{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/02\n"
     ]
    }
   ],
   "source": [
    "import ROOT as rt\n",
    "import uproot\n",
    "#from root_pandas import read_root\n",
    "import h5py \n",
    "\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as nlr\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu-ibanks-2.hep.caltech.edu  Tue Aug 17 14:44:24 2021  470.57.02\n",
      "[0] NVIDIA TITAN Xp  | 28'C,   0 % |     1 / 12196 MB |\n",
      "[1] NVIDIA TITAN Xp  | 28'C,   0 % |     1 / 12196 MB |\n",
      "[2] NVIDIA TITAN Xp  | 30'C,   0 % |     1 / 12196 MB |\n",
      "[3] NVIDIA TITAN Xp  | 25'C,   0 % |     1 / 12196 MB |\n",
      "[4] NVIDIA TITAN Xp  | 26'C,   0 % |     1 / 12196 MB |\n",
      "[5] NVIDIA TITAN Xp  | 24'C,   0 % |     1 / 12196 MB |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "import json\n",
    "from prettytable import PrettyTable \n",
    "\n",
    "import gpustat\n",
    "gpustat.print_gpustat()\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp_to_df(samp_name, total_num):\n",
    "    dfs = []\n",
    "    for i in range(total_num+1):\n",
    "        file_name = f'/storage/af/user/schen7/CMSSW_9_4_2/src/Higgs/HHbbgg/HHbbggAna/condor/output/{samp_name}{i}.root'\n",
    "        samp_file = uproot.open(file_name)\n",
    "        samp_array = samp_file['tree'].arrays()\n",
    "        samp_df = pd.DataFrame(samp_array)\n",
    "        dfs.append(samp_df)\n",
    "    combine_df = pd.concat(dfs, ignore_index=True)\n",
    "    return combine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataframes - 2018 (recon == 1)\n",
    "\n",
    "# Signals\n",
    "GluGluToHH_df_2018 = samp_to_df('job_5_ntuple20180805v1/GluGluToHHTo2B2G_node_cHHH1_TuneCP5_PSWeights_13TeV-powheg-pythia8Job5ifile', 0)\n",
    "VBFHH_df_2018 = samp_to_df('job_1_ntuple20180809v2/VBFHHTo2B2G_CV_1_C2V_1_C3_1_TuneCP5_PSWeights_13TeV-madgraph-pythia8Job1ifile', 2)\n",
    "\n",
    "# Backgrounds\n",
    "TTGJets_df_2018 = samp_to_df('job_2_ntuple20180805v1/TTGJets_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8Job2ifile', 19)\n",
    "TTGG_0Jets_df_2018 = samp_to_df('job_3_ntuple20180805v1/TTGG_0Jets_TuneCP5_13TeV_amcatnlo_madspin_pythia8Job3ifile', 28)\n",
    "TTJets_df_2018 = samp_to_df('job_4_ntuple20180805v1/TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8Job4ifile', 304)\n",
    "VHToGG_df_2018 = samp_to_df('job_6_ntuple20180805v1/VHToGG_M125_13TeV_amcatnloFXFX_madspin_pythia8Job6ifile', 1)\n",
    "ttHToGG_df_2018 = samp_to_df('job_7_ntuple20180805v1/ttHToGG_M125_TuneCP5_PSweights_13TeV-powheg-pythia8Job7ifile', 2)\n",
    "VBFHToGG_df_2018 = samp_to_df('job_8_ntuple20180805v1/VBFHToGG_M125_13TeV_amcatnlo_pythia8Job8ifile', 4)\n",
    "GluGluHToGG_df_2018 = samp_to_df('job_9_ntuple20180805v1/GluGluHToGG_M125_TuneCP5_13TeV-amcatnloFXFX-pythia8Job9ifile', 3)\n",
    "GJet_SmallPt_df_2018 = samp_to_df('job_10_ntuple20180805v1/GJet_Pt-20to40_DoubleEMEnriched_MGG-80toInf_TuneCP5_13TeV_Pythia8Job10ifile', 15)\n",
    "GJet_BigPt_df_2018 = samp_to_df('job_11_ntuple20180805v1/GJet_Pt-40toInf_DoubleEMEnriched_MGG-80toInf_TuneCP5_13TeV_Pythia8Job11ifile', 10)\n",
    "DiPhotonJetsBox2B_df_2018 = samp_to_df('job_12_ntuple20180805v1/DiPhotonJetsBox2BJets_MGG-80toInf_13TeV-SherpaJob12ifile', 2)\n",
    "DiPhotonJetsBox1B_df_2018 = samp_to_df('job_13_ntuple20180805v1/DiPhotonJetsBox1BJet_MGG-80toInf_13TeV-SherpaJob13ifile', 3)\n",
    "DiPhotonJetsBox_df_2018 = samp_to_df('job_14_ntuple20180805v1/DiPhotonJetsBox_MGG-80toInf_13TeV-SherpaJob14ifile', 23)\n",
    "QCD_Jets_df_2018 = samp_to_df('job_2_ntuple20180809v2/QCD_Pt-40toInf_DoubleEMEnriched_MGG-80toInf_TuneCP5_13TeV_Pythia8Job2ifile', 78)\n",
    "\n",
    "\n",
    "# Set up dataframes - 2017 (recon == 1)\n",
    "\n",
    "# Signals\n",
    "GluGluToHH_df_2017 = samp_to_df('job_8_ntuple20170805v1/GluGluToHHTo2B2G_node_cHHH1_TuneCP5_PSWeights_13TeV-powheg-pythia8Job8ifile', 15)\n",
    "VBFHH_df_2017 = samp_to_df('job_1_ntuple20170809v2/VBFHHTo2B2G_CV_1_C2V_1_C3_1_13TeV-madgraphJob1ifile', 6)\n",
    "\n",
    "# Backgrounds\n",
    "DiPhotonJetsBox1B_df_2017 = samp_to_df('job_2_ntuple20170805v1/DiPhotonJetsBox1BJet_MGG-80toInf_13TeV-SherpaJob2ifile', 15)\n",
    "DiPhotonJetsBox2B_df_2017 = samp_to_df('job_3_ntuple20170805v1/DiPhotonJetsBox2BJets_MGG-80toInf_13TeV-SherpaJob3ifile', 25)\n",
    "DiPhotonJetsBox_df_2017 = samp_to_df('job_4_ntuple20170805v1/DiPhotonJetsBox_MGG-80toInf_13TeV-SherpaJob4ifile', 83)\n",
    "GJet_SmallPt_df_2017 = samp_to_df('job_5_ntuple20170805v1/GJet_Pt-20to40_DoubleEMEnriched_MGG-80toInf_TuneCP5_13TeV_Pythia8Job5ifile', 11)\n",
    "GJet_BigPt_df_2017 = samp_to_df('job_6_ntuple20170805v1/GJet_Pt-40toInf_DoubleEMEnriched_MGG-80toInf_TuneCP5_13TeV_Pythia8Job6ifile', 104)\n",
    "GluGluHToGG_df_2017 = samp_to_df('job_7_ntuple20170805v1/GluGluHToGG_M-125_13TeV_powheg_pythia8Job7ifile', 1)\n",
    "TTGG_0Jets_df_2017 = samp_to_df('job_9_ntuple20170805v1/TTGG_0Jets_TuneCP5_13TeV_amcatnlo_madspin_pythia8Job9ifile', 8)\n",
    "TTGJets_df_2017 = samp_to_df('job_10_ntuple20170805v1/TTGJets_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8Job10ifile', 53)\n",
    "TTJets_df_2017 = samp_to_df('job_11_ntuple20170805v1/TTJets_TuneCP5_13TeV-amcatnloFXFX-pythia8Job11ifile', 194)\n",
    "VBFHToGG_df_2017 = samp_to_df('job_12_ntuple20170805v1/VBFHToGG_M-125_13TeV_powheg_pythia8Job12ifile', 11)\n",
    "VHToGG_df_2017 = samp_to_df('job_13_ntuple20170805v1/VHToGG_M125_13TeV_amcatnloFXFX_madspin_pythia8Job13ifile', 0)\n",
    "ttHToGG_df_2017 = samp_to_df('job_14_ntuple20170805v1/ttHToGG_M125_13TeV_powheg_pythia8Job14ifile', 3)\n",
    "QCD_Jets_df_2017 = samp_to_df('job_2_ntuple20180809v2/QCD_Pt-40toInf_DoubleEMEnriched_MGG-80toInf_TuneCP5_13TeV_Pythia8Job2ifile', 18)\n",
    "\n",
    "\n",
    "# Set up dataframes - 2016 (recon == 1)\n",
    "\n",
    "# Signals\n",
    "GluGluToHH_df_2016 = samp_to_df('job_8_ntuple20160805v1/GluGluToHHTo2B2G_node_cHHH1_TuneCUETP8M1_PSWeights_13TeV-powheg-pythia8Job8ifile', 3)\n",
    "VBFHH_df_2016 = samp_to_df('job_1_ntuple20160809v2/VBFHHTo2B2G_CV_1_C2V_1_C3_1_13TeV-madgraphJob1ifile', 0)\n",
    "\n",
    "# Background\n",
    "DiPhotonJetsBox1B_df_2016 = samp_to_df('job_2_ntuple20160805v1/DiPhotonJetsBox1BJet_MGG-80toInf_TuneSherpa_13TeV-SherpaJob2ifile', 17)\n",
    "DiPhotonJetsBox2B_df_2016 = samp_to_df('job_3_ntuple20160805v1/DiPhotonJetsBox2BJets_MGG-80toInf_TuneSherpa_13TeV-SherpaJob3ifile', 23)\n",
    "DiPhotonJetsBox_df_2016 = samp_to_df('job_4_ntuple20160805v1/DiPhotonJetsBox_MGG-80toInf_13TeV-SherpaJob4ifile', 48)\n",
    "GJet_SmallPt_df_2016 = samp_to_df('job_5_ntuple20160805v1/GJet_Pt-20to40_DoubleEMEnriched_MGG-80toInf_TuneCUETP8M1_13TeV_Pythia8Job5ifile', 13)\n",
    "GJet_BigPt_df_2016 = samp_to_df('job_6_ntuple20160805v1/GJet_Pt-40toInf_DoubleEMEnriched_MGG-80toInf_TuneCUETP8M1_13TeV_Pythia8Job6ifile', 64)\n",
    "GluGluHToGG_df_2016 = samp_to_df('job_7_ntuple20160805v1/GluGluHToGG_M125_13TeV_amcatnloFXFX_pythiaJob7ifile', 0)\n",
    "TTGG_0Jets_df_2016 = samp_to_df('job_9_ntuple20160805v1/TTGG_0Jets_TuneCUETP8M1_13TeV_amcatnlo_madspin_pythia8Job9ifile', 2)\n",
    "TTGJets_df_2016 = samp_to_df('job_10_ntuple20160805v1/TTGJets_TuneCUETP8M1_13TeV-amcatnloFXFX-madspin-pythia8Job10ifile', 12)\n",
    "TTJets_df_2016 = samp_to_df('job_11_ntuple20160805v1/TTJets_TuneCUETP8M2T4_13TeV-amcatnloFXFX-pythia8Job11ifile', 43)\n",
    "VBFHToGG_df_2016 = samp_to_df('job_12_ntuple20160805v1/VBFHToGG_M125_13TeV_amcatnlo_pythia8_v2Job12ifile', 11)\n",
    "VHToGG_df_2016 = samp_to_df('job_13_ntuple20160805v1/VHToGG_M125_13TeV_amcatnloFXFX_madspin_pythia8Job13ifile', 1)\n",
    "ttHToGG_df_2016 = samp_to_df('job_14_ntuple20160805v1/ttHToGG_M125_13TeV_powheg_pythia8_v2Job14ifile', 0)\n",
    "QCD_Jets_df_2016 = samp_to_df('job_2_ntuple20160809v2/QCD_Pt-40toInf_DoubleEMEnriched_MGG-80toInf_TuneCUETP8M1_13TeV_Pythia8Job2ifile', 15)\n",
    "\n",
    "\n",
    "# Combine by sample (recon == 1)\n",
    "\n",
    "# Signals \n",
    "GluGluToHH_df = pd.concat([GluGluToHH_df_2018, GluGluToHH_df_2017, GluGluToHH_df_2016], ignore_index=True)\n",
    "VBFHH_df = pd.concat([VBFHH_df_2018, VBFHH_df_2017, VBFHH_df_2016], ignore_index=True)\n",
    "\n",
    "# Backgrounds\n",
    "TTGJets_df = pd.concat([TTGJets_df_2018, TTGJets_df_2017, TTGJets_df_2016], ignore_index=True)\n",
    "TTGG_0Jets_df = pd.concat([TTGG_0Jets_df_2018, TTGG_0Jets_df_2017, TTGG_0Jets_df_2016], ignore_index=True)\n",
    "TTJets_df = pd.concat([TTJets_df_2018, TTJets_df_2017, TTJets_df_2016], ignore_index=True)\n",
    "VHToGG_df = pd.concat([VHToGG_df_2018, VHToGG_df_2017, VHToGG_df_2016], ignore_index=True)\n",
    "ttHToGG_df = pd.concat([ttHToGG_df_2018, ttHToGG_df_2017, ttHToGG_df_2016], ignore_index=True)\n",
    "VBFHToGG_df = pd.concat([VBFHToGG_df_2018, VBFHToGG_df_2017, VBFHToGG_df_2016], ignore_index=True)\n",
    "GluGluHToGG_df = pd.concat([GluGluHToGG_df_2018, GluGluHToGG_df_2017, GluGluHToGG_df_2016], ignore_index=True)\n",
    "GJet_SmallPt_df = pd.concat([GJet_SmallPt_df_2018, GJet_SmallPt_df_2017, GJet_SmallPt_df_2016], ignore_index=True)\n",
    "GJet_BigPt_df = pd.concat([GJet_BigPt_df_2018, GJet_BigPt_df_2017, GJet_BigPt_df_2016], ignore_index=True)\n",
    "DiPhotonJetsBox2B_df = pd.concat([DiPhotonJetsBox2B_df_2018, DiPhotonJetsBox2B_df_2017, DiPhotonJetsBox2B_df_2016], ignore_index=True)\n",
    "DiPhotonJetsBox1B_df = pd.concat([DiPhotonJetsBox1B_df_2018, DiPhotonJetsBox1B_df_2017, DiPhotonJetsBox1B_df_2016], ignore_index=True)\n",
    "DiPhotonJetsBox_df = pd.concat([DiPhotonJetsBox_df_2018, DiPhotonJetsBox_df_2017, DiPhotonJetsBox_df_2016], ignore_index=True)\n",
    "QCD_Jets_df = pd.concat([QCD_Jets_df_2018, QCD_Jets_df_2017, QCD_Jets_df_2016], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSGrad(optim.Optimizer):\n",
    "    \"\"\"Implements AMSGrad algorithm.\n",
    "    It has been proposed in `On the Convergence of Adam and Beyond (https://openreview.net/forum?id=ryQu7f-RZ)`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AMSGrad, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AMSGrad, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', True)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "training_vars = [b'leading_photon_pt', b'leading_photon_eta', b'leading_photon_phi', \n",
    "                 b'subleading_photon_pt', b'subleading_photon_eta', b'subleading_photon_phi',\n",
    "                 b'leading_bjet_pt_corr', b'leading_bjet_eta', b'leading_bjet_phi',\n",
    "                 b'subleading_bjet_pt_corr', b'subleading_bjet_eta', b'subleading_bjet_phi',\n",
    "                 b'leadingDeepBscore', b'subleadingDeepBscore', b'sumDeepBscore',\n",
    "                 b'leading_pho_pt_over_dimass', b'leading_bjet_pt_over_dimass', b'leading_bjet_pt_over_dimass_corr'\n",
    "                ]\n",
    "\n",
    "aug_vars = [b'dibjet_mass', b'diphoton_mass', b'event']\n",
    "\n",
    "w_var = [b'genweight']\n",
    "\n",
    "BKD_dfs = [TTGJets_df, TTGG_0Jets_df, TTJets_df, VHToGG_df, ttHToGG_df, VBFHToGG_df, GluGluHToGG_df,\n",
    "            GJet_SmallPt_df, GJet_BigPt_df, DiPhotonJetsBox2B_df, DiPhotonJetsBox1B_df, DiPhotonJetsBox_df, QCD_Jets_df]\n",
    "\n",
    "names = ['TTG Jets', 'TTGG_0Jets', 'TTJets', 'VHToGG', 'ttHToGG', 'VBFHToGG', 'GluGluHToGG',\n",
    "         'GJet_SmallPt', 'GJet_BigPt', 'DiPhotonJetsBox2B', 'DiPhotonJetsBox1B', 'DiPhotonJetsBox', 'QCD_Jets']\n",
    "\n",
    "i = 1 # select sample\n",
    "\n",
    "sig_frame_all = GluGluToHH_df\n",
    "bkg_frame_all = BKD_dfs[i]\n",
    "\n",
    "sig_frame = sig_frame_all[sig_frame_all[b'leading_photon_pt'] > 0][sig_frame_all[b'leading_bjet_pt'] > 0][training_vars]\n",
    "bkg_frame = bkg_frame_all[bkg_frame_all[b'leading_photon_pt'] > 0][bkg_frame_all[b'leading_bjet_pt'] > 0][training_vars]\n",
    "\n",
    "sig_aux_frame = sig_frame_all[sig_frame_all[b'leading_photon_pt'] > 0][sig_frame_all[b'leading_bjet_pt'] > 0][aug_vars]\n",
    "bkg_aux_frame = bkg_frame_all[bkg_frame_all[b'leading_photon_pt'] > 0][bkg_frame_all[b'leading_bjet_pt'] > 0][aug_vars]\n",
    "\n",
    "sig_df = sig_frame_all[sig_frame_all[b'leading_photon_pt'] > 0][sig_frame_all[b'leading_bjet_pt'] > 0][training_vars + w_var + aug_vars]\n",
    "bkg_df = bkg_frame_all[bkg_frame_all[b'leading_photon_pt'] > 0][bkg_frame_all[b'leading_bjet_pt'] > 0][training_vars + w_var + aug_vars]\n",
    "\n",
    "sig_weight = sig_frame_all[sig_frame_all[b'leading_photon_pt'] > 0][sig_frame_all[b'leading_bjet_pt'] > 0][w_var]\n",
    "bkg_weight = bkg_frame_all[bkg_frame_all[b'leading_photon_pt'] > 0][bkg_frame_all[b'leading_bjet_pt'] > 0][w_var]\n",
    "\n",
    "# Standardize\n",
    "x_mean = bkg_frame.mean()\n",
    "x_std = bkg_frame.std()\n",
    "\n",
    "sig_frame = (sig_frame-x_mean)/x_std\n",
    "bkg_frame = (bkg_frame-x_mean)/x_std\n",
    "signal = sig_frame.values\n",
    "background = bkg_frame.values\n",
    "#print signal, background\n",
    "signal_all = sig_df.values\n",
    "background_all = bkg_df.values\n",
    "\n",
    "# Shuffle before splitting into train-val\n",
    "randix = np.arange(len(background))\n",
    "np.random.shuffle(randix)\n",
    "#np.random.shuffle(background)\n",
    "#np.random.shuffle(signal)\n",
    "\n",
    "randix2 = np.arange(len(signal))\n",
    "np.random.shuffle(randix2)\n",
    "signal_ = signal[randix2]\n",
    "signal_all_=signal_all[randix2]\n",
    "#signal_ = signal_[:len(background)]\n",
    "\n",
    "background_ = background[randix]\n",
    "background_all_ = background_all[randix]\n",
    "background_ = background_[:len(signal)] # downsampling\n",
    "\n",
    "sig_label = np.ones(len(signal_))\n",
    "bkg_label = np.zeros(len(background_))\n",
    "\n",
    "data = np.concatenate((signal_,background_))\n",
    "label = np.concatenate((sig_label,bkg_label))\n",
    "data_all = np.concatenate((signal_all_,background_all_))\n",
    "\n",
    "sig_weight_np = sig_weight.values\n",
    "bkg_weight_np = bkg_weight.values\n",
    "sig_weight_np_sfl = sig_weight_np[randix2]\n",
    "bkg_weight_np_sfl = bkg_weight_np[randix]\n",
    "weights = np.concatenate((sig_weight_np_sfl, bkg_weight_np_sfl))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "skf.get_n_splits(data, label)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "model_file = 'SimpleDNN.torch'\n",
    "config_file = 'BestConfig.json'\n",
    "retrain=True\n",
    "\n",
    "class EventHLF(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.len = data_x.shape[0]\n",
    "        self.data_x = torch.from_numpy(data_x).float()\n",
    "        self.data_y = torch.from_numpy(data_y).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data_x[idx], self.data_y[idx])\n",
    "\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, num_hiddens=2, initial_node=500, dropout=0.5):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(18, initial_node)\n",
    "        self.dropout = dropout\n",
    "        self.hiddens = nn.ModuleList()\n",
    "        nodes = [initial_node]\n",
    "        for i in range(num_hiddens):\n",
    "            nodes.append(int(nodes[i]/2))\n",
    "            self.hiddens.append(nn.Linear(nodes[i],nodes[i+1]))\n",
    "        self.out = nn.Linear(nodes[-1],2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for i in range(len(self.hiddens)):\n",
    "            x = F.relu(self.hiddens[i](x))\n",
    "            x = F.dropout(x, training=self.training, p=self.dropout)\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early Stopping to terminate training early under certain conditions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 monitor='val_loss',\n",
    "                 min_delta=0,\n",
    "                 patience=10):\n",
    "        self.monitor = monitor\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.stopped_epoch = 0\n",
    "        self.stop_training= False\n",
    "        #print(\"This is my patience {}\".format(patience))\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.wait = 0\n",
    "        self.best_loss = 1e15\n",
    "    \n",
    "    def on_epoch_end(self, epoch, current_loss):\n",
    "        if current_loss is None:\n",
    "            pass\n",
    "        else:\n",
    "            if (current_loss - self.best_loss) < -self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait = 1\n",
    "            else:\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stopped_epoch = epoch + 1\n",
    "                    self.stop_training = True\n",
    "                self.wait += 1\n",
    "            return  self.stop_training\n",
    "        \n",
    "    def on_train_end(self):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print('\\nTerminated training for early stopping at epoch %04i' % \n",
    "                (self.stopped_epoch))\n",
    "\n",
    "def train(num_epochs, model, criterion, optimizer,scheduler,volatile=False, data_loader=None):\n",
    "    best_model = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    train_losses ,val_losses = [],[]\n",
    "    callback = EarlyStopping(patience=20)\n",
    "    callback.on_train_begin()\n",
    "    breakdown = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if breakdown:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['training', 'validation']:\n",
    "            if phase == 'training':\n",
    "                model.train() # Set model to training mode\n",
    "                volatile=False\n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "                volatile=True\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch_idx, (x_data, y_data) in enumerate(data_loader[phase]):\n",
    "                x_data = Variable(x_data, volatile).cuda()\n",
    "                y_data = Variable(y_data, volatile=volatile, requires_grad=False).cuda()\n",
    "                if phase == 'training':\n",
    "                    optimizer.zero_grad()\n",
    "                # forward pass\n",
    "                outputs = model(x_data)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, y_data)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'training':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == y_data.data)\n",
    "                #print(\"I finished %d batch\" % batch_idx)\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = 100. * running_corrects / len(data_loader[phase].dataset)\n",
    "            if phase == 'training':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                scheduler.step(epoch_loss)\n",
    "                val_losses.append(epoch_loss)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                        'net': model, #.module if use_cuda else net,\n",
    "                        'epoch': epoch,\n",
    "                        'best_acc':epoch_acc,\n",
    "                        'train_loss':train_losses,\n",
    "                        'val_loss':val_losses,\n",
    "                        }\n",
    "                torch.save(state, model_file)\n",
    "                best_acc = epoch_acc\n",
    "                best_model = model.state_dict()\n",
    "            if phase == 'validation':\n",
    "                breakdown = callback.on_epoch_end(epoch, -epoch_acc)\n",
    "                \n",
    "         \n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    print('-' * 10)\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New configuration: {'learning_rate': 0.19486241836466403, 'initial_nodes': 875, 'dropout': 0.7640540475178719, 'batch_size': 331, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.5080 Acc: 49.8333\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 49.6429\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 50.0923\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 49.9821\n",
      "validation Loss: 0.0022 Acc: 49.9762\n",
      "Epoch 4/99\n",
      "training Loss: 0.0028 Acc: 49.7708\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0116 Acc: 49.8065\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 50.1786\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 50.2827\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 49.5506\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 50.4345\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 50.1905\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 49.5774\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 49.8661\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 49.8780\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 49.8810\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 49.7083\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 50.8095\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 50.0863\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 50.0357\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 50.2411\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 50.0923\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.6513 Acc: 50.2619\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 50.2678\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 49.5596\n",
      "validation Loss: 0.0021 Acc: 50.0238\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 49.8572\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0021 Acc: 50.3065\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 49.6131\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 50.5625\n",
      "validation Loss: 0.0022 Acc: 49.9881\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 49.8095\n",
      "validation Loss: 0.0022 Acc: 50.0119\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 50.0238\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 50.1012\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 50.1042\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 49.8661\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 49.9940\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 49.8631\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 49.8691\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 50.0536\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 49.8333\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 49.8482\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 49.8929\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 50.0982\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 49.7381\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 21/99\n",
      "training Loss: 0.0021 Acc: 49.6548\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 22/99\n",
      "training Loss: 0.0021 Acc: 50.1488\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Early stopped.\n",
      "Best val acc: 50.023810\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.5735 Acc: 49.1191\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 50.1399\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0023 Acc: 49.8899\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0023 Acc: 50.1369\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0021 Acc: 50.1845\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 49.6578\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 49.7411\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 49.8958\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 49.7143\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 49.6012\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 50.4166\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 49.7530\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 50.4226\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 49.7917\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 50.0179\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 49.5744\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 49.7084\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 49.5715\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 49.8869\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 49.3542\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 49.5060\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.5252 Acc: 50.2143\n",
      "validation Loss: 0.0022 Acc: 49.9881\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 49.7708\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 49.5357\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 50.0298\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0021 Acc: 50.0655\n",
      "validation Loss: 0.0021 Acc: 49.9881\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 49.6845\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 49.8363\n",
      "validation Loss: 0.0021 Acc: 49.9881\n",
      "Epoch 7/99\n",
      "training Loss: 0.0024 Acc: 49.7708\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 50.3363\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 50.1815\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 50.4762\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 49.9851\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 50.0327\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 50.6904\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 50.2143\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 49.8244\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 49.8333\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 50.4404\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 50.4137\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 49.7351\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 50.3006\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 21/99\n",
      "training Loss: 0.0021 Acc: 50.2232\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.6279 Acc: 50.4345\n",
      "validation Loss: 0.0022 Acc: 49.9762\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0038 Acc: 50.3512\n",
      "validation Loss: 0.0021 Acc: 50.0238\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 50.1190\n",
      "validation Loss: 0.0021 Acc: 50.0238\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 49.9970\n",
      "validation Loss: 0.0021 Acc: 50.0238\n",
      "Epoch 4/99\n",
      "training Loss: 0.0021 Acc: 50.0327\n",
      "validation Loss: 0.0021 Acc: 50.0238\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 49.5625\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 50.0863\n",
      "validation Loss: 0.0022 Acc: 50.0119\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 50.4137\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 49.7411\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 50.0476\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 50.1934\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 50.2708\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 49.9673\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 49.8006\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 50.0833\n",
      "validation Loss: 0.0022 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 49.9911\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 50.1905\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 49.5625\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 49.5923\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 50.3690\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 50.1548\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0021 Acc: 49.7798\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Early stopped.\n",
      "Best val acc: 50.023810\n",
      "----------\n",
      "Average best_acc across k-fold: 50.0095238095\n",
      "New configuration: {'learning_rate': 1.921165975741198e-05, 'initial_nodes': 438, 'dropout': 0.2526641021763008, 'batch_size': 261, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0026 Acc: 66.4196\n",
      "validation Loss: 0.0025 Acc: 78.4337\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 78.3810\n",
      "validation Loss: 0.0021 Acc: 79.2906\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 79.5536\n",
      "validation Loss: 0.0017 Acc: 80.8379\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0017 Acc: 80.9940\n",
      "validation Loss: 0.0016 Acc: 82.0162\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.6399\n",
      "validation Loss: 0.0015 Acc: 82.8493\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.9286\n",
      "validation Loss: 0.0015 Acc: 83.0159\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.0893\n",
      "validation Loss: 0.0015 Acc: 83.1112\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.3393\n",
      "validation Loss: 0.0015 Acc: 83.2540\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 82.4940\n",
      "validation Loss: 0.0015 Acc: 83.3492\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.5774\n",
      "validation Loss: 0.0015 Acc: 83.2778\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.6399\n",
      "validation Loss: 0.0015 Acc: 83.3254\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.8333\n",
      "validation Loss: 0.0015 Acc: 83.5039\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 82.7351\n",
      "validation Loss: 0.0015 Acc: 83.3968\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 82.9851\n",
      "validation Loss: 0.0015 Acc: 83.3611\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 82.8780\n",
      "validation Loss: 0.0014 Acc: 83.3730\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 82.9762\n",
      "validation Loss: 0.0015 Acc: 83.4325\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.0298\n",
      "validation Loss: 0.0014 Acc: 83.4325\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.0863\n",
      "validation Loss: 0.0014 Acc: 83.5039\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.1637\n",
      "validation Loss: 0.0015 Acc: 83.4206\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.1339\n",
      "validation Loss: 0.0014 Acc: 83.4563\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.2202\n",
      "validation Loss: 0.0014 Acc: 83.5753\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.3065\n",
      "validation Loss: 0.0015 Acc: 83.4206\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.2708\n",
      "validation Loss: 0.0015 Acc: 83.5396\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.3512\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.3363\n",
      "validation Loss: 0.0014 Acc: 83.4682\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.2262\n",
      "validation Loss: 0.0014 Acc: 83.5277\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.3244\n",
      "validation Loss: 0.0014 Acc: 83.4801\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.3958\n",
      "validation Loss: 0.0014 Acc: 83.5634\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.3661\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.3542\n",
      "validation Loss: 0.0014 Acc: 83.6229\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.3958\n",
      "validation Loss: 0.0014 Acc: 83.6110\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.4464\n",
      "validation Loss: 0.0014 Acc: 83.5991\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.4137\n",
      "validation Loss: 0.0014 Acc: 83.5515\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.5030\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.4107\n",
      "validation Loss: 0.0014 Acc: 83.6229\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.4286\n",
      "validation Loss: 0.0014 Acc: 83.6110\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.5476\n",
      "validation Loss: 0.0014 Acc: 83.6825\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.4107\n",
      "validation Loss: 0.0014 Acc: 83.7658\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.6429\n",
      "validation Loss: 0.0014 Acc: 83.6468\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.5208\n",
      "validation Loss: 0.0015 Acc: 83.5991\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.7024\n",
      "validation Loss: 0.0014 Acc: 83.6587\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.7054\n",
      "validation Loss: 0.0014 Acc: 83.6229\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.5208\n",
      "validation Loss: 0.0014 Acc: 83.6348\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.5417\n",
      "validation Loss: 0.0014 Acc: 83.6110\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 83.6667\n",
      "validation Loss: 0.0014 Acc: 83.5634\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.5179\n",
      "validation Loss: 0.0014 Acc: 83.6110\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 83.6429\n",
      "validation Loss: 0.0014 Acc: 83.5753\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 83.5774\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 83.6488\n",
      "validation Loss: 0.0014 Acc: 83.6348\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 83.5923\n",
      "validation Loss: 0.0014 Acc: 83.6587\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 83.6756\n",
      "validation Loss: 0.0014 Acc: 83.6110\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 83.5476\n",
      "validation Loss: 0.0014 Acc: 83.5991\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 83.6458\n",
      "validation Loss: 0.0014 Acc: 83.5753\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 83.6696\n",
      "validation Loss: 0.0014 Acc: 83.6229\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 83.5000\n",
      "validation Loss: 0.0014 Acc: 83.6348\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 83.6786\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 83.8155\n",
      "validation Loss: 0.0014 Acc: 83.6110\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 83.6488\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Early stopped.\n",
      "Best val acc: 83.765770\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0026 Acc: 69.4393\n",
      "validation Loss: 0.0025 Acc: 78.5833\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 78.4804\n",
      "validation Loss: 0.0021 Acc: 79.4762\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 79.7780\n",
      "validation Loss: 0.0017 Acc: 80.5476\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0017 Acc: 80.9119\n",
      "validation Loss: 0.0016 Acc: 81.7381\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.7035\n",
      "validation Loss: 0.0015 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0015 Acc: 82.2600\n",
      "validation Loss: 0.0015 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.5695\n",
      "validation Loss: 0.0015 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.4624\n",
      "validation Loss: 0.0015 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 82.6498\n",
      "validation Loss: 0.0015 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.8701\n",
      "validation Loss: 0.0015 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.8552\n",
      "validation Loss: 0.0015 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.9236\n",
      "validation Loss: 0.0015 Acc: 83.2619\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.0070\n",
      "validation Loss: 0.0015 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 82.9177\n",
      "validation Loss: 0.0015 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 82.9623\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 82.9593\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.0873\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.1290\n",
      "validation Loss: 0.0015 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.3135\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.2808\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.3581\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.3046\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.2808\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.3313\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.2748\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.3462\n",
      "validation Loss: 0.0014 Acc: 83.6548\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.4087\n",
      "validation Loss: 0.0014 Acc: 83.6429\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.4415\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.3284\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.4772\n",
      "validation Loss: 0.0014 Acc: 83.6429\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.3581\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.3879\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.3552\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.5903\n",
      "validation Loss: 0.0014 Acc: 83.6429\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.4623\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.4593\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.4682\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.4712\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.4980\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.6409\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.5248\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.6170\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.5635\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.7599\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 83.5337\n",
      "validation Loss: 0.0014 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.6379\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 83.6319\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 83.6349\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 83.6885\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 83.5486\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 83.5754\n",
      "validation Loss: 0.0014 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 83.6706\n",
      "validation Loss: 0.0014 Acc: 83.7738\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 83.6141\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 83.7034\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 83.7599\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 83.5665\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 83.6557\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 83.7748\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 83.7093\n",
      "validation Loss: 0.0014 Acc: 83.7738\n",
      "Epoch 59/99\n",
      "training Loss: 0.0014 Acc: 83.6170\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Epoch 60/99\n",
      "training Loss: 0.0014 Acc: 83.7897\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Epoch 61/99\n",
      "training Loss: 0.0014 Acc: 83.6944\n",
      "validation Loss: 0.0014 Acc: 83.6429\n",
      "Epoch 62/99\n",
      "training Loss: 0.0014 Acc: 83.7450\n",
      "validation Loss: 0.0014 Acc: 83.6548\n",
      "Epoch 63/99\n",
      "training Loss: 0.0014 Acc: 83.6170\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 64/99\n",
      "training Loss: 0.0014 Acc: 83.6498\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 65/99\n",
      "training Loss: 0.0014 Acc: 83.8343\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Epoch 66/99\n",
      "training Loss: 0.0014 Acc: 83.7093\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 67/99\n",
      "training Loss: 0.0014 Acc: 83.6676\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Epoch 68/99\n",
      "training Loss: 0.0014 Acc: 83.7004\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 69/99\n",
      "training Loss: 0.0014 Acc: 83.6647\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Epoch 70/99\n",
      "training Loss: 0.0014 Acc: 83.6498\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Early stopped.\n",
      "Best val acc: 83.833333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0026 Acc: 51.5684\n",
      "validation Loss: 0.0025 Acc: 65.2024\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0024 Acc: 68.2965\n",
      "validation Loss: 0.0022 Acc: 78.3452\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 78.7840\n",
      "validation Loss: 0.0019 Acc: 80.7143\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 80.8999\n",
      "validation Loss: 0.0017 Acc: 81.4048\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0017 Acc: 81.4981\n",
      "validation Loss: 0.0016 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.8999\n",
      "validation Loss: 0.0016 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 82.1796\n",
      "validation Loss: 0.0016 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 82.3433\n",
      "validation Loss: 0.0015 Acc: 82.9048\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 82.6052\n",
      "validation Loss: 0.0015 Acc: 82.8810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.8076\n",
      "validation Loss: 0.0015 Acc: 82.8333\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.6885\n",
      "validation Loss: 0.0015 Acc: 82.8571\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.8195\n",
      "validation Loss: 0.0015 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 82.9207\n",
      "validation Loss: 0.0015 Acc: 82.9524\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 82.9832\n",
      "validation Loss: 0.0015 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 82.9861\n",
      "validation Loss: 0.0015 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.0099\n",
      "validation Loss: 0.0015 Acc: 83.0357\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.2302\n",
      "validation Loss: 0.0015 Acc: 83.0357\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.0218\n",
      "validation Loss: 0.0015 Acc: 83.0833\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 82.9534\n",
      "validation Loss: 0.0015 Acc: 83.0952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.0516\n",
      "validation Loss: 0.0015 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.3135\n",
      "validation Loss: 0.0014 Acc: 83.1310\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.1915\n",
      "validation Loss: 0.0015 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.2956\n",
      "validation Loss: 0.0015 Acc: 83.1786\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.3016\n",
      "validation Loss: 0.0015 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.2480\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.5129\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.2956\n",
      "validation Loss: 0.0014 Acc: 83.0714\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.3403\n",
      "validation Loss: 0.0015 Acc: 83.1905\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.4236\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.3671\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.4028\n",
      "validation Loss: 0.0015 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.4117\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.4117\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.4028\n",
      "validation Loss: 0.0015 Acc: 83.3452\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.4593\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.5724\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.4147\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.4980\n",
      "validation Loss: 0.0015 Acc: 83.4286\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.5426\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.5784\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.5040\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.4950\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.4474\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.6349\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 83.6438\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.5903\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 83.5278\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 83.5516\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 83.6379\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 83.5337\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 83.5129\n",
      "validation Loss: 0.0015 Acc: 83.4048\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 83.6676\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 83.6557\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 83.5635\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 83.5099\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 83.5069\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 83.6914\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 83.6528\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 83.6200\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0014 Acc: 83.5635\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 60/99\n",
      "training Loss: 0.0014 Acc: 83.7301\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 61/99\n",
      "training Loss: 0.0014 Acc: 83.7629\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 62/99\n",
      "training Loss: 0.0014 Acc: 83.7926\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 63/99\n",
      "training Loss: 0.0014 Acc: 83.7391\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 64/99\n",
      "training Loss: 0.0014 Acc: 83.7658\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0014 Acc: 83.6200\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 66/99\n",
      "training Loss: 0.0014 Acc: 83.7123\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 67/99\n",
      "training Loss: 0.0014 Acc: 83.6736\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 68/99\n",
      "training Loss: 0.0014 Acc: 83.6736\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 69/99\n",
      "training Loss: 0.0014 Acc: 83.6706\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 70/99\n",
      "training Loss: 0.0014 Acc: 83.7182\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 71/99\n",
      "training Loss: 0.0014 Acc: 83.6706\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 72/99\n",
      "training Loss: 0.0014 Acc: 83.6557\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 73/99\n",
      "training Loss: 0.0014 Acc: 83.5932\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 74/99\n",
      "training Loss: 0.0014 Acc: 83.7956\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 75/99\n",
      "training Loss: 0.0014 Acc: 83.6855\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 76/99\n",
      "training Loss: 0.0014 Acc: 83.6290\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 77/99\n",
      "training Loss: 0.0014 Acc: 83.6944\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 78/99\n",
      "training Loss: 0.0014 Acc: 83.7034\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 79/99\n",
      "training Loss: 0.0014 Acc: 83.7480\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 80/99\n",
      "training Loss: 0.0014 Acc: 83.7897\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 81/99\n",
      "training Loss: 0.0014 Acc: 83.7272\n",
      "validation Loss: 0.0015 Acc: 83.5714\n",
      "Epoch 82/99\n",
      "training Loss: 0.0014 Acc: 83.7093\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 83/99\n",
      "training Loss: 0.0014 Acc: 83.6706\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 84/99\n",
      "training Loss: 0.0014 Acc: 83.7510\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Early stopped.\n",
      "Best val acc: 83.619048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0026 Acc: 68.8679\n",
      "validation Loss: 0.0026 Acc: 78.5595\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 78.2334\n",
      "validation Loss: 0.0021 Acc: 79.1667\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 79.5012\n",
      "validation Loss: 0.0017 Acc: 80.4405\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0017 Acc: 80.5845\n",
      "validation Loss: 0.0016 Acc: 81.5714\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.3553\n",
      "validation Loss: 0.0016 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.9654\n",
      "validation Loss: 0.0015 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 82.3016\n",
      "validation Loss: 0.0015 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.3106\n",
      "validation Loss: 0.0015 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 82.5784\n",
      "validation Loss: 0.0015 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.7451\n",
      "validation Loss: 0.0015 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.8225\n",
      "validation Loss: 0.0015 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.7659\n",
      "validation Loss: 0.0014 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 82.8671\n",
      "validation Loss: 0.0015 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 82.9742\n",
      "validation Loss: 0.0015 Acc: 83.0476\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.0546\n",
      "validation Loss: 0.0015 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.1052\n",
      "validation Loss: 0.0014 Acc: 83.1310\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.1260\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.2510\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.2927\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.2808\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.3492\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.3700\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.2480\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.3760\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.2540\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.4831\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.4801\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.4861\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.5069\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.6200\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.5813\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.5159\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.5397\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.5307\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.5159\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.5248\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.5724\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.4563\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.6885\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.4891\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.7301\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.5873\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.6170\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.5546\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 83.6914\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.5843\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 83.5813\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 83.6498\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 83.5992\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 83.5903\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 83.6974\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 83.6230\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Early stopped.\n",
      "Best val acc: 83.428571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0026 Acc: 68.3471\n",
      "validation Loss: 0.0025 Acc: 78.2024\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 78.6769\n",
      "validation Loss: 0.0020 Acc: 79.9048\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0018 Acc: 80.4535\n",
      "validation Loss: 0.0017 Acc: 80.6071\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0017 Acc: 81.3969\n",
      "validation Loss: 0.0016 Acc: 81.7143\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.9892\n",
      "validation Loss: 0.0016 Acc: 82.1667\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0015 Acc: 82.2630\n",
      "validation Loss: 0.0016 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.6826\n",
      "validation Loss: 0.0015 Acc: 82.3214\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.6231\n",
      "validation Loss: 0.0015 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 82.6885\n",
      "validation Loss: 0.0015 Acc: 82.6071\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.7808\n",
      "validation Loss: 0.0015 Acc: 82.5476\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.8522\n",
      "validation Loss: 0.0015 Acc: 82.6310\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.9921\n",
      "validation Loss: 0.0015 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.1349\n",
      "validation Loss: 0.0015 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.0516\n",
      "validation Loss: 0.0015 Acc: 82.6905\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.1825\n",
      "validation Loss: 0.0015 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 83.2927\n",
      "validation Loss: 0.0015 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.1528\n",
      "validation Loss: 0.0015 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.2480\n",
      "validation Loss: 0.0015 Acc: 82.7976\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.4415\n",
      "validation Loss: 0.0015 Acc: 82.7976\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.2331\n",
      "validation Loss: 0.0015 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.4504\n",
      "validation Loss: 0.0015 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.4028\n",
      "validation Loss: 0.0015 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.3581\n",
      "validation Loss: 0.0015 Acc: 82.8214\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.4921\n",
      "validation Loss: 0.0015 Acc: 82.9048\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.4653\n",
      "validation Loss: 0.0015 Acc: 82.9048\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.4206\n",
      "validation Loss: 0.0015 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.4891\n",
      "validation Loss: 0.0015 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.5486\n",
      "validation Loss: 0.0015 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.7539\n",
      "validation Loss: 0.0015 Acc: 82.9405\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.4801\n",
      "validation Loss: 0.0015 Acc: 83.0000\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.6319\n",
      "validation Loss: 0.0015 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.5635\n",
      "validation Loss: 0.0015 Acc: 83.0357\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.6766\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.6260\n",
      "validation Loss: 0.0015 Acc: 82.9881\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.5694\n",
      "validation Loss: 0.0015 Acc: 82.9762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.6825\n",
      "validation Loss: 0.0015 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.7093\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.7956\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.7718\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.6676\n",
      "validation Loss: 0.0014 Acc: 83.1548\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.5903\n",
      "validation Loss: 0.0015 Acc: 83.2143\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.6498\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.6885\n",
      "validation Loss: 0.0015 Acc: 83.1548\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.7599\n",
      "validation Loss: 0.0015 Acc: 83.1310\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 83.7123\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.8313\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 83.7510\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 83.7063\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 83.5932\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 83.8283\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 83.8283\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 83.7123\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 83.7658\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 83.8581\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 83.7897\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 83.7539\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 83.9325\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 83.7391\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 83.6944\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Epoch 59/99\n",
      "training Loss: 0.0014 Acc: 83.7123\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Epoch 60/99\n",
      "training Loss: 0.0014 Acc: 83.8194\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Epoch 61/99\n",
      "training Loss: 0.0014 Acc: 83.7658\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 62/99\n",
      "training Loss: 0.0014 Acc: 83.8611\n",
      "validation Loss: 0.0015 Acc: 83.2262\n",
      "Epoch 63/99\n",
      "training Loss: 0.0014 Acc: 83.7063\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 64/99\n",
      "training Loss: 0.0014 Acc: 83.8045\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 65/99\n",
      "training Loss: 0.0014 Acc: 83.8938\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 66/99\n",
      "training Loss: 0.0014 Acc: 83.9236\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 67/99\n",
      "training Loss: 0.0014 Acc: 83.6676\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 68/99\n",
      "training Loss: 0.0014 Acc: 83.7897\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 69/99\n",
      "training Loss: 0.0014 Acc: 83.7956\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 70/99\n",
      "training Loss: 0.0014 Acc: 83.8670\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0014 Acc: 83.7004\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 72/99\n",
      "training Loss: 0.0014 Acc: 83.9563\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 73/99\n",
      "training Loss: 0.0014 Acc: 83.8522\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 74/99\n",
      "training Loss: 0.0014 Acc: 83.7837\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 75/99\n",
      "training Loss: 0.0014 Acc: 83.7480\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0014 Acc: 83.9057\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 77/99\n",
      "training Loss: 0.0014 Acc: 83.9236\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 78/99\n",
      "training Loss: 0.0014 Acc: 83.8016\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 79/99\n",
      "training Loss: 0.0014 Acc: 83.8908\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 80/99\n",
      "training Loss: 0.0014 Acc: 83.7123\n",
      "validation Loss: 0.0015 Acc: 83.2857\n",
      "Epoch 81/99\n",
      "training Loss: 0.0014 Acc: 83.7748\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0014 Acc: 83.8313\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 83/99\n",
      "training Loss: 0.0014 Acc: 83.7718\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 84/99\n",
      "training Loss: 0.0014 Acc: 83.7748\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 85/99\n",
      "training Loss: 0.0014 Acc: 83.8968\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 86/99\n",
      "training Loss: 0.0014 Acc: 83.7986\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 87/99\n",
      "training Loss: 0.0014 Acc: 83.9236\n",
      "validation Loss: 0.0015 Acc: 83.2976\n",
      "Epoch 88/99\n",
      "training Loss: 0.0014 Acc: 83.7004\n",
      "validation Loss: 0.0015 Acc: 83.2857\n",
      "Epoch 89/99\n",
      "training Loss: 0.0014 Acc: 83.7658\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 90/99\n",
      "training Loss: 0.0014 Acc: 83.8760\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 91/99\n",
      "training Loss: 0.0014 Acc: 83.8254\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 92/99\n",
      "training Loss: 0.0014 Acc: 83.8908\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 93/99\n",
      "training Loss: 0.0014 Acc: 83.7867\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 94/99\n",
      "training Loss: 0.0014 Acc: 83.6944\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 95/99\n",
      "training Loss: 0.0014 Acc: 83.8164\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 96/99\n",
      "training Loss: 0.0014 Acc: 83.7480\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 97/99\n",
      "training Loss: 0.0014 Acc: 83.8670\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 98/99\n",
      "training Loss: 0.0014 Acc: 83.7510\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 99/99\n",
      "training Loss: 0.0014 Acc: 83.8283\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Best val acc: 83.297619\n",
      "----------\n",
      "Average best_acc across k-fold: 83.5888682967\n",
      "New configuration: {'learning_rate': 0.0009202884691104562, 'initial_nodes': 584, 'dropout': 0.7541100995482662, 'batch_size': 194, 'max_depth': 4}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0029 Acc: 70.4554\n",
      "validation Loss: 0.0020 Acc: 83.3016\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.8065\n",
      "validation Loss: 0.0020 Acc: 83.5634\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.2827\n",
      "validation Loss: 0.0019 Acc: 83.7063\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 81.6994\n",
      "validation Loss: 0.0019 Acc: 83.9919\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 81.8750\n",
      "validation Loss: 0.0019 Acc: 84.0871\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 82.4137\n",
      "validation Loss: 0.0019 Acc: 84.1585\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 82.1607\n",
      "validation Loss: 0.0019 Acc: 84.1585\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 82.3988\n",
      "validation Loss: 0.0019 Acc: 84.0514\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 82.5357\n",
      "validation Loss: 0.0019 Acc: 84.0752\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 82.4554\n",
      "validation Loss: 0.0019 Acc: 83.9800\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 82.7381\n",
      "validation Loss: 0.0019 Acc: 84.1347\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 83.0208\n",
      "validation Loss: 0.0019 Acc: 84.4442\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 82.6012\n",
      "validation Loss: 0.0019 Acc: 83.9443\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 82.7500\n",
      "validation Loss: 0.0019 Acc: 84.0038\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 82.7857\n",
      "validation Loss: 0.0019 Acc: 84.2895\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 82.9405\n",
      "validation Loss: 0.0019 Acc: 84.2418\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 82.9435\n",
      "validation Loss: 0.0019 Acc: 84.3728\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 83.0060\n",
      "validation Loss: 0.0019 Acc: 84.2776\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 83.1994\n",
      "validation Loss: 0.0019 Acc: 84.3252\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 82.9911\n",
      "validation Loss: 0.0019 Acc: 84.1466\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 83.3214\n",
      "validation Loss: 0.0019 Acc: 84.3966\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 83.3393\n",
      "validation Loss: 0.0019 Acc: 84.5156\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 83.2917\n",
      "validation Loss: 0.0019 Acc: 84.4204\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 83.3393\n",
      "validation Loss: 0.0019 Acc: 84.3371\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 83.3810\n",
      "validation Loss: 0.0018 Acc: 84.3966\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 83.4702\n",
      "validation Loss: 0.0019 Acc: 84.3252\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 83.3720\n",
      "validation Loss: 0.0018 Acc: 84.2895\n",
      "Epoch 27/99\n",
      "training Loss: 0.0020 Acc: 83.2976\n",
      "validation Loss: 0.0018 Acc: 84.3728\n",
      "Epoch 28/99\n",
      "training Loss: 0.0020 Acc: 83.2798\n",
      "validation Loss: 0.0018 Acc: 84.3490\n",
      "Epoch 29/99\n",
      "training Loss: 0.0020 Acc: 83.4315\n",
      "validation Loss: 0.0018 Acc: 84.1109\n",
      "Epoch 30/99\n",
      "training Loss: 0.0020 Acc: 83.3720\n",
      "validation Loss: 0.0018 Acc: 84.3252\n",
      "Epoch 31/99\n",
      "training Loss: 0.0020 Acc: 83.6548\n",
      "validation Loss: 0.0018 Acc: 84.4442\n",
      "Epoch 32/99\n",
      "training Loss: 0.0020 Acc: 83.4732\n",
      "validation Loss: 0.0018 Acc: 84.3966\n",
      "Epoch 33/99\n",
      "training Loss: 0.0020 Acc: 83.3304\n",
      "validation Loss: 0.0018 Acc: 84.2180\n",
      "Epoch 34/99\n",
      "training Loss: 0.0020 Acc: 83.5685\n",
      "validation Loss: 0.0018 Acc: 84.3014\n",
      "Epoch 35/99\n",
      "training Loss: 0.0020 Acc: 83.3958\n",
      "validation Loss: 0.0018 Acc: 84.3252\n",
      "Epoch 36/99\n",
      "training Loss: 0.0020 Acc: 83.5357\n",
      "validation Loss: 0.0018 Acc: 84.4204\n",
      "Epoch 37/99\n",
      "training Loss: 0.0020 Acc: 83.5060\n",
      "validation Loss: 0.0018 Acc: 84.4799\n",
      "Epoch 38/99\n",
      "training Loss: 0.0020 Acc: 83.6577\n",
      "validation Loss: 0.0018 Acc: 84.2537\n",
      "Epoch 39/99\n",
      "training Loss: 0.0020 Acc: 83.5446\n",
      "validation Loss: 0.0018 Acc: 84.3490\n",
      "Epoch 40/99\n",
      "training Loss: 0.0020 Acc: 83.6250\n",
      "validation Loss: 0.0018 Acc: 84.5037\n",
      "Epoch 41/99\n",
      "training Loss: 0.0020 Acc: 83.6280\n",
      "validation Loss: 0.0018 Acc: 84.5632\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0020 Acc: 83.3988\n",
      "validation Loss: 0.0018 Acc: 84.5037\n",
      "Epoch 43/99\n",
      "training Loss: 0.0020 Acc: 83.5268\n",
      "validation Loss: 0.0018 Acc: 84.5751\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 83.9286\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 45/99\n",
      "training Loss: 0.0020 Acc: 83.8333\n",
      "validation Loss: 0.0018 Acc: 84.4204\n",
      "Epoch 46/99\n",
      "training Loss: 0.0020 Acc: 83.8125\n",
      "validation Loss: 0.0018 Acc: 84.6227\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 83.8393\n",
      "validation Loss: 0.0018 Acc: 84.4918\n",
      "Epoch 48/99\n",
      "training Loss: 0.0020 Acc: 83.8095\n",
      "validation Loss: 0.0018 Acc: 84.4799\n",
      "Epoch 49/99\n",
      "training Loss: 0.0020 Acc: 83.8363\n",
      "validation Loss: 0.0018 Acc: 84.5156\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 83.9702\n",
      "validation Loss: 0.0018 Acc: 84.3490\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 83.8571\n",
      "validation Loss: 0.0018 Acc: 84.5751\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 84.0685\n",
      "validation Loss: 0.0018 Acc: 84.6227\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 83.9494\n",
      "validation Loss: 0.0018 Acc: 84.5751\n",
      "Epoch 54/99\n",
      "training Loss: 0.0020 Acc: 84.2262\n",
      "validation Loss: 0.0018 Acc: 84.5037\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 84.0714\n",
      "validation Loss: 0.0018 Acc: 84.6108\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 84.0833\n",
      "validation Loss: 0.0018 Acc: 84.4918\n",
      "Epoch 57/99\n",
      "training Loss: 0.0019 Acc: 84.1905\n",
      "validation Loss: 0.0018 Acc: 84.3966\n",
      "Epoch 58/99\n",
      "training Loss: 0.0019 Acc: 84.0506\n",
      "validation Loss: 0.0018 Acc: 84.4680\n",
      "Epoch 59/99\n",
      "training Loss: 0.0019 Acc: 84.1369\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 60/99\n",
      "training Loss: 0.0019 Acc: 84.0625\n",
      "validation Loss: 0.0018 Acc: 84.5156\n",
      "Epoch 61/99\n",
      "training Loss: 0.0019 Acc: 84.1607\n",
      "validation Loss: 0.0018 Acc: 84.5037\n",
      "Epoch 62/99\n",
      "training Loss: 0.0019 Acc: 84.2024\n",
      "validation Loss: 0.0018 Acc: 84.4918\n",
      "Epoch 63/99\n",
      "training Loss: 0.0019 Acc: 84.1399\n",
      "validation Loss: 0.0018 Acc: 84.5394\n",
      "Epoch 64/99\n",
      "training Loss: 0.0019 Acc: 84.1369\n",
      "validation Loss: 0.0018 Acc: 84.4442\n",
      "Epoch 65/99\n",
      "training Loss: 0.0019 Acc: 84.1696\n",
      "validation Loss: 0.0018 Acc: 84.4680\n",
      "Epoch 66/99\n",
      "training Loss: 0.0019 Acc: 84.1607\n",
      "validation Loss: 0.0018 Acc: 84.4323\n",
      "Early stopped.\n",
      "Best val acc: 84.622709\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 73.4361\n",
      "validation Loss: 0.0024 Acc: 81.1190\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0024 Acc: 80.4625\n",
      "validation Loss: 0.0020 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0023 Acc: 81.0904\n",
      "validation Loss: 0.0020 Acc: 82.5000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 81.2214\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 81.4059\n",
      "validation Loss: 0.0019 Acc: 83.1190\n",
      "Epoch 5/99\n",
      "training Loss: 0.0022 Acc: 81.6410\n",
      "validation Loss: 0.0019 Acc: 83.0833\n",
      "Epoch 6/99\n",
      "training Loss: 0.0022 Acc: 81.6320\n",
      "validation Loss: 0.0019 Acc: 82.7738\n",
      "Epoch 7/99\n",
      "training Loss: 0.0022 Acc: 81.6231\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 81.8731\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 81.9564\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 82.0933\n",
      "validation Loss: 0.0019 Acc: 83.1190\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 82.0636\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 82.0338\n",
      "validation Loss: 0.0019 Acc: 82.9405\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 82.3433\n",
      "validation Loss: 0.0019 Acc: 82.8095\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 82.1499\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 82.3284\n",
      "validation Loss: 0.0019 Acc: 83.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 82.6737\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 82.3582\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 82.2868\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 82.5397\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 82.8046\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 82.5368\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 82.9236\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 23/99\n",
      "training Loss: 0.0021 Acc: 82.8701\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 24/99\n",
      "training Loss: 0.0021 Acc: 82.6796\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 82.5070\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 82.7064\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 27/99\n",
      "training Loss: 0.0020 Acc: 82.7927\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Epoch 28/99\n",
      "training Loss: 0.0020 Acc: 82.8998\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Early stopped.\n",
      "Best val acc: 83.595238\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0029 Acc: 72.9808\n",
      "validation Loss: 0.0021 Acc: 81.6071\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 81.1440\n",
      "validation Loss: 0.0020 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.5338\n",
      "validation Loss: 0.0020 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 81.6559\n",
      "validation Loss: 0.0020 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 81.9802\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 82.3165\n",
      "validation Loss: 0.0020 Acc: 82.5476\n",
      "Epoch 6/99\n",
      "training Loss: 0.0022 Acc: 82.0933\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 82.6409\n",
      "validation Loss: 0.0020 Acc: 82.7500\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 82.4891\n",
      "validation Loss: 0.0020 Acc: 82.3810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 82.5248\n",
      "validation Loss: 0.0020 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 82.4832\n",
      "validation Loss: 0.0019 Acc: 83.0595\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 82.9236\n",
      "validation Loss: 0.0020 Acc: 83.0357\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 82.8701\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 82.6915\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 82.8314\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 82.9117\n",
      "validation Loss: 0.0020 Acc: 83.0238\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 82.6379\n",
      "validation Loss: 0.0019 Acc: 83.0714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 83.0873\n",
      "validation Loss: 0.0020 Acc: 83.2857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 83.1647\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 83.0873\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 82.9296\n",
      "validation Loss: 0.0019 Acc: 83.1429\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 82.7659\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 83.2361\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 83.2391\n",
      "validation Loss: 0.0020 Acc: 83.0119\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 83.0576\n",
      "validation Loss: 0.0020 Acc: 83.1667\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 83.3313\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 83.5367\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0020 Acc: 83.2808\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 28/99\n",
      "training Loss: 0.0020 Acc: 83.2302\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 29/99\n",
      "training Loss: 0.0020 Acc: 83.3700\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0020 Acc: 83.4861\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 31/99\n",
      "training Loss: 0.0020 Acc: 83.3135\n",
      "validation Loss: 0.0019 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0020 Acc: 83.6855\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0020 Acc: 83.4028\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0020 Acc: 83.4682\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 35/99\n",
      "training Loss: 0.0020 Acc: 83.2867\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 36/99\n",
      "training Loss: 0.0020 Acc: 83.6051\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 83.5754\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0019 Acc: 83.6170\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 83.5873\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 83.7986\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 83.5218\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 83.5694\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 83.9176\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 83.7926\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 83.5307\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 83.7926\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 83.7778\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 83.7658\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 83.5605\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 83.7331\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 83.8938\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 83.6319\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 54/99\n",
      "training Loss: 0.0019 Acc: 83.7926\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 83.7837\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 83.7301\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 57/99\n",
      "training Loss: 0.0019 Acc: 83.8998\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 58/99\n",
      "training Loss: 0.0019 Acc: 83.9355\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 59/99\n",
      "training Loss: 0.0019 Acc: 83.8581\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 60/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 61/99\n",
      "training Loss: 0.0019 Acc: 83.7658\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 62/99\n",
      "training Loss: 0.0019 Acc: 83.7926\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 63/99\n",
      "training Loss: 0.0019 Acc: 83.8581\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 64/99\n",
      "training Loss: 0.0019 Acc: 83.9236\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 65/99\n",
      "training Loss: 0.0019 Acc: 83.8373\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 66/99\n",
      "training Loss: 0.0019 Acc: 83.8194\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Early stopped.\n",
      "Best val acc: 83.583333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 68.5644\n",
      "validation Loss: 0.0020 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.8226\n",
      "validation Loss: 0.0020 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.6737\n",
      "validation Loss: 0.0020 Acc: 83.2024\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 81.7928\n",
      "validation Loss: 0.0020 Acc: 83.2381\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 82.1945\n",
      "validation Loss: 0.0020 Acc: 83.4405\n",
      "Epoch 5/99\n",
      "training Loss: 0.0022 Acc: 82.3999\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 6/99\n",
      "training Loss: 0.0022 Acc: 82.3731\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 82.5010\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 82.4683\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 82.6498\n",
      "validation Loss: 0.0020 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 82.6796\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 82.7004\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 82.8760\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 82.8105\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 82.5219\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 82.8849\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 82.9593\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 82.9177\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 82.8611\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 82.9207\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 82.9861\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 82.8849\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 22/99\n",
      "training Loss: 0.0021 Acc: 82.8046\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0021 Acc: 82.9326\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 83.0516\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 83.1617\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 83.3552\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0020 Acc: 83.1320\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 28/99\n",
      "training Loss: 0.0020 Acc: 83.3492\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0020 Acc: 83.0457\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 30/99\n",
      "training Loss: 0.0020 Acc: 83.3849\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0020 Acc: 83.1945\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0020 Acc: 83.3819\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 33/99\n",
      "training Loss: 0.0020 Acc: 83.4653\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 34/99\n",
      "training Loss: 0.0020 Acc: 83.2897\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 35/99\n",
      "training Loss: 0.0020 Acc: 83.2004\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 36/99\n",
      "training Loss: 0.0020 Acc: 83.4296\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 37/99\n",
      "training Loss: 0.0020 Acc: 83.3254\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 38/99\n",
      "training Loss: 0.0020 Acc: 83.5099\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 39/99\n",
      "training Loss: 0.0020 Acc: 83.5486\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0020 Acc: 83.6141\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 41/99\n",
      "training Loss: 0.0020 Acc: 83.6349\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 42/99\n",
      "training Loss: 0.0020 Acc: 83.3700\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 43/99\n",
      "training Loss: 0.0020 Acc: 83.4772\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 44/99\n",
      "training Loss: 0.0020 Acc: 83.6795\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Epoch 45/99\n",
      "training Loss: 0.0020 Acc: 83.5932\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Epoch 46/99\n",
      "training Loss: 0.0020 Acc: 83.3730\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 47/99\n",
      "training Loss: 0.0020 Acc: 83.6855\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 83.5099\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 49/99\n",
      "training Loss: 0.0020 Acc: 83.7539\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 83.5635\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 51/99\n",
      "training Loss: 0.0020 Acc: 83.5843\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 52/99\n",
      "training Loss: 0.0020 Acc: 83.6736\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 83.8611\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Epoch 54/99\n",
      "training Loss: 0.0019 Acc: 83.6379\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 83.7301\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 83.6349\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 57/99\n",
      "training Loss: 0.0020 Acc: 83.6647\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 58/99\n",
      "training Loss: 0.0020 Acc: 83.8373\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 59/99\n",
      "training Loss: 0.0019 Acc: 83.7420\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Early stopped.\n",
      "Best val acc: 83.797619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0029 Acc: 70.9958\n",
      "validation Loss: 0.0020 Acc: 82.4643\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.5339\n",
      "validation Loss: 0.0020 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.1380\n",
      "validation Loss: 0.0020 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 81.2868\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 81.8433\n",
      "validation Loss: 0.0020 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 82.0576\n",
      "validation Loss: 0.0020 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 82.2124\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 82.3284\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 82.6141\n",
      "validation Loss: 0.0020 Acc: 83.1905\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 82.5278\n",
      "validation Loss: 0.0019 Acc: 82.8810\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 82.7838\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 82.8433\n",
      "validation Loss: 0.0019 Acc: 83.0595\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 82.9028\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 83.0724\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 83.0576\n",
      "validation Loss: 0.0019 Acc: 83.0833\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 83.1290\n",
      "validation Loss: 0.0020 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 83.0814\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 83.3790\n",
      "validation Loss: 0.0019 Acc: 82.9405\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 83.3224\n",
      "validation Loss: 0.0019 Acc: 83.1190\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 83.1706\n",
      "validation Loss: 0.0019 Acc: 82.9524\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 83.2718\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 21/99\n",
      "training Loss: 0.0021 Acc: 83.1677\n",
      "validation Loss: 0.0019 Acc: 82.8452\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 83.2450\n",
      "validation Loss: 0.0019 Acc: 83.0357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 83.4355\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 83.2808\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 83.4801\n",
      "validation Loss: 0.0020 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 83.4385\n",
      "validation Loss: 0.0019 Acc: 83.1905\n",
      "Epoch 27/99\n",
      "training Loss: 0.0020 Acc: 83.1379\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0020 Acc: 83.3492\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0020 Acc: 83.3135\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 30/99\n",
      "training Loss: 0.0020 Acc: 83.4147\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 31/99\n",
      "training Loss: 0.0020 Acc: 83.6349\n",
      "validation Loss: 0.0020 Acc: 83.4405\n",
      "Epoch 32/99\n",
      "training Loss: 0.0020 Acc: 83.6468\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 33/99\n",
      "training Loss: 0.0020 Acc: 83.7629\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 34/99\n",
      "training Loss: 0.0020 Acc: 83.4831\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 35/99\n",
      "training Loss: 0.0020 Acc: 83.7629\n",
      "validation Loss: 0.0020 Acc: 83.3095\n",
      "Epoch 36/99\n",
      "training Loss: 0.0020 Acc: 83.5784\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 37/99\n",
      "training Loss: 0.0020 Acc: 83.6111\n",
      "validation Loss: 0.0019 Acc: 83.3810\n",
      "Epoch 38/99\n",
      "training Loss: 0.0020 Acc: 83.5962\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 83.8551\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 40/99\n",
      "training Loss: 0.0020 Acc: 83.8135\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 83.8194\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 83.9385\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 83.8968\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 83.8998\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 83.8760\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 83.8551\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 84.0962\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 84.0367\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 84.1081\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 83.9890\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 84.1349\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 84.1051\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 84.1825\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 54/99\n",
      "training Loss: 0.0019 Acc: 84.2926\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 84.0367\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 84.2122\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 57/99\n",
      "training Loss: 0.0019 Acc: 84.1200\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 58/99\n",
      "training Loss: 0.0019 Acc: 84.2807\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Epoch 59/99\n",
      "training Loss: 0.0019 Acc: 84.2420\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 60/99\n",
      "training Loss: 0.0019 Acc: 84.2301\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 61/99\n",
      "training Loss: 0.0019 Acc: 84.1884\n",
      "validation Loss: 0.0020 Acc: 83.4881\n",
      "Epoch 62/99\n",
      "training Loss: 0.0019 Acc: 84.2866\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 63/99\n",
      "training Loss: 0.0019 Acc: 84.0932\n",
      "validation Loss: 0.0020 Acc: 83.5833\n",
      "Epoch 64/99\n",
      "training Loss: 0.0019 Acc: 84.2122\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Epoch 65/99\n",
      "training Loss: 0.0019 Acc: 84.1914\n",
      "validation Loss: 0.0020 Acc: 83.5595\n",
      "Epoch 66/99\n",
      "training Loss: 0.0019 Acc: 84.1617\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 67/99\n",
      "training Loss: 0.0019 Acc: 84.0754\n",
      "validation Loss: 0.0020 Acc: 83.5119\n",
      "Epoch 68/99\n",
      "training Loss: 0.0019 Acc: 83.9861\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 69/99\n",
      "training Loss: 0.0019 Acc: 84.1230\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 70/99\n",
      "training Loss: 0.0019 Acc: 84.2688\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 71/99\n",
      "training Loss: 0.0019 Acc: 84.1914\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Early stopped.\n",
      "Best val acc: 83.595238\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8388274901\n",
      "New configuration: {'learning_rate': 0.6106267064873843, 'initial_nodes': 495, 'dropout': 0.1349121945672542, 'batch_size': 450, 'max_depth': 4}\n",
      "Epoch 0/99\n",
      "training Loss: 481.2111 Acc: 49.7530\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 49.9375\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0015 Acc: 50.1012\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 50.3393\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0015 Acc: 50.1429\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 50.1607\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 50.2470\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 50.0179\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 50.0536\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0331 Acc: 49.7827\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 49.9702\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 49.8810\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 50.1101\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 50.4256\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 50.0417\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0160 Acc: 50.2054\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 50.1607\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 49.9821\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 50.1131\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 49.7083\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 49.6488\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 51.9426 Acc: 49.5953\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0068 Acc: 50.3184\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.6248 Acc: 49.8661\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 49.6072\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 49.7232\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 49.9286\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 50.5059\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 49.9464\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 50.2024\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 49.5090\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 49.5000\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 49.6101\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0288 Acc: 49.7560\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 49.8214\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0016 Acc: 50.0774\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 50.0000\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 49.9048\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 50.3393\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 49.9048\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 50.1369\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0016 Acc: 49.6667\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 642.5498 Acc: 49.5090\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0249 Acc: 50.1875\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 49.9196\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0025 Acc: 49.8333\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 50.4107\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0017 Acc: 50.1339\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 49.5238\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 49.3959\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 49.9375\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 50.0446\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 49.7351\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 50.3036\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 2.0506 Acc: 49.6964\n",
      "validation Loss: 0.0025 Acc: 49.9881\n",
      "Epoch 13/99\n",
      "training Loss: 0.6292 Acc: 49.9762\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0079 Acc: 49.7887\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 1.2374 Acc: 50.4375\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0016 Acc: 49.6875\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0016 Acc: 50.3541\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 49.8185\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0016 Acc: 49.5685\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 49.5863\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 1025.6002 Acc: 50.1964\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.1708 Acc: 49.4405\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 50.1131\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0015 Acc: 49.9107\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.2016 Acc: 49.8452\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0030 Acc: 49.6726\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0034 Acc: 49.8631\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 49.9464\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 50.2738\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0030 Acc: 49.9643\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 50.1012\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 50.2738\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 50.0119\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0016 Acc: 49.6548\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 50.4464\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0059 Acc: 49.5179\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 49.8631\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 50.1428\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 49.5179\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 50.0536\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0262 Acc: 50.1905\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 130.2697 Acc: 50.9077\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 50.2857\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 49.5298\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 49.7024\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 4/99\n",
      "training Loss: 0.0015 Acc: 49.9077\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0015 Acc: 49.8661\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 49.7976\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 49.6697\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 49.8958\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 49.8869\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 50.3155\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 49.9256\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 49.6012\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0016 Acc: 49.6816\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 14/99\n",
      "training Loss: 0.0016 Acc: 49.7589\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 50.0298\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 49.9732\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 49.8066\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 49.8899\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 50.0089\n",
      "validation Loss: 0.0016 Acc: 50.0238\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 49.9583\n",
      "validation Loss: 0.0016 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.023810\n",
      "----------\n",
      "Average best_acc across k-fold: 50.0047619048\n",
      "New configuration: {'learning_rate': 0.004003016661601729, 'initial_nodes': 841, 'dropout': 0.6142027818058747, 'batch_size': 378, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.4911\n",
      "validation Loss: 0.0010 Acc: 82.3256\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1131\n",
      "validation Loss: 0.0010 Acc: 82.5756\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.2738\n",
      "validation Loss: 0.0010 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.4018\n",
      "validation Loss: 0.0010 Acc: 82.8731\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.5536\n",
      "validation Loss: 0.0010 Acc: 83.1231\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.7113\n",
      "validation Loss: 0.0010 Acc: 83.0517\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.5595\n",
      "validation Loss: 0.0010 Acc: 83.1588\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.7381\n",
      "validation Loss: 0.0010 Acc: 83.0755\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.6964\n",
      "validation Loss: 0.0010 Acc: 82.7422\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.8512\n",
      "validation Loss: 0.0010 Acc: 82.8969\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.7440\n",
      "validation Loss: 0.0010 Acc: 82.9088\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.8661\n",
      "validation Loss: 0.0010 Acc: 83.1707\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 84.1071\n",
      "validation Loss: 0.0010 Acc: 83.0040\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 84.2173\n",
      "validation Loss: 0.0010 Acc: 83.1826\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 84.2679\n",
      "validation Loss: 0.0010 Acc: 83.1826\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.3720\n",
      "validation Loss: 0.0010 Acc: 83.3730\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 84.4077\n",
      "validation Loss: 0.0010 Acc: 82.8731\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.3571\n",
      "validation Loss: 0.0010 Acc: 83.1826\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.4554\n",
      "validation Loss: 0.0010 Acc: 83.5158\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.4911\n",
      "validation Loss: 0.0010 Acc: 83.3016\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.7440\n",
      "validation Loss: 0.0010 Acc: 83.4920\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.6548\n",
      "validation Loss: 0.0010 Acc: 83.3492\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.9256\n",
      "validation Loss: 0.0010 Acc: 83.3968\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.9167\n",
      "validation Loss: 0.0010 Acc: 83.1469\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.8542\n",
      "validation Loss: 0.0010 Acc: 83.2897\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.9881\n",
      "validation Loss: 0.0010 Acc: 83.2421\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 85.2083\n",
      "validation Loss: 0.0010 Acc: 83.1707\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 85.3661\n",
      "validation Loss: 0.0010 Acc: 83.1350\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 85.1577\n",
      "validation Loss: 0.0010 Acc: 83.2183\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.5774\n",
      "validation Loss: 0.0010 Acc: 83.2183\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 85.4375\n",
      "validation Loss: 0.0010 Acc: 83.3016\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.6190\n",
      "validation Loss: 0.0010 Acc: 83.2897\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.7768\n",
      "validation Loss: 0.0010 Acc: 83.2302\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.7679\n",
      "validation Loss: 0.0010 Acc: 83.1707\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.5952\n",
      "validation Loss: 0.0010 Acc: 83.0636\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.8482\n",
      "validation Loss: 0.0010 Acc: 83.1469\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.9375\n",
      "validation Loss: 0.0010 Acc: 83.2540\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.7946\n",
      "validation Loss: 0.0010 Acc: 83.0279\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 86.0149\n",
      "validation Loss: 0.0010 Acc: 83.0636\n",
      "Early stopped.\n",
      "Best val acc: 83.515830\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.3404\n",
      "validation Loss: 0.0010 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0011 Acc: 82.7034\n",
      "validation Loss: 0.0010 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.1141\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.3075\n",
      "validation Loss: 0.0010 Acc: 83.6429\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.1945\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.4117\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.3998\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.3224\n",
      "validation Loss: 0.0010 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.2927\n",
      "validation Loss: 0.0010 Acc: 83.9286\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.3879\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.5724\n",
      "validation Loss: 0.0010 Acc: 83.8690\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.5546\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.3611\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 83.4355\n",
      "validation Loss: 0.0010 Acc: 83.7857\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 83.5069\n",
      "validation Loss: 0.0010 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 83.6260\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 83.6290\n",
      "validation Loss: 0.0010 Acc: 84.0476\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 83.9593\n",
      "validation Loss: 0.0009 Acc: 84.0357\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.1438\n",
      "validation Loss: 0.0010 Acc: 83.9524\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 83.9950\n",
      "validation Loss: 0.0010 Acc: 84.2738\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.1527\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.1795\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.1498\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.3105\n",
      "validation Loss: 0.0010 Acc: 84.2857\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.4801\n",
      "validation Loss: 0.0010 Acc: 84.1548\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.5872\n",
      "validation Loss: 0.0010 Acc: 83.9881\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.9086\n",
      "validation Loss: 0.0010 Acc: 83.9524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.7063\n",
      "validation Loss: 0.0010 Acc: 83.9524\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.7271\n",
      "validation Loss: 0.0010 Acc: 84.0000\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.7628\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.9771\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.1795\n",
      "validation Loss: 0.0010 Acc: 83.8810\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.1527\n",
      "validation Loss: 0.0010 Acc: 83.8452\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.3669\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.3848\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.3342\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.4652\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.4979\n",
      "validation Loss: 0.0010 Acc: 83.6548\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 85.5306\n",
      "validation Loss: 0.0010 Acc: 83.6667\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 85.5753\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 85.6110\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 85.7955\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 85.9354\n",
      "validation Loss: 0.0010 Acc: 83.6905\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 85.8252\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Early stopped.\n",
      "Best val acc: 84.285714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.3285\n",
      "validation Loss: 0.0010 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.3462\n",
      "validation Loss: 0.0010 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.2212\n",
      "validation Loss: 0.0010 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.3968\n",
      "validation Loss: 0.0011 Acc: 83.0714\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.5456\n",
      "validation Loss: 0.0010 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.5248\n",
      "validation Loss: 0.0010 Acc: 82.9881\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.5129\n",
      "validation Loss: 0.0010 Acc: 83.1667\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.7004\n",
      "validation Loss: 0.0010 Acc: 83.1548\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.5129\n",
      "validation Loss: 0.0010 Acc: 82.9762\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.6944\n",
      "validation Loss: 0.0010 Acc: 83.1190\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.6587\n",
      "validation Loss: 0.0010 Acc: 83.2024\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.5843\n",
      "validation Loss: 0.0010 Acc: 83.1786\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.5903\n",
      "validation Loss: 0.0010 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 83.6141\n",
      "validation Loss: 0.0010 Acc: 82.9881\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 83.6914\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 83.8641\n",
      "validation Loss: 0.0010 Acc: 83.2024\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 83.8373\n",
      "validation Loss: 0.0010 Acc: 83.1786\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 83.8730\n",
      "validation Loss: 0.0010 Acc: 83.1905\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 83.6795\n",
      "validation Loss: 0.0010 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 83.8670\n",
      "validation Loss: 0.0010 Acc: 83.0952\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 83.8016\n",
      "validation Loss: 0.0010 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.1617\n",
      "validation Loss: 0.0010 Acc: 83.3929\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.1855\n",
      "validation Loss: 0.0010 Acc: 83.1071\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.3134\n",
      "validation Loss: 0.0010 Acc: 83.2976\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.4890\n",
      "validation Loss: 0.0010 Acc: 83.4762\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.3402\n",
      "validation Loss: 0.0010 Acc: 83.2381\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.4920\n",
      "validation Loss: 0.0010 Acc: 83.3333\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.5575\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.7033\n",
      "validation Loss: 0.0010 Acc: 83.1310\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.0098\n",
      "validation Loss: 0.0010 Acc: 83.2857\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.8313\n",
      "validation Loss: 0.0010 Acc: 83.3095\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.8640\n",
      "validation Loss: 0.0010 Acc: 83.1548\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 84.9711\n",
      "validation Loss: 0.0010 Acc: 83.4048\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.2658\n",
      "validation Loss: 0.0010 Acc: 83.4048\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.1378\n",
      "validation Loss: 0.0010 Acc: 83.2857\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.3134\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.4890\n",
      "validation Loss: 0.0010 Acc: 83.4524\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.5812\n",
      "validation Loss: 0.0010 Acc: 83.4167\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 85.5455\n",
      "validation Loss: 0.0010 Acc: 83.3929\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 85.6735\n",
      "validation Loss: 0.0010 Acc: 83.5476\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 85.7062\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Early stopped.\n",
      "Best val acc: 83.607143\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.1737\n",
      "validation Loss: 0.0010 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 82.7778\n",
      "validation Loss: 0.0010 Acc: 83.6190\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.2391\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.0010\n",
      "validation Loss: 0.0010 Acc: 84.2857\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.1379\n",
      "validation Loss: 0.0010 Acc: 84.3690\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.0099\n",
      "validation Loss: 0.0010 Acc: 84.1190\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.2659\n",
      "validation Loss: 0.0010 Acc: 84.2024\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.3522\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.2599\n",
      "validation Loss: 0.0010 Acc: 84.3214\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.3522\n",
      "validation Loss: 0.0010 Acc: 84.2500\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.3135\n",
      "validation Loss: 0.0010 Acc: 83.9643\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.2956\n",
      "validation Loss: 0.0010 Acc: 83.9881\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.3879\n",
      "validation Loss: 0.0010 Acc: 84.0714\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 83.5367\n",
      "validation Loss: 0.0010 Acc: 84.2262\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 83.6528\n",
      "validation Loss: 0.0010 Acc: 84.3929\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 83.8075\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 83.9176\n",
      "validation Loss: 0.0010 Acc: 84.3095\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.0099\n",
      "validation Loss: 0.0010 Acc: 84.3095\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 83.9027\n",
      "validation Loss: 0.0010 Acc: 84.2976\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 83.8402\n",
      "validation Loss: 0.0010 Acc: 84.3571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.1587\n",
      "validation Loss: 0.0010 Acc: 84.0238\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.3343\n",
      "validation Loss: 0.0010 Acc: 84.3333\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.1765\n",
      "validation Loss: 0.0010 Acc: 84.1429\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.4414\n",
      "validation Loss: 0.0010 Acc: 84.3333\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.2807\n",
      "validation Loss: 0.0010 Acc: 84.0595\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.4355\n",
      "validation Loss: 0.0010 Acc: 84.2976\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.6438\n",
      "validation Loss: 0.0010 Acc: 84.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.4831\n",
      "validation Loss: 0.0010 Acc: 84.2262\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.6765\n",
      "validation Loss: 0.0010 Acc: 84.3690\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.8521\n",
      "validation Loss: 0.0010 Acc: 84.3810\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.8104\n",
      "validation Loss: 0.0010 Acc: 84.2857\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.8967\n",
      "validation Loss: 0.0010 Acc: 84.3452\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.0931\n",
      "validation Loss: 0.0010 Acc: 84.4286\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.0396\n",
      "validation Loss: 0.0010 Acc: 84.3214\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.1705\n",
      "validation Loss: 0.0010 Acc: 84.2381\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.0693\n",
      "validation Loss: 0.0010 Acc: 84.2738\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.2836\n",
      "validation Loss: 0.0010 Acc: 84.3452\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.2479\n",
      "validation Loss: 0.0010 Acc: 84.2738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 85.1765\n",
      "validation Loss: 0.0010 Acc: 84.1905\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 85.3015\n",
      "validation Loss: 0.0010 Acc: 84.1548\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 85.4413\n",
      "validation Loss: 0.0010 Acc: 84.2976\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 85.3074\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 85.5931\n",
      "validation Loss: 0.0010 Acc: 84.3214\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 85.5128\n",
      "validation Loss: 0.0010 Acc: 84.2976\n",
      "Epoch 44/99\n",
      "training Loss: 0.0009 Acc: 85.4562\n",
      "validation Loss: 0.0010 Acc: 84.2262\n",
      "Epoch 45/99\n",
      "training Loss: 0.0009 Acc: 85.4652\n",
      "validation Loss: 0.0010 Acc: 84.2381\n",
      "Epoch 46/99\n",
      "training Loss: 0.0009 Acc: 85.5901\n",
      "validation Loss: 0.0010 Acc: 84.2381\n",
      "Epoch 47/99\n",
      "training Loss: 0.0009 Acc: 85.4443\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0009 Acc: 85.4681\n",
      "validation Loss: 0.0010 Acc: 84.3333\n",
      "Epoch 49/99\n",
      "training Loss: 0.0009 Acc: 85.5693\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 50/99\n",
      "training Loss: 0.0009 Acc: 85.4919\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Epoch 51/99\n",
      "training Loss: 0.0009 Acc: 85.5842\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 52/99\n",
      "training Loss: 0.0009 Acc: 85.4771\n",
      "validation Loss: 0.0010 Acc: 84.2262\n",
      "Early stopped.\n",
      "Best val acc: 84.428571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.3106\n",
      "validation Loss: 0.0010 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1320\n",
      "validation Loss: 0.0010 Acc: 83.2381\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.2897\n",
      "validation Loss: 0.0010 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.0843\n",
      "validation Loss: 0.0010 Acc: 83.2976\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.3581\n",
      "validation Loss: 0.0010 Acc: 83.3810\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.5040\n",
      "validation Loss: 0.0010 Acc: 83.2024\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.5516\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.6617\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.9236\n",
      "validation Loss: 0.0010 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.8968\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.8700\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.9742\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 84.0873\n",
      "validation Loss: 0.0010 Acc: 83.6905\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.8938\n",
      "validation Loss: 0.0010 Acc: 83.9286\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.9890\n",
      "validation Loss: 0.0010 Acc: 83.6429\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.0813\n",
      "validation Loss: 0.0010 Acc: 83.8690\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 83.9325\n",
      "validation Loss: 0.0010 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.0754\n",
      "validation Loss: 0.0010 Acc: 84.1190\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.2747\n",
      "validation Loss: 0.0010 Acc: 84.0238\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.2837\n",
      "validation Loss: 0.0010 Acc: 83.8810\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.2866\n",
      "validation Loss: 0.0010 Acc: 84.0595\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.2599\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.5545\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.5366\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.6527\n",
      "validation Loss: 0.0010 Acc: 84.0952\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.7539\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.7717\n",
      "validation Loss: 0.0010 Acc: 84.0714\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 85.0068\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.9503\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.2687\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 85.4830\n",
      "validation Loss: 0.0010 Acc: 83.7619\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.2717\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.3669\n",
      "validation Loss: 0.0010 Acc: 83.8452\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.4205\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.5217\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.5991\n",
      "validation Loss: 0.0010 Acc: 83.6429\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.7985\n",
      "validation Loss: 0.0010 Acc: 83.6786\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.6735\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Early stopped.\n",
      "Best val acc: 84.119048\n",
      "----------\n",
      "Average best_acc across k-fold: 83.991261151\n",
      "New configuration: {'learning_rate': 0.06209807272272614, 'initial_nodes': 630, 'dropout': 0.10425777039713502, 'batch_size': 259, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0104 Acc: 77.9375\n",
      "validation Loss: 0.0016 Acc: 82.8374\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 81.7500\n",
      "validation Loss: 0.0015 Acc: 82.6946\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.1875\n",
      "validation Loss: 0.0015 Acc: 82.0876\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 81.9613\n",
      "validation Loss: 0.0015 Acc: 82.4566\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.9137\n",
      "validation Loss: 0.0015 Acc: 82.4923\n",
      "Epoch 5/99\n",
      "training Loss: 0.0015 Acc: 82.4167\n",
      "validation Loss: 0.0016 Acc: 82.6470\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.5357\n",
      "validation Loss: 0.0015 Acc: 82.8731\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.8482\n",
      "validation Loss: 0.0015 Acc: 82.5280\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 82.6756\n",
      "validation Loss: 0.0015 Acc: 82.5994\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 83.0000\n",
      "validation Loss: 0.0015 Acc: 83.0159\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.9196\n",
      "validation Loss: 0.0015 Acc: 82.0519\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.0774\n",
      "validation Loss: 0.0015 Acc: 83.1588\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.1250\n",
      "validation Loss: 0.0015 Acc: 82.9564\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.1637\n",
      "validation Loss: 0.0015 Acc: 83.0517\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.2619\n",
      "validation Loss: 0.0015 Acc: 82.2542\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.0149\n",
      "validation Loss: 0.0015 Acc: 83.1350\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.0060\n",
      "validation Loss: 0.0016 Acc: 82.4447\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.0744\n",
      "validation Loss: 0.0015 Acc: 82.7422\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 82.9107\n",
      "validation Loss: 0.0015 Acc: 82.7779\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 83.1131\n",
      "validation Loss: 0.0015 Acc: 82.7541\n",
      "Epoch 20/99\n",
      "training Loss: 0.0016 Acc: 82.7232\n",
      "validation Loss: 0.0016 Acc: 80.9093\n",
      "Epoch 21/99\n",
      "training Loss: 0.0016 Acc: 82.4554\n",
      "validation Loss: 0.0016 Acc: 82.3613\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 82.6280\n",
      "validation Loss: 0.0015 Acc: 82.9207\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 82.6726\n",
      "validation Loss: 0.0015 Acc: 83.1945\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 83.0268\n",
      "validation Loss: 0.0015 Acc: 82.9326\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 83.0536\n",
      "validation Loss: 0.0015 Acc: 83.3492\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 82.9881\n",
      "validation Loss: 0.0015 Acc: 83.3135\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 83.1042\n",
      "validation Loss: 0.0015 Acc: 83.3016\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 83.1637\n",
      "validation Loss: 0.0015 Acc: 82.9683\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 83.3571\n",
      "validation Loss: 0.0015 Acc: 83.1588\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 83.3006\n",
      "validation Loss: 0.0015 Acc: 83.1469\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.3720\n",
      "validation Loss: 0.0015 Acc: 83.2659\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 83.4524\n",
      "validation Loss: 0.0015 Acc: 82.7660\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 83.5357\n",
      "validation Loss: 0.0015 Acc: 83.4087\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.4464\n",
      "validation Loss: 0.0014 Acc: 83.1231\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.5357\n",
      "validation Loss: 0.0015 Acc: 83.2659\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.4643\n",
      "validation Loss: 0.0014 Acc: 83.0159\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.6637\n",
      "validation Loss: 0.0014 Acc: 83.2659\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.8185\n",
      "validation Loss: 0.0015 Acc: 83.4682\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.7321\n",
      "validation Loss: 0.0014 Acc: 83.1945\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.9018\n",
      "validation Loss: 0.0014 Acc: 83.3968\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.7946\n",
      "validation Loss: 0.0014 Acc: 83.3611\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.6012\n",
      "validation Loss: 0.0015 Acc: 83.2778\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.9673\n",
      "validation Loss: 0.0014 Acc: 83.4087\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 84.0298\n",
      "validation Loss: 0.0014 Acc: 83.4206\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.9375\n",
      "validation Loss: 0.0014 Acc: 83.3849\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 84.0119\n",
      "validation Loss: 0.0015 Acc: 83.1945\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 84.0804\n",
      "validation Loss: 0.0015 Acc: 83.3373\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 84.1042\n",
      "validation Loss: 0.0014 Acc: 83.0159\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 84.2619\n",
      "validation Loss: 0.0014 Acc: 83.3016\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 84.1935\n",
      "validation Loss: 0.0014 Acc: 83.4087\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 84.2560\n",
      "validation Loss: 0.0014 Acc: 83.3849\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 84.4137\n",
      "validation Loss: 0.0014 Acc: 83.3611\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 84.3155\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 84.4970\n",
      "validation Loss: 0.0014 Acc: 83.4087\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 84.2649\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 84.5119\n",
      "validation Loss: 0.0015 Acc: 83.4563\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 84.3482\n",
      "validation Loss: 0.0014 Acc: 83.3254\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 84.5357\n",
      "validation Loss: 0.0014 Acc: 83.1588\n",
      "Early stopped.\n",
      "Best val acc: 83.468222\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0242 Acc: 78.2245\n",
      "validation Loss: 0.0020 Acc: 79.2381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0018 Acc: 80.1351\n",
      "validation Loss: 0.0017 Acc: 80.5714\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 79.4655\n",
      "validation Loss: 0.0017 Acc: 80.7619\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 80.2601\n",
      "validation Loss: 0.0017 Acc: 81.5476\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 80.6797\n",
      "validation Loss: 0.0016 Acc: 81.7619\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 80.3315\n",
      "validation Loss: 0.0017 Acc: 81.7500\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 80.2631\n",
      "validation Loss: 0.0016 Acc: 80.9167\n",
      "Epoch 7/99\n",
      "training Loss: 0.0018 Acc: 79.2959\n",
      "validation Loss: 0.0017 Acc: 80.3929\n",
      "Epoch 8/99\n",
      "training Loss: 0.0017 Acc: 80.3791\n",
      "validation Loss: 0.0017 Acc: 80.9524\n",
      "Epoch 9/99\n",
      "training Loss: 0.0017 Acc: 81.1172\n",
      "validation Loss: 0.0017 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 81.0101\n",
      "validation Loss: 0.0017 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0017 Acc: 81.0636\n",
      "validation Loss: 0.0016 Acc: 82.4405\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 81.5041\n",
      "validation Loss: 0.0016 Acc: 82.3571\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 81.6291\n",
      "validation Loss: 0.0016 Acc: 82.9643\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 81.8939\n",
      "validation Loss: 0.0016 Acc: 82.9881\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 81.8106\n",
      "validation Loss: 0.0016 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 81.9505\n",
      "validation Loss: 0.0016 Acc: 83.1310\n",
      "Epoch 17/99\n",
      "training Loss: 0.0017 Acc: 81.8493\n",
      "validation Loss: 0.0016 Acc: 82.5476\n",
      "Epoch 18/99\n",
      "training Loss: 0.0017 Acc: 81.9564\n",
      "validation Loss: 0.0016 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0017 Acc: 81.7362\n",
      "validation Loss: 0.0016 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0017 Acc: 81.5517\n",
      "validation Loss: 0.0015 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0017 Acc: 82.0040\n",
      "validation Loss: 0.0016 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0017 Acc: 82.2153\n",
      "validation Loss: 0.0016 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0016 Acc: 82.2272\n",
      "validation Loss: 0.0016 Acc: 83.1786\n",
      "Epoch 24/99\n",
      "training Loss: 0.0016 Acc: 82.6112\n",
      "validation Loss: 0.0016 Acc: 83.1786\n",
      "Epoch 25/99\n",
      "training Loss: 0.0017 Acc: 82.0576\n",
      "validation Loss: 0.0016 Acc: 83.4048\n",
      "Epoch 26/99\n",
      "training Loss: 0.0016 Acc: 82.2421\n",
      "validation Loss: 0.0016 Acc: 83.4762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0016 Acc: 81.9267\n",
      "validation Loss: 0.0016 Acc: 82.8095\n",
      "Epoch 28/99\n",
      "training Loss: 0.0016 Acc: 81.8999\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0016 Acc: 82.4921\n",
      "validation Loss: 0.0016 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0016 Acc: 82.5189\n",
      "validation Loss: 0.0016 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 82.5784\n",
      "validation Loss: 0.0016 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 82.2332\n",
      "validation Loss: 0.0016 Acc: 83.4643\n",
      "Epoch 33/99\n",
      "training Loss: 0.0016 Acc: 82.4564\n",
      "validation Loss: 0.0015 Acc: 83.6548\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 82.7451\n",
      "validation Loss: 0.0015 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 82.8611\n",
      "validation Loss: 0.0015 Acc: 83.7619\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 82.6379\n",
      "validation Loss: 0.0015 Acc: 83.6071\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 82.8046\n",
      "validation Loss: 0.0015 Acc: 83.5476\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 82.6737\n",
      "validation Loss: 0.0015 Acc: 83.5595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 82.6498\n",
      "validation Loss: 0.0015 Acc: 83.6905\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 82.7600\n",
      "validation Loss: 0.0015 Acc: 83.6786\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 82.8671\n",
      "validation Loss: 0.0015 Acc: 83.4167\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 82.9266\n",
      "validation Loss: 0.0015 Acc: 83.1190\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 82.7838\n",
      "validation Loss: 0.0015 Acc: 83.1905\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 83.0843\n",
      "validation Loss: 0.0015 Acc: 83.1429\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 82.9326\n",
      "validation Loss: 0.0015 Acc: 83.2976\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 82.9415\n",
      "validation Loss: 0.0015 Acc: 83.3095\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 82.8582\n",
      "validation Loss: 0.0015 Acc: 83.2024\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 83.0337\n",
      "validation Loss: 0.0015 Acc: 83.1667\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 82.9623\n",
      "validation Loss: 0.0015 Acc: 82.9643\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 82.9355\n",
      "validation Loss: 0.0015 Acc: 82.9286\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 82.8225\n",
      "validation Loss: 0.0015 Acc: 82.5119\n",
      "Epoch 52/99\n",
      "training Loss: 0.0015 Acc: 83.0218\n",
      "validation Loss: 0.0015 Acc: 82.9524\n",
      "Epoch 53/99\n",
      "training Loss: 0.0016 Acc: 83.1736\n",
      "validation Loss: 0.0015 Acc: 82.7262\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 83.0010\n",
      "validation Loss: 0.0015 Acc: 82.8929\n",
      "Early stopped.\n",
      "Best val acc: 83.809524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0080 Acc: 78.4358\n",
      "validation Loss: 0.0017 Acc: 81.3095\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0017 Acc: 81.5666\n",
      "validation Loss: 0.0016 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.4772\n",
      "validation Loss: 0.0015 Acc: 82.5952\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 78.0817\n",
      "validation Loss: 0.0016 Acc: 81.8690\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.9118\n",
      "validation Loss: 0.0015 Acc: 82.2976\n",
      "Epoch 5/99\n",
      "training Loss: 0.0015 Acc: 82.5070\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.2570\n",
      "validation Loss: 0.0015 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.7540\n",
      "validation Loss: 0.0015 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 82.1767\n",
      "validation Loss: 0.0018 Acc: 81.0238\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 81.7451\n",
      "validation Loss: 0.0016 Acc: 82.2619\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.4356\n",
      "validation Loss: 0.0015 Acc: 82.9881\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.8522\n",
      "validation Loss: 0.0015 Acc: 83.0119\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.2510\n",
      "validation Loss: 0.0015 Acc: 83.1190\n",
      "Epoch 13/99\n",
      "training Loss: 0.0016 Acc: 82.9058\n",
      "validation Loss: 0.0015 Acc: 83.0714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.2689\n",
      "validation Loss: 0.0015 Acc: 83.1429\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 83.4504\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.3700\n",
      "validation Loss: 0.0014 Acc: 83.1310\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.4444\n",
      "validation Loss: 0.0015 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.5605\n",
      "validation Loss: 0.0015 Acc: 83.3452\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.2837\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.3938\n",
      "validation Loss: 0.0015 Acc: 83.4524\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.4921\n",
      "validation Loss: 0.0014 Acc: 82.9881\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.4355\n",
      "validation Loss: 0.0015 Acc: 82.9524\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.5575\n",
      "validation Loss: 0.0015 Acc: 83.0476\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.6528\n",
      "validation Loss: 0.0014 Acc: 83.0714\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.6200\n",
      "validation Loss: 0.0015 Acc: 83.1667\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.7391\n",
      "validation Loss: 0.0014 Acc: 82.5476\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.7956\n",
      "validation Loss: 0.0015 Acc: 82.7619\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.8313\n",
      "validation Loss: 0.0015 Acc: 82.9881\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.9623\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.8194\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.9742\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.8611\n",
      "validation Loss: 0.0014 Acc: 83.0714\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.9861\n",
      "validation Loss: 0.0014 Acc: 83.0595\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 84.0188\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 84.0129\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 84.0664\n",
      "validation Loss: 0.0014 Acc: 82.9524\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.9742\n",
      "validation Loss: 0.0015 Acc: 83.2976\n",
      "Early stopped.\n",
      "Best val acc: 83.583333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0156 Acc: 77.8971\n",
      "validation Loss: 0.0016 Acc: 81.4405\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0018 Acc: 81.2184\n",
      "validation Loss: 0.0015 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.1767\n",
      "validation Loss: 0.0015 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 82.0040\n",
      "validation Loss: 0.0015 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 82.1915\n",
      "validation Loss: 0.0015 Acc: 82.6310\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 82.2927\n",
      "validation Loss: 0.0016 Acc: 82.0595\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 81.9505\n",
      "validation Loss: 0.0016 Acc: 82.7262\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 82.3403\n",
      "validation Loss: 0.0015 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 82.5040\n",
      "validation Loss: 0.0015 Acc: 83.2024\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.5278\n",
      "validation Loss: 0.0015 Acc: 83.3095\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.4564\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.7897\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 82.7838\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 83.2569\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 83.0724\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 83.0873\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 82.9117\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.1498\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.0814\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.1647\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.1379\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.0992\n",
      "validation Loss: 0.0014 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 82.9891\n",
      "validation Loss: 0.0015 Acc: 83.4643\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 82.9445\n",
      "validation Loss: 0.0014 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.5367\n",
      "validation Loss: 0.0014 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.3194\n",
      "validation Loss: 0.0014 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.4831\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.5575\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.4593\n",
      "validation Loss: 0.0014 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.6230\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.5397\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.9266\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.5635\n",
      "validation Loss: 0.0014 Acc: 83.8214\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.8105\n",
      "validation Loss: 0.0014 Acc: 83.8690\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.8551\n",
      "validation Loss: 0.0014 Acc: 83.8810\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 84.1706\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.7837\n",
      "validation Loss: 0.0014 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 83.6409\n",
      "validation Loss: 0.0014 Acc: 83.8571\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 83.9920\n",
      "validation Loss: 0.0014 Acc: 83.8095\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 83.9355\n",
      "validation Loss: 0.0014 Acc: 83.9167\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 84.0129\n",
      "validation Loss: 0.0014 Acc: 83.7976\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 84.0754\n",
      "validation Loss: 0.0014 Acc: 83.8452\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 84.1914\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 84.1944\n",
      "validation Loss: 0.0014 Acc: 83.7976\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 84.0664\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 84.1736\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 84.2122\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 84.0783\n",
      "validation Loss: 0.0014 Acc: 83.8452\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 84.3581\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 84.2480\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 84.1974\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 84.4712\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 52/99\n",
      "training Loss: 0.0013 Acc: 84.3878\n",
      "validation Loss: 0.0014 Acc: 83.8095\n",
      "Epoch 53/99\n",
      "training Loss: 0.0013 Acc: 84.4801\n",
      "validation Loss: 0.0014 Acc: 83.7976\n",
      "Epoch 54/99\n",
      "training Loss: 0.0013 Acc: 84.2807\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Epoch 55/99\n",
      "training Loss: 0.0013 Acc: 84.3908\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 56/99\n",
      "training Loss: 0.0013 Acc: 84.4087\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Early stopped.\n",
      "Best val acc: 83.952381\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0098 Acc: 78.6263\n",
      "validation Loss: 0.0016 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 82.1171\n",
      "validation Loss: 0.0015 Acc: 82.5952\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.5278\n",
      "validation Loss: 0.0015 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 81.9862\n",
      "validation Loss: 0.0015 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 82.4385\n",
      "validation Loss: 0.0015 Acc: 82.8214\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 82.0189\n",
      "validation Loss: 0.0015 Acc: 83.2262\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.5993\n",
      "validation Loss: 0.0015 Acc: 83.5595\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 83.0337\n",
      "validation Loss: 0.0015 Acc: 83.4167\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 83.1617\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 83.2450\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 83.3016\n",
      "validation Loss: 0.0015 Acc: 83.3214\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.1111\n",
      "validation Loss: 0.0015 Acc: 83.1905\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.2510\n",
      "validation Loss: 0.0014 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.5426\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.3879\n",
      "validation Loss: 0.0015 Acc: 83.6429\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.1111\n",
      "validation Loss: 0.0015 Acc: 83.6429\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.6498\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.7510\n",
      "validation Loss: 0.0015 Acc: 83.7857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.7748\n",
      "validation Loss: 0.0014 Acc: 84.0238\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.8432\n",
      "validation Loss: 0.0014 Acc: 83.7976\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.8879\n",
      "validation Loss: 0.0014 Acc: 83.8929\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.9682\n",
      "validation Loss: 0.0015 Acc: 83.9643\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.7956\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.7718\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.8789\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.9623\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 84.0456\n",
      "validation Loss: 0.0014 Acc: 83.8929\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 84.1646\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 84.2063\n",
      "validation Loss: 0.0014 Acc: 83.9643\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 84.1289\n",
      "validation Loss: 0.0015 Acc: 83.8333\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 84.2331\n",
      "validation Loss: 0.0014 Acc: 83.9167\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 84.2777\n",
      "validation Loss: 0.0014 Acc: 83.9881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 84.4235\n",
      "validation Loss: 0.0014 Acc: 83.9524\n",
      "Epoch 33/99\n",
      "training Loss: 0.0013 Acc: 84.3105\n",
      "validation Loss: 0.0014 Acc: 83.8333\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 84.2093\n",
      "validation Loss: 0.0014 Acc: 83.9524\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 84.3253\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 84.3105\n",
      "validation Loss: 0.0014 Acc: 84.0119\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 84.3551\n",
      "validation Loss: 0.0014 Acc: 84.0238\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 84.4474\n",
      "validation Loss: 0.0014 Acc: 83.9286\n",
      "Early stopped.\n",
      "Best val acc: 84.023810\n",
      "----------\n",
      "Average best_acc across k-fold: 83.7674538942\n",
      "New configuration: {'learning_rate': 0.00012099063947513859, 'initial_nodes': 790, 'dropout': 0.13034417433085235, 'batch_size': 188, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0022 Acc: 80.9167\n",
      "validation Loss: 0.0020 Acc: 82.9564\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 83.1101\n",
      "validation Loss: 0.0019 Acc: 83.2302\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 83.2321\n",
      "validation Loss: 0.0019 Acc: 83.2064\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 83.4940\n",
      "validation Loss: 0.0019 Acc: 83.5634\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 83.7292\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.6786\n",
      "validation Loss: 0.0019 Acc: 83.4325\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.6429\n",
      "validation Loss: 0.0019 Acc: 83.3373\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.8690\n",
      "validation Loss: 0.0020 Acc: 83.0993\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.9077\n",
      "validation Loss: 0.0019 Acc: 83.5872\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.8631\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.8631\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 84.2024\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 84.0923\n",
      "validation Loss: 0.0019 Acc: 83.6110\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 84.2143\n",
      "validation Loss: 0.0019 Acc: 83.6825\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 84.1518\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 84.3214\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 84.3542\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 84.3601\n",
      "validation Loss: 0.0019 Acc: 83.3254\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 84.3423\n",
      "validation Loss: 0.0019 Acc: 83.4087\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 84.3958\n",
      "validation Loss: 0.0019 Acc: 83.2659\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 84.5923\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 84.7113\n",
      "validation Loss: 0.0019 Acc: 83.6825\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 84.6250\n",
      "validation Loss: 0.0019 Acc: 83.3254\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 84.7232\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 84.6399\n",
      "validation Loss: 0.0019 Acc: 83.3016\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 84.7946\n",
      "validation Loss: 0.0019 Acc: 83.1588\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 84.8839\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 84.8393\n",
      "validation Loss: 0.0019 Acc: 83.4325\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 85.0060\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 84.9583\n",
      "validation Loss: 0.0019 Acc: 83.5872\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 84.9613\n",
      "validation Loss: 0.0019 Acc: 83.7182\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 85.0536\n",
      "validation Loss: 0.0019 Acc: 83.6825\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 84.9554\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 85.0685\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 85.1131\n",
      "validation Loss: 0.0019 Acc: 83.5872\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 85.0149\n",
      "validation Loss: 0.0019 Acc: 83.4682\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 85.1875\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 85.2500\n",
      "validation Loss: 0.0019 Acc: 83.4206\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 85.2560\n",
      "validation Loss: 0.0019 Acc: 83.6468\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 85.2083\n",
      "validation Loss: 0.0019 Acc: 83.6110\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 85.1280\n",
      "validation Loss: 0.0019 Acc: 83.6229\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 85.1280\n",
      "validation Loss: 0.0019 Acc: 83.5872\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 85.2202\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 85.1994\n",
      "validation Loss: 0.0019 Acc: 83.4563\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 85.2946\n",
      "validation Loss: 0.0019 Acc: 83.6229\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 85.1964\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 85.3274\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 85.2321\n",
      "validation Loss: 0.0019 Acc: 83.5039\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 85.2292\n",
      "validation Loss: 0.0019 Acc: 83.6468\n",
      "Epoch 49/99\n",
      "training Loss: 0.0017 Acc: 85.1994\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Epoch 50/99\n",
      "training Loss: 0.0017 Acc: 85.1905\n",
      "validation Loss: 0.0019 Acc: 83.6229\n",
      "Early stopped.\n",
      "Best val acc: 83.718162\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0022 Acc: 81.0398\n",
      "validation Loss: 0.0020 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 83.1409\n",
      "validation Loss: 0.0020 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 83.3849\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 83.5307\n",
      "validation Loss: 0.0020 Acc: 83.2738\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 83.4891\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.8105\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.8045\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.8224\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.7956\n",
      "validation Loss: 0.0019 Acc: 84.0119\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.9742\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.9682\n",
      "validation Loss: 0.0019 Acc: 83.9524\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 84.0456\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 84.0545\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 84.1527\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 84.2480\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 84.1468\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 84.2271\n",
      "validation Loss: 0.0019 Acc: 84.2619\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 84.2628\n",
      "validation Loss: 0.0019 Acc: 84.0952\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 84.2747\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 84.3968\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 84.5069\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 84.6706\n",
      "validation Loss: 0.0019 Acc: 84.1429\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 84.7033\n",
      "validation Loss: 0.0019 Acc: 83.9881\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 84.7271\n",
      "validation Loss: 0.0019 Acc: 84.0357\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 84.5962\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 84.7152\n",
      "validation Loss: 0.0019 Acc: 83.9762\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 84.7182\n",
      "validation Loss: 0.0019 Acc: 84.1190\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 84.7331\n",
      "validation Loss: 0.0019 Acc: 84.2024\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 84.7479\n",
      "validation Loss: 0.0019 Acc: 84.3571\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 84.9830\n",
      "validation Loss: 0.0019 Acc: 84.2262\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 84.9443\n",
      "validation Loss: 0.0019 Acc: 84.2262\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 84.9741\n",
      "validation Loss: 0.0019 Acc: 84.3214\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 85.1408\n",
      "validation Loss: 0.0019 Acc: 84.2262\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 84.9949\n",
      "validation Loss: 0.0019 Acc: 84.2143\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 85.1765\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 85.1140\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 85.1943\n",
      "validation Loss: 0.0019 Acc: 84.2857\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 85.1259\n",
      "validation Loss: 0.0019 Acc: 84.1667\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 85.0455\n",
      "validation Loss: 0.0019 Acc: 84.2262\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 85.2539\n",
      "validation Loss: 0.0019 Acc: 84.0476\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 85.1973\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 85.2360\n",
      "validation Loss: 0.0019 Acc: 84.1905\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 85.0842\n",
      "validation Loss: 0.0019 Acc: 84.1667\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 85.1735\n",
      "validation Loss: 0.0019 Acc: 84.2024\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 85.1318\n",
      "validation Loss: 0.0019 Acc: 84.1190\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 85.2390\n",
      "validation Loss: 0.0019 Acc: 84.2024\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 85.4027\n",
      "validation Loss: 0.0019 Acc: 84.0714\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 85.1795\n",
      "validation Loss: 0.0019 Acc: 84.1429\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 85.3431\n",
      "validation Loss: 0.0019 Acc: 84.1548\n",
      "Early stopped.\n",
      "Best val acc: 84.357143\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0022 Acc: 81.3315\n",
      "validation Loss: 0.0020 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 83.2748\n",
      "validation Loss: 0.0020 Acc: 82.9405\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 83.4623\n",
      "validation Loss: 0.0020 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 83.5159\n",
      "validation Loss: 0.0020 Acc: 83.0595\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 83.7867\n",
      "validation Loss: 0.0020 Acc: 82.9286\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.8492\n",
      "validation Loss: 0.0020 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.8998\n",
      "validation Loss: 0.0020 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 84.0992\n",
      "validation Loss: 0.0020 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 84.0932\n",
      "validation Loss: 0.0020 Acc: 82.9405\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 84.1021\n",
      "validation Loss: 0.0020 Acc: 82.9881\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.9950\n",
      "validation Loss: 0.0020 Acc: 83.2381\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 84.2093\n",
      "validation Loss: 0.0020 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0018 Acc: 84.2331\n",
      "validation Loss: 0.0020 Acc: 83.2262\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 84.2569\n",
      "validation Loss: 0.0020 Acc: 83.0714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 84.4831\n",
      "validation Loss: 0.0020 Acc: 83.2857\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 84.4652\n",
      "validation Loss: 0.0020 Acc: 83.1429\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 84.5634\n",
      "validation Loss: 0.0020 Acc: 83.3214\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 84.5307\n",
      "validation Loss: 0.0020 Acc: 83.4167\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 84.5337\n",
      "validation Loss: 0.0020 Acc: 83.0714\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 84.7598\n",
      "validation Loss: 0.0020 Acc: 83.1310\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 84.7866\n",
      "validation Loss: 0.0020 Acc: 83.2619\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 84.8640\n",
      "validation Loss: 0.0020 Acc: 83.2619\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 84.8015\n",
      "validation Loss: 0.0020 Acc: 83.3214\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 85.0068\n",
      "validation Loss: 0.0020 Acc: 83.1429\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 84.9979\n",
      "validation Loss: 0.0020 Acc: 83.3929\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 85.0158\n",
      "validation Loss: 0.0020 Acc: 83.2976\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 85.0039\n",
      "validation Loss: 0.0020 Acc: 83.4048\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 85.1437\n",
      "validation Loss: 0.0020 Acc: 83.2976\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 85.2003\n",
      "validation Loss: 0.0020 Acc: 83.3452\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 85.2330\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 85.2241\n",
      "validation Loss: 0.0020 Acc: 83.2381\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 85.2509\n",
      "validation Loss: 0.0020 Acc: 83.3929\n",
      "Early stopped.\n",
      "Best val acc: 83.500000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0022 Acc: 81.4951\n",
      "validation Loss: 0.0020 Acc: 82.5833\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 83.3433\n",
      "validation Loss: 0.0020 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 83.4355\n",
      "validation Loss: 0.0020 Acc: 82.5952\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 83.6081\n",
      "validation Loss: 0.0020 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 83.6885\n",
      "validation Loss: 0.0020 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.8611\n",
      "validation Loss: 0.0020 Acc: 83.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.8492\n",
      "validation Loss: 0.0020 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.8462\n",
      "validation Loss: 0.0020 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.9920\n",
      "validation Loss: 0.0020 Acc: 83.0952\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 84.0634\n",
      "validation Loss: 0.0020 Acc: 82.8690\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 84.1557\n",
      "validation Loss: 0.0020 Acc: 83.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 84.2242\n",
      "validation Loss: 0.0020 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 84.2361\n",
      "validation Loss: 0.0020 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 84.2718\n",
      "validation Loss: 0.0020 Acc: 83.0833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 84.1587\n",
      "validation Loss: 0.0020 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 84.3105\n",
      "validation Loss: 0.0020 Acc: 83.0952\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 84.3581\n",
      "validation Loss: 0.0020 Acc: 83.0595\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 84.4593\n",
      "validation Loss: 0.0020 Acc: 83.0833\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 84.5277\n",
      "validation Loss: 0.0020 Acc: 83.4048\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 84.5723\n",
      "validation Loss: 0.0020 Acc: 83.4643\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 84.7509\n",
      "validation Loss: 0.0020 Acc: 83.4048\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 84.6081\n",
      "validation Loss: 0.0020 Acc: 83.0595\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 84.8015\n",
      "validation Loss: 0.0020 Acc: 83.4762\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 84.8342\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 84.7569\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 84.8640\n",
      "validation Loss: 0.0020 Acc: 83.2143\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 84.9682\n",
      "validation Loss: 0.0020 Acc: 83.4048\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 84.8253\n",
      "validation Loss: 0.0020 Acc: 83.1310\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 84.8848\n",
      "validation Loss: 0.0020 Acc: 83.4524\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 84.9741\n",
      "validation Loss: 0.0020 Acc: 83.1071\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 85.0872\n",
      "validation Loss: 0.0020 Acc: 83.1786\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 85.0812\n",
      "validation Loss: 0.0020 Acc: 83.2262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 85.0812\n",
      "validation Loss: 0.0020 Acc: 83.3571\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 85.1973\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 85.1914\n",
      "validation Loss: 0.0020 Acc: 83.3452\n",
      "Early stopped.\n",
      "Best val acc: 83.559524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0022 Acc: 81.3404\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 82.9564\n",
      "validation Loss: 0.0019 Acc: 83.7619\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 83.1320\n",
      "validation Loss: 0.0019 Acc: 84.2857\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 83.4057\n",
      "validation Loss: 0.0019 Acc: 84.2143\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 83.4861\n",
      "validation Loss: 0.0019 Acc: 84.1190\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.6974\n",
      "validation Loss: 0.0019 Acc: 84.1310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.6409\n",
      "validation Loss: 0.0019 Acc: 84.0952\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.7063\n",
      "validation Loss: 0.0019 Acc: 84.1190\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.7510\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.9414\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 84.0010\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.8462\n",
      "validation Loss: 0.0019 Acc: 84.1786\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 84.1289\n",
      "validation Loss: 0.0019 Acc: 84.0238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 84.0813\n",
      "validation Loss: 0.0019 Acc: 84.1786\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 84.1408\n",
      "validation Loss: 0.0019 Acc: 84.0119\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 84.2390\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 84.2718\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 84.2986\n",
      "validation Loss: 0.0019 Acc: 84.1667\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 84.4384\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 84.4503\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 84.3640\n",
      "validation Loss: 0.0018 Acc: 84.1667\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 84.4682\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 84.4325\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Early stopped.\n",
      "Best val acc: 84.285714\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8841086589\n",
      "New configuration: {'learning_rate': 0.0008559431203360228, 'initial_nodes': 378, 'dropout': 0.8133126432211704, 'batch_size': 248, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0019 Acc: 78.2976\n",
      "validation Loss: 0.0015 Acc: 83.2659\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0017 Acc: 82.3274\n",
      "validation Loss: 0.0015 Acc: 83.5753\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.8244\n",
      "validation Loss: 0.0015 Acc: 83.8134\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 82.8542\n",
      "validation Loss: 0.0015 Acc: 84.0157\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 82.9792\n",
      "validation Loss: 0.0015 Acc: 83.9919\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 83.0804\n",
      "validation Loss: 0.0015 Acc: 83.8253\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 83.1607\n",
      "validation Loss: 0.0015 Acc: 83.8134\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 83.2262\n",
      "validation Loss: 0.0014 Acc: 83.9086\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 83.3780\n",
      "validation Loss: 0.0015 Acc: 83.6825\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 83.2589\n",
      "validation Loss: 0.0015 Acc: 83.8372\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 83.3393\n",
      "validation Loss: 0.0014 Acc: 83.9562\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.2917\n",
      "validation Loss: 0.0014 Acc: 84.0157\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.4167\n",
      "validation Loss: 0.0015 Acc: 84.1466\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.4077\n",
      "validation Loss: 0.0014 Acc: 84.0752\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.5655\n",
      "validation Loss: 0.0014 Acc: 84.1109\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.5238\n",
      "validation Loss: 0.0014 Acc: 84.0990\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.4524\n",
      "validation Loss: 0.0014 Acc: 84.0276\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.7470\n",
      "validation Loss: 0.0014 Acc: 84.0752\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 83.7649\n",
      "validation Loss: 0.0014 Acc: 84.0157\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 83.6131\n",
      "validation Loss: 0.0014 Acc: 83.9800\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 83.6369\n",
      "validation Loss: 0.0014 Acc: 84.1228\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 83.6905\n",
      "validation Loss: 0.0014 Acc: 84.2418\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 83.6518\n",
      "validation Loss: 0.0014 Acc: 84.1109\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 83.6964\n",
      "validation Loss: 0.0014 Acc: 84.0633\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 83.7768\n",
      "validation Loss: 0.0014 Acc: 84.1585\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 83.8423\n",
      "validation Loss: 0.0014 Acc: 84.1942\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 83.8929\n",
      "validation Loss: 0.0014 Acc: 84.2299\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 84.0655\n",
      "validation Loss: 0.0014 Acc: 84.2180\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 83.9018\n",
      "validation Loss: 0.0014 Acc: 84.1942\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 83.8988\n",
      "validation Loss: 0.0014 Acc: 84.2418\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 83.8512\n",
      "validation Loss: 0.0014 Acc: 84.2895\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0015 Acc: 83.9970\n",
      "validation Loss: 0.0014 Acc: 84.1704\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 83.9970\n",
      "validation Loss: 0.0014 Acc: 84.2657\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 83.8095\n",
      "validation Loss: 0.0014 Acc: 84.2299\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 84.0268\n",
      "validation Loss: 0.0014 Acc: 84.2061\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 84.0863\n",
      "validation Loss: 0.0014 Acc: 84.2061\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 83.8869\n",
      "validation Loss: 0.0014 Acc: 84.2537\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 83.9286\n",
      "validation Loss: 0.0014 Acc: 84.2537\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 83.8780\n",
      "validation Loss: 0.0014 Acc: 84.3252\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0015 Acc: 84.0833\n",
      "validation Loss: 0.0014 Acc: 84.1347\n",
      "Epoch 40/99\n",
      "training Loss: 0.0015 Acc: 83.9762\n",
      "validation Loss: 0.0014 Acc: 84.2180\n",
      "Epoch 41/99\n",
      "training Loss: 0.0015 Acc: 84.0804\n",
      "validation Loss: 0.0014 Acc: 84.1942\n",
      "Epoch 42/99\n",
      "training Loss: 0.0015 Acc: 84.0982\n",
      "validation Loss: 0.0014 Acc: 84.2061\n",
      "Epoch 43/99\n",
      "training Loss: 0.0015 Acc: 83.9018\n",
      "validation Loss: 0.0014 Acc: 84.2776\n",
      "Epoch 44/99\n",
      "training Loss: 0.0015 Acc: 84.1726\n",
      "validation Loss: 0.0014 Acc: 84.2061\n",
      "Epoch 45/99\n",
      "training Loss: 0.0015 Acc: 84.1875\n",
      "validation Loss: 0.0014 Acc: 84.2776\n",
      "Epoch 46/99\n",
      "training Loss: 0.0015 Acc: 84.0417\n",
      "validation Loss: 0.0014 Acc: 84.1823\n",
      "Epoch 47/99\n",
      "training Loss: 0.0015 Acc: 84.1518\n",
      "validation Loss: 0.0014 Acc: 84.1585\n",
      "Epoch 48/99\n",
      "training Loss: 0.0015 Acc: 84.1815\n",
      "validation Loss: 0.0014 Acc: 84.2418\n",
      "Epoch 49/99\n",
      "training Loss: 0.0015 Acc: 84.3333\n",
      "validation Loss: 0.0014 Acc: 84.3014\n",
      "Epoch 50/99\n",
      "training Loss: 0.0015 Acc: 84.1429\n",
      "validation Loss: 0.0014 Acc: 84.1704\n",
      "Epoch 51/99\n",
      "training Loss: 0.0015 Acc: 84.2321\n",
      "validation Loss: 0.0014 Acc: 84.2061\n",
      "Epoch 52/99\n",
      "training Loss: 0.0015 Acc: 84.3125\n",
      "validation Loss: 0.0014 Acc: 84.2180\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 84.2292\n",
      "validation Loss: 0.0014 Acc: 84.2418\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 84.2560\n",
      "validation Loss: 0.0014 Acc: 84.2180\n",
      "Epoch 55/99\n",
      "training Loss: 0.0015 Acc: 84.2083\n",
      "validation Loss: 0.0014 Acc: 84.3133\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 84.0893\n",
      "validation Loss: 0.0014 Acc: 84.3133\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 84.3065\n",
      "validation Loss: 0.0014 Acc: 84.3252\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 84.3482\n",
      "validation Loss: 0.0014 Acc: 84.2776\n",
      "Early stopped.\n",
      "Best val acc: 84.325161\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0019 Acc: 78.3406\n",
      "validation Loss: 0.0016 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 82.6350\n",
      "validation Loss: 0.0015 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.9593\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 83.2391\n",
      "validation Loss: 0.0015 Acc: 82.3095\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 83.5278\n",
      "validation Loss: 0.0015 Acc: 82.5000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 83.3938\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 83.3135\n",
      "validation Loss: 0.0015 Acc: 82.4167\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 83.5248\n",
      "validation Loss: 0.0015 Acc: 82.3571\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 83.4801\n",
      "validation Loss: 0.0015 Acc: 82.2738\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 83.5665\n",
      "validation Loss: 0.0015 Acc: 82.5595\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 83.6230\n",
      "validation Loss: 0.0015 Acc: 82.3810\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.8462\n",
      "validation Loss: 0.0015 Acc: 82.5476\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.6260\n",
      "validation Loss: 0.0015 Acc: 82.4167\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.7837\n",
      "validation Loss: 0.0015 Acc: 82.4643\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.7272\n",
      "validation Loss: 0.0015 Acc: 82.5714\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.5516\n",
      "validation Loss: 0.0015 Acc: 82.3214\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.7718\n",
      "validation Loss: 0.0015 Acc: 82.5952\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.7420\n",
      "validation Loss: 0.0015 Acc: 82.4405\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 83.8373\n",
      "validation Loss: 0.0015 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 83.8730\n",
      "validation Loss: 0.0015 Acc: 82.7500\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 83.9563\n",
      "validation Loss: 0.0015 Acc: 82.6071\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 84.0575\n",
      "validation Loss: 0.0015 Acc: 82.5952\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 84.0337\n",
      "validation Loss: 0.0015 Acc: 82.5714\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 84.0486\n",
      "validation Loss: 0.0015 Acc: 82.6786\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 83.9117\n",
      "validation Loss: 0.0015 Acc: 82.4762\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 84.0664\n",
      "validation Loss: 0.0015 Acc: 82.7976\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 84.0396\n",
      "validation Loss: 0.0015 Acc: 82.5357\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 84.0843\n",
      "validation Loss: 0.0015 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 84.1438\n",
      "validation Loss: 0.0015 Acc: 82.6429\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 84.0962\n",
      "validation Loss: 0.0015 Acc: 82.5476\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 84.1051\n",
      "validation Loss: 0.0015 Acc: 82.6548\n",
      "Epoch 31/99\n",
      "training Loss: 0.0015 Acc: 84.3045\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 84.2033\n",
      "validation Loss: 0.0015 Acc: 82.7500\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 84.3015\n",
      "validation Loss: 0.0015 Acc: 82.7262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 84.4652\n",
      "validation Loss: 0.0015 Acc: 82.6429\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 84.3849\n",
      "validation Loss: 0.0015 Acc: 82.5119\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 84.4206\n",
      "validation Loss: 0.0015 Acc: 82.7262\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 84.4355\n",
      "validation Loss: 0.0015 Acc: 82.6905\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 84.4176\n",
      "validation Loss: 0.0015 Acc: 82.7024\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 84.5962\n",
      "validation Loss: 0.0015 Acc: 82.6548\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 84.4682\n",
      "validation Loss: 0.0015 Acc: 82.7857\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 84.6289\n",
      "validation Loss: 0.0015 Acc: 82.5595\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 84.5515\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 84.5783\n",
      "validation Loss: 0.0015 Acc: 82.6548\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 84.5485\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 84.6110\n",
      "validation Loss: 0.0015 Acc: 82.6429\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 84.4860\n",
      "validation Loss: 0.0015 Acc: 82.6310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 84.6587\n",
      "validation Loss: 0.0015 Acc: 82.6786\n",
      "Early stopped.\n",
      "Best val acc: 82.809524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0019 Acc: 77.9775\n",
      "validation Loss: 0.0015 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 82.4028\n",
      "validation Loss: 0.0015 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.5963\n",
      "validation Loss: 0.0015 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 82.9326\n",
      "validation Loss: 0.0015 Acc: 83.1310\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 83.2183\n",
      "validation Loss: 0.0015 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 83.2689\n",
      "validation Loss: 0.0015 Acc: 83.1548\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 83.4385\n",
      "validation Loss: 0.0015 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 83.2927\n",
      "validation Loss: 0.0015 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 83.2837\n",
      "validation Loss: 0.0015 Acc: 83.3452\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 83.4296\n",
      "validation Loss: 0.0015 Acc: 83.1548\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 83.2153\n",
      "validation Loss: 0.0015 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.4921\n",
      "validation Loss: 0.0015 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.4117\n",
      "validation Loss: 0.0015 Acc: 83.5000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.6081\n",
      "validation Loss: 0.0015 Acc: 83.4881\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.5188\n",
      "validation Loss: 0.0015 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.6170\n",
      "validation Loss: 0.0015 Acc: 83.4048\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.6706\n",
      "validation Loss: 0.0015 Acc: 83.4643\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.6290\n",
      "validation Loss: 0.0015 Acc: 83.4286\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 83.6111\n",
      "validation Loss: 0.0015 Acc: 83.4881\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 83.4861\n",
      "validation Loss: 0.0015 Acc: 83.5714\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 83.6914\n",
      "validation Loss: 0.0015 Acc: 83.5833\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 83.5992\n",
      "validation Loss: 0.0015 Acc: 83.3810\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 83.5962\n",
      "validation Loss: 0.0015 Acc: 83.5357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 83.7123\n",
      "validation Loss: 0.0015 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 83.6022\n",
      "validation Loss: 0.0015 Acc: 83.5714\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 83.7153\n",
      "validation Loss: 0.0015 Acc: 83.5714\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 83.7599\n",
      "validation Loss: 0.0015 Acc: 83.3929\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 83.8313\n",
      "validation Loss: 0.0015 Acc: 83.5595\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 83.8135\n",
      "validation Loss: 0.0015 Acc: 83.4881\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 83.7867\n",
      "validation Loss: 0.0015 Acc: 83.5357\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 83.8938\n",
      "validation Loss: 0.0015 Acc: 83.4405\n",
      "Epoch 31/99\n",
      "training Loss: 0.0015 Acc: 83.9027\n",
      "validation Loss: 0.0015 Acc: 83.3333\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 83.8283\n",
      "validation Loss: 0.0015 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 83.8194\n",
      "validation Loss: 0.0015 Acc: 83.6071\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 84.0367\n",
      "validation Loss: 0.0015 Acc: 83.3452\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 83.8045\n",
      "validation Loss: 0.0015 Acc: 83.6548\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 83.8373\n",
      "validation Loss: 0.0015 Acc: 83.6071\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 84.0039\n",
      "validation Loss: 0.0015 Acc: 83.6190\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 83.9087\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 39/99\n",
      "training Loss: 0.0015 Acc: 84.0575\n",
      "validation Loss: 0.0014 Acc: 83.8214\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0015 Acc: 84.0426\n",
      "validation Loss: 0.0015 Acc: 83.6905\n",
      "Epoch 41/99\n",
      "training Loss: 0.0015 Acc: 83.8938\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 42/99\n",
      "training Loss: 0.0015 Acc: 83.9146\n",
      "validation Loss: 0.0015 Acc: 83.6667\n",
      "Epoch 43/99\n",
      "training Loss: 0.0015 Acc: 83.9980\n",
      "validation Loss: 0.0015 Acc: 83.6548\n",
      "Epoch 44/99\n",
      "training Loss: 0.0015 Acc: 84.0277\n",
      "validation Loss: 0.0015 Acc: 83.4881\n",
      "Epoch 45/99\n",
      "training Loss: 0.0015 Acc: 84.0307\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 46/99\n",
      "training Loss: 0.0015 Acc: 84.1706\n",
      "validation Loss: 0.0014 Acc: 83.7738\n",
      "Epoch 47/99\n",
      "training Loss: 0.0015 Acc: 84.2301\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 84.2182\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 84.2480\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 84.1795\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 84.4087\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 84.4682\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 84.4325\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 84.2628\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 84.4474\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 84.6051\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 84.4533\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 84.4979\n",
      "validation Loss: 0.0014 Acc: 83.7976\n",
      "Epoch 59/99\n",
      "training Loss: 0.0014 Acc: 84.4503\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Early stopped.\n",
      "Best val acc: 83.821429\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0019 Acc: 78.1501\n",
      "validation Loss: 0.0015 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0017 Acc: 82.3522\n",
      "validation Loss: 0.0015 Acc: 83.4524\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.5665\n",
      "validation Loss: 0.0015 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 82.8016\n",
      "validation Loss: 0.0014 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 83.0308\n",
      "validation Loss: 0.0014 Acc: 83.8214\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 83.2034\n",
      "validation Loss: 0.0014 Acc: 83.7381\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 83.2361\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 83.0695\n",
      "validation Loss: 0.0014 Acc: 84.0238\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 83.1320\n",
      "validation Loss: 0.0014 Acc: 84.0476\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 83.1885\n",
      "validation Loss: 0.0014 Acc: 83.9881\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 82.9980\n",
      "validation Loss: 0.0014 Acc: 83.9762\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.2331\n",
      "validation Loss: 0.0014 Acc: 83.9643\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.2897\n",
      "validation Loss: 0.0014 Acc: 84.0238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.3194\n",
      "validation Loss: 0.0014 Acc: 84.2500\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.3968\n",
      "validation Loss: 0.0014 Acc: 84.0952\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.3552\n",
      "validation Loss: 0.0014 Acc: 84.1071\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.3492\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.4296\n",
      "validation Loss: 0.0014 Acc: 84.1905\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 83.4444\n",
      "validation Loss: 0.0014 Acc: 84.1071\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 83.3998\n",
      "validation Loss: 0.0014 Acc: 84.1310\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 83.5307\n",
      "validation Loss: 0.0014 Acc: 84.1667\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 83.4504\n",
      "validation Loss: 0.0014 Acc: 83.9762\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 83.5694\n",
      "validation Loss: 0.0014 Acc: 84.0595\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 83.4891\n",
      "validation Loss: 0.0014 Acc: 84.1310\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 83.4801\n",
      "validation Loss: 0.0014 Acc: 84.1429\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 83.5218\n",
      "validation Loss: 0.0014 Acc: 84.1786\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 83.4563\n",
      "validation Loss: 0.0014 Acc: 84.1310\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 83.5337\n",
      "validation Loss: 0.0014 Acc: 84.3333\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 83.6498\n",
      "validation Loss: 0.0014 Acc: 84.3571\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 83.7331\n",
      "validation Loss: 0.0014 Acc: 84.4048\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 83.6468\n",
      "validation Loss: 0.0014 Acc: 84.6429\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0015 Acc: 83.6795\n",
      "validation Loss: 0.0014 Acc: 84.1667\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 83.6914\n",
      "validation Loss: 0.0014 Acc: 84.2976\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 83.6736\n",
      "validation Loss: 0.0014 Acc: 84.2262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 83.8551\n",
      "validation Loss: 0.0014 Acc: 84.2262\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 83.7837\n",
      "validation Loss: 0.0014 Acc: 84.0714\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 83.8373\n",
      "validation Loss: 0.0014 Acc: 84.2738\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 83.9266\n",
      "validation Loss: 0.0014 Acc: 84.2024\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 83.9295\n",
      "validation Loss: 0.0014 Acc: 84.2262\n",
      "Epoch 39/99\n",
      "training Loss: 0.0015 Acc: 83.9771\n",
      "validation Loss: 0.0014 Acc: 84.2024\n",
      "Epoch 40/99\n",
      "training Loss: 0.0015 Acc: 83.9117\n",
      "validation Loss: 0.0014 Acc: 84.2976\n",
      "Epoch 41/99\n",
      "training Loss: 0.0015 Acc: 84.0337\n",
      "validation Loss: 0.0014 Acc: 84.3452\n",
      "Epoch 42/99\n",
      "training Loss: 0.0015 Acc: 84.0396\n",
      "validation Loss: 0.0014 Acc: 84.2500\n",
      "Epoch 43/99\n",
      "training Loss: 0.0015 Acc: 84.0486\n",
      "validation Loss: 0.0014 Acc: 84.1667\n",
      "Epoch 44/99\n",
      "training Loss: 0.0015 Acc: 84.1170\n",
      "validation Loss: 0.0014 Acc: 84.2143\n",
      "Epoch 45/99\n",
      "training Loss: 0.0015 Acc: 84.0664\n",
      "validation Loss: 0.0014 Acc: 84.2024\n",
      "Epoch 46/99\n",
      "training Loss: 0.0015 Acc: 84.1289\n",
      "validation Loss: 0.0014 Acc: 84.2857\n",
      "Epoch 47/99\n",
      "training Loss: 0.0015 Acc: 84.1557\n",
      "validation Loss: 0.0014 Acc: 84.2381\n",
      "Epoch 48/99\n",
      "training Loss: 0.0015 Acc: 84.1498\n",
      "validation Loss: 0.0014 Acc: 84.2024\n",
      "Epoch 49/99\n",
      "training Loss: 0.0015 Acc: 84.2956\n",
      "validation Loss: 0.0014 Acc: 84.1786\n",
      "Epoch 50/99\n",
      "training Loss: 0.0015 Acc: 84.1438\n",
      "validation Loss: 0.0014 Acc: 84.1786\n",
      "Early stopped.\n",
      "Best val acc: 84.642857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0019 Acc: 78.4299\n",
      "validation Loss: 0.0015 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 82.4653\n",
      "validation Loss: 0.0015 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 82.6617\n",
      "validation Loss: 0.0015 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 82.9832\n",
      "validation Loss: 0.0015 Acc: 83.6548\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 83.0308\n",
      "validation Loss: 0.0015 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 82.8671\n",
      "validation Loss: 0.0015 Acc: 83.6310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 83.1141\n",
      "validation Loss: 0.0015 Acc: 83.8690\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 83.2540\n",
      "validation Loss: 0.0015 Acc: 83.8095\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 83.1171\n",
      "validation Loss: 0.0015 Acc: 83.7500\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 83.2361\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 83.2361\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 83.3165\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 83.3373\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 83.4801\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 83.2897\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 83.4474\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 83.4177\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 83.5784\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 83.5724\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 83.4147\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 83.5337\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 83.6825\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 83.6111\n",
      "validation Loss: 0.0014 Acc: 83.7976\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 83.8045\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 83.6647\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Early stopped.\n",
      "Best val acc: 83.904762\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9007464209\n",
      "New configuration: {'learning_rate': 3.1361855667835625e-05, 'initial_nodes': 922, 'dropout': 0.8731300702946067, 'batch_size': 346, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 50.6935\n",
      "validation Loss: 0.0020 Acc: 59.3549\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 53.3839\n",
      "validation Loss: 0.0020 Acc: 71.3640\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 56.6905\n",
      "validation Loss: 0.0020 Acc: 78.3504\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 60.6548\n",
      "validation Loss: 0.0019 Acc: 78.6122\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 64.5923\n",
      "validation Loss: 0.0019 Acc: 79.1716\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 69.4345\n",
      "validation Loss: 0.0018 Acc: 79.9214\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0017 Acc: 72.9345\n",
      "validation Loss: 0.0017 Acc: 80.7427\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0017 Acc: 76.1726\n",
      "validation Loss: 0.0017 Acc: 81.1711\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0017 Acc: 78.1339\n",
      "validation Loss: 0.0016 Acc: 81.7067\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 79.4524\n",
      "validation Loss: 0.0016 Acc: 81.9805\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 80.5744\n",
      "validation Loss: 0.0016 Acc: 82.1352\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 80.7173\n",
      "validation Loss: 0.0015 Acc: 82.2661\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0016 Acc: 81.3274\n",
      "validation Loss: 0.0015 Acc: 82.1709\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 81.3393\n",
      "validation Loss: 0.0015 Acc: 82.0400\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 81.4970\n",
      "validation Loss: 0.0015 Acc: 82.2185\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 81.2917\n",
      "validation Loss: 0.0015 Acc: 82.2066\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 81.5952\n",
      "validation Loss: 0.0015 Acc: 82.1709\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 81.6012\n",
      "validation Loss: 0.0014 Acc: 82.1471\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 81.7649\n",
      "validation Loss: 0.0014 Acc: 82.1947\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 81.7649\n",
      "validation Loss: 0.0014 Acc: 82.2066\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 81.6935\n",
      "validation Loss: 0.0014 Acc: 82.2304\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 81.6250\n",
      "validation Loss: 0.0014 Acc: 82.1590\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 81.7768\n",
      "validation Loss: 0.0014 Acc: 82.3018\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 81.9464\n",
      "validation Loss: 0.0014 Acc: 82.2066\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 81.6577\n",
      "validation Loss: 0.0013 Acc: 82.1352\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 81.8810\n",
      "validation Loss: 0.0013 Acc: 82.1709\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 81.9256\n",
      "validation Loss: 0.0013 Acc: 82.3018\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 81.9048\n",
      "validation Loss: 0.0013 Acc: 82.3256\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 81.8304\n",
      "validation Loss: 0.0013 Acc: 82.3494\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 81.8750\n",
      "validation Loss: 0.0013 Acc: 82.5042\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 81.9494\n",
      "validation Loss: 0.0013 Acc: 82.4447\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 81.8720\n",
      "validation Loss: 0.0013 Acc: 82.7660\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 82.0298\n",
      "validation Loss: 0.0012 Acc: 82.7422\n",
      "Epoch 33/99\n",
      "training Loss: 0.0013 Acc: 82.0417\n",
      "validation Loss: 0.0012 Acc: 82.6589\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 82.0208\n",
      "validation Loss: 0.0012 Acc: 82.8493\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 81.9554\n",
      "validation Loss: 0.0012 Acc: 82.7779\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 82.1042\n",
      "validation Loss: 0.0012 Acc: 82.7660\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 81.9940\n",
      "validation Loss: 0.0012 Acc: 83.0159\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 81.9851\n",
      "validation Loss: 0.0012 Acc: 82.9683\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 82.2530\n",
      "validation Loss: 0.0012 Acc: 82.9564\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 82.2679\n",
      "validation Loss: 0.0012 Acc: 83.0040\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 82.0893\n",
      "validation Loss: 0.0012 Acc: 82.9921\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 82.2292\n",
      "validation Loss: 0.0012 Acc: 83.0517\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 82.1012\n",
      "validation Loss: 0.0012 Acc: 82.9683\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 82.2381\n",
      "validation Loss: 0.0012 Acc: 83.0040\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 82.0804\n",
      "validation Loss: 0.0012 Acc: 83.0398\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 82.2708\n",
      "validation Loss: 0.0012 Acc: 83.1469\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 82.0089\n",
      "validation Loss: 0.0012 Acc: 83.0040\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 82.2381\n",
      "validation Loss: 0.0012 Acc: 83.0398\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 82.0893\n",
      "validation Loss: 0.0012 Acc: 82.9683\n",
      "Epoch 50/99\n",
      "training Loss: 0.0012 Acc: 82.2202\n",
      "validation Loss: 0.0012 Acc: 83.0398\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 82.4464\n",
      "validation Loss: 0.0012 Acc: 83.0636\n",
      "Epoch 52/99\n",
      "training Loss: 0.0012 Acc: 82.4911\n",
      "validation Loss: 0.0012 Acc: 83.1112\n",
      "Epoch 53/99\n",
      "training Loss: 0.0012 Acc: 82.4554\n",
      "validation Loss: 0.0011 Acc: 83.1469\n",
      "Epoch 54/99\n",
      "training Loss: 0.0012 Acc: 82.5863\n",
      "validation Loss: 0.0012 Acc: 83.1350\n",
      "Epoch 55/99\n",
      "training Loss: 0.0012 Acc: 82.4702\n",
      "validation Loss: 0.0011 Acc: 83.0517\n",
      "Epoch 56/99\n",
      "training Loss: 0.0012 Acc: 82.4554\n",
      "validation Loss: 0.0011 Acc: 83.1231\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 82.5952\n",
      "validation Loss: 0.0011 Acc: 83.1588\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0012 Acc: 82.6339\n",
      "validation Loss: 0.0011 Acc: 83.0040\n",
      "Epoch 59/99\n",
      "training Loss: 0.0012 Acc: 82.6637\n",
      "validation Loss: 0.0011 Acc: 83.1231\n",
      "Epoch 60/99\n",
      "training Loss: 0.0012 Acc: 82.7351\n",
      "validation Loss: 0.0011 Acc: 83.1707\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0012 Acc: 82.6905\n",
      "validation Loss: 0.0011 Acc: 83.0517\n",
      "Epoch 62/99\n",
      "training Loss: 0.0012 Acc: 82.5744\n",
      "validation Loss: 0.0011 Acc: 83.1826\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0012 Acc: 82.6399\n",
      "validation Loss: 0.0011 Acc: 83.1231\n",
      "Epoch 64/99\n",
      "training Loss: 0.0012 Acc: 82.7619\n",
      "validation Loss: 0.0011 Acc: 83.0993\n",
      "Epoch 65/99\n",
      "training Loss: 0.0012 Acc: 82.5655\n",
      "validation Loss: 0.0011 Acc: 83.1707\n",
      "Epoch 66/99\n",
      "training Loss: 0.0012 Acc: 82.4137\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Saving..\n",
      "Epoch 67/99\n",
      "training Loss: 0.0012 Acc: 82.7232\n",
      "validation Loss: 0.0011 Acc: 83.2183\n",
      "Epoch 68/99\n",
      "training Loss: 0.0012 Acc: 82.6399\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 69/99\n",
      "training Loss: 0.0012 Acc: 82.7887\n",
      "validation Loss: 0.0011 Acc: 83.2778\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0012 Acc: 82.7083\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 71/99\n",
      "training Loss: 0.0012 Acc: 82.4702\n",
      "validation Loss: 0.0011 Acc: 83.1350\n",
      "Epoch 72/99\n",
      "training Loss: 0.0012 Acc: 82.6756\n",
      "validation Loss: 0.0011 Acc: 83.2183\n",
      "Epoch 73/99\n",
      "training Loss: 0.0012 Acc: 82.6369\n",
      "validation Loss: 0.0011 Acc: 83.3135\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0012 Acc: 82.7768\n",
      "validation Loss: 0.0011 Acc: 83.2302\n",
      "Epoch 75/99\n",
      "training Loss: 0.0012 Acc: 82.8720\n",
      "validation Loss: 0.0011 Acc: 83.1350\n",
      "Epoch 76/99\n",
      "training Loss: 0.0012 Acc: 82.5685\n",
      "validation Loss: 0.0011 Acc: 83.2183\n",
      "Epoch 77/99\n",
      "training Loss: 0.0012 Acc: 82.4583\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Epoch 78/99\n",
      "training Loss: 0.0012 Acc: 82.7262\n",
      "validation Loss: 0.0011 Acc: 83.2897\n",
      "Epoch 79/99\n",
      "training Loss: 0.0012 Acc: 82.7738\n",
      "validation Loss: 0.0011 Acc: 83.3016\n",
      "Epoch 80/99\n",
      "training Loss: 0.0012 Acc: 82.5625\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Epoch 81/99\n",
      "training Loss: 0.0012 Acc: 82.8482\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 82/99\n",
      "training Loss: 0.0012 Acc: 82.8571\n",
      "validation Loss: 0.0011 Acc: 83.3135\n",
      "Epoch 83/99\n",
      "training Loss: 0.0012 Acc: 82.7143\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 84/99\n",
      "training Loss: 0.0012 Acc: 82.8185\n",
      "validation Loss: 0.0011 Acc: 83.2778\n",
      "Epoch 85/99\n",
      "training Loss: 0.0012 Acc: 82.8810\n",
      "validation Loss: 0.0011 Acc: 83.2302\n",
      "Epoch 86/99\n",
      "training Loss: 0.0012 Acc: 82.9405\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 87/99\n",
      "training Loss: 0.0012 Acc: 82.9911\n",
      "validation Loss: 0.0011 Acc: 83.1588\n",
      "Epoch 88/99\n",
      "training Loss: 0.0012 Acc: 82.7649\n",
      "validation Loss: 0.0011 Acc: 83.1707\n",
      "Epoch 89/99\n",
      "training Loss: 0.0012 Acc: 82.6667\n",
      "validation Loss: 0.0011 Acc: 83.1826\n",
      "Epoch 90/99\n",
      "training Loss: 0.0012 Acc: 82.6280\n",
      "validation Loss: 0.0011 Acc: 83.1707\n",
      "Epoch 91/99\n",
      "training Loss: 0.0012 Acc: 82.8452\n",
      "validation Loss: 0.0011 Acc: 83.1588\n",
      "Epoch 92/99\n",
      "training Loss: 0.0012 Acc: 82.9226\n",
      "validation Loss: 0.0011 Acc: 83.1588\n",
      "Epoch 93/99\n",
      "training Loss: 0.0012 Acc: 82.7440\n",
      "validation Loss: 0.0011 Acc: 83.1707\n",
      "Early stopped.\n",
      "Best val acc: 83.313497\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 51.2202\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 53.1814\n",
      "validation Loss: 0.0020 Acc: 68.2381\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 55.6306\n",
      "validation Loss: 0.0020 Acc: 77.0476\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0019 Acc: 58.9459\n",
      "validation Loss: 0.0019 Acc: 78.7738\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 61.3565\n",
      "validation Loss: 0.0018 Acc: 79.6429\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 64.6985\n",
      "validation Loss: 0.0017 Acc: 80.2262\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 67.4811\n",
      "validation Loss: 0.0016 Acc: 80.3690\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0017 Acc: 69.9036\n",
      "validation Loss: 0.0015 Acc: 80.7857\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 72.5463\n",
      "validation Loss: 0.0014 Acc: 81.2024\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 74.3557\n",
      "validation Loss: 0.0013 Acc: 81.6786\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 75.6889\n",
      "validation Loss: 0.0013 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 76.4330\n",
      "validation Loss: 0.0012 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 77.8138\n",
      "validation Loss: 0.0012 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 78.6531\n",
      "validation Loss: 0.0012 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 79.0340\n",
      "validation Loss: 0.0012 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 79.4060\n",
      "validation Loss: 0.0012 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 79.9179\n",
      "validation Loss: 0.0011 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 80.2333\n",
      "validation Loss: 0.0011 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 80.3583\n",
      "validation Loss: 0.0011 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 80.6142\n",
      "validation Loss: 0.0011 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 81.0220\n",
      "validation Loss: 0.0011 Acc: 83.4524\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 81.0577\n",
      "validation Loss: 0.0011 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 81.2303\n",
      "validation Loss: 0.0011 Acc: 83.7619\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 81.4148\n",
      "validation Loss: 0.0011 Acc: 83.7619\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 81.7094\n",
      "validation Loss: 0.0011 Acc: 83.7738\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 81.6826\n",
      "validation Loss: 0.0011 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 81.7422\n",
      "validation Loss: 0.0011 Acc: 83.8810\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 81.7898\n",
      "validation Loss: 0.0011 Acc: 83.7857\n",
      "Epoch 28/99\n",
      "training Loss: 0.0012 Acc: 81.8493\n",
      "validation Loss: 0.0011 Acc: 83.9048\n",
      "Epoch 29/99\n",
      "training Loss: 0.0012 Acc: 81.7987\n",
      "validation Loss: 0.0011 Acc: 83.8452\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 81.9177\n",
      "validation Loss: 0.0011 Acc: 83.8690\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 81.9892\n",
      "validation Loss: 0.0011 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 81.9654\n",
      "validation Loss: 0.0011 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 82.2897\n",
      "validation Loss: 0.0011 Acc: 83.8929\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 82.3165\n",
      "validation Loss: 0.0011 Acc: 83.9524\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 82.2362\n",
      "validation Loss: 0.0011 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 82.2124\n",
      "validation Loss: 0.0011 Acc: 83.9643\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 82.2808\n",
      "validation Loss: 0.0011 Acc: 83.9762\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 82.3880\n",
      "validation Loss: 0.0011 Acc: 83.9048\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 82.2897\n",
      "validation Loss: 0.0011 Acc: 83.9762\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 82.4266\n",
      "validation Loss: 0.0011 Acc: 83.9881\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 82.3820\n",
      "validation Loss: 0.0011 Acc: 84.0476\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 82.5070\n",
      "validation Loss: 0.0011 Acc: 83.9048\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 82.5457\n",
      "validation Loss: 0.0011 Acc: 83.9762\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 82.6409\n",
      "validation Loss: 0.0011 Acc: 84.1310\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0012 Acc: 82.3701\n",
      "validation Loss: 0.0011 Acc: 84.0714\n",
      "Epoch 46/99\n",
      "training Loss: 0.0012 Acc: 82.5368\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0012 Acc: 82.5754\n",
      "validation Loss: 0.0011 Acc: 84.0714\n",
      "Epoch 48/99\n",
      "training Loss: 0.0012 Acc: 82.5278\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 49/99\n",
      "training Loss: 0.0012 Acc: 82.6737\n",
      "validation Loss: 0.0011 Acc: 84.0357\n",
      "Epoch 50/99\n",
      "training Loss: 0.0012 Acc: 82.5278\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Epoch 51/99\n",
      "training Loss: 0.0012 Acc: 82.7183\n",
      "validation Loss: 0.0011 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0012 Acc: 82.6052\n",
      "validation Loss: 0.0011 Acc: 84.0595\n",
      "Epoch 53/99\n",
      "training Loss: 0.0012 Acc: 82.6856\n",
      "validation Loss: 0.0011 Acc: 84.0833\n",
      "Epoch 54/99\n",
      "training Loss: 0.0012 Acc: 82.7600\n",
      "validation Loss: 0.0011 Acc: 84.1548\n",
      "Epoch 55/99\n",
      "training Loss: 0.0012 Acc: 82.9445\n",
      "validation Loss: 0.0011 Acc: 84.0476\n",
      "Epoch 56/99\n",
      "training Loss: 0.0012 Acc: 82.7213\n",
      "validation Loss: 0.0011 Acc: 84.0476\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 82.7213\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Epoch 58/99\n",
      "training Loss: 0.0012 Acc: 82.8135\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 59/99\n",
      "training Loss: 0.0012 Acc: 82.9474\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Epoch 60/99\n",
      "training Loss: 0.0012 Acc: 82.6856\n",
      "validation Loss: 0.0011 Acc: 84.1548\n",
      "Epoch 61/99\n",
      "training Loss: 0.0012 Acc: 82.9534\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 62/99\n",
      "training Loss: 0.0012 Acc: 83.0635\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 63/99\n",
      "training Loss: 0.0012 Acc: 82.8641\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 64/99\n",
      "training Loss: 0.0012 Acc: 82.7481\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 65/99\n",
      "training Loss: 0.0012 Acc: 82.7957\n",
      "validation Loss: 0.0011 Acc: 84.0952\n",
      "Epoch 66/99\n",
      "training Loss: 0.0012 Acc: 82.9653\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 67/99\n",
      "training Loss: 0.0012 Acc: 83.0099\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 68/99\n",
      "training Loss: 0.0012 Acc: 83.1230\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Epoch 69/99\n",
      "training Loss: 0.0012 Acc: 83.0099\n",
      "validation Loss: 0.0011 Acc: 84.1667\n",
      "Epoch 70/99\n",
      "training Loss: 0.0012 Acc: 83.0010\n",
      "validation Loss: 0.0011 Acc: 84.1310\n",
      "Epoch 71/99\n",
      "training Loss: 0.0012 Acc: 82.9802\n",
      "validation Loss: 0.0011 Acc: 84.2024\n",
      "Early stopped.\n",
      "Best val acc: 84.238095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 49.8393\n",
      "validation Loss: 0.0020 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 51.3154\n",
      "validation Loss: 0.0020 Acc: 50.5000\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 54.0623\n",
      "validation Loss: 0.0020 Acc: 60.1190\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 55.9342\n",
      "validation Loss: 0.0019 Acc: 71.7619\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 58.7614\n",
      "validation Loss: 0.0019 Acc: 78.4643\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 62.5141\n",
      "validation Loss: 0.0018 Acc: 80.0357\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 65.9217\n",
      "validation Loss: 0.0016 Acc: 80.3095\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0017 Acc: 68.6685\n",
      "validation Loss: 0.0015 Acc: 80.9048\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 71.3767\n",
      "validation Loss: 0.0014 Acc: 81.2143\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 73.9777\n",
      "validation Loss: 0.0013 Acc: 81.3690\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 74.9479\n",
      "validation Loss: 0.0013 Acc: 81.9643\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 76.4478\n",
      "validation Loss: 0.0013 Acc: 82.1071\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 77.5787\n",
      "validation Loss: 0.0012 Acc: 82.2857\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 78.5638\n",
      "validation Loss: 0.0012 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 79.0876\n",
      "validation Loss: 0.0012 Acc: 82.4286\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 79.5518\n",
      "validation Loss: 0.0012 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 79.7393\n",
      "validation Loss: 0.0012 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 80.5637\n",
      "validation Loss: 0.0012 Acc: 82.6310\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 80.5160\n",
      "validation Loss: 0.0011 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 81.0011\n",
      "validation Loss: 0.0011 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 81.1321\n",
      "validation Loss: 0.0011 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 81.0785\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 81.2928\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 81.5636\n",
      "validation Loss: 0.0011 Acc: 83.0238\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 81.6231\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 81.6856\n",
      "validation Loss: 0.0011 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0012 Acc: 81.9237\n",
      "validation Loss: 0.0011 Acc: 83.0238\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 81.8285\n",
      "validation Loss: 0.0011 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0012 Acc: 81.9237\n",
      "validation Loss: 0.0011 Acc: 83.0833\n",
      "Epoch 29/99\n",
      "training Loss: 0.0012 Acc: 81.9743\n",
      "validation Loss: 0.0011 Acc: 83.0238\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 82.1767\n",
      "validation Loss: 0.0011 Acc: 83.0952\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 81.9445\n",
      "validation Loss: 0.0011 Acc: 83.1190\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 81.8433\n",
      "validation Loss: 0.0011 Acc: 83.0833\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 82.1112\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 82.2451\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 82.3671\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 82.3790\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 82.4326\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 82.3106\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 82.2570\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 82.3612\n",
      "validation Loss: 0.0011 Acc: 83.2500\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 82.7094\n",
      "validation Loss: 0.0011 Acc: 83.2857\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 82.4981\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 82.3850\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 82.6588\n",
      "validation Loss: 0.0011 Acc: 83.2738\n",
      "Epoch 45/99\n",
      "training Loss: 0.0012 Acc: 82.7064\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 46/99\n",
      "training Loss: 0.0012 Acc: 82.8760\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 47/99\n",
      "training Loss: 0.0012 Acc: 82.7689\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 48/99\n",
      "training Loss: 0.0012 Acc: 82.6677\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 49/99\n",
      "training Loss: 0.0012 Acc: 82.6022\n",
      "validation Loss: 0.0011 Acc: 83.2262\n",
      "Epoch 50/99\n",
      "training Loss: 0.0012 Acc: 82.7064\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 51/99\n",
      "training Loss: 0.0012 Acc: 82.6677\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 52/99\n",
      "training Loss: 0.0012 Acc: 82.6677\n",
      "validation Loss: 0.0011 Acc: 83.2738\n",
      "Epoch 53/99\n",
      "training Loss: 0.0012 Acc: 82.7540\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 54/99\n",
      "training Loss: 0.0012 Acc: 82.8522\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 55/99\n",
      "training Loss: 0.0012 Acc: 82.9713\n",
      "validation Loss: 0.0011 Acc: 83.2857\n",
      "Epoch 56/99\n",
      "training Loss: 0.0012 Acc: 82.7153\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 82.7927\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Early stopped.\n",
      "Best val acc: 83.321429\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 50.3958\n",
      "validation Loss: 0.0020 Acc: 60.9643\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 52.1933\n",
      "validation Loss: 0.0020 Acc: 76.5119\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 54.2349\n",
      "validation Loss: 0.0020 Acc: 77.7381\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 56.4341\n",
      "validation Loss: 0.0019 Acc: 78.6667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 59.0977\n",
      "validation Loss: 0.0019 Acc: 79.4286\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 63.0468\n",
      "validation Loss: 0.0017 Acc: 80.0952\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 66.3681\n",
      "validation Loss: 0.0016 Acc: 80.5000\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0017 Acc: 69.6923\n",
      "validation Loss: 0.0015 Acc: 80.7024\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 72.6326\n",
      "validation Loss: 0.0014 Acc: 81.1548\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 74.9390\n",
      "validation Loss: 0.0013 Acc: 81.5357\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 76.1949\n",
      "validation Loss: 0.0013 Acc: 81.9643\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0014 Acc: 77.6859\n",
      "validation Loss: 0.0012 Acc: 82.0238\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 78.4418\n",
      "validation Loss: 0.0012 Acc: 82.2024\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 78.8852\n",
      "validation Loss: 0.0012 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 79.5905\n",
      "validation Loss: 0.0012 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 79.9506\n",
      "validation Loss: 0.0012 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 80.3553\n",
      "validation Loss: 0.0012 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 80.6648\n",
      "validation Loss: 0.0012 Acc: 82.6548\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 80.8642\n",
      "validation Loss: 0.0011 Acc: 82.7024\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 81.0279\n",
      "validation Loss: 0.0011 Acc: 82.7619\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 81.2243\n",
      "validation Loss: 0.0011 Acc: 82.6667\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 81.2749\n",
      "validation Loss: 0.0011 Acc: 82.6667\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 81.2958\n",
      "validation Loss: 0.0011 Acc: 82.7143\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 81.6737\n",
      "validation Loss: 0.0011 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 81.6112\n",
      "validation Loss: 0.0011 Acc: 82.7500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0012 Acc: 81.8374\n",
      "validation Loss: 0.0011 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0012 Acc: 82.0219\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0012 Acc: 82.0249\n",
      "validation Loss: 0.0011 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0012 Acc: 81.9088\n",
      "validation Loss: 0.0011 Acc: 82.9762\n",
      "Epoch 29/99\n",
      "training Loss: 0.0012 Acc: 82.0398\n",
      "validation Loss: 0.0011 Acc: 83.0119\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 82.2481\n",
      "validation Loss: 0.0011 Acc: 82.9881\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 82.2124\n",
      "validation Loss: 0.0011 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 82.3582\n",
      "validation Loss: 0.0011 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 82.1558\n",
      "validation Loss: 0.0011 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 82.2749\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 82.3463\n",
      "validation Loss: 0.0011 Acc: 83.1429\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 82.5933\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 82.4326\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 82.5397\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 82.4445\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 82.5129\n",
      "validation Loss: 0.0011 Acc: 83.0595\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 82.7302\n",
      "validation Loss: 0.0011 Acc: 83.1548\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 82.7719\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 82.7838\n",
      "validation Loss: 0.0011 Acc: 83.1786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 82.6112\n",
      "validation Loss: 0.0011 Acc: 83.1786\n",
      "Epoch 45/99\n",
      "training Loss: 0.0012 Acc: 82.5963\n",
      "validation Loss: 0.0011 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0012 Acc: 82.7600\n",
      "validation Loss: 0.0011 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0012 Acc: 82.7064\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 48/99\n",
      "training Loss: 0.0012 Acc: 82.7123\n",
      "validation Loss: 0.0011 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0012 Acc: 82.8463\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 50/99\n",
      "training Loss: 0.0012 Acc: 82.8939\n",
      "validation Loss: 0.0011 Acc: 83.1190\n",
      "Epoch 51/99\n",
      "training Loss: 0.0012 Acc: 82.8492\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Epoch 52/99\n",
      "training Loss: 0.0012 Acc: 82.7897\n",
      "validation Loss: 0.0011 Acc: 83.2738\n",
      "Epoch 53/99\n",
      "training Loss: 0.0012 Acc: 82.6201\n",
      "validation Loss: 0.0011 Acc: 83.2143\n",
      "Epoch 54/99\n",
      "training Loss: 0.0012 Acc: 82.9385\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 55/99\n",
      "training Loss: 0.0012 Acc: 82.8046\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 56/99\n",
      "training Loss: 0.0012 Acc: 83.0040\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 82.8730\n",
      "validation Loss: 0.0011 Acc: 83.2143\n",
      "Epoch 58/99\n",
      "training Loss: 0.0012 Acc: 82.9564\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 59/99\n",
      "training Loss: 0.0012 Acc: 82.9891\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 60/99\n",
      "training Loss: 0.0012 Acc: 82.8463\n",
      "validation Loss: 0.0011 Acc: 83.1786\n",
      "Epoch 61/99\n",
      "training Loss: 0.0012 Acc: 83.0337\n",
      "validation Loss: 0.0011 Acc: 83.3571\n",
      "Epoch 62/99\n",
      "training Loss: 0.0012 Acc: 82.9326\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 63/99\n",
      "training Loss: 0.0012 Acc: 82.9147\n",
      "validation Loss: 0.0011 Acc: 83.2857\n",
      "Epoch 64/99\n",
      "training Loss: 0.0012 Acc: 83.2004\n",
      "validation Loss: 0.0011 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0012 Acc: 83.0099\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 66/99\n",
      "training Loss: 0.0012 Acc: 82.9802\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 67/99\n",
      "training Loss: 0.0012 Acc: 82.9355\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 68/99\n",
      "training Loss: 0.0012 Acc: 82.8076\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 69/99\n",
      "training Loss: 0.0012 Acc: 83.1915\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 70/99\n",
      "training Loss: 0.0012 Acc: 82.9564\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 71/99\n",
      "training Loss: 0.0012 Acc: 82.8701\n",
      "validation Loss: 0.0011 Acc: 83.3452\n",
      "Epoch 72/99\n",
      "training Loss: 0.0012 Acc: 82.9236\n",
      "validation Loss: 0.0011 Acc: 83.3690\n",
      "Epoch 73/99\n",
      "training Loss: 0.0012 Acc: 83.1647\n",
      "validation Loss: 0.0011 Acc: 83.3571\n",
      "Epoch 74/99\n",
      "training Loss: 0.0012 Acc: 83.0070\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 75/99\n",
      "training Loss: 0.0012 Acc: 82.8284\n",
      "validation Loss: 0.0011 Acc: 83.3690\n",
      "Epoch 76/99\n",
      "training Loss: 0.0012 Acc: 83.1945\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 77/99\n",
      "training Loss: 0.0012 Acc: 83.1260\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 78/99\n",
      "training Loss: 0.0012 Acc: 82.9921\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 79/99\n",
      "training Loss: 0.0012 Acc: 83.1022\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 80/99\n",
      "training Loss: 0.0012 Acc: 82.9772\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 81/99\n",
      "training Loss: 0.0012 Acc: 82.9058\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 82/99\n",
      "training Loss: 0.0012 Acc: 83.0367\n",
      "validation Loss: 0.0011 Acc: 83.3452\n",
      "Epoch 83/99\n",
      "training Loss: 0.0012 Acc: 83.1647\n",
      "validation Loss: 0.0011 Acc: 83.3452\n",
      "Epoch 84/99\n",
      "training Loss: 0.0012 Acc: 83.0873\n",
      "validation Loss: 0.0011 Acc: 83.3571\n",
      "Early stopped.\n",
      "Best val acc: 83.428571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 50.7916\n",
      "validation Loss: 0.0020 Acc: 73.2381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 52.4582\n",
      "validation Loss: 0.0020 Acc: 77.0357\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 53.6605\n",
      "validation Loss: 0.0020 Acc: 78.0714\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 55.4044\n",
      "validation Loss: 0.0020 Acc: 78.9643\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0019 Acc: 58.0888\n",
      "validation Loss: 0.0019 Acc: 79.1429\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 60.9904\n",
      "validation Loss: 0.0018 Acc: 79.8095\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 65.0557\n",
      "validation Loss: 0.0017 Acc: 80.5000\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0017 Acc: 68.6626\n",
      "validation Loss: 0.0016 Acc: 80.7262\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 71.3678\n",
      "validation Loss: 0.0014 Acc: 81.1786\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 73.9599\n",
      "validation Loss: 0.0014 Acc: 81.5357\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 75.8050\n",
      "validation Loss: 0.0013 Acc: 81.9167\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 76.9389\n",
      "validation Loss: 0.0013 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 77.9894\n",
      "validation Loss: 0.0012 Acc: 82.2262\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 78.6977\n",
      "validation Loss: 0.0012 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 79.5935\n",
      "validation Loss: 0.0012 Acc: 82.6071\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 79.8226\n",
      "validation Loss: 0.0012 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 80.2363\n",
      "validation Loss: 0.0012 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 80.5547\n",
      "validation Loss: 0.0012 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 80.8255\n",
      "validation Loss: 0.0012 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 80.8910\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 81.1678\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 81.5338\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 81.6410\n",
      "validation Loss: 0.0011 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 81.6975\n",
      "validation Loss: 0.0011 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 81.6678\n",
      "validation Loss: 0.0011 Acc: 83.1429\n",
      "Epoch 25/99\n",
      "training Loss: 0.0012 Acc: 81.8999\n",
      "validation Loss: 0.0011 Acc: 83.0952\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 82.0130\n",
      "validation Loss: 0.0011 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0012 Acc: 82.0993\n",
      "validation Loss: 0.0011 Acc: 83.2262\n",
      "Epoch 28/99\n",
      "training Loss: 0.0012 Acc: 82.1082\n",
      "validation Loss: 0.0011 Acc: 83.1429\n",
      "Epoch 29/99\n",
      "training Loss: 0.0012 Acc: 82.1856\n",
      "validation Loss: 0.0011 Acc: 83.2143\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 82.2659\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 82.2064\n",
      "validation Loss: 0.0011 Acc: 83.2262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 82.2153\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 82.2034\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 82.2511\n",
      "validation Loss: 0.0011 Acc: 83.2143\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 82.3641\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 82.4028\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 82.3136\n",
      "validation Loss: 0.0011 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 82.3552\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 82.4534\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 82.4504\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 82.5100\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 82.5516\n",
      "validation Loss: 0.0011 Acc: 83.2262\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 82.6647\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 82.4088\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 45/99\n",
      "training Loss: 0.0012 Acc: 82.6141\n",
      "validation Loss: 0.0011 Acc: 83.1786\n",
      "Epoch 46/99\n",
      "training Loss: 0.0012 Acc: 82.6617\n",
      "validation Loss: 0.0011 Acc: 83.1786\n",
      "Epoch 47/99\n",
      "training Loss: 0.0012 Acc: 82.8046\n",
      "validation Loss: 0.0011 Acc: 83.2500\n",
      "Epoch 48/99\n",
      "training Loss: 0.0012 Acc: 82.6498\n",
      "validation Loss: 0.0011 Acc: 83.2262\n",
      "Epoch 49/99\n",
      "training Loss: 0.0012 Acc: 82.8701\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 50/99\n",
      "training Loss: 0.0012 Acc: 82.7481\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 51/99\n",
      "training Loss: 0.0012 Acc: 82.9951\n",
      "validation Loss: 0.0011 Acc: 83.0833\n",
      "Epoch 52/99\n",
      "training Loss: 0.0012 Acc: 82.7808\n",
      "validation Loss: 0.0011 Acc: 83.2857\n",
      "Epoch 53/99\n",
      "training Loss: 0.0012 Acc: 82.9385\n",
      "validation Loss: 0.0011 Acc: 83.1310\n",
      "Epoch 54/99\n",
      "training Loss: 0.0012 Acc: 82.8016\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Epoch 55/99\n",
      "training Loss: 0.0012 Acc: 82.8016\n",
      "validation Loss: 0.0011 Acc: 83.2143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0012 Acc: 82.8790\n",
      "validation Loss: 0.0011 Acc: 83.0714\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 82.8076\n",
      "validation Loss: 0.0011 Acc: 83.0952\n",
      "Early stopped.\n",
      "Best val acc: 83.369048\n",
      "----------\n",
      "Average best_acc across k-fold: 83.5341279287\n",
      "New configuration: {'learning_rate': 0.05668011282865983, 'initial_nodes': 487, 'dropout': 0.5509692951567664, 'batch_size': 188, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0060 Acc: 75.2381\n",
      "validation Loss: 0.0024 Acc: 81.9448\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0028 Acc: 77.3363\n",
      "validation Loss: 0.0025 Acc: 81.2188\n",
      "Epoch 2/99\n",
      "training Loss: 0.0028 Acc: 76.8988\n",
      "validation Loss: 0.0023 Acc: 81.5044\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 76.2589\n",
      "validation Loss: 0.0026 Acc: 81.1831\n",
      "Epoch 4/99\n",
      "training Loss: 0.0029 Acc: 74.4494\n",
      "validation Loss: 0.0026 Acc: 80.8141\n",
      "Epoch 5/99\n",
      "training Loss: 0.0029 Acc: 75.6994\n",
      "validation Loss: 0.0027 Acc: 78.3861\n",
      "Epoch 6/99\n",
      "training Loss: 0.0030 Acc: 74.7321\n",
      "validation Loss: 0.0027 Acc: 79.5644\n",
      "Epoch 7/99\n",
      "training Loss: 0.0032 Acc: 74.0982\n",
      "validation Loss: 0.0030 Acc: 77.8624\n",
      "Epoch 8/99\n",
      "training Loss: 0.0033 Acc: 66.7500\n",
      "validation Loss: 0.0030 Acc: 79.0883\n",
      "Epoch 9/99\n",
      "training Loss: 0.0031 Acc: 70.8512\n",
      "validation Loss: 0.0028 Acc: 78.8741\n",
      "Epoch 10/99\n",
      "training Loss: 0.0031 Acc: 71.3125\n",
      "validation Loss: 0.0028 Acc: 80.6237\n",
      "Epoch 11/99\n",
      "training Loss: 0.0031 Acc: 71.6905\n",
      "validation Loss: 0.0028 Acc: 79.4573\n",
      "Epoch 12/99\n",
      "training Loss: 0.0031 Acc: 73.2649\n",
      "validation Loss: 0.0028 Acc: 79.0169\n",
      "Epoch 13/99\n",
      "training Loss: 0.0030 Acc: 74.3750\n",
      "validation Loss: 0.0026 Acc: 82.2066\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0030 Acc: 73.3750\n",
      "validation Loss: 0.0027 Acc: 81.7067\n",
      "Epoch 15/99\n",
      "training Loss: 0.0030 Acc: 72.7798\n",
      "validation Loss: 0.0027 Acc: 80.8141\n",
      "Epoch 16/99\n",
      "training Loss: 0.0030 Acc: 74.0655\n",
      "validation Loss: 0.0027 Acc: 80.8855\n",
      "Epoch 17/99\n",
      "training Loss: 0.0030 Acc: 73.3036\n",
      "validation Loss: 0.0026 Acc: 80.8379\n",
      "Epoch 18/99\n",
      "training Loss: 0.0030 Acc: 73.7560\n",
      "validation Loss: 0.0026 Acc: 81.4687\n",
      "Epoch 19/99\n",
      "training Loss: 0.0030 Acc: 72.7470\n",
      "validation Loss: 0.0026 Acc: 80.9926\n",
      "Epoch 20/99\n",
      "training Loss: 0.0030 Acc: 72.6815\n",
      "validation Loss: 0.0026 Acc: 81.5758\n",
      "Epoch 21/99\n",
      "training Loss: 0.0030 Acc: 72.6875\n",
      "validation Loss: 0.0026 Acc: 81.9805\n",
      "Epoch 22/99\n",
      "training Loss: 0.0030 Acc: 72.6756\n",
      "validation Loss: 0.0026 Acc: 81.9686\n",
      "Epoch 23/99\n",
      "training Loss: 0.0030 Acc: 72.4524\n",
      "validation Loss: 0.0026 Acc: 81.8734\n",
      "Epoch 24/99\n",
      "training Loss: 0.0030 Acc: 72.6726\n",
      "validation Loss: 0.0026 Acc: 82.0995\n",
      "Epoch 25/99\n",
      "training Loss: 0.0029 Acc: 72.9762\n",
      "validation Loss: 0.0026 Acc: 81.6472\n",
      "Epoch 26/99\n",
      "training Loss: 0.0029 Acc: 73.6548\n",
      "validation Loss: 0.0026 Acc: 81.8734\n",
      "Epoch 27/99\n",
      "training Loss: 0.0029 Acc: 73.6161\n",
      "validation Loss: 0.0026 Acc: 81.8496\n",
      "Epoch 28/99\n",
      "training Loss: 0.0029 Acc: 72.9821\n",
      "validation Loss: 0.0026 Acc: 81.9805\n",
      "Epoch 29/99\n",
      "training Loss: 0.0029 Acc: 72.8810\n",
      "validation Loss: 0.0026 Acc: 82.0400\n",
      "Epoch 30/99\n",
      "training Loss: 0.0029 Acc: 73.0774\n",
      "validation Loss: 0.0026 Acc: 81.9448\n",
      "Epoch 31/99\n",
      "training Loss: 0.0029 Acc: 73.4583\n",
      "validation Loss: 0.0026 Acc: 81.9329\n",
      "Epoch 32/99\n",
      "training Loss: 0.0029 Acc: 73.0893\n",
      "validation Loss: 0.0026 Acc: 82.0400\n",
      "Epoch 33/99\n",
      "training Loss: 0.0029 Acc: 73.4970\n",
      "validation Loss: 0.0026 Acc: 81.8972\n",
      "Early stopped.\n",
      "Best val acc: 82.206617\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0073 Acc: 70.0583\n",
      "validation Loss: 0.0024 Acc: 79.1548\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0027 Acc: 74.6890\n",
      "validation Loss: 0.0022 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 75.9746\n",
      "validation Loss: 0.0027 Acc: 80.2738\n",
      "Epoch 3/99\n",
      "training Loss: 0.0030 Acc: 74.6265\n",
      "validation Loss: 0.0027 Acc: 82.0833\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0031 Acc: 74.5104\n",
      "validation Loss: 0.0028 Acc: 78.6071\n",
      "Epoch 5/99\n",
      "training Loss: 0.0030 Acc: 75.2098\n",
      "validation Loss: 0.0026 Acc: 81.8095\n",
      "Epoch 6/99\n",
      "training Loss: 0.0031 Acc: 75.8258\n",
      "validation Loss: 0.0027 Acc: 80.2619\n",
      "Epoch 7/99\n",
      "training Loss: 0.0031 Acc: 73.1266\n",
      "validation Loss: 0.0032 Acc: 72.4881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0032 Acc: 72.1534\n",
      "validation Loss: 0.0028 Acc: 78.8810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0030 Acc: 75.7574\n",
      "validation Loss: 0.0027 Acc: 80.9405\n",
      "Epoch 10/99\n",
      "training Loss: 0.0030 Acc: 75.0551\n",
      "validation Loss: 0.0027 Acc: 79.4167\n",
      "Epoch 11/99\n",
      "training Loss: 0.0030 Acc: 75.4628\n",
      "validation Loss: 0.0028 Acc: 79.3571\n",
      "Epoch 12/99\n",
      "training Loss: 0.0029 Acc: 76.4627\n",
      "validation Loss: 0.0027 Acc: 80.4048\n",
      "Epoch 13/99\n",
      "training Loss: 0.0029 Acc: 77.2156\n",
      "validation Loss: 0.0027 Acc: 78.7619\n",
      "Epoch 14/99\n",
      "training Loss: 0.0029 Acc: 76.3556\n",
      "validation Loss: 0.0026 Acc: 81.8810\n",
      "Epoch 15/99\n",
      "training Loss: 0.0029 Acc: 77.2811\n",
      "validation Loss: 0.0027 Acc: 80.6190\n",
      "Epoch 16/99\n",
      "training Loss: 0.0029 Acc: 76.5282\n",
      "validation Loss: 0.0027 Acc: 79.7619\n",
      "Epoch 17/99\n",
      "training Loss: 0.0029 Acc: 76.4746\n",
      "validation Loss: 0.0027 Acc: 78.7143\n",
      "Epoch 18/99\n",
      "training Loss: 0.0029 Acc: 76.1830\n",
      "validation Loss: 0.0026 Acc: 80.3690\n",
      "Epoch 19/99\n",
      "training Loss: 0.0029 Acc: 76.6919\n",
      "validation Loss: 0.0026 Acc: 81.3333\n",
      "Epoch 20/99\n",
      "training Loss: 0.0029 Acc: 77.2484\n",
      "validation Loss: 0.0026 Acc: 80.7857\n",
      "Epoch 21/99\n",
      "training Loss: 0.0029 Acc: 76.8466\n",
      "validation Loss: 0.0026 Acc: 80.9048\n",
      "Epoch 22/99\n",
      "training Loss: 0.0029 Acc: 76.8377\n",
      "validation Loss: 0.0027 Acc: 80.5476\n",
      "Epoch 23/99\n",
      "training Loss: 0.0029 Acc: 77.0490\n",
      "validation Loss: 0.0026 Acc: 81.0476\n",
      "Early stopped.\n",
      "Best val acc: 82.083333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0059 Acc: 71.2368\n",
      "validation Loss: 0.0025 Acc: 81.2024\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 70.7041\n",
      "validation Loss: 0.0024 Acc: 79.2381\n",
      "Epoch 2/99\n",
      "training Loss: 0.0031 Acc: 69.8173\n",
      "validation Loss: 0.0023 Acc: 81.3095\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0031 Acc: 68.2221\n",
      "validation Loss: 0.0026 Acc: 81.2024\n",
      "Epoch 4/99\n",
      "training Loss: 0.0032 Acc: 68.1120\n",
      "validation Loss: 0.0030 Acc: 73.5119\n",
      "Epoch 5/99\n",
      "training Loss: 0.0032 Acc: 68.3293\n",
      "validation Loss: 0.0026 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0033 Acc: 65.5467\n",
      "validation Loss: 0.0034 Acc: 67.6905\n",
      "Epoch 7/99\n",
      "training Loss: 0.0040 Acc: 55.5890\n",
      "validation Loss: 0.0033 Acc: 76.7500\n",
      "Epoch 8/99\n",
      "training Loss: 0.0035 Acc: 55.6693\n",
      "validation Loss: 0.0031 Acc: 81.6786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0035 Acc: 56.2883\n",
      "validation Loss: 0.0033 Acc: 79.9762\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 56.5323\n",
      "validation Loss: 0.0032 Acc: 82.1429\n",
      "Epoch 11/99\n",
      "training Loss: 0.0035 Acc: 56.6811\n",
      "validation Loss: 0.0033 Acc: 81.4167\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 56.4788\n",
      "validation Loss: 0.0033 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 56.4966\n",
      "validation Loss: 0.0031 Acc: 81.9762\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 56.6692\n",
      "validation Loss: 0.0032 Acc: 82.2976\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 56.7585\n",
      "validation Loss: 0.0033 Acc: 80.6310\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 56.7526\n",
      "validation Loss: 0.0032 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 56.6722\n",
      "validation Loss: 0.0033 Acc: 80.5714\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 56.4579\n",
      "validation Loss: 0.0033 Acc: 80.1071\n",
      "Epoch 19/99\n",
      "training Loss: 0.0035 Acc: 56.7794\n",
      "validation Loss: 0.0033 Acc: 81.3810\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 56.7317\n",
      "validation Loss: 0.0034 Acc: 79.6190\n",
      "Epoch 21/99\n",
      "training Loss: 0.0035 Acc: 56.4103\n",
      "validation Loss: 0.0032 Acc: 81.3690\n",
      "Epoch 22/99\n",
      "training Loss: 0.0035 Acc: 56.8954\n",
      "validation Loss: 0.0032 Acc: 82.6548\n",
      "Epoch 23/99\n",
      "training Loss: 0.0035 Acc: 56.5502\n",
      "validation Loss: 0.0033 Acc: 82.0357\n",
      "Epoch 24/99\n",
      "training Loss: 0.0035 Acc: 56.8508\n",
      "validation Loss: 0.0032 Acc: 80.9524\n",
      "Epoch 25/99\n",
      "training Loss: 0.0035 Acc: 56.6692\n",
      "validation Loss: 0.0032 Acc: 82.7024\n",
      "Epoch 26/99\n",
      "training Loss: 0.0035 Acc: 56.5383\n",
      "validation Loss: 0.0033 Acc: 81.4643\n",
      "Epoch 27/99\n",
      "training Loss: 0.0035 Acc: 56.7913\n",
      "validation Loss: 0.0032 Acc: 82.3333\n",
      "Epoch 28/99\n",
      "training Loss: 0.0035 Acc: 56.5889\n",
      "validation Loss: 0.0033 Acc: 82.1667\n",
      "Epoch 29/99\n",
      "training Loss: 0.0035 Acc: 56.7317\n",
      "validation Loss: 0.0033 Acc: 82.2381\n",
      "Epoch 30/99\n",
      "training Loss: 0.0035 Acc: 56.7198\n",
      "validation Loss: 0.0032 Acc: 82.4405\n",
      "Epoch 31/99\n",
      "training Loss: 0.0035 Acc: 56.7436\n",
      "validation Loss: 0.0033 Acc: 81.7143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0035 Acc: 56.7109\n",
      "validation Loss: 0.0032 Acc: 82.4286\n",
      "Epoch 33/99\n",
      "training Loss: 0.0035 Acc: 56.9996\n",
      "validation Loss: 0.0032 Acc: 82.5952\n",
      "Epoch 34/99\n",
      "training Loss: 0.0035 Acc: 56.9073\n",
      "validation Loss: 0.0032 Acc: 82.0119\n",
      "Epoch 35/99\n",
      "training Loss: 0.0035 Acc: 56.8865\n",
      "validation Loss: 0.0032 Acc: 82.5714\n",
      "Epoch 36/99\n",
      "training Loss: 0.0035 Acc: 56.8865\n",
      "validation Loss: 0.0032 Acc: 82.2024\n",
      "Early stopped.\n",
      "Best val acc: 82.714286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0061 Acc: 70.1238\n",
      "validation Loss: 0.0027 Acc: 80.9524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 73.7486\n",
      "validation Loss: 0.0026 Acc: 81.4405\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0032 Acc: 69.2935\n",
      "validation Loss: 0.0033 Acc: 80.0952\n",
      "Epoch 3/99\n",
      "training Loss: 0.0045 Acc: 64.4069\n",
      "validation Loss: 0.0034 Acc: 76.2738\n",
      "Epoch 4/99\n",
      "training Loss: 0.0035 Acc: 62.9963\n",
      "validation Loss: 0.0033 Acc: 69.4048\n",
      "Epoch 5/99\n",
      "training Loss: 0.0034 Acc: 66.0526\n",
      "validation Loss: 0.0032 Acc: 77.5238\n",
      "Epoch 6/99\n",
      "training Loss: 0.0035 Acc: 66.5318\n",
      "validation Loss: 0.0031 Acc: 74.8929\n",
      "Epoch 7/99\n",
      "training Loss: 0.0034 Acc: 68.1537\n",
      "validation Loss: 0.0031 Acc: 76.1667\n",
      "Epoch 8/99\n",
      "training Loss: 0.0033 Acc: 69.1745\n",
      "validation Loss: 0.0031 Acc: 76.7857\n",
      "Epoch 9/99\n",
      "training Loss: 0.0034 Acc: 68.4126\n",
      "validation Loss: 0.0032 Acc: 76.9643\n",
      "Epoch 10/99\n",
      "training Loss: 0.0033 Acc: 68.3799\n",
      "validation Loss: 0.0030 Acc: 78.7500\n",
      "Epoch 11/99\n",
      "training Loss: 0.0034 Acc: 69.1209\n",
      "validation Loss: 0.0030 Acc: 79.0714\n",
      "Epoch 12/99\n",
      "training Loss: 0.0034 Acc: 68.6834\n",
      "validation Loss: 0.0031 Acc: 77.4524\n",
      "Epoch 13/99\n",
      "training Loss: 0.0034 Acc: 69.3560\n",
      "validation Loss: 0.0031 Acc: 75.9405\n",
      "Epoch 14/99\n",
      "training Loss: 0.0034 Acc: 69.4602\n",
      "validation Loss: 0.0031 Acc: 77.1905\n",
      "Epoch 15/99\n",
      "training Loss: 0.0034 Acc: 69.7042\n",
      "validation Loss: 0.0031 Acc: 75.9762\n",
      "Epoch 16/99\n",
      "training Loss: 0.0034 Acc: 69.3441\n",
      "validation Loss: 0.0030 Acc: 77.7619\n",
      "Epoch 17/99\n",
      "training Loss: 0.0033 Acc: 69.5911\n",
      "validation Loss: 0.0031 Acc: 78.1310\n",
      "Epoch 18/99\n",
      "training Loss: 0.0033 Acc: 70.3143\n",
      "validation Loss: 0.0030 Acc: 78.4762\n",
      "Epoch 19/99\n",
      "training Loss: 0.0033 Acc: 70.5732\n",
      "validation Loss: 0.0030 Acc: 79.0833\n",
      "Epoch 20/99\n",
      "training Loss: 0.0033 Acc: 70.6059\n",
      "validation Loss: 0.0030 Acc: 79.6429\n",
      "Epoch 21/99\n",
      "training Loss: 0.0033 Acc: 71.0226\n",
      "validation Loss: 0.0030 Acc: 79.6548\n",
      "Early stopped.\n",
      "Best val acc: 81.440476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0070 Acc: 67.6091\n",
      "validation Loss: 0.0026 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0035 Acc: 65.2164\n",
      "validation Loss: 0.0030 Acc: 79.9643\n",
      "Epoch 2/99\n",
      "training Loss: 0.0035 Acc: 61.1749\n",
      "validation Loss: 0.0033 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0035 Acc: 62.3386\n",
      "validation Loss: 0.0030 Acc: 79.4881\n",
      "Epoch 4/99\n",
      "training Loss: 0.0035 Acc: 63.1986\n",
      "validation Loss: 0.0037 Acc: 55.1071\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 58.4221\n",
      "validation Loss: 0.0034 Acc: 67.8810\n",
      "Epoch 6/99\n",
      "training Loss: 0.0034 Acc: 63.9813\n",
      "validation Loss: 0.0029 Acc: 78.4405\n",
      "Epoch 7/99\n",
      "training Loss: 0.0034 Acc: 63.9992\n",
      "validation Loss: 0.0030 Acc: 78.7976\n",
      "Epoch 8/99\n",
      "training Loss: 0.0034 Acc: 64.6479\n",
      "validation Loss: 0.0030 Acc: 79.1786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0034 Acc: 64.3295\n",
      "validation Loss: 0.0031 Acc: 78.9286\n",
      "Epoch 10/99\n",
      "training Loss: 0.0034 Acc: 62.9695\n",
      "validation Loss: 0.0030 Acc: 80.4643\n",
      "Epoch 11/99\n",
      "training Loss: 0.0034 Acc: 63.1361\n",
      "validation Loss: 0.0030 Acc: 80.0952\n",
      "Epoch 12/99\n",
      "training Loss: 0.0034 Acc: 65.3205\n",
      "validation Loss: 0.0030 Acc: 79.7024\n",
      "Epoch 13/99\n",
      "training Loss: 0.0034 Acc: 66.5615\n",
      "validation Loss: 0.0029 Acc: 80.8095\n",
      "Epoch 14/99\n",
      "training Loss: 0.0034 Acc: 66.1240\n",
      "validation Loss: 0.0031 Acc: 81.0238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0034 Acc: 66.2639\n",
      "validation Loss: 0.0029 Acc: 81.9286\n",
      "Epoch 16/99\n",
      "training Loss: 0.0034 Acc: 67.2341\n",
      "validation Loss: 0.0029 Acc: 81.2857\n",
      "Epoch 17/99\n",
      "training Loss: 0.0034 Acc: 66.6508\n",
      "validation Loss: 0.0030 Acc: 81.4643\n",
      "Epoch 18/99\n",
      "training Loss: 0.0033 Acc: 66.9424\n",
      "validation Loss: 0.0029 Acc: 81.2500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0034 Acc: 66.5496\n",
      "validation Loss: 0.0029 Acc: 81.6786\n",
      "Epoch 20/99\n",
      "training Loss: 0.0033 Acc: 67.4305\n",
      "validation Loss: 0.0029 Acc: 82.0357\n",
      "Early stopped.\n",
      "Best val acc: 82.214286\n",
      "----------\n",
      "Average best_acc across k-fold: 82.1317996849\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 1000, 'dropout': 0.01, 'batch_size': 194, 'max_depth': 4}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 69.3333\n",
      "validation Loss: 0.0031 Acc: 80.2190\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 80.2083\n",
      "validation Loss: 0.0022 Acc: 82.0995\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 81.7321\n",
      "validation Loss: 0.0020 Acc: 83.1231\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.2351\n",
      "validation Loss: 0.0019 Acc: 83.4563\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.6845\n",
      "validation Loss: 0.0019 Acc: 83.5039\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.9077\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 83.0804\n",
      "validation Loss: 0.0019 Acc: 83.5396\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.1220\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.0833\n",
      "validation Loss: 0.0019 Acc: 83.7420\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.0982\n",
      "validation Loss: 0.0019 Acc: 83.7539\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.2411\n",
      "validation Loss: 0.0019 Acc: 83.8253\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.2411\n",
      "validation Loss: 0.0019 Acc: 83.8967\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.3125\n",
      "validation Loss: 0.0019 Acc: 83.8491\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.3958\n",
      "validation Loss: 0.0019 Acc: 83.9086\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.3958\n",
      "validation Loss: 0.0019 Acc: 83.8253\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.3780\n",
      "validation Loss: 0.0019 Acc: 83.8729\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.4018\n",
      "validation Loss: 0.0019 Acc: 83.8491\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.2946\n",
      "validation Loss: 0.0019 Acc: 84.2180\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.4464\n",
      "validation Loss: 0.0019 Acc: 83.9324\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.4792\n",
      "validation Loss: 0.0018 Acc: 84.1823\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.4643\n",
      "validation Loss: 0.0018 Acc: 84.1942\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.5060\n",
      "validation Loss: 0.0018 Acc: 84.1466\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.5327\n",
      "validation Loss: 0.0018 Acc: 84.2657\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.5387\n",
      "validation Loss: 0.0018 Acc: 84.0395\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.5327\n",
      "validation Loss: 0.0018 Acc: 84.0752\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.6012\n",
      "validation Loss: 0.0018 Acc: 84.2537\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.5952\n",
      "validation Loss: 0.0018 Acc: 84.1585\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.5506\n",
      "validation Loss: 0.0018 Acc: 84.3609\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.5714\n",
      "validation Loss: 0.0018 Acc: 84.3490\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.5952\n",
      "validation Loss: 0.0018 Acc: 84.3371\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.5952\n",
      "validation Loss: 0.0018 Acc: 84.2061\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 83.6905\n",
      "validation Loss: 0.0018 Acc: 84.0157\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 83.7173\n",
      "validation Loss: 0.0018 Acc: 84.1704\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 83.5595\n",
      "validation Loss: 0.0018 Acc: 84.0871\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.7143\n",
      "validation Loss: 0.0018 Acc: 84.4680\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.7292\n",
      "validation Loss: 0.0018 Acc: 84.3609\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.8095\n",
      "validation Loss: 0.0018 Acc: 84.3847\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.7232\n",
      "validation Loss: 0.0018 Acc: 84.3609\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.7708\n",
      "validation Loss: 0.0018 Acc: 84.3847\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.7054\n",
      "validation Loss: 0.0018 Acc: 84.3847\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.7232\n",
      "validation Loss: 0.0018 Acc: 84.4085\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.7381\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.7589\n",
      "validation Loss: 0.0018 Acc: 84.5394\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.8006\n",
      "validation Loss: 0.0018 Acc: 84.4799\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.7887\n",
      "validation Loss: 0.0018 Acc: 84.3014\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 83.7232\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.7708\n",
      "validation Loss: 0.0018 Acc: 84.5037\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.7976\n",
      "validation Loss: 0.0018 Acc: 84.5275\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 83.8214\n",
      "validation Loss: 0.0018 Acc: 84.4680\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 83.8304\n",
      "validation Loss: 0.0018 Acc: 84.5156\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 83.8988\n",
      "validation Loss: 0.0018 Acc: 84.3609\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 83.8125\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 83.8452\n",
      "validation Loss: 0.0018 Acc: 84.4799\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 83.8155\n",
      "validation Loss: 0.0018 Acc: 84.5275\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 83.7292\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 83.8601\n",
      "validation Loss: 0.0018 Acc: 84.2776\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 83.8661\n",
      "validation Loss: 0.0018 Acc: 84.3847\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 83.8601\n",
      "validation Loss: 0.0018 Acc: 84.3847\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 83.8958\n",
      "validation Loss: 0.0018 Acc: 84.4204\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 83.8631\n",
      "validation Loss: 0.0018 Acc: 84.4204\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 83.8065\n",
      "validation Loss: 0.0018 Acc: 84.5870\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 83.9345\n",
      "validation Loss: 0.0018 Acc: 84.4323\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 83.7887\n",
      "validation Loss: 0.0018 Acc: 84.5394\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 83.8869\n",
      "validation Loss: 0.0018 Acc: 84.5394\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 83.8899\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 83.8482\n",
      "validation Loss: 0.0018 Acc: 84.5156\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 83.8958\n",
      "validation Loss: 0.0018 Acc: 84.5751\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 83.8244\n",
      "validation Loss: 0.0018 Acc: 84.4323\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 83.8333\n",
      "validation Loss: 0.0018 Acc: 84.4323\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 83.8631\n",
      "validation Loss: 0.0018 Acc: 84.5394\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 83.8065\n",
      "validation Loss: 0.0018 Acc: 84.5513\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 83.9524\n",
      "validation Loss: 0.0018 Acc: 84.4680\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 83.8542\n",
      "validation Loss: 0.0018 Acc: 84.4442\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 83.8452\n",
      "validation Loss: 0.0018 Acc: 84.4442\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 83.9107\n",
      "validation Loss: 0.0018 Acc: 84.4442\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 83.8899\n",
      "validation Loss: 0.0018 Acc: 84.5632\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 83.8452\n",
      "validation Loss: 0.0018 Acc: 84.4680\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 83.8750\n",
      "validation Loss: 0.0018 Acc: 84.5632\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 83.9345\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 83.9286\n",
      "validation Loss: 0.0018 Acc: 84.4561\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 83.8631\n",
      "validation Loss: 0.0018 Acc: 84.4799\n",
      "Early stopped.\n",
      "Best val acc: 84.587003\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 63.0736\n",
      "validation Loss: 0.0032 Acc: 79.7024\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 80.1798\n",
      "validation Loss: 0.0022 Acc: 81.5238\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 81.8969\n",
      "validation Loss: 0.0020 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.4028\n",
      "validation Loss: 0.0020 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.7927\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.8165\n",
      "validation Loss: 0.0020 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 82.9802\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.0129\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.1111\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.0665\n",
      "validation Loss: 0.0019 Acc: 83.7619\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.2450\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.2004\n",
      "validation Loss: 0.0019 Acc: 83.8452\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.2569\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.2867\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.2629\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.2748\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.3105\n",
      "validation Loss: 0.0019 Acc: 84.0357\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.3552\n",
      "validation Loss: 0.0019 Acc: 84.2143\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.3938\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.4325\n",
      "validation Loss: 0.0019 Acc: 84.1429\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.5367\n",
      "validation Loss: 0.0019 Acc: 84.1071\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.4087\n",
      "validation Loss: 0.0019 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.5754\n",
      "validation Loss: 0.0019 Acc: 84.2381\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.5873\n",
      "validation Loss: 0.0019 Acc: 84.1429\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.4444\n",
      "validation Loss: 0.0019 Acc: 84.1905\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.5754\n",
      "validation Loss: 0.0019 Acc: 84.1429\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.6319\n",
      "validation Loss: 0.0018 Acc: 84.2381\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.6022\n",
      "validation Loss: 0.0019 Acc: 84.2738\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 83.6141\n",
      "validation Loss: 0.0019 Acc: 84.2381\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.7539\n",
      "validation Loss: 0.0019 Acc: 84.2500\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.7212\n",
      "validation Loss: 0.0018 Acc: 84.2143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 83.7480\n",
      "validation Loss: 0.0018 Acc: 84.2738\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 83.7718\n",
      "validation Loss: 0.0018 Acc: 84.3095\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 83.7242\n",
      "validation Loss: 0.0019 Acc: 84.2738\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.7391\n",
      "validation Loss: 0.0018 Acc: 84.1786\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.6706\n",
      "validation Loss: 0.0018 Acc: 84.2381\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.6795\n",
      "validation Loss: 0.0018 Acc: 84.2738\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.7510\n",
      "validation Loss: 0.0018 Acc: 84.2381\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.7599\n",
      "validation Loss: 0.0018 Acc: 84.3214\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.7599\n",
      "validation Loss: 0.0018 Acc: 84.2143\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.7956\n",
      "validation Loss: 0.0019 Acc: 84.2381\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.8373\n",
      "validation Loss: 0.0018 Acc: 84.3095\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.7986\n",
      "validation Loss: 0.0019 Acc: 84.3095\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.8045\n",
      "validation Loss: 0.0018 Acc: 84.2619\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.7539\n",
      "validation Loss: 0.0018 Acc: 84.2500\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 83.9236\n",
      "validation Loss: 0.0018 Acc: 84.3333\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.8313\n",
      "validation Loss: 0.0018 Acc: 84.3452\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.7926\n",
      "validation Loss: 0.0018 Acc: 84.3095\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 83.8462\n",
      "validation Loss: 0.0018 Acc: 84.2619\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 83.9385\n",
      "validation Loss: 0.0019 Acc: 84.2857\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 83.8522\n",
      "validation Loss: 0.0018 Acc: 84.2738\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 83.8135\n",
      "validation Loss: 0.0018 Acc: 84.3095\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 83.8224\n",
      "validation Loss: 0.0018 Acc: 84.2857\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 83.9652\n",
      "validation Loss: 0.0018 Acc: 84.3810\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 83.8343\n",
      "validation Loss: 0.0018 Acc: 84.3214\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 83.8343\n",
      "validation Loss: 0.0018 Acc: 84.3095\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 83.8075\n",
      "validation Loss: 0.0018 Acc: 84.2619\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 84.0426\n",
      "validation Loss: 0.0019 Acc: 84.3214\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 83.9444\n",
      "validation Loss: 0.0018 Acc: 84.2619\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 83.8760\n",
      "validation Loss: 0.0018 Acc: 84.3452\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 83.9920\n",
      "validation Loss: 0.0018 Acc: 84.3452\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 83.9712\n",
      "validation Loss: 0.0018 Acc: 84.3452\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.0515\n",
      "validation Loss: 0.0018 Acc: 84.3810\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 83.9117\n",
      "validation Loss: 0.0018 Acc: 84.3810\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 83.9355\n",
      "validation Loss: 0.0018 Acc: 84.3095\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 83.9623\n",
      "validation Loss: 0.0019 Acc: 84.3333\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.0248\n",
      "validation Loss: 0.0018 Acc: 84.3333\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0018 Acc: 84.3333\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 83.9920\n",
      "validation Loss: 0.0018 Acc: 84.3690\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0018 Acc: 84.3690\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 83.9057\n",
      "validation Loss: 0.0018 Acc: 84.3690\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 83.8819\n",
      "validation Loss: 0.0018 Acc: 84.3690\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 83.9325\n",
      "validation Loss: 0.0018 Acc: 84.3333\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 83.9831\n",
      "validation Loss: 0.0018 Acc: 84.4405\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 83.9117\n",
      "validation Loss: 0.0018 Acc: 84.4167\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.1081\n",
      "validation Loss: 0.0019 Acc: 84.3810\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 83.9652\n",
      "validation Loss: 0.0018 Acc: 84.3452\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 83.9950\n",
      "validation Loss: 0.0018 Acc: 84.3571\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 83.9295\n",
      "validation Loss: 0.0018 Acc: 84.4167\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 84.0664\n",
      "validation Loss: 0.0018 Acc: 84.3929\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 83.9295\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 83.8968\n",
      "validation Loss: 0.0018 Acc: 84.4167\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 83.9117\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 83/99\n",
      "training Loss: 0.0018 Acc: 83.8581\n",
      "validation Loss: 0.0018 Acc: 84.4167\n",
      "Epoch 84/99\n",
      "training Loss: 0.0018 Acc: 84.0486\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 85/99\n",
      "training Loss: 0.0018 Acc: 83.9176\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 86/99\n",
      "training Loss: 0.0018 Acc: 83.9623\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 87/99\n",
      "training Loss: 0.0018 Acc: 83.9652\n",
      "validation Loss: 0.0018 Acc: 84.4167\n",
      "Epoch 88/99\n",
      "training Loss: 0.0018 Acc: 83.9087\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 89/99\n",
      "training Loss: 0.0018 Acc: 83.9295\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 90/99\n",
      "training Loss: 0.0018 Acc: 83.9176\n",
      "validation Loss: 0.0018 Acc: 84.4167\n",
      "Epoch 91/99\n",
      "training Loss: 0.0018 Acc: 83.9385\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 92/99\n",
      "training Loss: 0.0018 Acc: 84.0039\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Epoch 93/99\n",
      "training Loss: 0.0018 Acc: 83.8730\n",
      "validation Loss: 0.0018 Acc: 84.4286\n",
      "Early stopped.\n",
      "Best val acc: 84.440476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0035 Acc: 61.0589\n",
      "validation Loss: 0.0032 Acc: 79.8333\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 80.4476\n",
      "validation Loss: 0.0022 Acc: 80.9643\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.0933\n",
      "validation Loss: 0.0021 Acc: 81.9762\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.6498\n",
      "validation Loss: 0.0020 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 83.0278\n",
      "validation Loss: 0.0020 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.1647\n",
      "validation Loss: 0.0020 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.2064\n",
      "validation Loss: 0.0020 Acc: 82.6310\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.2897\n",
      "validation Loss: 0.0020 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.3849\n",
      "validation Loss: 0.0020 Acc: 82.6786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.3968\n",
      "validation Loss: 0.0019 Acc: 82.6429\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.3790\n",
      "validation Loss: 0.0019 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.4891\n",
      "validation Loss: 0.0019 Acc: 82.7143\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.5486\n",
      "validation Loss: 0.0019 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.5426\n",
      "validation Loss: 0.0019 Acc: 82.7976\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.5992\n",
      "validation Loss: 0.0019 Acc: 82.7619\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.6260\n",
      "validation Loss: 0.0019 Acc: 82.9405\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.6795\n",
      "validation Loss: 0.0019 Acc: 83.0119\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.5546\n",
      "validation Loss: 0.0019 Acc: 82.9881\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.6111\n",
      "validation Loss: 0.0019 Acc: 83.0714\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.7926\n",
      "validation Loss: 0.0019 Acc: 82.9405\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.7242\n",
      "validation Loss: 0.0019 Acc: 82.7143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.6795\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7480\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.7986\n",
      "validation Loss: 0.0019 Acc: 82.8095\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 83.7539\n",
      "validation Loss: 0.0019 Acc: 82.9881\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 83.8194\n",
      "validation Loss: 0.0019 Acc: 82.9643\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 83.8581\n",
      "validation Loss: 0.0019 Acc: 82.8452\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 83.8462\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 83.8849\n",
      "validation Loss: 0.0019 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 83.8313\n",
      "validation Loss: 0.0019 Acc: 82.9762\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.8402\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 83.8313\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 83.9504\n",
      "validation Loss: 0.0019 Acc: 83.0119\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 83.8908\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.8789\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.9236\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.9474\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.9087\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.9117\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 84.0664\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 84.0634\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 84.1111\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 84.1021\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 84.0992\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 84.1438\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 84.1736\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 84.0396\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 84.1706\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 84.1408\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 84.1140\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.1795\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 84.1200\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 84.1498\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 84.1557\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 84.0962\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 84.1646\n",
      "validation Loss: 0.0019 Acc: 83.3810\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.1498\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 84.1349\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.2063\n",
      "validation Loss: 0.0019 Acc: 83.2024\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.0992\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.2152\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.1855\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.1557\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.1736\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.1765\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.1765\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.1736\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.2003\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.2212\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Early stopped.\n",
      "Best val acc: 83.416667\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 69.4602\n",
      "validation Loss: 0.0032 Acc: 78.5595\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0027 Acc: 80.0518\n",
      "validation Loss: 0.0024 Acc: 81.1310\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 81.9862\n",
      "validation Loss: 0.0021 Acc: 81.8095\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.7361\n",
      "validation Loss: 0.0021 Acc: 82.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.9028\n",
      "validation Loss: 0.0020 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 83.0129\n",
      "validation Loss: 0.0020 Acc: 82.3333\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.1439\n",
      "validation Loss: 0.0020 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.3016\n",
      "validation Loss: 0.0020 Acc: 82.2738\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.3224\n",
      "validation Loss: 0.0020 Acc: 82.4524\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.3462\n",
      "validation Loss: 0.0020 Acc: 82.6429\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.3343\n",
      "validation Loss: 0.0020 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.3343\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.4355\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.4682\n",
      "validation Loss: 0.0020 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.6170\n",
      "validation Loss: 0.0020 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.6676\n",
      "validation Loss: 0.0020 Acc: 82.9524\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.5397\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.5962\n",
      "validation Loss: 0.0019 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.6557\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.6111\n",
      "validation Loss: 0.0020 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.7658\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 83.7629\n",
      "validation Loss: 0.0019 Acc: 83.0714\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7629\n",
      "validation Loss: 0.0019 Acc: 83.0119\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.8105\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 83.8551\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 83.7629\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 83.8432\n",
      "validation Loss: 0.0019 Acc: 83.0476\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 83.8432\n",
      "validation Loss: 0.0019 Acc: 83.1071\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 83.8402\n",
      "validation Loss: 0.0019 Acc: 83.0595\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 83.9206\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.9325\n",
      "validation Loss: 0.0019 Acc: 82.9881\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 83.9146\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 83.9504\n",
      "validation Loss: 0.0019 Acc: 83.0833\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 83.9652\n",
      "validation Loss: 0.0019 Acc: 83.1429\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.9355\n",
      "validation Loss: 0.0019 Acc: 83.1905\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.9980\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 84.0129\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.9682\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 84.0545\n",
      "validation Loss: 0.0019 Acc: 83.1905\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.9980\n",
      "validation Loss: 0.0019 Acc: 83.1905\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 84.0873\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 84.0396\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 84.0932\n",
      "validation Loss: 0.0019 Acc: 83.1190\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 84.0545\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 84.1111\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 84.1111\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 84.0545\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 84.1378\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 84.0248\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 84.1021\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.1349\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 84.2122\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 84.1944\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 84.2182\n",
      "validation Loss: 0.0019 Acc: 83.1786\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 84.1795\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 84.1646\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.1587\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 84.1884\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.1646\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.0367\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.2152\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Early stopped.\n",
      "Best val acc: 83.285714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 60.2702\n",
      "validation Loss: 0.0031 Acc: 78.0476\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 80.2542\n",
      "validation Loss: 0.0023 Acc: 80.9048\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.0874\n",
      "validation Loss: 0.0021 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.6528\n",
      "validation Loss: 0.0020 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.8284\n",
      "validation Loss: 0.0020 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0019 Acc: 82.9980\n",
      "validation Loss: 0.0020 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 83.1825\n",
      "validation Loss: 0.0020 Acc: 83.1429\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 83.2778\n",
      "validation Loss: 0.0020 Acc: 83.1190\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.2837\n",
      "validation Loss: 0.0020 Acc: 83.1429\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.4087\n",
      "validation Loss: 0.0019 Acc: 82.9881\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.3790\n",
      "validation Loss: 0.0019 Acc: 83.0119\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.4623\n",
      "validation Loss: 0.0019 Acc: 82.9762\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.5010\n",
      "validation Loss: 0.0019 Acc: 82.9762\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.5040\n",
      "validation Loss: 0.0019 Acc: 83.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.5694\n",
      "validation Loss: 0.0019 Acc: 82.9048\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.6290\n",
      "validation Loss: 0.0019 Acc: 82.9762\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.5843\n",
      "validation Loss: 0.0019 Acc: 83.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.6230\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.7748\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.7778\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.7272\n",
      "validation Loss: 0.0019 Acc: 83.0119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.7420\n",
      "validation Loss: 0.0019 Acc: 83.0476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7212\n",
      "validation Loss: 0.0019 Acc: 83.0357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.7867\n",
      "validation Loss: 0.0019 Acc: 83.0833\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.8730\n",
      "validation Loss: 0.0019 Acc: 83.0238\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.7718\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Early stopped.\n",
      "Best val acc: 83.214286\n",
      "----------\n",
      "Average best_acc across k-fold: 83.7888291903\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 984, 'dropout': 0.9, 'batch_size': 358, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 50.1845\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.2679\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 49.7024\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 49.4524\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 50.0685\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 50.0000\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 49.4464\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 49.3899\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 49.6726\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 48.9702\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 49.0804\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 48.9494\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 48.6815\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 48.6994\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 48.5923\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 48.8274\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 48.5327\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 48.8690\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 49.0952\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 48.7857\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 49.1637\n",
      "validation Loss: 0.0019 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 49.9018\n",
      "validation Loss: 0.0020 Acc: 49.9762\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.1905\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.7321\n",
      "validation Loss: 0.0020 Acc: 50.0357\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 51.0654\n",
      "validation Loss: 0.0020 Acc: 50.2857\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 51.4493\n",
      "validation Loss: 0.0020 Acc: 50.9881\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 51.0416\n",
      "validation Loss: 0.0020 Acc: 52.3810\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 51.7142\n",
      "validation Loss: 0.0020 Acc: 54.8452\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 51.9344\n",
      "validation Loss: 0.0020 Acc: 57.5357\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 52.0743\n",
      "validation Loss: 0.0019 Acc: 60.3214\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 52.6010\n",
      "validation Loss: 0.0019 Acc: 63.6786\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 52.5981\n",
      "validation Loss: 0.0019 Acc: 66.6310\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 53.0802\n",
      "validation Loss: 0.0019 Acc: 69.9405\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 53.4730\n",
      "validation Loss: 0.0019 Acc: 71.9286\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 53.5206\n",
      "validation Loss: 0.0019 Acc: 74.7381\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 54.1158\n",
      "validation Loss: 0.0019 Acc: 76.3810\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 54.6604\n",
      "validation Loss: 0.0019 Acc: 77.0357\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 55.0146\n",
      "validation Loss: 0.0019 Acc: 77.4405\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 55.5741\n",
      "validation Loss: 0.0019 Acc: 77.9762\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 56.2228\n",
      "validation Loss: 0.0019 Acc: 78.3690\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 56.5294\n",
      "validation Loss: 0.0019 Acc: 78.4286\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 57.1156\n",
      "validation Loss: 0.0019 Acc: 78.7262\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 58.3061\n",
      "validation Loss: 0.0018 Acc: 78.9405\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 58.7227\n",
      "validation Loss: 0.0018 Acc: 79.0238\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 59.3477\n",
      "validation Loss: 0.0018 Acc: 79.4643\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 60.0202\n",
      "validation Loss: 0.0018 Acc: 79.5833\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 60.5172\n",
      "validation Loss: 0.0018 Acc: 79.7738\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 61.7047\n",
      "validation Loss: 0.0017 Acc: 79.7619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 62.3594\n",
      "validation Loss: 0.0017 Acc: 79.9643\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 63.0647\n",
      "validation Loss: 0.0017 Acc: 80.1190\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 63.5795\n",
      "validation Loss: 0.0017 Acc: 80.2381\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 64.9485\n",
      "validation Loss: 0.0016 Acc: 80.3095\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 65.2699\n",
      "validation Loss: 0.0016 Acc: 80.5238\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 66.2133\n",
      "validation Loss: 0.0016 Acc: 80.6071\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 66.6776\n",
      "validation Loss: 0.0015 Acc: 80.6548\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 67.4632\n",
      "validation Loss: 0.0015 Acc: 80.7738\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 68.2727\n",
      "validation Loss: 0.0015 Acc: 80.8929\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 68.5406\n",
      "validation Loss: 0.0015 Acc: 80.8810\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 69.6714\n",
      "validation Loss: 0.0014 Acc: 81.0119\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 70.5315\n",
      "validation Loss: 0.0014 Acc: 81.0952\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 70.1744\n",
      "validation Loss: 0.0014 Acc: 81.2619\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 70.9928\n",
      "validation Loss: 0.0014 Acc: 81.3810\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 71.6713\n",
      "validation Loss: 0.0014 Acc: 81.5119\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0015 Acc: 72.2040\n",
      "validation Loss: 0.0013 Acc: 81.6310\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0015 Acc: 72.7367\n",
      "validation Loss: 0.0013 Acc: 81.7262\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0015 Acc: 73.0760\n",
      "validation Loss: 0.0013 Acc: 81.7857\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0015 Acc: 73.4302\n",
      "validation Loss: 0.0013 Acc: 81.7857\n",
      "Epoch 46/99\n",
      "training Loss: 0.0015 Acc: 73.8051\n",
      "validation Loss: 0.0013 Acc: 81.7976\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0015 Acc: 74.4063\n",
      "validation Loss: 0.0013 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0015 Acc: 74.4063\n",
      "validation Loss: 0.0013 Acc: 81.9643\n",
      "Epoch 49/99\n",
      "training Loss: 0.0015 Acc: 75.0283\n",
      "validation Loss: 0.0012 Acc: 81.9524\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 75.1562\n",
      "validation Loss: 0.0012 Acc: 81.9286\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 75.3378\n",
      "validation Loss: 0.0012 Acc: 82.0357\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 75.7455\n",
      "validation Loss: 0.0012 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 75.9211\n",
      "validation Loss: 0.0012 Acc: 82.2619\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 76.1086\n",
      "validation Loss: 0.0012 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 76.2068\n",
      "validation Loss: 0.0012 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 76.5966\n",
      "validation Loss: 0.0012 Acc: 82.4643\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 77.0162\n",
      "validation Loss: 0.0012 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 77.2395\n",
      "validation Loss: 0.0012 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0014 Acc: 77.1740\n",
      "validation Loss: 0.0012 Acc: 82.5714\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0014 Acc: 77.5728\n",
      "validation Loss: 0.0012 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0014 Acc: 77.5430\n",
      "validation Loss: 0.0012 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0014 Acc: 77.7364\n",
      "validation Loss: 0.0012 Acc: 82.6548\n",
      "Epoch 63/99\n",
      "training Loss: 0.0014 Acc: 78.0102\n",
      "validation Loss: 0.0011 Acc: 82.6786\n",
      "Epoch 64/99\n",
      "training Loss: 0.0014 Acc: 77.9835\n",
      "validation Loss: 0.0012 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0014 Acc: 78.3793\n",
      "validation Loss: 0.0011 Acc: 82.7381\n",
      "Epoch 66/99\n",
      "training Loss: 0.0014 Acc: 77.9864\n",
      "validation Loss: 0.0011 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 67/99\n",
      "training Loss: 0.0013 Acc: 78.6679\n",
      "validation Loss: 0.0011 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0013 Acc: 78.5816\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0013 Acc: 78.6560\n",
      "validation Loss: 0.0011 Acc: 82.7381\n",
      "Epoch 70/99\n",
      "training Loss: 0.0013 Acc: 78.7364\n",
      "validation Loss: 0.0011 Acc: 82.8095\n",
      "Epoch 71/99\n",
      "training Loss: 0.0013 Acc: 78.9268\n",
      "validation Loss: 0.0011 Acc: 82.8214\n",
      "Epoch 72/99\n",
      "training Loss: 0.0013 Acc: 79.2869\n",
      "validation Loss: 0.0011 Acc: 82.8214\n",
      "Epoch 73/99\n",
      "training Loss: 0.0013 Acc: 79.1411\n",
      "validation Loss: 0.0011 Acc: 82.7738\n",
      "Epoch 74/99\n",
      "training Loss: 0.0013 Acc: 79.3703\n",
      "validation Loss: 0.0011 Acc: 82.8214\n",
      "Epoch 75/99\n",
      "training Loss: 0.0013 Acc: 79.0995\n",
      "validation Loss: 0.0011 Acc: 82.8214\n",
      "Epoch 76/99\n",
      "training Loss: 0.0013 Acc: 79.3703\n",
      "validation Loss: 0.0011 Acc: 82.7976\n",
      "Epoch 77/99\n",
      "training Loss: 0.0013 Acc: 79.4596\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Epoch 78/99\n",
      "training Loss: 0.0013 Acc: 79.4566\n",
      "validation Loss: 0.0011 Acc: 82.7857\n",
      "Epoch 79/99\n",
      "training Loss: 0.0013 Acc: 79.7572\n",
      "validation Loss: 0.0011 Acc: 82.7738\n",
      "Epoch 80/99\n",
      "training Loss: 0.0013 Acc: 79.7542\n",
      "validation Loss: 0.0011 Acc: 82.7857\n",
      "Epoch 81/99\n",
      "training Loss: 0.0013 Acc: 79.8673\n",
      "validation Loss: 0.0011 Acc: 82.7381\n",
      "Epoch 82/99\n",
      "training Loss: 0.0013 Acc: 79.8762\n",
      "validation Loss: 0.0011 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0013 Acc: 80.3137\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 84/99\n",
      "training Loss: 0.0013 Acc: 80.0637\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Epoch 85/99\n",
      "training Loss: 0.0013 Acc: 80.0726\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 86/99\n",
      "training Loss: 0.0013 Acc: 79.9744\n",
      "validation Loss: 0.0011 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0013 Acc: 80.1083\n",
      "validation Loss: 0.0011 Acc: 82.9286\n",
      "Epoch 88/99\n",
      "training Loss: 0.0013 Acc: 80.3137\n",
      "validation Loss: 0.0011 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 89/99\n",
      "training Loss: 0.0013 Acc: 80.2720\n",
      "validation Loss: 0.0011 Acc: 82.9524\n",
      "Epoch 90/99\n",
      "training Loss: 0.0013 Acc: 80.5220\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 91/99\n",
      "training Loss: 0.0013 Acc: 80.4238\n",
      "validation Loss: 0.0011 Acc: 82.9286\n",
      "Epoch 92/99\n",
      "training Loss: 0.0013 Acc: 80.3762\n",
      "validation Loss: 0.0011 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 93/99\n",
      "training Loss: 0.0013 Acc: 80.7006\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 94/99\n",
      "training Loss: 0.0013 Acc: 80.4922\n",
      "validation Loss: 0.0011 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0013 Acc: 80.8196\n",
      "validation Loss: 0.0011 Acc: 83.0119\n",
      "Epoch 96/99\n",
      "training Loss: 0.0013 Acc: 80.5458\n",
      "validation Loss: 0.0011 Acc: 83.0000\n",
      "Epoch 97/99\n",
      "training Loss: 0.0013 Acc: 80.4774\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Epoch 98/99\n",
      "training Loss: 0.0012 Acc: 81.0101\n",
      "validation Loss: 0.0011 Acc: 83.0238\n",
      "Epoch 99/99\n",
      "training Loss: 0.0013 Acc: 80.6946\n",
      "validation Loss: 0.0011 Acc: 83.0119\n",
      "Best val acc: 83.047619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 49.7143\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.1964\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.4256\n",
      "validation Loss: 0.0020 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 50.9613\n",
      "validation Loss: 0.0020 Acc: 50.2500\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 51.2946\n",
      "validation Loss: 0.0020 Acc: 51.3214\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 51.4493\n",
      "validation Loss: 0.0020 Acc: 52.9524\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 51.3511\n",
      "validation Loss: 0.0020 Acc: 55.7857\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 51.8213\n",
      "validation Loss: 0.0020 Acc: 59.2976\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 52.1844\n",
      "validation Loss: 0.0019 Acc: 62.6548\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 52.3421\n",
      "validation Loss: 0.0019 Acc: 65.7262\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 52.4879\n",
      "validation Loss: 0.0019 Acc: 68.6190\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 52.8332\n",
      "validation Loss: 0.0019 Acc: 71.1190\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 53.1337\n",
      "validation Loss: 0.0019 Acc: 72.8690\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 52.9998\n",
      "validation Loss: 0.0019 Acc: 74.1667\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 53.5801\n",
      "validation Loss: 0.0019 Acc: 75.7500\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 53.9462\n",
      "validation Loss: 0.0019 Acc: 76.3214\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 54.3896\n",
      "validation Loss: 0.0019 Acc: 77.0952\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 54.6188\n",
      "validation Loss: 0.0019 Acc: 77.6667\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 55.7467\n",
      "validation Loss: 0.0019 Acc: 78.0476\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 55.2943\n",
      "validation Loss: 0.0019 Acc: 78.3095\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 56.2556\n",
      "validation Loss: 0.0019 Acc: 78.4167\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 56.8389\n",
      "validation Loss: 0.0019 Acc: 78.7143\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 57.2496\n",
      "validation Loss: 0.0018 Acc: 78.8214\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 57.4341\n",
      "validation Loss: 0.0018 Acc: 78.9881\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 58.1959\n",
      "validation Loss: 0.0018 Acc: 79.0000\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 58.9310\n",
      "validation Loss: 0.0018 Acc: 79.1667\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 59.6601\n",
      "validation Loss: 0.0018 Acc: 79.1429\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 60.3178\n",
      "validation Loss: 0.0018 Acc: 79.2143\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 60.9279\n",
      "validation Loss: 0.0018 Acc: 79.4048\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 61.1987\n",
      "validation Loss: 0.0017 Acc: 79.5119\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 62.2522\n",
      "validation Loss: 0.0017 Acc: 79.6071\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 62.6034\n",
      "validation Loss: 0.0017 Acc: 79.6548\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 63.3087\n",
      "validation Loss: 0.0017 Acc: 79.6905\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 64.0646\n",
      "validation Loss: 0.0017 Acc: 79.7976\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 64.2759\n",
      "validation Loss: 0.0017 Acc: 79.8571\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 65.1301\n",
      "validation Loss: 0.0017 Acc: 80.0000\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 66.1002\n",
      "validation Loss: 0.0016 Acc: 80.1786\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 66.3651\n",
      "validation Loss: 0.0016 Acc: 80.2024\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 67.0764\n",
      "validation Loss: 0.0016 Acc: 80.3571\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 67.4990\n",
      "validation Loss: 0.0016 Acc: 80.5833\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 68.0852\n",
      "validation Loss: 0.0016 Acc: 80.6310\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 68.5941\n",
      "validation Loss: 0.0016 Acc: 80.6786\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 68.9393\n",
      "validation Loss: 0.0015 Acc: 80.7381\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 69.6476\n",
      "validation Loss: 0.0015 Acc: 80.8452\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 69.8381\n",
      "validation Loss: 0.0015 Acc: 80.8690\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 70.8172\n",
      "validation Loss: 0.0015 Acc: 80.9643\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 71.0791\n",
      "validation Loss: 0.0015 Acc: 80.9881\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 71.6862\n",
      "validation Loss: 0.0015 Acc: 81.0476\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 71.7308\n",
      "validation Loss: 0.0014 Acc: 81.1190\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 72.5225\n",
      "validation Loss: 0.0014 Acc: 81.0714\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 72.6772\n",
      "validation Loss: 0.0014 Acc: 81.1429\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0015 Acc: 73.3825\n",
      "validation Loss: 0.0014 Acc: 81.2143\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0015 Acc: 73.4063\n",
      "validation Loss: 0.0014 Acc: 81.2381\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0015 Acc: 73.5700\n",
      "validation Loss: 0.0014 Acc: 81.2619\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 74.0730\n",
      "validation Loss: 0.0013 Acc: 81.2857\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0015 Acc: 74.5878\n",
      "validation Loss: 0.0013 Acc: 81.3690\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0015 Acc: 74.7366\n",
      "validation Loss: 0.0013 Acc: 81.3690\n",
      "Epoch 57/99\n",
      "training Loss: 0.0015 Acc: 74.9539\n",
      "validation Loss: 0.0013 Acc: 81.4762\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0015 Acc: 75.2991\n",
      "validation Loss: 0.0013 Acc: 81.5714\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0015 Acc: 75.6889\n",
      "validation Loss: 0.0013 Acc: 81.5714\n",
      "Epoch 60/99\n",
      "training Loss: 0.0015 Acc: 75.6473\n",
      "validation Loss: 0.0013 Acc: 81.6071\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0015 Acc: 76.0431\n",
      "validation Loss: 0.0013 Acc: 81.5952\n",
      "Epoch 62/99\n",
      "training Loss: 0.0014 Acc: 76.5252\n",
      "validation Loss: 0.0012 Acc: 81.6548\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0014 Acc: 76.6026\n",
      "validation Loss: 0.0012 Acc: 81.6667\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0014 Acc: 76.6383\n",
      "validation Loss: 0.0012 Acc: 81.7024\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0014 Acc: 76.9657\n",
      "validation Loss: 0.0012 Acc: 81.7262\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0014 Acc: 77.0609\n",
      "validation Loss: 0.0012 Acc: 81.7024\n",
      "Epoch 67/99\n",
      "training Loss: 0.0014 Acc: 77.2841\n",
      "validation Loss: 0.0012 Acc: 81.7857\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0014 Acc: 77.3793\n",
      "validation Loss: 0.0012 Acc: 81.8333\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0014 Acc: 77.3496\n",
      "validation Loss: 0.0012 Acc: 81.9048\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0014 Acc: 77.6204\n",
      "validation Loss: 0.0012 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0014 Acc: 77.6769\n",
      "validation Loss: 0.0012 Acc: 81.9643\n",
      "Saving..\n",
      "Epoch 72/99\n",
      "training Loss: 0.0014 Acc: 77.8198\n",
      "validation Loss: 0.0012 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 73/99\n",
      "training Loss: 0.0014 Acc: 77.9894\n",
      "validation Loss: 0.0012 Acc: 81.9762\n",
      "Epoch 74/99\n",
      "training Loss: 0.0014 Acc: 78.1025\n",
      "validation Loss: 0.0012 Acc: 82.0000\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0014 Acc: 78.4447\n",
      "validation Loss: 0.0012 Acc: 82.0714\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0013 Acc: 78.7602\n",
      "validation Loss: 0.0012 Acc: 82.0833\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0014 Acc: 78.4507\n",
      "validation Loss: 0.0012 Acc: 82.0714\n",
      "Epoch 78/99\n",
      "training Loss: 0.0013 Acc: 79.0370\n",
      "validation Loss: 0.0012 Acc: 82.1310\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0013 Acc: 79.1381\n",
      "validation Loss: 0.0012 Acc: 82.1071\n",
      "Epoch 80/99\n",
      "training Loss: 0.0013 Acc: 78.9149\n",
      "validation Loss: 0.0011 Acc: 82.1429\n",
      "Saving..\n",
      "Epoch 81/99\n",
      "training Loss: 0.0013 Acc: 78.9268\n",
      "validation Loss: 0.0011 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0013 Acc: 79.3852\n",
      "validation Loss: 0.0011 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0013 Acc: 79.0042\n",
      "validation Loss: 0.0011 Acc: 82.2262\n",
      "Epoch 84/99\n",
      "training Loss: 0.0013 Acc: 79.1798\n",
      "validation Loss: 0.0011 Acc: 82.1786\n",
      "Epoch 85/99\n",
      "training Loss: 0.0013 Acc: 79.3762\n",
      "validation Loss: 0.0011 Acc: 82.1786\n",
      "Epoch 86/99\n",
      "training Loss: 0.0013 Acc: 79.3941\n",
      "validation Loss: 0.0011 Acc: 82.2500\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0013 Acc: 79.4387\n",
      "validation Loss: 0.0011 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 88/99\n",
      "training Loss: 0.0013 Acc: 79.5042\n",
      "validation Loss: 0.0011 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 89/99\n",
      "training Loss: 0.0013 Acc: 79.6589\n",
      "validation Loss: 0.0011 Acc: 82.2500\n",
      "Epoch 90/99\n",
      "training Loss: 0.0013 Acc: 79.8077\n",
      "validation Loss: 0.0011 Acc: 82.3452\n",
      "Epoch 91/99\n",
      "training Loss: 0.0013 Acc: 79.7661\n",
      "validation Loss: 0.0011 Acc: 82.2976\n",
      "Epoch 92/99\n",
      "training Loss: 0.0013 Acc: 79.7869\n",
      "validation Loss: 0.0011 Acc: 82.3214\n",
      "Epoch 93/99\n",
      "training Loss: 0.0013 Acc: 80.1351\n",
      "validation Loss: 0.0011 Acc: 82.3095\n",
      "Epoch 94/99\n",
      "training Loss: 0.0013 Acc: 80.0042\n",
      "validation Loss: 0.0011 Acc: 82.3095\n",
      "Epoch 95/99\n",
      "training Loss: 0.0013 Acc: 80.0101\n",
      "validation Loss: 0.0011 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 96/99\n",
      "training Loss: 0.0013 Acc: 80.2720\n",
      "validation Loss: 0.0011 Acc: 82.3571\n",
      "Epoch 97/99\n",
      "training Loss: 0.0013 Acc: 79.9506\n",
      "validation Loss: 0.0011 Acc: 82.3452\n",
      "Epoch 98/99\n",
      "training Loss: 0.0013 Acc: 79.9268\n",
      "validation Loss: 0.0011 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 99/99\n",
      "training Loss: 0.0013 Acc: 80.4327\n",
      "validation Loss: 0.0011 Acc: 82.3690\n",
      "Best val acc: 82.404762\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 50.3393\n",
      "validation Loss: 0.0020 Acc: 50.5952\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.4881\n",
      "validation Loss: 0.0020 Acc: 52.1905\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.8630\n",
      "validation Loss: 0.0020 Acc: 54.9048\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 51.1755\n",
      "validation Loss: 0.0020 Acc: 58.5119\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 51.2737\n",
      "validation Loss: 0.0020 Acc: 62.8333\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 51.9582\n",
      "validation Loss: 0.0020 Acc: 66.7619\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 52.4879\n",
      "validation Loss: 0.0019 Acc: 69.8095\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 52.4314\n",
      "validation Loss: 0.0019 Acc: 72.5119\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 53.0831\n",
      "validation Loss: 0.0019 Acc: 75.0119\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 53.4432\n",
      "validation Loss: 0.0019 Acc: 77.3333\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 54.1009\n",
      "validation Loss: 0.0019 Acc: 78.6310\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 54.6039\n",
      "validation Loss: 0.0019 Acc: 79.3690\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 55.0533\n",
      "validation Loss: 0.0019 Acc: 79.9286\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 55.1306\n",
      "validation Loss: 0.0019 Acc: 80.2500\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 56.4788\n",
      "validation Loss: 0.0019 Acc: 80.2857\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 57.3656\n",
      "validation Loss: 0.0019 Acc: 80.3333\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 58.1573\n",
      "validation Loss: 0.0019 Acc: 80.2857\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 58.8685\n",
      "validation Loss: 0.0019 Acc: 80.2857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 60.1512\n",
      "validation Loss: 0.0019 Acc: 80.1667\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 60.9220\n",
      "validation Loss: 0.0018 Acc: 80.3095\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 61.7523\n",
      "validation Loss: 0.0018 Acc: 80.3571\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 62.7165\n",
      "validation Loss: 0.0018 Acc: 80.4881\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 63.6450\n",
      "validation Loss: 0.0018 Acc: 80.6548\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 65.0705\n",
      "validation Loss: 0.0018 Acc: 81.0119\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 66.1746\n",
      "validation Loss: 0.0018 Acc: 81.2024\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0017 Acc: 67.3025\n",
      "validation Loss: 0.0017 Acc: 81.4167\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 68.0674\n",
      "validation Loss: 0.0017 Acc: 81.7143\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 69.3381\n",
      "validation Loss: 0.0017 Acc: 81.8095\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 70.2369\n",
      "validation Loss: 0.0017 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 71.3410\n",
      "validation Loss: 0.0017 Acc: 82.0000\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 72.3350\n",
      "validation Loss: 0.0017 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 72.9689\n",
      "validation Loss: 0.0017 Acc: 82.3929\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 73.8825\n",
      "validation Loss: 0.0017 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 74.6057\n",
      "validation Loss: 0.0016 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 75.5788\n",
      "validation Loss: 0.0016 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 76.3080\n",
      "validation Loss: 0.0016 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 76.4627\n",
      "validation Loss: 0.0016 Acc: 82.5833\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 77.0192\n",
      "validation Loss: 0.0016 Acc: 82.5238\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 77.7335\n",
      "validation Loss: 0.0016 Acc: 82.4881\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 77.8882\n",
      "validation Loss: 0.0016 Acc: 82.4286\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 78.1680\n",
      "validation Loss: 0.0016 Acc: 82.5119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 78.5668\n",
      "validation Loss: 0.0016 Acc: 82.6429\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 78.5995\n",
      "validation Loss: 0.0016 Acc: 82.5476\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 79.2185\n",
      "validation Loss: 0.0015 Acc: 82.6548\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 79.4536\n",
      "validation Loss: 0.0015 Acc: 82.6071\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 79.6292\n",
      "validation Loss: 0.0015 Acc: 82.5476\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 79.6589\n",
      "validation Loss: 0.0015 Acc: 82.6310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 79.9833\n",
      "validation Loss: 0.0015 Acc: 82.7143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0015 Acc: 80.0310\n",
      "validation Loss: 0.0015 Acc: 82.5952\n",
      "Epoch 49/99\n",
      "training Loss: 0.0015 Acc: 80.0786\n",
      "validation Loss: 0.0015 Acc: 82.6071\n",
      "Epoch 50/99\n",
      "training Loss: 0.0015 Acc: 80.0845\n",
      "validation Loss: 0.0015 Acc: 82.5833\n",
      "Epoch 51/99\n",
      "training Loss: 0.0015 Acc: 80.3256\n",
      "validation Loss: 0.0015 Acc: 82.6310\n",
      "Epoch 52/99\n",
      "training Loss: 0.0015 Acc: 80.3672\n",
      "validation Loss: 0.0015 Acc: 82.5476\n",
      "Epoch 53/99\n",
      "training Loss: 0.0015 Acc: 80.6738\n",
      "validation Loss: 0.0015 Acc: 82.5119\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 80.4684\n",
      "validation Loss: 0.0015 Acc: 82.5714\n",
      "Epoch 55/99\n",
      "training Loss: 0.0015 Acc: 80.6529\n",
      "validation Loss: 0.0015 Acc: 82.5119\n",
      "Early stopped.\n",
      "Best val acc: 82.714286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0021 Acc: 50.4315\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.0684\n",
      "validation Loss: 0.0020 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.5238\n",
      "validation Loss: 0.0020 Acc: 50.0238\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 51.0029\n",
      "validation Loss: 0.0020 Acc: 50.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 51.6636\n",
      "validation Loss: 0.0020 Acc: 50.2857\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 51.7023\n",
      "validation Loss: 0.0020 Acc: 50.4643\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 51.8749\n",
      "validation Loss: 0.0020 Acc: 50.8452\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 51.8243\n",
      "validation Loss: 0.0020 Acc: 51.4167\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 52.2439\n",
      "validation Loss: 0.0020 Acc: 52.5595\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 52.9314\n",
      "validation Loss: 0.0020 Acc: 54.6905\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 52.9016\n",
      "validation Loss: 0.0020 Acc: 57.6429\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 53.8688\n",
      "validation Loss: 0.0019 Acc: 61.9286\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 53.9402\n",
      "validation Loss: 0.0019 Acc: 65.3214\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 54.2111\n",
      "validation Loss: 0.0019 Acc: 67.8690\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 55.2914\n",
      "validation Loss: 0.0019 Acc: 70.3095\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 54.6961\n",
      "validation Loss: 0.0019 Acc: 73.0357\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 55.6247\n",
      "validation Loss: 0.0019 Acc: 75.3929\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 56.2526\n",
      "validation Loss: 0.0019 Acc: 77.5238\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 57.2139\n",
      "validation Loss: 0.0019 Acc: 78.9286\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 57.6811\n",
      "validation Loss: 0.0019 Acc: 79.7619\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 58.3328\n",
      "validation Loss: 0.0019 Acc: 80.0238\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 58.6483\n",
      "validation Loss: 0.0019 Acc: 80.2024\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 60.0292\n",
      "validation Loss: 0.0018 Acc: 80.2619\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 60.3089\n",
      "validation Loss: 0.0018 Acc: 80.3810\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 61.1214\n",
      "validation Loss: 0.0018 Acc: 80.6071\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 62.8058\n",
      "validation Loss: 0.0018 Acc: 80.6310\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 63.4754\n",
      "validation Loss: 0.0018 Acc: 80.7381\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 64.0944\n",
      "validation Loss: 0.0018 Acc: 80.9286\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 65.6151\n",
      "validation Loss: 0.0017 Acc: 80.9524\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 66.8085\n",
      "validation Loss: 0.0017 Acc: 81.0357\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 67.6150\n",
      "validation Loss: 0.0017 Acc: 81.1071\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 68.9810\n",
      "validation Loss: 0.0017 Acc: 81.4048\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 69.9333\n",
      "validation Loss: 0.0017 Acc: 81.4881\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 70.8410\n",
      "validation Loss: 0.0017 Acc: 81.6429\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 72.0344\n",
      "validation Loss: 0.0017 Acc: 81.6667\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 72.7665\n",
      "validation Loss: 0.0017 Acc: 81.8571\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 73.9480\n",
      "validation Loss: 0.0016 Acc: 82.0476\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 74.6771\n",
      "validation Loss: 0.0016 Acc: 82.0952\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 75.7336\n",
      "validation Loss: 0.0016 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 76.0490\n",
      "validation Loss: 0.0016 Acc: 82.3333\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 76.8823\n",
      "validation Loss: 0.0016 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 77.2573\n",
      "validation Loss: 0.0016 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 77.7513\n",
      "validation Loss: 0.0016 Acc: 82.4524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 78.0489\n",
      "validation Loss: 0.0016 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 78.5489\n",
      "validation Loss: 0.0016 Acc: 82.4881\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 78.9149\n",
      "validation Loss: 0.0016 Acc: 82.4762\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 79.0399\n",
      "validation Loss: 0.0016 Acc: 82.5238\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 79.3137\n",
      "validation Loss: 0.0015 Acc: 82.4881\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 79.5429\n",
      "validation Loss: 0.0015 Acc: 82.5119\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 79.7125\n",
      "validation Loss: 0.0015 Acc: 82.4881\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 79.9298\n",
      "validation Loss: 0.0015 Acc: 82.5119\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 79.8792\n",
      "validation Loss: 0.0015 Acc: 82.4881\n",
      "Epoch 52/99\n",
      "training Loss: 0.0016 Acc: 80.0250\n",
      "validation Loss: 0.0015 Acc: 82.4762\n",
      "Epoch 53/99\n",
      "training Loss: 0.0015 Acc: 80.0845\n",
      "validation Loss: 0.0015 Acc: 82.4286\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 80.5726\n",
      "validation Loss: 0.0015 Acc: 82.4405\n",
      "Epoch 55/99\n",
      "training Loss: 0.0015 Acc: 80.3047\n",
      "validation Loss: 0.0015 Acc: 82.3333\n",
      "Epoch 56/99\n",
      "training Loss: 0.0015 Acc: 80.4922\n",
      "validation Loss: 0.0015 Acc: 82.2738\n",
      "Epoch 57/99\n",
      "training Loss: 0.0015 Acc: 80.4535\n",
      "validation Loss: 0.0015 Acc: 82.3095\n",
      "Epoch 58/99\n",
      "training Loss: 0.0015 Acc: 80.4357\n",
      "validation Loss: 0.0015 Acc: 82.3571\n",
      "Epoch 59/99\n",
      "training Loss: 0.0015 Acc: 80.6053\n",
      "validation Loss: 0.0015 Acc: 82.3452\n",
      "Epoch 60/99\n",
      "training Loss: 0.0015 Acc: 80.5160\n",
      "validation Loss: 0.0015 Acc: 82.2262\n",
      "Epoch 61/99\n",
      "training Loss: 0.0015 Acc: 80.7273\n",
      "validation Loss: 0.0015 Acc: 82.2024\n",
      "Epoch 62/99\n",
      "training Loss: 0.0015 Acc: 80.6381\n",
      "validation Loss: 0.0015 Acc: 82.1905\n",
      "Epoch 63/99\n",
      "training Loss: 0.0015 Acc: 80.6797\n",
      "validation Loss: 0.0015 Acc: 82.1786\n",
      "Early stopped.\n",
      "Best val acc: 82.535714\n",
      "----------\n",
      "Average best_acc across k-fold: 76.1404761905\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 637, 'dropout': 0.5789762929956876, 'batch_size': 348, 'max_depth': 5}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 50.0119\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.0119\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.0030\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 50.0089\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 50.0089\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 50.0506\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 50.1101\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 50.3958\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 51.2738\n",
      "validation Loss: 0.0020 Acc: 50.1190\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 53.5893\n",
      "validation Loss: 0.0020 Acc: 51.0117\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 58.7143\n",
      "validation Loss: 0.0020 Acc: 61.0807\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 64.2917\n",
      "validation Loss: 0.0019 Acc: 77.4220\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0018 Acc: 68.8244\n",
      "validation Loss: 0.0018 Acc: 80.5165\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 71.3214\n",
      "validation Loss: 0.0017 Acc: 80.7070\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 73.0030\n",
      "validation Loss: 0.0016 Acc: 81.1235\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 74.0327\n",
      "validation Loss: 0.0016 Acc: 81.4330\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 74.7470\n",
      "validation Loss: 0.0015 Acc: 81.6591\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0016 Acc: 75.5060\n",
      "validation Loss: 0.0015 Acc: 82.2304\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0016 Acc: 75.7798\n",
      "validation Loss: 0.0015 Acc: 82.2185\n",
      "Epoch 19/99\n",
      "training Loss: 0.0016 Acc: 75.5774\n",
      "validation Loss: 0.0014 Acc: 82.1114\n",
      "Epoch 20/99\n",
      "training Loss: 0.0016 Acc: 76.4018\n",
      "validation Loss: 0.0014 Acc: 82.2423\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 76.5833\n",
      "validation Loss: 0.0013 Acc: 82.3375\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 77.0625\n",
      "validation Loss: 0.0013 Acc: 82.3613\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 77.1280\n",
      "validation Loss: 0.0013 Acc: 82.4923\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 77.2321\n",
      "validation Loss: 0.0013 Acc: 82.6232\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 77.6488\n",
      "validation Loss: 0.0013 Acc: 82.7184\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 78.1101\n",
      "validation Loss: 0.0012 Acc: 82.7898\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 78.1994\n",
      "validation Loss: 0.0012 Acc: 82.8136\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 78.1310\n",
      "validation Loss: 0.0012 Acc: 82.7779\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 78.3155\n",
      "validation Loss: 0.0012 Acc: 82.9207\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 78.8571\n",
      "validation Loss: 0.0012 Acc: 82.8493\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 78.8125\n",
      "validation Loss: 0.0012 Acc: 82.9207\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 78.9970\n",
      "validation Loss: 0.0012 Acc: 82.8731\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 79.3571\n",
      "validation Loss: 0.0012 Acc: 82.8850\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 79.0923\n",
      "validation Loss: 0.0012 Acc: 82.9088\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 79.4226\n",
      "validation Loss: 0.0012 Acc: 82.8136\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 79.4851\n",
      "validation Loss: 0.0012 Acc: 82.9564\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 79.8958\n",
      "validation Loss: 0.0012 Acc: 82.8255\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 80.1101\n",
      "validation Loss: 0.0012 Acc: 82.9921\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 80.0089\n",
      "validation Loss: 0.0012 Acc: 82.9564\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 80.1161\n",
      "validation Loss: 0.0011 Acc: 83.0279\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 80.2202\n",
      "validation Loss: 0.0011 Acc: 83.0636\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 80.2381\n",
      "validation Loss: 0.0011 Acc: 83.1350\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 80.3214\n",
      "validation Loss: 0.0011 Acc: 83.1350\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 80.2173\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 80.6071\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 80.4851\n",
      "validation Loss: 0.0011 Acc: 83.1350\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 80.7321\n",
      "validation Loss: 0.0011 Acc: 83.1826\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 80.5119\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 80.9881\n",
      "validation Loss: 0.0011 Acc: 83.1588\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 80.7708\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 80.9613\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 52/99\n",
      "training Loss: 0.0013 Acc: 81.2054\n",
      "validation Loss: 0.0011 Acc: 83.2183\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0013 Acc: 81.0536\n",
      "validation Loss: 0.0011 Acc: 83.2183\n",
      "Epoch 54/99\n",
      "training Loss: 0.0013 Acc: 80.9970\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 55/99\n",
      "training Loss: 0.0013 Acc: 81.0298\n",
      "validation Loss: 0.0011 Acc: 83.1469\n",
      "Epoch 56/99\n",
      "training Loss: 0.0013 Acc: 81.1429\n",
      "validation Loss: 0.0011 Acc: 83.1469\n",
      "Epoch 57/99\n",
      "training Loss: 0.0013 Acc: 81.1012\n",
      "validation Loss: 0.0011 Acc: 83.1826\n",
      "Epoch 58/99\n",
      "training Loss: 0.0013 Acc: 81.2321\n",
      "validation Loss: 0.0011 Acc: 83.1350\n",
      "Epoch 59/99\n",
      "training Loss: 0.0013 Acc: 81.2440\n",
      "validation Loss: 0.0011 Acc: 83.1707\n",
      "Epoch 60/99\n",
      "training Loss: 0.0013 Acc: 81.2530\n",
      "validation Loss: 0.0011 Acc: 83.1826\n",
      "Epoch 61/99\n",
      "training Loss: 0.0013 Acc: 81.5089\n",
      "validation Loss: 0.0011 Acc: 83.2302\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0013 Acc: 81.4435\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Epoch 63/99\n",
      "training Loss: 0.0013 Acc: 81.4196\n",
      "validation Loss: 0.0011 Acc: 83.2302\n",
      "Epoch 64/99\n",
      "training Loss: 0.0013 Acc: 81.5833\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 65/99\n",
      "training Loss: 0.0013 Acc: 81.4583\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0013 Acc: 81.5833\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Epoch 67/99\n",
      "training Loss: 0.0013 Acc: 81.4613\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Epoch 68/99\n",
      "training Loss: 0.0013 Acc: 81.1905\n",
      "validation Loss: 0.0011 Acc: 83.1588\n",
      "Epoch 69/99\n",
      "training Loss: 0.0013 Acc: 81.6964\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Epoch 70/99\n",
      "training Loss: 0.0012 Acc: 81.5774\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Epoch 71/99\n",
      "training Loss: 0.0012 Acc: 81.6071\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Saving..\n",
      "Epoch 72/99\n",
      "training Loss: 0.0012 Acc: 81.6696\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 73/99\n",
      "training Loss: 0.0013 Acc: 81.6369\n",
      "validation Loss: 0.0011 Acc: 83.1826\n",
      "Epoch 74/99\n",
      "training Loss: 0.0012 Acc: 81.4494\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Epoch 75/99\n",
      "training Loss: 0.0012 Acc: 81.6935\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Epoch 76/99\n",
      "training Loss: 0.0012 Acc: 81.7024\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Epoch 77/99\n",
      "training Loss: 0.0012 Acc: 81.6429\n",
      "validation Loss: 0.0011 Acc: 83.1945\n",
      "Epoch 78/99\n",
      "training Loss: 0.0012 Acc: 81.8631\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 79/99\n",
      "training Loss: 0.0012 Acc: 81.8304\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 80/99\n",
      "training Loss: 0.0012 Acc: 81.6518\n",
      "validation Loss: 0.0011 Acc: 83.2302\n",
      "Epoch 81/99\n",
      "training Loss: 0.0012 Acc: 81.9881\n",
      "validation Loss: 0.0011 Acc: 83.2778\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0012 Acc: 81.6875\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 83/99\n",
      "training Loss: 0.0012 Acc: 81.5833\n",
      "validation Loss: 0.0011 Acc: 83.2778\n",
      "Epoch 84/99\n",
      "training Loss: 0.0012 Acc: 81.7649\n",
      "validation Loss: 0.0011 Acc: 83.2897\n",
      "Saving..\n",
      "Epoch 85/99\n",
      "training Loss: 0.0012 Acc: 81.7232\n",
      "validation Loss: 0.0011 Acc: 83.2897\n",
      "Epoch 86/99\n",
      "training Loss: 0.0012 Acc: 81.8661\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 87/99\n",
      "training Loss: 0.0012 Acc: 81.7024\n",
      "validation Loss: 0.0011 Acc: 83.3016\n",
      "Saving..\n",
      "Epoch 88/99\n",
      "training Loss: 0.0012 Acc: 81.8333\n",
      "validation Loss: 0.0011 Acc: 83.2778\n",
      "Epoch 89/99\n",
      "training Loss: 0.0012 Acc: 82.0446\n",
      "validation Loss: 0.0011 Acc: 83.2183\n",
      "Epoch 90/99\n",
      "training Loss: 0.0012 Acc: 81.7619\n",
      "validation Loss: 0.0011 Acc: 83.2778\n",
      "Epoch 91/99\n",
      "training Loss: 0.0012 Acc: 82.0833\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 92/99\n",
      "training Loss: 0.0012 Acc: 81.7798\n",
      "validation Loss: 0.0011 Acc: 83.3016\n",
      "Epoch 93/99\n",
      "training Loss: 0.0012 Acc: 81.7411\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Epoch 94/99\n",
      "training Loss: 0.0012 Acc: 81.9494\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 95/99\n",
      "training Loss: 0.0012 Acc: 82.1220\n",
      "validation Loss: 0.0011 Acc: 83.2659\n",
      "Epoch 96/99\n",
      "training Loss: 0.0012 Acc: 81.8512\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 97/99\n",
      "training Loss: 0.0012 Acc: 82.0952\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 98/99\n",
      "training Loss: 0.0012 Acc: 82.1131\n",
      "validation Loss: 0.0011 Acc: 83.2540\n",
      "Epoch 99/99\n",
      "training Loss: 0.0012 Acc: 82.0030\n",
      "validation Loss: 0.0011 Acc: 83.2064\n",
      "Best val acc: 83.301595\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 50.0030\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.0000\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.0863\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 50.2500\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 51.2380\n",
      "validation Loss: 0.0020 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 53.4700\n",
      "validation Loss: 0.0020 Acc: 49.9881\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 56.7228\n",
      "validation Loss: 0.0020 Acc: 57.6190\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 61.2642\n",
      "validation Loss: 0.0019 Acc: 76.3690\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 64.8295\n",
      "validation Loss: 0.0018 Acc: 77.6429\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 67.9483\n",
      "validation Loss: 0.0018 Acc: 78.5357\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 70.1536\n",
      "validation Loss: 0.0017 Acc: 78.8929\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 71.1862\n",
      "validation Loss: 0.0017 Acc: 79.2024\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 72.7248\n",
      "validation Loss: 0.0016 Acc: 79.4643\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 73.4986\n",
      "validation Loss: 0.0016 Acc: 79.7381\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 74.1920\n",
      "validation Loss: 0.0016 Acc: 79.9881\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 74.8289\n",
      "validation Loss: 0.0016 Acc: 80.2500\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 75.8824\n",
      "validation Loss: 0.0015 Acc: 80.5000\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0016 Acc: 76.4746\n",
      "validation Loss: 0.0015 Acc: 80.5476\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0016 Acc: 76.7157\n",
      "validation Loss: 0.0015 Acc: 80.7143\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0016 Acc: 77.1650\n",
      "validation Loss: 0.0015 Acc: 80.9167\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0016 Acc: 77.3377\n",
      "validation Loss: 0.0015 Acc: 81.0238\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0016 Acc: 77.4002\n",
      "validation Loss: 0.0014 Acc: 81.1667\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0016 Acc: 77.8495\n",
      "validation Loss: 0.0014 Acc: 81.1548\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 77.7543\n",
      "validation Loss: 0.0014 Acc: 81.2976\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 77.6472\n",
      "validation Loss: 0.0014 Acc: 81.3690\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 78.0906\n",
      "validation Loss: 0.0014 Acc: 81.3810\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 78.2186\n",
      "validation Loss: 0.0013 Acc: 81.6310\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 78.5935\n",
      "validation Loss: 0.0013 Acc: 81.6905\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 78.3852\n",
      "validation Loss: 0.0013 Acc: 81.7738\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 78.8405\n",
      "validation Loss: 0.0013 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 78.5370\n",
      "validation Loss: 0.0013 Acc: 82.0714\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 78.8763\n",
      "validation Loss: 0.0013 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 79.2572\n",
      "validation Loss: 0.0013 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 79.4060\n",
      "validation Loss: 0.0012 Acc: 82.2619\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 79.5994\n",
      "validation Loss: 0.0012 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 79.3971\n",
      "validation Loss: 0.0012 Acc: 82.3333\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 79.5726\n",
      "validation Loss: 0.0012 Acc: 82.3690\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 79.5637\n",
      "validation Loss: 0.0012 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 79.6768\n",
      "validation Loss: 0.0012 Acc: 82.3810\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 79.9506\n",
      "validation Loss: 0.0012 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 79.7214\n",
      "validation Loss: 0.0012 Acc: 82.5119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 80.1262\n",
      "validation Loss: 0.0012 Acc: 82.5714\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 80.1619\n",
      "validation Loss: 0.0012 Acc: 82.5476\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 80.0815\n",
      "validation Loss: 0.0012 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 80.3553\n",
      "validation Loss: 0.0012 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 80.2512\n",
      "validation Loss: 0.0012 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 80.4119\n",
      "validation Loss: 0.0012 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 80.3464\n",
      "validation Loss: 0.0012 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 80.3137\n",
      "validation Loss: 0.0012 Acc: 82.7143\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 80.6767\n",
      "validation Loss: 0.0012 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 80.7065\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 80.6142\n",
      "validation Loss: 0.0012 Acc: 82.8333\n",
      "Epoch 52/99\n",
      "training Loss: 0.0013 Acc: 80.8047\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0013 Acc: 80.9535\n",
      "validation Loss: 0.0011 Acc: 82.8095\n",
      "Epoch 54/99\n",
      "training Loss: 0.0013 Acc: 80.7660\n",
      "validation Loss: 0.0011 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0013 Acc: 80.8523\n",
      "validation Loss: 0.0011 Acc: 82.9167\n",
      "Epoch 56/99\n",
      "training Loss: 0.0013 Acc: 80.7363\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0013 Acc: 80.8851\n",
      "validation Loss: 0.0011 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0013 Acc: 80.7065\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0013 Acc: 81.0755\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Epoch 60/99\n",
      "training Loss: 0.0013 Acc: 81.1470\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Epoch 61/99\n",
      "training Loss: 0.0013 Acc: 81.2005\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 62/99\n",
      "training Loss: 0.0013 Acc: 81.1678\n",
      "validation Loss: 0.0011 Acc: 82.9524\n",
      "Epoch 63/99\n",
      "training Loss: 0.0013 Acc: 81.2154\n",
      "validation Loss: 0.0011 Acc: 82.9286\n",
      "Epoch 64/99\n",
      "training Loss: 0.0013 Acc: 81.2362\n",
      "validation Loss: 0.0011 Acc: 82.9524\n",
      "Epoch 65/99\n",
      "training Loss: 0.0013 Acc: 81.2600\n",
      "validation Loss: 0.0011 Acc: 82.9286\n",
      "Epoch 66/99\n",
      "training Loss: 0.0013 Acc: 81.1559\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 67/99\n",
      "training Loss: 0.0013 Acc: 81.3434\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 68/99\n",
      "training Loss: 0.0013 Acc: 81.3493\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 69/99\n",
      "training Loss: 0.0013 Acc: 81.3255\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Epoch 70/99\n",
      "training Loss: 0.0013 Acc: 81.4535\n",
      "validation Loss: 0.0011 Acc: 82.9167\n",
      "Epoch 71/99\n",
      "training Loss: 0.0013 Acc: 81.3940\n",
      "validation Loss: 0.0011 Acc: 82.8095\n",
      "Epoch 72/99\n",
      "training Loss: 0.0013 Acc: 81.6439\n",
      "validation Loss: 0.0011 Acc: 82.9048\n",
      "Epoch 73/99\n",
      "training Loss: 0.0013 Acc: 81.6499\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 74/99\n",
      "training Loss: 0.0012 Acc: 81.6142\n",
      "validation Loss: 0.0011 Acc: 82.8452\n",
      "Epoch 75/99\n",
      "training Loss: 0.0012 Acc: 81.6469\n",
      "validation Loss: 0.0011 Acc: 82.7976\n",
      "Epoch 76/99\n",
      "training Loss: 0.0013 Acc: 81.4237\n",
      "validation Loss: 0.0011 Acc: 82.8690\n",
      "Epoch 77/99\n",
      "training Loss: 0.0012 Acc: 81.6053\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 78/99\n",
      "training Loss: 0.0012 Acc: 81.5993\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Early stopped.\n",
      "Best val acc: 83.035714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 50.0030\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.0179\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.0268\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 50.0655\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 50.2500\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 50.7351\n",
      "validation Loss: 0.0020 Acc: 50.0595\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 52.5862\n",
      "validation Loss: 0.0020 Acc: 50.5000\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 55.8211\n",
      "validation Loss: 0.0020 Acc: 54.8333\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 60.5797\n",
      "validation Loss: 0.0019 Acc: 68.9048\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 65.2521\n",
      "validation Loss: 0.0018 Acc: 78.7143\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 69.1417\n",
      "validation Loss: 0.0018 Acc: 79.0476\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 71.6535\n",
      "validation Loss: 0.0017 Acc: 79.2024\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 73.2248\n",
      "validation Loss: 0.0016 Acc: 80.0000\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 74.5343\n",
      "validation Loss: 0.0016 Acc: 80.4524\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0016 Acc: 75.3705\n",
      "validation Loss: 0.0015 Acc: 80.9167\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0016 Acc: 76.3407\n",
      "validation Loss: 0.0015 Acc: 81.2976\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0016 Acc: 76.7216\n",
      "validation Loss: 0.0014 Acc: 81.4524\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 77.0877\n",
      "validation Loss: 0.0014 Acc: 81.4524\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 77.4269\n",
      "validation Loss: 0.0014 Acc: 81.4881\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 77.6591\n",
      "validation Loss: 0.0013 Acc: 81.5238\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 77.9954\n",
      "validation Loss: 0.0013 Acc: 81.5119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 78.3971\n",
      "validation Loss: 0.0013 Acc: 81.5119\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 78.5935\n",
      "validation Loss: 0.0013 Acc: 81.6667\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 78.8763\n",
      "validation Loss: 0.0013 Acc: 81.5119\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 78.7751\n",
      "validation Loss: 0.0012 Acc: 81.5714\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 78.9090\n",
      "validation Loss: 0.0012 Acc: 81.7500\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 79.3703\n",
      "validation Loss: 0.0012 Acc: 81.7976\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 79.2840\n",
      "validation Loss: 0.0012 Acc: 81.8929\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 79.4149\n",
      "validation Loss: 0.0012 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 79.4655\n",
      "validation Loss: 0.0012 Acc: 82.0119\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 79.9893\n",
      "validation Loss: 0.0012 Acc: 82.0000\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 79.9506\n",
      "validation Loss: 0.0012 Acc: 82.0357\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 79.8583\n",
      "validation Loss: 0.0012 Acc: 82.0476\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 80.1173\n",
      "validation Loss: 0.0012 Acc: 82.0357\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 79.9476\n",
      "validation Loss: 0.0012 Acc: 82.1071\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 80.6559\n",
      "validation Loss: 0.0012 Acc: 82.1429\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 80.3226\n",
      "validation Loss: 0.0012 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 80.5190\n",
      "validation Loss: 0.0012 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 80.5339\n",
      "validation Loss: 0.0012 Acc: 82.2738\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 80.7154\n",
      "validation Loss: 0.0012 Acc: 82.3452\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 80.6886\n",
      "validation Loss: 0.0012 Acc: 82.3571\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 80.4922\n",
      "validation Loss: 0.0012 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 80.7541\n",
      "validation Loss: 0.0012 Acc: 82.4286\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 81.0160\n",
      "validation Loss: 0.0012 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 80.7839\n",
      "validation Loss: 0.0012 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 81.0517\n",
      "validation Loss: 0.0012 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 80.8613\n",
      "validation Loss: 0.0012 Acc: 82.5357\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 80.8315\n",
      "validation Loss: 0.0012 Acc: 82.5595\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 80.8613\n",
      "validation Loss: 0.0012 Acc: 82.3333\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 81.1708\n",
      "validation Loss: 0.0011 Acc: 82.4643\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 81.1529\n",
      "validation Loss: 0.0012 Acc: 82.3571\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 81.0249\n",
      "validation Loss: 0.0011 Acc: 82.5595\n",
      "Epoch 52/99\n",
      "training Loss: 0.0013 Acc: 81.0607\n",
      "validation Loss: 0.0011 Acc: 82.5357\n",
      "Epoch 53/99\n",
      "training Loss: 0.0013 Acc: 81.0726\n",
      "validation Loss: 0.0012 Acc: 82.5595\n",
      "Epoch 54/99\n",
      "training Loss: 0.0013 Acc: 81.2243\n",
      "validation Loss: 0.0011 Acc: 82.5357\n",
      "Epoch 55/99\n",
      "training Loss: 0.0013 Acc: 81.1529\n",
      "validation Loss: 0.0012 Acc: 82.5476\n",
      "Epoch 56/99\n",
      "training Loss: 0.0013 Acc: 81.2690\n",
      "validation Loss: 0.0011 Acc: 82.5238\n",
      "Epoch 57/99\n",
      "training Loss: 0.0013 Acc: 81.2481\n",
      "validation Loss: 0.0011 Acc: 82.5000\n",
      "Epoch 58/99\n",
      "training Loss: 0.0013 Acc: 81.4416\n",
      "validation Loss: 0.0011 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0013 Acc: 81.3315\n",
      "validation Loss: 0.0011 Acc: 82.5714\n",
      "Epoch 60/99\n",
      "training Loss: 0.0013 Acc: 81.1470\n",
      "validation Loss: 0.0011 Acc: 82.5714\n",
      "Epoch 61/99\n",
      "training Loss: 0.0013 Acc: 81.4624\n",
      "validation Loss: 0.0011 Acc: 82.5476\n",
      "Epoch 62/99\n",
      "training Loss: 0.0013 Acc: 81.4059\n",
      "validation Loss: 0.0011 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0013 Acc: 81.2749\n",
      "validation Loss: 0.0011 Acc: 82.6190\n",
      "Epoch 64/99\n",
      "training Loss: 0.0013 Acc: 81.5011\n",
      "validation Loss: 0.0011 Acc: 82.5357\n",
      "Epoch 65/99\n",
      "training Loss: 0.0013 Acc: 81.4624\n",
      "validation Loss: 0.0011 Acc: 82.5833\n",
      "Epoch 66/99\n",
      "training Loss: 0.0012 Acc: 81.5517\n",
      "validation Loss: 0.0011 Acc: 82.5595\n",
      "Epoch 67/99\n",
      "training Loss: 0.0012 Acc: 81.6053\n",
      "validation Loss: 0.0011 Acc: 82.6548\n",
      "Epoch 68/99\n",
      "training Loss: 0.0012 Acc: 81.6261\n",
      "validation Loss: 0.0011 Acc: 82.6071\n",
      "Epoch 69/99\n",
      "training Loss: 0.0013 Acc: 81.6172\n",
      "validation Loss: 0.0011 Acc: 82.5952\n",
      "Epoch 70/99\n",
      "training Loss: 0.0012 Acc: 81.6053\n",
      "validation Loss: 0.0011 Acc: 82.6548\n",
      "Epoch 71/99\n",
      "training Loss: 0.0012 Acc: 81.7005\n",
      "validation Loss: 0.0011 Acc: 82.6429\n",
      "Epoch 72/99\n",
      "training Loss: 0.0012 Acc: 81.6678\n",
      "validation Loss: 0.0011 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 73/99\n",
      "training Loss: 0.0012 Acc: 81.7570\n",
      "validation Loss: 0.0011 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0012 Acc: 81.8195\n",
      "validation Loss: 0.0011 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0012 Acc: 81.4773\n",
      "validation Loss: 0.0011 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0012 Acc: 82.1142\n",
      "validation Loss: 0.0011 Acc: 82.7738\n",
      "Epoch 77/99\n",
      "training Loss: 0.0012 Acc: 81.6529\n",
      "validation Loss: 0.0011 Acc: 82.6310\n",
      "Epoch 78/99\n",
      "training Loss: 0.0012 Acc: 81.6291\n",
      "validation Loss: 0.0011 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0012 Acc: 81.8910\n",
      "validation Loss: 0.0011 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 80/99\n",
      "training Loss: 0.0012 Acc: 81.8285\n",
      "validation Loss: 0.0011 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 81/99\n",
      "training Loss: 0.0012 Acc: 81.7511\n",
      "validation Loss: 0.0011 Acc: 82.8571\n",
      "Epoch 82/99\n",
      "training Loss: 0.0012 Acc: 81.9743\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Epoch 83/99\n",
      "training Loss: 0.0012 Acc: 81.8552\n",
      "validation Loss: 0.0011 Acc: 82.8452\n",
      "Epoch 84/99\n",
      "training Loss: 0.0012 Acc: 81.9088\n",
      "validation Loss: 0.0011 Acc: 82.8690\n",
      "Epoch 85/99\n",
      "training Loss: 0.0012 Acc: 81.7957\n",
      "validation Loss: 0.0011 Acc: 82.9048\n",
      "Epoch 86/99\n",
      "training Loss: 0.0012 Acc: 81.8166\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0012 Acc: 81.7303\n",
      "validation Loss: 0.0011 Acc: 82.8690\n",
      "Epoch 88/99\n",
      "training Loss: 0.0012 Acc: 82.0189\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Epoch 89/99\n",
      "training Loss: 0.0012 Acc: 81.8344\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 90/99\n",
      "training Loss: 0.0012 Acc: 82.1112\n",
      "validation Loss: 0.0011 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 91/99\n",
      "training Loss: 0.0012 Acc: 81.9773\n",
      "validation Loss: 0.0011 Acc: 82.8690\n",
      "Epoch 92/99\n",
      "training Loss: 0.0012 Acc: 82.0517\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Epoch 93/99\n",
      "training Loss: 0.0012 Acc: 82.1082\n",
      "validation Loss: 0.0011 Acc: 82.9167\n",
      "Epoch 94/99\n",
      "training Loss: 0.0012 Acc: 82.1320\n",
      "validation Loss: 0.0011 Acc: 82.9286\n",
      "Epoch 95/99\n",
      "training Loss: 0.0012 Acc: 82.1082\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 96/99\n",
      "training Loss: 0.0012 Acc: 82.1171\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 97/99\n",
      "training Loss: 0.0012 Acc: 81.8166\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 98/99\n",
      "training Loss: 0.0012 Acc: 81.8939\n",
      "validation Loss: 0.0011 Acc: 82.9167\n",
      "Epoch 99/99\n",
      "training Loss: 0.0012 Acc: 81.7779\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Best val acc: 82.964286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 50.0268\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 50.2172\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 50.7172\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 51.2350\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 53.5593\n",
      "validation Loss: 0.0020 Acc: 50.3333\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 56.7377\n",
      "validation Loss: 0.0020 Acc: 55.2143\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 61.9904\n",
      "validation Loss: 0.0019 Acc: 73.0714\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 66.3591\n",
      "validation Loss: 0.0018 Acc: 79.4762\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0018 Acc: 69.7488\n",
      "validation Loss: 0.0017 Acc: 79.4405\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 72.1296\n",
      "validation Loss: 0.0017 Acc: 79.9167\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0017 Acc: 74.0879\n",
      "validation Loss: 0.0016 Acc: 80.4405\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0017 Acc: 75.6503\n",
      "validation Loss: 0.0016 Acc: 80.9643\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 76.0401\n",
      "validation Loss: 0.0016 Acc: 81.5238\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0016 Acc: 77.1531\n",
      "validation Loss: 0.0015 Acc: 81.9167\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0016 Acc: 77.9894\n",
      "validation Loss: 0.0015 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0016 Acc: 78.0162\n",
      "validation Loss: 0.0015 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0016 Acc: 78.6709\n",
      "validation Loss: 0.0015 Acc: 82.4405\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0016 Acc: 79.2780\n",
      "validation Loss: 0.0014 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 79.5816\n",
      "validation Loss: 0.0014 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 79.5726\n",
      "validation Loss: 0.0014 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 79.8197\n",
      "validation Loss: 0.0014 Acc: 82.6667\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 80.0071\n",
      "validation Loss: 0.0014 Acc: 82.6905\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 79.9327\n",
      "validation Loss: 0.0014 Acc: 82.7143\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 79.8226\n",
      "validation Loss: 0.0014 Acc: 82.7381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 80.0548\n",
      "validation Loss: 0.0013 Acc: 82.6786\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 80.1113\n",
      "validation Loss: 0.0013 Acc: 82.4048\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 79.7899\n",
      "validation Loss: 0.0013 Acc: 82.4762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 80.2601\n",
      "validation Loss: 0.0013 Acc: 82.4881\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 80.0042\n",
      "validation Loss: 0.0013 Acc: 82.2976\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 79.8435\n",
      "validation Loss: 0.0013 Acc: 82.3571\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 80.4506\n",
      "validation Loss: 0.0012 Acc: 82.4286\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 80.4297\n",
      "validation Loss: 0.0012 Acc: 82.3810\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 80.0161\n",
      "validation Loss: 0.0012 Acc: 82.5357\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 80.5160\n",
      "validation Loss: 0.0012 Acc: 82.7262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 80.3672\n",
      "validation Loss: 0.0012 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 80.5071\n",
      "validation Loss: 0.0012 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 80.7601\n",
      "validation Loss: 0.0012 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 80.8196\n",
      "validation Loss: 0.0012 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 80.8017\n",
      "validation Loss: 0.0012 Acc: 83.0119\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 80.8226\n",
      "validation Loss: 0.0012 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 80.9357\n",
      "validation Loss: 0.0012 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 81.0487\n",
      "validation Loss: 0.0012 Acc: 83.2262\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 81.0279\n",
      "validation Loss: 0.0012 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 81.0934\n",
      "validation Loss: 0.0011 Acc: 83.2857\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 81.1618\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 81.2660\n",
      "validation Loss: 0.0012 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 81.4624\n",
      "validation Loss: 0.0011 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 81.3612\n",
      "validation Loss: 0.0011 Acc: 83.3690\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 81.2273\n",
      "validation Loss: 0.0011 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 81.4416\n",
      "validation Loss: 0.0011 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 81.3791\n",
      "validation Loss: 0.0011 Acc: 83.4405\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 81.5636\n",
      "validation Loss: 0.0011 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0013 Acc: 81.4207\n",
      "validation Loss: 0.0011 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0013 Acc: 81.4951\n",
      "validation Loss: 0.0011 Acc: 83.4881\n",
      "Epoch 54/99\n",
      "training Loss: 0.0013 Acc: 81.7243\n",
      "validation Loss: 0.0011 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0013 Acc: 81.7035\n",
      "validation Loss: 0.0011 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0013 Acc: 81.5309\n",
      "validation Loss: 0.0011 Acc: 83.5476\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 81.7570\n",
      "validation Loss: 0.0011 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0013 Acc: 81.6826\n",
      "validation Loss: 0.0011 Acc: 83.6310\n",
      "Epoch 59/99\n",
      "training Loss: 0.0013 Acc: 81.6410\n",
      "validation Loss: 0.0011 Acc: 83.6429\n",
      "Epoch 60/99\n",
      "training Loss: 0.0012 Acc: 81.6886\n",
      "validation Loss: 0.0011 Acc: 83.6190\n",
      "Epoch 61/99\n",
      "training Loss: 0.0012 Acc: 81.8404\n",
      "validation Loss: 0.0011 Acc: 83.6548\n",
      "Epoch 62/99\n",
      "training Loss: 0.0012 Acc: 81.7035\n",
      "validation Loss: 0.0011 Acc: 83.5952\n",
      "Epoch 63/99\n",
      "training Loss: 0.0012 Acc: 81.6469\n",
      "validation Loss: 0.0011 Acc: 83.5714\n",
      "Epoch 64/99\n",
      "training Loss: 0.0012 Acc: 81.8880\n",
      "validation Loss: 0.0011 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0012 Acc: 81.8642\n",
      "validation Loss: 0.0011 Acc: 83.5952\n",
      "Epoch 66/99\n",
      "training Loss: 0.0012 Acc: 81.8285\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 67/99\n",
      "training Loss: 0.0012 Acc: 81.8761\n",
      "validation Loss: 0.0011 Acc: 83.5595\n",
      "Epoch 68/99\n",
      "training Loss: 0.0012 Acc: 81.8076\n",
      "validation Loss: 0.0011 Acc: 83.5476\n",
      "Epoch 69/99\n",
      "training Loss: 0.0012 Acc: 81.7898\n",
      "validation Loss: 0.0011 Acc: 83.6429\n",
      "Epoch 70/99\n",
      "training Loss: 0.0012 Acc: 82.0130\n",
      "validation Loss: 0.0011 Acc: 83.6667\n",
      "Epoch 71/99\n",
      "training Loss: 0.0012 Acc: 81.8314\n",
      "validation Loss: 0.0011 Acc: 83.6071\n",
      "Epoch 72/99\n",
      "training Loss: 0.0012 Acc: 81.8285\n",
      "validation Loss: 0.0011 Acc: 83.5833\n",
      "Epoch 73/99\n",
      "training Loss: 0.0012 Acc: 81.8106\n",
      "validation Loss: 0.0011 Acc: 83.6429\n",
      "Epoch 74/99\n",
      "training Loss: 0.0012 Acc: 81.9207\n",
      "validation Loss: 0.0011 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0012 Acc: 82.0576\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 76/99\n",
      "training Loss: 0.0012 Acc: 81.9892\n",
      "validation Loss: 0.0011 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0012 Acc: 82.0576\n",
      "validation Loss: 0.0011 Acc: 83.7262\n",
      "Epoch 78/99\n",
      "training Loss: 0.0012 Acc: 81.8433\n",
      "validation Loss: 0.0011 Acc: 83.7500\n",
      "Epoch 79/99\n",
      "training Loss: 0.0012 Acc: 82.1350\n",
      "validation Loss: 0.0011 Acc: 83.6310\n",
      "Epoch 80/99\n",
      "training Loss: 0.0012 Acc: 81.9564\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 81/99\n",
      "training Loss: 0.0012 Acc: 82.1231\n",
      "validation Loss: 0.0011 Acc: 83.7143\n",
      "Epoch 82/99\n",
      "training Loss: 0.0012 Acc: 82.0904\n",
      "validation Loss: 0.0011 Acc: 83.7500\n",
      "Epoch 83/99\n",
      "training Loss: 0.0012 Acc: 81.8285\n",
      "validation Loss: 0.0011 Acc: 83.6310\n",
      "Epoch 84/99\n",
      "training Loss: 0.0012 Acc: 82.0844\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 85/99\n",
      "training Loss: 0.0012 Acc: 82.0130\n",
      "validation Loss: 0.0011 Acc: 83.6310\n",
      "Epoch 86/99\n",
      "training Loss: 0.0012 Acc: 81.9802\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 87/99\n",
      "training Loss: 0.0012 Acc: 81.9296\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 88/99\n",
      "training Loss: 0.0012 Acc: 81.9892\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 89/99\n",
      "training Loss: 0.0012 Acc: 81.9207\n",
      "validation Loss: 0.0011 Acc: 83.7143\n",
      "Epoch 90/99\n",
      "training Loss: 0.0012 Acc: 82.2689\n",
      "validation Loss: 0.0011 Acc: 83.7500\n",
      "Epoch 91/99\n",
      "training Loss: 0.0012 Acc: 81.8820\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 92/99\n",
      "training Loss: 0.0012 Acc: 82.1737\n",
      "validation Loss: 0.0011 Acc: 83.7262\n",
      "Epoch 93/99\n",
      "training Loss: 0.0012 Acc: 82.1588\n",
      "validation Loss: 0.0011 Acc: 83.7024\n",
      "Epoch 94/99\n",
      "training Loss: 0.0012 Acc: 82.1112\n",
      "validation Loss: 0.0011 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0012 Acc: 82.3433\n",
      "validation Loss: 0.0011 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 96/99\n",
      "training Loss: 0.0012 Acc: 82.1856\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 97/99\n",
      "training Loss: 0.0012 Acc: 82.1171\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 98/99\n",
      "training Loss: 0.0012 Acc: 82.1767\n",
      "validation Loss: 0.0011 Acc: 83.7024\n",
      "Epoch 99/99\n",
      "training Loss: 0.0012 Acc: 82.2153\n",
      "validation Loss: 0.0011 Acc: 83.7143\n",
      "Best val acc: 83.809524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 50.3452\n",
      "validation Loss: 0.0021 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 51.3660\n",
      "validation Loss: 0.0021 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 52.0237\n",
      "validation Loss: 0.0021 Acc: 50.0833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 54.2855\n",
      "validation Loss: 0.0021 Acc: 50.9524\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 57.0621\n",
      "validation Loss: 0.0020 Acc: 61.3690\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 60.4904\n",
      "validation Loss: 0.0020 Acc: 77.1905\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0019 Acc: 63.8772\n",
      "validation Loss: 0.0020 Acc: 80.1310\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 66.6954\n",
      "validation Loss: 0.0019 Acc: 79.0119\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 68.5554\n",
      "validation Loss: 0.0019 Acc: 78.9643\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 70.9749\n",
      "validation Loss: 0.0018 Acc: 79.4762\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 72.2011\n",
      "validation Loss: 0.0017 Acc: 80.0476\n",
      "Epoch 11/99\n",
      "training Loss: 0.0017 Acc: 74.3200\n",
      "validation Loss: 0.0017 Acc: 80.9048\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 75.2812\n",
      "validation Loss: 0.0017 Acc: 81.4405\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 76.1175\n",
      "validation Loss: 0.0016 Acc: 81.6667\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 76.9805\n",
      "validation Loss: 0.0016 Acc: 82.1310\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0016 Acc: 78.0073\n",
      "validation Loss: 0.0016 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0016 Acc: 78.2781\n",
      "validation Loss: 0.0016 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0016 Acc: 78.7423\n",
      "validation Loss: 0.0016 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0016 Acc: 78.9268\n",
      "validation Loss: 0.0016 Acc: 82.5833\n",
      "Epoch 19/99\n",
      "training Loss: 0.0016 Acc: 79.1828\n",
      "validation Loss: 0.0015 Acc: 82.6429\n",
      "Epoch 20/99\n",
      "training Loss: 0.0016 Acc: 79.1798\n",
      "validation Loss: 0.0015 Acc: 82.5595\n",
      "Epoch 21/99\n",
      "training Loss: 0.0016 Acc: 79.4774\n",
      "validation Loss: 0.0015 Acc: 82.3690\n",
      "Epoch 22/99\n",
      "training Loss: 0.0016 Acc: 78.9745\n",
      "validation Loss: 0.0015 Acc: 82.3810\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 79.0132\n",
      "validation Loss: 0.0015 Acc: 82.3929\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 78.8644\n",
      "validation Loss: 0.0015 Acc: 82.3214\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 78.9655\n",
      "validation Loss: 0.0014 Acc: 82.3452\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 78.7483\n",
      "validation Loss: 0.0014 Acc: 82.3690\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 78.7304\n",
      "validation Loss: 0.0014 Acc: 82.2024\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 78.5162\n",
      "validation Loss: 0.0014 Acc: 82.2381\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 78.3614\n",
      "validation Loss: 0.0013 Acc: 82.2024\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 78.6917\n",
      "validation Loss: 0.0013 Acc: 82.2381\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 78.7721\n",
      "validation Loss: 0.0013 Acc: 82.3214\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 79.0608\n",
      "validation Loss: 0.0013 Acc: 82.2262\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 79.2334\n",
      "validation Loss: 0.0013 Acc: 82.3095\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 79.3048\n",
      "validation Loss: 0.0013 Acc: 82.3333\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 79.3167\n",
      "validation Loss: 0.0013 Acc: 82.3452\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 79.6411\n",
      "validation Loss: 0.0013 Acc: 82.4524\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 79.7066\n",
      "validation Loss: 0.0012 Acc: 82.5595\n",
      "Early stopped.\n",
      "Best val acc: 82.654762\n",
      "----------\n",
      "Average best_acc across k-fold: 83.1531761145\n",
      "New configuration: {'learning_rate': 0.006356147163416415, 'initial_nodes': 705, 'dropout': 0.3974657706000091, 'batch_size': 379, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 80.9226\n",
      "validation Loss: 0.0010 Acc: 83.5872\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1429\n",
      "validation Loss: 0.0010 Acc: 83.3849\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.3125\n",
      "validation Loss: 0.0010 Acc: 83.4206\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.3125\n",
      "validation Loss: 0.0010 Acc: 83.6229\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.4643\n",
      "validation Loss: 0.0010 Acc: 83.4444\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.5357\n",
      "validation Loss: 0.0011 Acc: 83.0636\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.4762\n",
      "validation Loss: 0.0010 Acc: 83.4563\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.6012\n",
      "validation Loss: 0.0010 Acc: 83.5515\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.5923\n",
      "validation Loss: 0.0010 Acc: 83.3135\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.4613\n",
      "validation Loss: 0.0010 Acc: 83.3492\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.9851\n",
      "validation Loss: 0.0010 Acc: 83.3968\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 84.0625\n",
      "validation Loss: 0.0010 Acc: 83.7182\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 84.2589\n",
      "validation Loss: 0.0010 Acc: 83.6110\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 84.0060\n",
      "validation Loss: 0.0010 Acc: 83.8253\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.9107\n",
      "validation Loss: 0.0010 Acc: 83.6587\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.2202\n",
      "validation Loss: 0.0010 Acc: 83.5753\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 84.2560\n",
      "validation Loss: 0.0010 Acc: 83.8372\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.4792\n",
      "validation Loss: 0.0010 Acc: 83.5277\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.6399\n",
      "validation Loss: 0.0010 Acc: 83.8134\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.6815\n",
      "validation Loss: 0.0010 Acc: 83.7896\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.7083\n",
      "validation Loss: 0.0010 Acc: 83.6110\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.7024\n",
      "validation Loss: 0.0010 Acc: 83.6587\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.8988\n",
      "validation Loss: 0.0010 Acc: 83.7658\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 85.0089\n",
      "validation Loss: 0.0010 Acc: 83.6587\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.9167\n",
      "validation Loss: 0.0010 Acc: 83.4920\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 85.0268\n",
      "validation Loss: 0.0010 Acc: 83.6944\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 85.1964\n",
      "validation Loss: 0.0010 Acc: 83.6468\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 85.4107\n",
      "validation Loss: 0.0010 Acc: 83.9562\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 85.4315\n",
      "validation Loss: 0.0010 Acc: 83.7182\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.5268\n",
      "validation Loss: 0.0010 Acc: 83.7063\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 85.5655\n",
      "validation Loss: 0.0010 Acc: 83.7063\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.6310\n",
      "validation Loss: 0.0010 Acc: 83.6110\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.7619\n",
      "validation Loss: 0.0010 Acc: 83.4325\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.8512\n",
      "validation Loss: 0.0010 Acc: 83.5039\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.7500\n",
      "validation Loss: 0.0010 Acc: 83.6110\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 86.0625\n",
      "validation Loss: 0.0010 Acc: 83.6587\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.9018\n",
      "validation Loss: 0.0010 Acc: 83.6229\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.8839\n",
      "validation Loss: 0.0010 Acc: 83.7182\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 85.9940\n",
      "validation Loss: 0.0010 Acc: 83.4920\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 85.9375\n",
      "validation Loss: 0.0010 Acc: 83.6587\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 86.1458\n",
      "validation Loss: 0.0010 Acc: 83.5872\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 86.1131\n",
      "validation Loss: 0.0010 Acc: 83.6110\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 86.1012\n",
      "validation Loss: 0.0010 Acc: 83.6229\n",
      "Epoch 43/99\n",
      "training Loss: 0.0008 Acc: 86.0595\n",
      "validation Loss: 0.0010 Acc: 83.5039\n",
      "Epoch 44/99\n",
      "training Loss: 0.0008 Acc: 86.2113\n",
      "validation Loss: 0.0010 Acc: 83.6348\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 86.2768\n",
      "validation Loss: 0.0010 Acc: 83.5991\n",
      "Epoch 46/99\n",
      "training Loss: 0.0008 Acc: 86.1964\n",
      "validation Loss: 0.0010 Acc: 83.5634\n",
      "Epoch 47/99\n",
      "training Loss: 0.0008 Acc: 86.1399\n",
      "validation Loss: 0.0010 Acc: 83.5991\n",
      "Early stopped.\n",
      "Best val acc: 83.956201\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.6975\n",
      "validation Loss: 0.0010 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.3105\n",
      "validation Loss: 0.0010 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.5099\n",
      "validation Loss: 0.0010 Acc: 83.2381\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.5159\n",
      "validation Loss: 0.0010 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.4712\n",
      "validation Loss: 0.0010 Acc: 82.9524\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.4891\n",
      "validation Loss: 0.0010 Acc: 83.2857\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.6855\n",
      "validation Loss: 0.0010 Acc: 83.5000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.4861\n",
      "validation Loss: 0.0010 Acc: 83.4286\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.7539\n",
      "validation Loss: 0.0010 Acc: 83.4524\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.7956\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.6795\n",
      "validation Loss: 0.0010 Acc: 83.2738\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.5516\n",
      "validation Loss: 0.0010 Acc: 83.5476\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.8313\n",
      "validation Loss: 0.0010 Acc: 83.3214\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 83.7004\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 83.8254\n",
      "validation Loss: 0.0010 Acc: 83.4405\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 83.7361\n",
      "validation Loss: 0.0010 Acc: 83.1905\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 83.5099\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 83.6795\n",
      "validation Loss: 0.0010 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 83.7361\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.1140\n",
      "validation Loss: 0.0010 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.3551\n",
      "validation Loss: 0.0010 Acc: 84.1190\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.2361\n",
      "validation Loss: 0.0010 Acc: 83.7857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.3105\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.3670\n",
      "validation Loss: 0.0010 Acc: 83.4405\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.4087\n",
      "validation Loss: 0.0010 Acc: 83.7262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.7836\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.7836\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.9414\n",
      "validation Loss: 0.0010 Acc: 83.7143\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 85.0485\n",
      "validation Loss: 0.0010 Acc: 83.9524\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.0426\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 85.2003\n",
      "validation Loss: 0.0010 Acc: 83.6786\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.4146\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.4919\n",
      "validation Loss: 0.0010 Acc: 83.7143\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.6378\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.7151\n",
      "validation Loss: 0.0010 Acc: 83.6071\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.7360\n",
      "validation Loss: 0.0010 Acc: 83.6905\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.9354\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 86.1169\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 86.1496\n",
      "validation Loss: 0.0010 Acc: 83.6786\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 86.2002\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 86.3341\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Early stopped.\n",
      "Best val acc: 84.119048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 82.0368\n",
      "validation Loss: 0.0011 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.2927\n",
      "validation Loss: 0.0010 Acc: 82.6786\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.4593\n",
      "validation Loss: 0.0010 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.6498\n",
      "validation Loss: 0.0011 Acc: 82.5476\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.6349\n",
      "validation Loss: 0.0010 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.6587\n",
      "validation Loss: 0.0011 Acc: 82.7619\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.3938\n",
      "validation Loss: 0.0010 Acc: 82.4524\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.6051\n",
      "validation Loss: 0.0010 Acc: 82.4286\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.8551\n",
      "validation Loss: 0.0010 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.7391\n",
      "validation Loss: 0.0010 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.7926\n",
      "validation Loss: 0.0010 Acc: 83.0714\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.9355\n",
      "validation Loss: 0.0010 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.9533\n",
      "validation Loss: 0.0010 Acc: 83.0833\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 83.8075\n",
      "validation Loss: 0.0010 Acc: 82.9643\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 84.0962\n",
      "validation Loss: 0.0010 Acc: 82.5714\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.0992\n",
      "validation Loss: 0.0010 Acc: 82.9167\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 84.0307\n",
      "validation Loss: 0.0010 Acc: 82.7857\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 83.9533\n",
      "validation Loss: 0.0010 Acc: 82.7143\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.2271\n",
      "validation Loss: 0.0010 Acc: 83.1786\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.3491\n",
      "validation Loss: 0.0010 Acc: 83.0119\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.3640\n",
      "validation Loss: 0.0010 Acc: 83.0119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.5069\n",
      "validation Loss: 0.0010 Acc: 82.7262\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.4950\n",
      "validation Loss: 0.0010 Acc: 82.7619\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.5694\n",
      "validation Loss: 0.0010 Acc: 82.7976\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.8104\n",
      "validation Loss: 0.0010 Acc: 82.7262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.6021\n",
      "validation Loss: 0.0010 Acc: 82.9048\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.9503\n",
      "validation Loss: 0.0010 Acc: 82.7500\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 85.1556\n",
      "validation Loss: 0.0010 Acc: 82.9405\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 85.3164\n",
      "validation Loss: 0.0010 Acc: 82.9048\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.3044\n",
      "validation Loss: 0.0010 Acc: 82.6667\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 85.3491\n",
      "validation Loss: 0.0011 Acc: 82.5119\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.4860\n",
      "validation Loss: 0.0010 Acc: 82.8214\n",
      "Early stopped.\n",
      "Best val acc: 83.321429\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.7064\n",
      "validation Loss: 0.0010 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.2093\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.3790\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.1439\n",
      "validation Loss: 0.0010 Acc: 83.3929\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.6468\n",
      "validation Loss: 0.0010 Acc: 83.2857\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.4117\n",
      "validation Loss: 0.0010 Acc: 83.2262\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.6051\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.5546\n",
      "validation Loss: 0.0010 Acc: 83.8690\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.5516\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.5516\n",
      "validation Loss: 0.0010 Acc: 83.8690\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.7123\n",
      "validation Loss: 0.0010 Acc: 83.5357\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.7688\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.7361\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 84.0069\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 84.0515\n",
      "validation Loss: 0.0010 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.1081\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 84.0902\n",
      "validation Loss: 0.0010 Acc: 83.4762\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.2331\n",
      "validation Loss: 0.0010 Acc: 84.0357\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.3700\n",
      "validation Loss: 0.0010 Acc: 83.5476\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.4027\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.4682\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.3462\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.5962\n",
      "validation Loss: 0.0010 Acc: 83.7857\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.6110\n",
      "validation Loss: 0.0010 Acc: 83.7857\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.8342\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 85.0068\n",
      "validation Loss: 0.0010 Acc: 84.0714\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 85.1259\n",
      "validation Loss: 0.0010 Acc: 83.5238\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 85.1467\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 85.1735\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 85.3223\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 85.7598\n",
      "validation Loss: 0.0010 Acc: 83.6548\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 85.9086\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.9919\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 86.1348\n",
      "validation Loss: 0.0010 Acc: 83.6905\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 86.1080\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Early stopped.\n",
      "Best val acc: 84.083333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.7957\n",
      "validation Loss: 0.0010 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.2391\n",
      "validation Loss: 0.0010 Acc: 83.4286\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.3075\n",
      "validation Loss: 0.0010 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.5010\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.4415\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.3819\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.3968\n",
      "validation Loss: 0.0010 Acc: 83.9286\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.4385\n",
      "validation Loss: 0.0010 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.5635\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.3790\n",
      "validation Loss: 0.0010 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.6260\n",
      "validation Loss: 0.0010 Acc: 84.0119\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.5843\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 83.5516\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 83.6349\n",
      "validation Loss: 0.0010 Acc: 83.7619\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 83.5486\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 83.5813\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 83.4772\n",
      "validation Loss: 0.0010 Acc: 84.0952\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 83.5218\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 83.5575\n",
      "validation Loss: 0.0010 Acc: 82.9881\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 83.8432\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.0634\n",
      "validation Loss: 0.0010 Acc: 84.0595\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.1617\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.1021\n",
      "validation Loss: 0.0010 Acc: 83.8690\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.2122\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.0664\n",
      "validation Loss: 0.0010 Acc: 84.1548\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.3372\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.1319\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.4920\n",
      "validation Loss: 0.0010 Acc: 84.2976\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.6259\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.7271\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.8521\n",
      "validation Loss: 0.0010 Acc: 83.9643\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.8789\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 85.1616\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 85.1140\n",
      "validation Loss: 0.0010 Acc: 84.0357\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.1973\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.2152\n",
      "validation Loss: 0.0010 Acc: 83.8452\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.2062\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.2806\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 85.6199\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 85.4771\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 85.6378\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 85.7211\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 85.7032\n",
      "validation Loss: 0.0010 Acc: 83.7262\n",
      "Epoch 43/99\n",
      "training Loss: 0.0008 Acc: 85.8104\n",
      "validation Loss: 0.0010 Acc: 83.7262\n",
      "Epoch 44/99\n",
      "training Loss: 0.0008 Acc: 85.6794\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 85.9681\n",
      "validation Loss: 0.0011 Acc: 83.9405\n",
      "Epoch 46/99\n",
      "training Loss: 0.0008 Acc: 85.8312\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 47/99\n",
      "training Loss: 0.0008 Acc: 85.8491\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Early stopped.\n",
      "Best val acc: 84.297619\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9555258952\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 467, 'dropout': 0.7580695892055032, 'batch_size': 192, 'max_depth': 4}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0036 Acc: 50.0060\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0036 Acc: 50.1042\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0036 Acc: 50.5655\n",
      "validation Loss: 0.0036 Acc: 50.0833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0036 Acc: 50.6905\n",
      "validation Loss: 0.0036 Acc: 51.9519\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0036 Acc: 51.0446\n",
      "validation Loss: 0.0036 Acc: 62.9136\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 51.3185\n",
      "validation Loss: 0.0036 Acc: 72.6137\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0036 Acc: 51.6696\n",
      "validation Loss: 0.0036 Acc: 75.7558\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0036 Acc: 52.2321\n",
      "validation Loss: 0.0036 Acc: 76.8388\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0036 Acc: 52.8036\n",
      "validation Loss: 0.0036 Acc: 77.7553\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0036 Acc: 53.6101\n",
      "validation Loss: 0.0036 Acc: 78.3147\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0036 Acc: 53.8661\n",
      "validation Loss: 0.0036 Acc: 78.5170\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0036 Acc: 54.6815\n",
      "validation Loss: 0.0035 Acc: 78.3742\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 56.0774\n",
      "validation Loss: 0.0035 Acc: 78.1838\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 56.2917\n",
      "validation Loss: 0.0035 Acc: 77.9695\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 57.3661\n",
      "validation Loss: 0.0034 Acc: 77.9814\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 58.3214\n",
      "validation Loss: 0.0034 Acc: 77.8862\n",
      "Epoch 16/99\n",
      "training Loss: 0.0034 Acc: 59.8482\n",
      "validation Loss: 0.0034 Acc: 78.0409\n",
      "Epoch 17/99\n",
      "training Loss: 0.0034 Acc: 60.3542\n",
      "validation Loss: 0.0033 Acc: 78.1957\n",
      "Epoch 18/99\n",
      "training Loss: 0.0034 Acc: 61.1935\n",
      "validation Loss: 0.0032 Acc: 78.6955\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0034 Acc: 62.7411\n",
      "validation Loss: 0.0032 Acc: 78.9931\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0033 Acc: 63.7679\n",
      "validation Loss: 0.0031 Acc: 79.0764\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0033 Acc: 64.8661\n",
      "validation Loss: 0.0031 Acc: 79.3383\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0033 Acc: 65.0804\n",
      "validation Loss: 0.0030 Acc: 79.5525\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0033 Acc: 65.6339\n",
      "validation Loss: 0.0030 Acc: 79.8024\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0032 Acc: 67.1905\n",
      "validation Loss: 0.0029 Acc: 79.9333\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0032 Acc: 67.8155\n",
      "validation Loss: 0.0029 Acc: 80.1595\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0032 Acc: 67.9940\n",
      "validation Loss: 0.0028 Acc: 80.4213\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0031 Acc: 69.0744\n",
      "validation Loss: 0.0028 Acc: 80.5046\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0031 Acc: 69.5119\n",
      "validation Loss: 0.0027 Acc: 80.5403\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0031 Acc: 69.9792\n",
      "validation Loss: 0.0027 Acc: 80.7427\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0030 Acc: 70.6815\n",
      "validation Loss: 0.0026 Acc: 80.7784\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0030 Acc: 71.0536\n",
      "validation Loss: 0.0026 Acc: 80.8617\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0030 Acc: 71.8690\n",
      "validation Loss: 0.0026 Acc: 80.9450\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0030 Acc: 72.0327\n",
      "validation Loss: 0.0025 Acc: 81.0402\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0029 Acc: 72.5655\n",
      "validation Loss: 0.0025 Acc: 81.2188\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0029 Acc: 72.8810\n",
      "validation Loss: 0.0025 Acc: 81.3021\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0029 Acc: 73.4107\n",
      "validation Loss: 0.0024 Acc: 81.3973\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0029 Acc: 73.7202\n",
      "validation Loss: 0.0024 Acc: 81.3497\n",
      "Epoch 38/99\n",
      "training Loss: 0.0029 Acc: 74.1042\n",
      "validation Loss: 0.0024 Acc: 81.5282\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0028 Acc: 74.3333\n",
      "validation Loss: 0.0024 Acc: 81.5520\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0028 Acc: 74.7470\n",
      "validation Loss: 0.0023 Acc: 81.7067\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0028 Acc: 75.1042\n",
      "validation Loss: 0.0023 Acc: 81.7543\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0028 Acc: 75.0565\n",
      "validation Loss: 0.0023 Acc: 81.8496\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0028 Acc: 75.7708\n",
      "validation Loss: 0.0023 Acc: 81.8258\n",
      "Epoch 44/99\n",
      "training Loss: 0.0028 Acc: 75.6667\n",
      "validation Loss: 0.0023 Acc: 81.8734\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0027 Acc: 75.7798\n",
      "validation Loss: 0.0022 Acc: 81.9210\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0027 Acc: 75.8274\n",
      "validation Loss: 0.0022 Acc: 81.9329\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0027 Acc: 76.4137\n",
      "validation Loss: 0.0022 Acc: 82.0757\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0027 Acc: 76.8036\n",
      "validation Loss: 0.0022 Acc: 82.1352\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0027 Acc: 77.2321\n",
      "validation Loss: 0.0022 Acc: 82.1828\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0027 Acc: 77.1310\n",
      "validation Loss: 0.0022 Acc: 82.1947\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0026 Acc: 77.1012\n",
      "validation Loss: 0.0022 Acc: 82.3494\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0026 Acc: 77.5268\n",
      "validation Loss: 0.0021 Acc: 82.3137\n",
      "Epoch 53/99\n",
      "training Loss: 0.0026 Acc: 77.6220\n",
      "validation Loss: 0.0021 Acc: 82.3137\n",
      "Epoch 54/99\n",
      "training Loss: 0.0026 Acc: 77.5179\n",
      "validation Loss: 0.0021 Acc: 82.3137\n",
      "Epoch 55/99\n",
      "training Loss: 0.0026 Acc: 77.7321\n",
      "validation Loss: 0.0021 Acc: 82.2899\n",
      "Epoch 56/99\n",
      "training Loss: 0.0026 Acc: 77.8393\n",
      "validation Loss: 0.0021 Acc: 82.2304\n",
      "Epoch 57/99\n",
      "training Loss: 0.0026 Acc: 78.1518\n",
      "validation Loss: 0.0021 Acc: 82.2661\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 78.3095\n",
      "validation Loss: 0.0021 Acc: 82.3732\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 78.2946\n",
      "validation Loss: 0.0021 Acc: 82.3970\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0026 Acc: 78.4732\n",
      "validation Loss: 0.0021 Acc: 82.3732\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 78.6071\n",
      "validation Loss: 0.0021 Acc: 82.5161\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0025 Acc: 78.4851\n",
      "validation Loss: 0.0021 Acc: 82.5399\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 78.7679\n",
      "validation Loss: 0.0021 Acc: 82.5518\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0025 Acc: 78.9792\n",
      "validation Loss: 0.0021 Acc: 82.6232\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0025 Acc: 78.9256\n",
      "validation Loss: 0.0021 Acc: 82.6351\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0025 Acc: 79.1042\n",
      "validation Loss: 0.0020 Acc: 82.6113\n",
      "Epoch 67/99\n",
      "training Loss: 0.0025 Acc: 78.9167\n",
      "validation Loss: 0.0020 Acc: 82.6708\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0025 Acc: 79.4256\n",
      "validation Loss: 0.0020 Acc: 82.6351\n",
      "Epoch 69/99\n",
      "training Loss: 0.0025 Acc: 79.3720\n",
      "validation Loss: 0.0020 Acc: 82.7541\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0025 Acc: 79.4226\n",
      "validation Loss: 0.0020 Acc: 82.7660\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0025 Acc: 79.6161\n",
      "validation Loss: 0.0020 Acc: 82.7660\n",
      "Epoch 72/99\n",
      "training Loss: 0.0025 Acc: 79.6964\n",
      "validation Loss: 0.0020 Acc: 82.8255\n",
      "Saving..\n",
      "Epoch 73/99\n",
      "training Loss: 0.0025 Acc: 79.4762\n",
      "validation Loss: 0.0020 Acc: 82.8493\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0025 Acc: 79.4851\n",
      "validation Loss: 0.0020 Acc: 82.8850\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0025 Acc: 79.3869\n",
      "validation Loss: 0.0020 Acc: 82.8255\n",
      "Epoch 76/99\n",
      "training Loss: 0.0024 Acc: 79.7857\n",
      "validation Loss: 0.0020 Acc: 82.8255\n",
      "Epoch 77/99\n",
      "training Loss: 0.0024 Acc: 80.0030\n",
      "validation Loss: 0.0020 Acc: 82.9088\n",
      "Saving..\n",
      "Epoch 78/99\n",
      "training Loss: 0.0025 Acc: 79.9375\n",
      "validation Loss: 0.0020 Acc: 82.9326\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0025 Acc: 79.9911\n",
      "validation Loss: 0.0020 Acc: 82.9564\n",
      "Saving..\n",
      "Epoch 80/99\n",
      "training Loss: 0.0024 Acc: 79.8423\n",
      "validation Loss: 0.0020 Acc: 82.9802\n",
      "Saving..\n",
      "Epoch 81/99\n",
      "training Loss: 0.0024 Acc: 80.0863\n",
      "validation Loss: 0.0020 Acc: 82.9564\n",
      "Epoch 82/99\n",
      "training Loss: 0.0024 Acc: 80.1994\n",
      "validation Loss: 0.0020 Acc: 83.0040\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0024 Acc: 80.2381\n",
      "validation Loss: 0.0020 Acc: 82.9445\n",
      "Epoch 84/99\n",
      "training Loss: 0.0024 Acc: 80.1339\n",
      "validation Loss: 0.0020 Acc: 82.9564\n",
      "Epoch 85/99\n",
      "training Loss: 0.0024 Acc: 80.2708\n",
      "validation Loss: 0.0020 Acc: 82.9088\n",
      "Epoch 86/99\n",
      "training Loss: 0.0024 Acc: 80.5149\n",
      "validation Loss: 0.0020 Acc: 82.9564\n",
      "Epoch 87/99\n",
      "training Loss: 0.0024 Acc: 80.5089\n",
      "validation Loss: 0.0020 Acc: 82.9445\n",
      "Epoch 88/99\n",
      "training Loss: 0.0024 Acc: 80.4911\n",
      "validation Loss: 0.0020 Acc: 82.9445\n",
      "Epoch 89/99\n",
      "training Loss: 0.0024 Acc: 80.3036\n",
      "validation Loss: 0.0020 Acc: 82.9802\n",
      "Epoch 90/99\n",
      "training Loss: 0.0024 Acc: 80.4821\n",
      "validation Loss: 0.0020 Acc: 83.0517\n",
      "Saving..\n",
      "Epoch 91/99\n",
      "training Loss: 0.0024 Acc: 80.2708\n",
      "validation Loss: 0.0020 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 92/99\n",
      "training Loss: 0.0024 Acc: 80.4048\n",
      "validation Loss: 0.0020 Acc: 83.1231\n",
      "Saving..\n",
      "Epoch 93/99\n",
      "training Loss: 0.0024 Acc: 80.7351\n",
      "validation Loss: 0.0020 Acc: 83.0874\n",
      "Epoch 94/99\n",
      "training Loss: 0.0024 Acc: 80.5119\n",
      "validation Loss: 0.0020 Acc: 83.1350\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0024 Acc: 80.8125\n",
      "validation Loss: 0.0020 Acc: 83.1469\n",
      "Saving..\n",
      "Epoch 96/99\n",
      "training Loss: 0.0024 Acc: 80.7351\n",
      "validation Loss: 0.0020 Acc: 83.1231\n",
      "Epoch 97/99\n",
      "training Loss: 0.0024 Acc: 80.5357\n",
      "validation Loss: 0.0020 Acc: 83.0874\n",
      "Epoch 98/99\n",
      "training Loss: 0.0024 Acc: 80.9077\n",
      "validation Loss: 0.0020 Acc: 83.0993\n",
      "Epoch 99/99\n",
      "training Loss: 0.0024 Acc: 80.9375\n",
      "validation Loss: 0.0020 Acc: 83.0517\n",
      "Best val acc: 83.146870\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0036 Acc: 50.3720\n",
      "validation Loss: 0.0036 Acc: 50.1190\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0036 Acc: 49.6459\n",
      "validation Loss: 0.0036 Acc: 50.1190\n",
      "Epoch 2/99\n",
      "training Loss: 0.0036 Acc: 50.8244\n",
      "validation Loss: 0.0036 Acc: 50.1310\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0036 Acc: 50.5238\n",
      "validation Loss: 0.0036 Acc: 50.2143\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0036 Acc: 50.3690\n",
      "validation Loss: 0.0036 Acc: 50.2976\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 50.5744\n",
      "validation Loss: 0.0036 Acc: 50.5000\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0036 Acc: 50.6637\n",
      "validation Loss: 0.0036 Acc: 51.0238\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0036 Acc: 51.2112\n",
      "validation Loss: 0.0036 Acc: 52.4524\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0036 Acc: 51.2053\n",
      "validation Loss: 0.0036 Acc: 54.6786\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0036 Acc: 51.2648\n",
      "validation Loss: 0.0036 Acc: 57.4286\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0036 Acc: 51.5951\n",
      "validation Loss: 0.0036 Acc: 61.6905\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0036 Acc: 51.8422\n",
      "validation Loss: 0.0036 Acc: 65.9405\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0036 Acc: 52.5028\n",
      "validation Loss: 0.0036 Acc: 69.5119\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0036 Acc: 52.0951\n",
      "validation Loss: 0.0036 Acc: 72.4643\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0036 Acc: 52.9849\n",
      "validation Loss: 0.0036 Acc: 75.4643\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0036 Acc: 53.3004\n",
      "validation Loss: 0.0036 Acc: 77.0952\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0036 Acc: 53.6248\n",
      "validation Loss: 0.0036 Acc: 78.2738\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0036 Acc: 54.4491\n",
      "validation Loss: 0.0036 Acc: 78.4643\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0036 Acc: 54.9194\n",
      "validation Loss: 0.0035 Acc: 78.5952\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0036 Acc: 55.7645\n",
      "validation Loss: 0.0035 Acc: 78.2976\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 56.8835\n",
      "validation Loss: 0.0035 Acc: 78.2143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0035 Acc: 57.7912\n",
      "validation Loss: 0.0035 Acc: 78.2500\n",
      "Epoch 22/99\n",
      "training Loss: 0.0035 Acc: 58.8358\n",
      "validation Loss: 0.0034 Acc: 78.4286\n",
      "Epoch 23/99\n",
      "training Loss: 0.0035 Acc: 59.7048\n",
      "validation Loss: 0.0034 Acc: 78.4524\n",
      "Epoch 24/99\n",
      "training Loss: 0.0035 Acc: 60.1095\n",
      "validation Loss: 0.0033 Acc: 78.6310\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0034 Acc: 61.7791\n",
      "validation Loss: 0.0033 Acc: 78.7143\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0034 Acc: 62.4427\n",
      "validation Loss: 0.0032 Acc: 78.9881\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0034 Acc: 63.1956\n",
      "validation Loss: 0.0032 Acc: 79.2976\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0033 Acc: 64.5468\n",
      "validation Loss: 0.0031 Acc: 79.7024\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0033 Acc: 65.1777\n",
      "validation Loss: 0.0031 Acc: 79.9524\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0033 Acc: 66.0348\n",
      "validation Loss: 0.0030 Acc: 80.1429\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0032 Acc: 67.0079\n",
      "validation Loss: 0.0030 Acc: 80.2976\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0032 Acc: 67.6269\n",
      "validation Loss: 0.0029 Acc: 80.4643\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0032 Acc: 68.6031\n",
      "validation Loss: 0.0029 Acc: 80.6548\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0031 Acc: 69.2161\n",
      "validation Loss: 0.0028 Acc: 80.6190\n",
      "Epoch 35/99\n",
      "training Loss: 0.0031 Acc: 69.6953\n",
      "validation Loss: 0.0028 Acc: 80.7143\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0031 Acc: 70.1417\n",
      "validation Loss: 0.0027 Acc: 80.7143\n",
      "Epoch 37/99\n",
      "training Loss: 0.0030 Acc: 70.9571\n",
      "validation Loss: 0.0027 Acc: 80.7262\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0030 Acc: 71.7606\n",
      "validation Loss: 0.0026 Acc: 80.8214\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0030 Acc: 71.8261\n",
      "validation Loss: 0.0026 Acc: 80.8690\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0030 Acc: 72.5016\n",
      "validation Loss: 0.0025 Acc: 81.0000\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0030 Acc: 72.5641\n",
      "validation Loss: 0.0025 Acc: 81.1667\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0029 Acc: 73.4421\n",
      "validation Loss: 0.0025 Acc: 81.3095\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0029 Acc: 73.7664\n",
      "validation Loss: 0.0024 Acc: 81.2976\n",
      "Epoch 44/99\n",
      "training Loss: 0.0028 Acc: 73.9271\n",
      "validation Loss: 0.0024 Acc: 81.3690\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0028 Acc: 74.3378\n",
      "validation Loss: 0.0024 Acc: 81.4762\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0028 Acc: 74.5640\n",
      "validation Loss: 0.0024 Acc: 81.5357\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0028 Acc: 74.8051\n",
      "validation Loss: 0.0023 Acc: 81.5952\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0028 Acc: 75.2485\n",
      "validation Loss: 0.0023 Acc: 81.7024\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0028 Acc: 75.3140\n",
      "validation Loss: 0.0023 Acc: 81.7262\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0028 Acc: 75.9151\n",
      "validation Loss: 0.0023 Acc: 81.8095\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0027 Acc: 76.1353\n",
      "validation Loss: 0.0023 Acc: 81.8810\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0027 Acc: 76.0193\n",
      "validation Loss: 0.0023 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0027 Acc: 76.4776\n",
      "validation Loss: 0.0022 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0027 Acc: 76.1889\n",
      "validation Loss: 0.0022 Acc: 82.1190\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0027 Acc: 76.7395\n",
      "validation Loss: 0.0022 Acc: 82.1310\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0027 Acc: 76.9210\n",
      "validation Loss: 0.0022 Acc: 82.1190\n",
      "Epoch 57/99\n",
      "training Loss: 0.0027 Acc: 77.0371\n",
      "validation Loss: 0.0022 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 77.0490\n",
      "validation Loss: 0.0022 Acc: 82.1310\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 77.5817\n",
      "validation Loss: 0.0022 Acc: 82.1310\n",
      "Epoch 60/99\n",
      "training Loss: 0.0027 Acc: 77.4269\n",
      "validation Loss: 0.0022 Acc: 82.2024\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 77.5906\n",
      "validation Loss: 0.0022 Acc: 82.1667\n",
      "Epoch 62/99\n",
      "training Loss: 0.0026 Acc: 78.0430\n",
      "validation Loss: 0.0022 Acc: 82.2262\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 78.1590\n",
      "validation Loss: 0.0022 Acc: 82.2500\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0026 Acc: 78.0668\n",
      "validation Loss: 0.0021 Acc: 82.2619\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0026 Acc: 78.2275\n",
      "validation Loss: 0.0021 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0026 Acc: 78.4031\n",
      "validation Loss: 0.0021 Acc: 82.2976\n",
      "Saving..\n",
      "Epoch 67/99\n",
      "training Loss: 0.0026 Acc: 78.4596\n",
      "validation Loss: 0.0021 Acc: 82.2381\n",
      "Epoch 68/99\n",
      "training Loss: 0.0026 Acc: 78.4269\n",
      "validation Loss: 0.0021 Acc: 82.3214\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0026 Acc: 78.5548\n",
      "validation Loss: 0.0021 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0026 Acc: 78.3882\n",
      "validation Loss: 0.0021 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0026 Acc: 78.8852\n",
      "validation Loss: 0.0021 Acc: 82.3571\n",
      "Epoch 72/99\n",
      "training Loss: 0.0025 Acc: 78.8524\n",
      "validation Loss: 0.0021 Acc: 82.3333\n",
      "Epoch 73/99\n",
      "training Loss: 0.0025 Acc: 78.5935\n",
      "validation Loss: 0.0021 Acc: 82.3333\n",
      "Epoch 74/99\n",
      "training Loss: 0.0025 Acc: 79.0608\n",
      "validation Loss: 0.0021 Acc: 82.2976\n",
      "Epoch 75/99\n",
      "training Loss: 0.0026 Acc: 78.9804\n",
      "validation Loss: 0.0021 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0026 Acc: 79.1054\n",
      "validation Loss: 0.0021 Acc: 82.3571\n",
      "Epoch 77/99\n",
      "training Loss: 0.0026 Acc: 79.2572\n",
      "validation Loss: 0.0021 Acc: 82.3452\n",
      "Epoch 78/99\n",
      "training Loss: 0.0027 Acc: 79.2185\n",
      "validation Loss: 0.0021 Acc: 82.3690\n",
      "Epoch 79/99\n",
      "training Loss: 0.0026 Acc: 79.4298\n",
      "validation Loss: 0.0021 Acc: 82.3929\n",
      "Saving..\n",
      "Epoch 80/99\n",
      "training Loss: 0.0025 Acc: 79.3197\n",
      "validation Loss: 0.0021 Acc: 82.4405\n",
      "Saving..\n",
      "Epoch 81/99\n",
      "training Loss: 0.0025 Acc: 79.3881\n",
      "validation Loss: 0.0021 Acc: 82.4286\n",
      "Epoch 82/99\n",
      "training Loss: 0.0025 Acc: 79.3375\n",
      "validation Loss: 0.0021 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0025 Acc: 79.5607\n",
      "validation Loss: 0.0021 Acc: 82.4524\n",
      "Epoch 84/99\n",
      "training Loss: 0.0025 Acc: 79.5369\n",
      "validation Loss: 0.0021 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 85/99\n",
      "training Loss: 0.0025 Acc: 79.4209\n",
      "validation Loss: 0.0021 Acc: 82.4405\n",
      "Epoch 86/99\n",
      "training Loss: 0.0025 Acc: 79.7720\n",
      "validation Loss: 0.0021 Acc: 82.4762\n",
      "Epoch 87/99\n",
      "training Loss: 0.0025 Acc: 79.5578\n",
      "validation Loss: 0.0021 Acc: 82.4643\n",
      "Epoch 88/99\n",
      "training Loss: 0.0025 Acc: 79.7363\n",
      "validation Loss: 0.0021 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 89/99\n",
      "training Loss: 0.0025 Acc: 79.7095\n",
      "validation Loss: 0.0021 Acc: 82.4524\n",
      "Epoch 90/99\n",
      "training Loss: 0.0025 Acc: 79.9030\n",
      "validation Loss: 0.0021 Acc: 82.4762\n",
      "Epoch 91/99\n",
      "training Loss: 0.0024 Acc: 79.8167\n",
      "validation Loss: 0.0021 Acc: 82.4643\n",
      "Epoch 92/99\n",
      "training Loss: 0.0025 Acc: 79.8821\n",
      "validation Loss: 0.0021 Acc: 82.4286\n",
      "Epoch 93/99\n",
      "training Loss: 0.0024 Acc: 79.9685\n",
      "validation Loss: 0.0021 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 94/99\n",
      "training Loss: 0.0025 Acc: 79.9625\n",
      "validation Loss: 0.0021 Acc: 82.4881\n",
      "Epoch 95/99\n",
      "training Loss: 0.0024 Acc: 80.1589\n",
      "validation Loss: 0.0021 Acc: 82.5476\n",
      "Epoch 96/99\n",
      "training Loss: 0.0025 Acc: 80.0637\n",
      "validation Loss: 0.0021 Acc: 82.5476\n",
      "Epoch 97/99\n",
      "training Loss: 0.0024 Acc: 80.0250\n",
      "validation Loss: 0.0021 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 98/99\n",
      "training Loss: 0.0024 Acc: 80.3583\n",
      "validation Loss: 0.0021 Acc: 82.6310\n",
      "Epoch 99/99\n",
      "training Loss: 0.0024 Acc: 80.2244\n",
      "validation Loss: 0.0020 Acc: 82.5833\n",
      "Best val acc: 82.642857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0037 Acc: 49.7828\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0037 Acc: 49.9226\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0036 Acc: 49.8810\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0036 Acc: 50.0684\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0036 Acc: 49.9821\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 50.6756\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0036 Acc: 50.5833\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0036 Acc: 50.6071\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0036 Acc: 51.1487\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0036 Acc: 51.0892\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0036 Acc: 52.1398\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0036 Acc: 52.3153\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0036 Acc: 53.1189\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 54.2884\n",
      "validation Loss: 0.0035 Acc: 51.1071\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 54.1426\n",
      "validation Loss: 0.0035 Acc: 61.8214\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 54.7854\n",
      "validation Loss: 0.0034 Acc: 72.2738\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 55.5175\n",
      "validation Loss: 0.0034 Acc: 76.5119\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 56.1455\n",
      "validation Loss: 0.0033 Acc: 77.6786\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0034 Acc: 57.4876\n",
      "validation Loss: 0.0033 Acc: 78.0595\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0034 Acc: 58.1096\n",
      "validation Loss: 0.0032 Acc: 78.3690\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0034 Acc: 59.4935\n",
      "validation Loss: 0.0032 Acc: 78.5952\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0033 Acc: 60.2256\n",
      "validation Loss: 0.0031 Acc: 78.9048\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0033 Acc: 61.2523\n",
      "validation Loss: 0.0031 Acc: 79.7024\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0033 Acc: 62.2314\n",
      "validation Loss: 0.0030 Acc: 80.0119\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0032 Acc: 63.1242\n",
      "validation Loss: 0.0030 Acc: 80.3452\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0032 Acc: 64.5110\n",
      "validation Loss: 0.0030 Acc: 80.6071\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0032 Acc: 65.4336\n",
      "validation Loss: 0.0029 Acc: 80.9405\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0032 Acc: 66.1240\n",
      "validation Loss: 0.0029 Acc: 81.3690\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0031 Acc: 67.4722\n",
      "validation Loss: 0.0028 Acc: 81.3929\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0031 Acc: 67.8352\n",
      "validation Loss: 0.0028 Acc: 81.5476\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0031 Acc: 68.8322\n",
      "validation Loss: 0.0027 Acc: 81.6786\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0030 Acc: 69.4542\n",
      "validation Loss: 0.0027 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0030 Acc: 70.0018\n",
      "validation Loss: 0.0026 Acc: 81.9405\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0030 Acc: 70.5166\n",
      "validation Loss: 0.0026 Acc: 82.0595\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0029 Acc: 70.7755\n",
      "validation Loss: 0.0026 Acc: 82.0238\n",
      "Epoch 35/99\n",
      "training Loss: 0.0029 Acc: 71.5285\n",
      "validation Loss: 0.0025 Acc: 81.9881\n",
      "Epoch 36/99\n",
      "training Loss: 0.0029 Acc: 71.9600\n",
      "validation Loss: 0.0025 Acc: 82.1190\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0029 Acc: 72.3856\n",
      "validation Loss: 0.0025 Acc: 82.1071\n",
      "Epoch 38/99\n",
      "training Loss: 0.0028 Acc: 73.4212\n",
      "validation Loss: 0.0024 Acc: 82.0952\n",
      "Epoch 39/99\n",
      "training Loss: 0.0028 Acc: 73.5998\n",
      "validation Loss: 0.0024 Acc: 82.1310\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0028 Acc: 73.8349\n",
      "validation Loss: 0.0024 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0028 Acc: 74.1027\n",
      "validation Loss: 0.0023 Acc: 82.2143\n",
      "Epoch 42/99\n",
      "training Loss: 0.0028 Acc: 74.4509\n",
      "validation Loss: 0.0023 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0028 Acc: 74.9122\n",
      "validation Loss: 0.0023 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0028 Acc: 75.3973\n",
      "validation Loss: 0.0023 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0028 Acc: 75.6443\n",
      "validation Loss: 0.0023 Acc: 82.3214\n",
      "Epoch 46/99\n",
      "training Loss: 0.0027 Acc: 75.6681\n",
      "validation Loss: 0.0023 Acc: 82.3214\n",
      "Epoch 47/99\n",
      "training Loss: 0.0027 Acc: 76.0788\n",
      "validation Loss: 0.0022 Acc: 82.3095\n",
      "Epoch 48/99\n",
      "training Loss: 0.0027 Acc: 76.4568\n",
      "validation Loss: 0.0022 Acc: 82.2738\n",
      "Epoch 49/99\n",
      "training Loss: 0.0027 Acc: 76.5639\n",
      "validation Loss: 0.0022 Acc: 82.3214\n",
      "Epoch 50/99\n",
      "training Loss: 0.0027 Acc: 76.8942\n",
      "validation Loss: 0.0022 Acc: 82.3452\n",
      "Epoch 51/99\n",
      "training Loss: 0.0027 Acc: 76.8823\n",
      "validation Loss: 0.0022 Acc: 82.3333\n",
      "Epoch 52/99\n",
      "training Loss: 0.0027 Acc: 77.0341\n",
      "validation Loss: 0.0022 Acc: 82.2976\n",
      "Epoch 53/99\n",
      "training Loss: 0.0026 Acc: 77.4478\n",
      "validation Loss: 0.0022 Acc: 82.3333\n",
      "Epoch 54/99\n",
      "training Loss: 0.0027 Acc: 77.3674\n",
      "validation Loss: 0.0022 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0026 Acc: 77.9120\n",
      "validation Loss: 0.0021 Acc: 82.4048\n",
      "Epoch 56/99\n",
      "training Loss: 0.0026 Acc: 77.7900\n",
      "validation Loss: 0.0021 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0026 Acc: 77.9091\n",
      "validation Loss: 0.0021 Acc: 82.4643\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 78.1352\n",
      "validation Loss: 0.0021 Acc: 82.3929\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 78.1382\n",
      "validation Loss: 0.0021 Acc: 82.4524\n",
      "Epoch 60/99\n",
      "training Loss: 0.0026 Acc: 78.4388\n",
      "validation Loss: 0.0021 Acc: 82.3810\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 78.5519\n",
      "validation Loss: 0.0021 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0026 Acc: 78.5727\n",
      "validation Loss: 0.0021 Acc: 82.5119\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 78.5757\n",
      "validation Loss: 0.0021 Acc: 82.5238\n",
      "Epoch 64/99\n",
      "training Loss: 0.0026 Acc: 78.5489\n",
      "validation Loss: 0.0021 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0026 Acc: 78.9298\n",
      "validation Loss: 0.0021 Acc: 82.5238\n",
      "Epoch 66/99\n",
      "training Loss: 0.0026 Acc: 79.1768\n",
      "validation Loss: 0.0021 Acc: 82.5119\n",
      "Epoch 67/99\n",
      "training Loss: 0.0026 Acc: 78.7096\n",
      "validation Loss: 0.0021 Acc: 82.4524\n",
      "Epoch 68/99\n",
      "training Loss: 0.0025 Acc: 79.3078\n",
      "validation Loss: 0.0021 Acc: 82.5357\n",
      "Epoch 69/99\n",
      "training Loss: 0.0025 Acc: 79.1828\n",
      "validation Loss: 0.0021 Acc: 82.5238\n",
      "Epoch 70/99\n",
      "training Loss: 0.0025 Acc: 79.2602\n",
      "validation Loss: 0.0021 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0025 Acc: 79.3048\n",
      "validation Loss: 0.0021 Acc: 82.4881\n",
      "Epoch 72/99\n",
      "training Loss: 0.0025 Acc: 79.0757\n",
      "validation Loss: 0.0021 Acc: 82.5833\n",
      "Epoch 73/99\n",
      "training Loss: 0.0025 Acc: 79.6173\n",
      "validation Loss: 0.0021 Acc: 82.6071\n",
      "Epoch 74/99\n",
      "training Loss: 0.0025 Acc: 79.4209\n",
      "validation Loss: 0.0020 Acc: 82.5476\n",
      "Epoch 75/99\n",
      "training Loss: 0.0025 Acc: 79.7304\n",
      "validation Loss: 0.0020 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0025 Acc: 79.4953\n",
      "validation Loss: 0.0020 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0025 Acc: 80.1678\n",
      "validation Loss: 0.0020 Acc: 82.7024\n",
      "Epoch 78/99\n",
      "training Loss: 0.0025 Acc: 79.7899\n",
      "validation Loss: 0.0020 Acc: 82.7143\n",
      "Epoch 79/99\n",
      "training Loss: 0.0025 Acc: 79.9268\n",
      "validation Loss: 0.0020 Acc: 82.7024\n",
      "Epoch 80/99\n",
      "training Loss: 0.0025 Acc: 80.0815\n",
      "validation Loss: 0.0020 Acc: 82.6905\n",
      "Epoch 81/99\n",
      "training Loss: 0.0025 Acc: 79.8375\n",
      "validation Loss: 0.0020 Acc: 82.6905\n",
      "Epoch 82/99\n",
      "training Loss: 0.0025 Acc: 80.1827\n",
      "validation Loss: 0.0020 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0025 Acc: 80.2333\n",
      "validation Loss: 0.0020 Acc: 82.7857\n",
      "Epoch 84/99\n",
      "training Loss: 0.0025 Acc: 79.8405\n",
      "validation Loss: 0.0020 Acc: 82.8095\n",
      "Epoch 85/99\n",
      "training Loss: 0.0025 Acc: 80.1381\n",
      "validation Loss: 0.0020 Acc: 82.8452\n",
      "Epoch 86/99\n",
      "training Loss: 0.0024 Acc: 80.1500\n",
      "validation Loss: 0.0020 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0025 Acc: 80.1381\n",
      "validation Loss: 0.0020 Acc: 82.8452\n",
      "Epoch 88/99\n",
      "training Loss: 0.0025 Acc: 80.1173\n",
      "validation Loss: 0.0020 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 89/99\n",
      "training Loss: 0.0025 Acc: 79.9804\n",
      "validation Loss: 0.0020 Acc: 82.8810\n",
      "Epoch 90/99\n",
      "training Loss: 0.0025 Acc: 80.1946\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Epoch 91/99\n",
      "training Loss: 0.0025 Acc: 80.5190\n",
      "validation Loss: 0.0020 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 92/99\n",
      "training Loss: 0.0024 Acc: 80.4982\n",
      "validation Loss: 0.0020 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 93/99\n",
      "training Loss: 0.0025 Acc: 80.4119\n",
      "validation Loss: 0.0020 Acc: 83.0357\n",
      "Epoch 94/99\n",
      "training Loss: 0.0024 Acc: 80.5190\n",
      "validation Loss: 0.0020 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0024 Acc: 80.6440\n",
      "validation Loss: 0.0020 Acc: 83.0357\n",
      "Epoch 96/99\n",
      "training Loss: 0.0024 Acc: 80.4893\n",
      "validation Loss: 0.0020 Acc: 83.0357\n",
      "Epoch 97/99\n",
      "training Loss: 0.0024 Acc: 80.6976\n",
      "validation Loss: 0.0020 Acc: 83.0238\n",
      "Epoch 98/99\n",
      "training Loss: 0.0024 Acc: 80.7333\n",
      "validation Loss: 0.0020 Acc: 82.9881\n",
      "Epoch 99/99\n",
      "training Loss: 0.0024 Acc: 80.6559\n",
      "validation Loss: 0.0020 Acc: 83.0000\n",
      "Best val acc: 83.047619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0037 Acc: 49.7113\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0037 Acc: 49.9167\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0036 Acc: 49.9464\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0036 Acc: 50.3690\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0036 Acc: 50.2262\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 50.8749\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0036 Acc: 51.1220\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0036 Acc: 51.0684\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0036 Acc: 50.9107\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0036 Acc: 51.2975\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0036 Acc: 51.3035\n",
      "validation Loss: 0.0036 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0036 Acc: 51.8570\n",
      "validation Loss: 0.0035 Acc: 50.0595\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0036 Acc: 52.2320\n",
      "validation Loss: 0.0035 Acc: 50.1310\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0036 Acc: 52.6248\n",
      "validation Loss: 0.0035 Acc: 50.4048\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0036 Acc: 53.5087\n",
      "validation Loss: 0.0035 Acc: 51.2738\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 54.1129\n",
      "validation Loss: 0.0035 Acc: 53.2857\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 54.5533\n",
      "validation Loss: 0.0034 Acc: 58.1429\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 55.5741\n",
      "validation Loss: 0.0034 Acc: 65.8095\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 56.1336\n",
      "validation Loss: 0.0033 Acc: 71.0952\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0034 Acc: 56.8805\n",
      "validation Loss: 0.0033 Acc: 74.8452\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0034 Acc: 57.9370\n",
      "validation Loss: 0.0033 Acc: 77.7619\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0034 Acc: 59.4250\n",
      "validation Loss: 0.0032 Acc: 78.9643\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0033 Acc: 60.3595\n",
      "validation Loss: 0.0032 Acc: 79.6905\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0033 Acc: 61.2583\n",
      "validation Loss: 0.0031 Acc: 80.0357\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0033 Acc: 62.9933\n",
      "validation Loss: 0.0031 Acc: 80.0952\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0033 Acc: 63.5290\n",
      "validation Loss: 0.0030 Acc: 80.4048\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0032 Acc: 63.9158\n",
      "validation Loss: 0.0029 Acc: 80.6071\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0032 Acc: 65.3324\n",
      "validation Loss: 0.0029 Acc: 80.7381\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0032 Acc: 66.1598\n",
      "validation Loss: 0.0028 Acc: 81.0238\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0031 Acc: 66.7550\n",
      "validation Loss: 0.0028 Acc: 81.0595\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0031 Acc: 67.7251\n",
      "validation Loss: 0.0027 Acc: 81.2738\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0031 Acc: 68.2965\n",
      "validation Loss: 0.0027 Acc: 81.3333\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0031 Acc: 69.1209\n",
      "validation Loss: 0.0027 Acc: 81.4762\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0030 Acc: 69.7518\n",
      "validation Loss: 0.0026 Acc: 81.5833\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0030 Acc: 70.1059\n",
      "validation Loss: 0.0026 Acc: 81.5476\n",
      "Epoch 35/99\n",
      "training Loss: 0.0030 Acc: 70.7934\n",
      "validation Loss: 0.0025 Acc: 81.6071\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0029 Acc: 71.2279\n",
      "validation Loss: 0.0025 Acc: 81.6905\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0029 Acc: 71.5017\n",
      "validation Loss: 0.0025 Acc: 81.7619\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0029 Acc: 72.2040\n",
      "validation Loss: 0.0024 Acc: 81.8452\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0029 Acc: 72.6475\n",
      "validation Loss: 0.0024 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0029 Acc: 73.4718\n",
      "validation Loss: 0.0024 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0028 Acc: 73.6712\n",
      "validation Loss: 0.0024 Acc: 82.0119\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0028 Acc: 73.9450\n",
      "validation Loss: 0.0024 Acc: 82.1071\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0028 Acc: 74.2099\n",
      "validation Loss: 0.0023 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0028 Acc: 74.8854\n",
      "validation Loss: 0.0023 Acc: 82.2500\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0028 Acc: 74.7456\n",
      "validation Loss: 0.0023 Acc: 82.2857\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0028 Acc: 75.3021\n",
      "validation Loss: 0.0023 Acc: 82.3214\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0028 Acc: 75.5818\n",
      "validation Loss: 0.0023 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0027 Acc: 75.7009\n",
      "validation Loss: 0.0022 Acc: 82.3571\n",
      "Epoch 49/99\n",
      "training Loss: 0.0028 Acc: 75.6919\n",
      "validation Loss: 0.0022 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0027 Acc: 76.0252\n",
      "validation Loss: 0.0022 Acc: 82.4286\n",
      "Epoch 51/99\n",
      "training Loss: 0.0027 Acc: 76.2187\n",
      "validation Loss: 0.0022 Acc: 82.4643\n",
      "Epoch 52/99\n",
      "training Loss: 0.0027 Acc: 76.8139\n",
      "validation Loss: 0.0022 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0027 Acc: 76.3139\n",
      "validation Loss: 0.0022 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0027 Acc: 76.7960\n",
      "validation Loss: 0.0022 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0027 Acc: 77.0668\n",
      "validation Loss: 0.0022 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0026 Acc: 77.4627\n",
      "validation Loss: 0.0022 Acc: 82.6310\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0026 Acc: 77.2097\n",
      "validation Loss: 0.0022 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 77.4418\n",
      "validation Loss: 0.0021 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 77.7305\n",
      "validation Loss: 0.0021 Acc: 82.7143\n",
      "Epoch 60/99\n",
      "training Loss: 0.0026 Acc: 77.8674\n",
      "validation Loss: 0.0021 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 77.7870\n",
      "validation Loss: 0.0021 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0026 Acc: 78.2513\n",
      "validation Loss: 0.0021 Acc: 82.7381\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 77.9537\n",
      "validation Loss: 0.0021 Acc: 82.7500\n",
      "Epoch 64/99\n",
      "training Loss: 0.0026 Acc: 78.3197\n",
      "validation Loss: 0.0021 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0026 Acc: 78.3822\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Epoch 66/99\n",
      "training Loss: 0.0025 Acc: 78.7572\n",
      "validation Loss: 0.0021 Acc: 82.7381\n",
      "Epoch 67/99\n",
      "training Loss: 0.0026 Acc: 78.8078\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Epoch 68/99\n",
      "training Loss: 0.0025 Acc: 78.7572\n",
      "validation Loss: 0.0021 Acc: 82.7619\n",
      "Epoch 69/99\n",
      "training Loss: 0.0026 Acc: 78.9030\n",
      "validation Loss: 0.0021 Acc: 82.7500\n",
      "Epoch 70/99\n",
      "training Loss: 0.0025 Acc: 79.1381\n",
      "validation Loss: 0.0021 Acc: 82.7976\n",
      "Epoch 71/99\n",
      "training Loss: 0.0025 Acc: 79.1649\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Epoch 72/99\n",
      "training Loss: 0.0025 Acc: 79.0667\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Epoch 73/99\n",
      "training Loss: 0.0025 Acc: 79.1501\n",
      "validation Loss: 0.0021 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0025 Acc: 79.3316\n",
      "validation Loss: 0.0021 Acc: 82.8214\n",
      "Epoch 75/99\n",
      "training Loss: 0.0025 Acc: 79.2602\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Epoch 76/99\n",
      "training Loss: 0.0025 Acc: 79.3524\n",
      "validation Loss: 0.0021 Acc: 82.8095\n",
      "Epoch 77/99\n",
      "training Loss: 0.0025 Acc: 79.5578\n",
      "validation Loss: 0.0020 Acc: 82.7976\n",
      "Epoch 78/99\n",
      "training Loss: 0.0025 Acc: 79.5310\n",
      "validation Loss: 0.0020 Acc: 82.8095\n",
      "Epoch 79/99\n",
      "training Loss: 0.0025 Acc: 79.4982\n",
      "validation Loss: 0.0020 Acc: 82.7857\n",
      "Epoch 80/99\n",
      "training Loss: 0.0025 Acc: 79.6589\n",
      "validation Loss: 0.0020 Acc: 82.7738\n",
      "Epoch 81/99\n",
      "training Loss: 0.0025 Acc: 79.6411\n",
      "validation Loss: 0.0020 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0025 Acc: 79.6441\n",
      "validation Loss: 0.0020 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0025 Acc: 79.8197\n",
      "validation Loss: 0.0020 Acc: 82.8929\n",
      "Epoch 84/99\n",
      "training Loss: 0.0024 Acc: 79.8077\n",
      "validation Loss: 0.0020 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 85/99\n",
      "training Loss: 0.0024 Acc: 79.7869\n",
      "validation Loss: 0.0020 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 86/99\n",
      "training Loss: 0.0025 Acc: 79.8911\n",
      "validation Loss: 0.0020 Acc: 82.8810\n",
      "Epoch 87/99\n",
      "training Loss: 0.0025 Acc: 79.8881\n",
      "validation Loss: 0.0020 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 88/99\n",
      "training Loss: 0.0024 Acc: 80.0101\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Epoch 89/99\n",
      "training Loss: 0.0025 Acc: 79.9030\n",
      "validation Loss: 0.0020 Acc: 82.8810\n",
      "Epoch 90/99\n",
      "training Loss: 0.0025 Acc: 80.0190\n",
      "validation Loss: 0.0020 Acc: 82.9881\n",
      "Epoch 91/99\n",
      "training Loss: 0.0024 Acc: 80.1083\n",
      "validation Loss: 0.0020 Acc: 82.9643\n",
      "Epoch 92/99\n",
      "training Loss: 0.0025 Acc: 79.8762\n",
      "validation Loss: 0.0020 Acc: 82.9643\n",
      "Epoch 93/99\n",
      "training Loss: 0.0024 Acc: 79.9952\n",
      "validation Loss: 0.0020 Acc: 82.9405\n",
      "Epoch 94/99\n",
      "training Loss: 0.0024 Acc: 80.2958\n",
      "validation Loss: 0.0020 Acc: 82.9524\n",
      "Epoch 95/99\n",
      "training Loss: 0.0024 Acc: 80.1946\n",
      "validation Loss: 0.0020 Acc: 82.9286\n",
      "Epoch 96/99\n",
      "training Loss: 0.0024 Acc: 80.2958\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Epoch 97/99\n",
      "training Loss: 0.0024 Acc: 80.5309\n",
      "validation Loss: 0.0020 Acc: 82.8929\n",
      "Epoch 98/99\n",
      "training Loss: 0.0024 Acc: 80.4595\n",
      "validation Loss: 0.0020 Acc: 82.9167\n",
      "Epoch 99/99\n",
      "training Loss: 0.0024 Acc: 80.3762\n",
      "validation Loss: 0.0020 Acc: 82.8810\n",
      "Best val acc: 83.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0037 Acc: 49.9464\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0037 Acc: 50.7291\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0037 Acc: 50.8392\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0037 Acc: 51.4523\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0036 Acc: 52.5088\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 53.7557\n",
      "validation Loss: 0.0036 Acc: 50.0714\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0036 Acc: 55.0265\n",
      "validation Loss: 0.0036 Acc: 50.2619\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0036 Acc: 56.8300\n",
      "validation Loss: 0.0036 Acc: 51.1310\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0035 Acc: 58.5382\n",
      "validation Loss: 0.0035 Acc: 54.1071\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0035 Acc: 60.1839\n",
      "validation Loss: 0.0035 Acc: 62.0476\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 62.0142\n",
      "validation Loss: 0.0034 Acc: 69.8690\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0034 Acc: 64.1301\n",
      "validation Loss: 0.0033 Acc: 74.4286\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0034 Acc: 65.2937\n",
      "validation Loss: 0.0033 Acc: 77.8095\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0033 Acc: 67.0853\n",
      "validation Loss: 0.0032 Acc: 79.6190\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0033 Acc: 68.7816\n",
      "validation Loss: 0.0032 Acc: 80.5000\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0032 Acc: 69.8143\n",
      "validation Loss: 0.0031 Acc: 81.1548\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0032 Acc: 70.6059\n",
      "validation Loss: 0.0030 Acc: 81.5476\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0032 Acc: 71.7070\n",
      "validation Loss: 0.0030 Acc: 81.6786\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0032 Acc: 72.2487\n",
      "validation Loss: 0.0030 Acc: 81.8571\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0031 Acc: 73.4510\n",
      "validation Loss: 0.0029 Acc: 81.8452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0031 Acc: 73.9420\n",
      "validation Loss: 0.0029 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0031 Acc: 75.0104\n",
      "validation Loss: 0.0029 Acc: 82.1190\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0031 Acc: 75.1771\n",
      "validation Loss: 0.0028 Acc: 82.2262\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0030 Acc: 75.4985\n",
      "validation Loss: 0.0028 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0030 Acc: 75.8110\n",
      "validation Loss: 0.0028 Acc: 82.2500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0030 Acc: 76.2722\n",
      "validation Loss: 0.0028 Acc: 82.2500\n",
      "Epoch 26/99\n",
      "training Loss: 0.0030 Acc: 76.7722\n",
      "validation Loss: 0.0027 Acc: 82.2024\n",
      "Epoch 27/99\n",
      "training Loss: 0.0030 Acc: 76.7960\n",
      "validation Loss: 0.0027 Acc: 82.3333\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0030 Acc: 77.2424\n",
      "validation Loss: 0.0027 Acc: 82.2500\n",
      "Epoch 29/99\n",
      "training Loss: 0.0030 Acc: 77.1174\n",
      "validation Loss: 0.0027 Acc: 82.1548\n",
      "Epoch 30/99\n",
      "training Loss: 0.0029 Acc: 77.5192\n",
      "validation Loss: 0.0027 Acc: 82.2262\n",
      "Epoch 31/99\n",
      "training Loss: 0.0029 Acc: 77.9091\n",
      "validation Loss: 0.0027 Acc: 82.2857\n",
      "Epoch 32/99\n",
      "training Loss: 0.0029 Acc: 77.8079\n",
      "validation Loss: 0.0027 Acc: 82.2619\n",
      "Epoch 33/99\n",
      "training Loss: 0.0029 Acc: 77.9686\n",
      "validation Loss: 0.0026 Acc: 82.2381\n",
      "Epoch 34/99\n",
      "training Loss: 0.0029 Acc: 77.9210\n",
      "validation Loss: 0.0026 Acc: 82.3333\n",
      "Epoch 35/99\n",
      "training Loss: 0.0029 Acc: 77.9983\n",
      "validation Loss: 0.0026 Acc: 82.3929\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0029 Acc: 78.2989\n",
      "validation Loss: 0.0026 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0029 Acc: 78.3584\n",
      "validation Loss: 0.0026 Acc: 82.6071\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0028 Acc: 78.3912\n",
      "validation Loss: 0.0026 Acc: 82.5595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0028 Acc: 78.5340\n",
      "validation Loss: 0.0026 Acc: 82.6071\n",
      "Epoch 40/99\n",
      "training Loss: 0.0028 Acc: 78.5310\n",
      "validation Loss: 0.0026 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0028 Acc: 78.7364\n",
      "validation Loss: 0.0026 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0028 Acc: 78.8614\n",
      "validation Loss: 0.0025 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0028 Acc: 79.1441\n",
      "validation Loss: 0.0025 Acc: 82.7500\n",
      "Epoch 44/99\n",
      "training Loss: 0.0028 Acc: 79.1024\n",
      "validation Loss: 0.0025 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0028 Acc: 79.1024\n",
      "validation Loss: 0.0025 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0028 Acc: 78.8346\n",
      "validation Loss: 0.0025 Acc: 82.8452\n",
      "Epoch 47/99\n",
      "training Loss: 0.0028 Acc: 78.9298\n",
      "validation Loss: 0.0025 Acc: 82.8095\n",
      "Epoch 48/99\n",
      "training Loss: 0.0028 Acc: 78.8316\n",
      "validation Loss: 0.0025 Acc: 82.6905\n",
      "Epoch 49/99\n",
      "training Loss: 0.0027 Acc: 79.1352\n",
      "validation Loss: 0.0025 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0028 Acc: 78.9834\n",
      "validation Loss: 0.0025 Acc: 82.7143\n",
      "Epoch 51/99\n",
      "training Loss: 0.0027 Acc: 79.1858\n",
      "validation Loss: 0.0025 Acc: 82.7024\n",
      "Epoch 52/99\n",
      "training Loss: 0.0027 Acc: 79.3227\n",
      "validation Loss: 0.0025 Acc: 82.7024\n",
      "Epoch 53/99\n",
      "training Loss: 0.0027 Acc: 79.1947\n",
      "validation Loss: 0.0025 Acc: 82.8095\n",
      "Epoch 54/99\n",
      "training Loss: 0.0027 Acc: 79.2869\n",
      "validation Loss: 0.0025 Acc: 82.7738\n",
      "Epoch 55/99\n",
      "training Loss: 0.0027 Acc: 79.5369\n",
      "validation Loss: 0.0025 Acc: 82.8095\n",
      "Epoch 56/99\n",
      "training Loss: 0.0027 Acc: 79.3465\n",
      "validation Loss: 0.0025 Acc: 82.8214\n",
      "Epoch 57/99\n",
      "training Loss: 0.0027 Acc: 79.4357\n",
      "validation Loss: 0.0025 Acc: 82.7500\n",
      "Epoch 58/99\n",
      "training Loss: 0.0027 Acc: 79.4506\n",
      "validation Loss: 0.0025 Acc: 82.8214\n",
      "Epoch 59/99\n",
      "training Loss: 0.0027 Acc: 79.2929\n",
      "validation Loss: 0.0025 Acc: 82.7024\n",
      "Epoch 60/99\n",
      "training Loss: 0.0027 Acc: 79.2989\n",
      "validation Loss: 0.0025 Acc: 82.6190\n",
      "Epoch 61/99\n",
      "training Loss: 0.0027 Acc: 79.3852\n",
      "validation Loss: 0.0024 Acc: 82.5595\n",
      "Epoch 62/99\n",
      "training Loss: 0.0027 Acc: 79.2810\n",
      "validation Loss: 0.0024 Acc: 82.5833\n",
      "Epoch 63/99\n",
      "training Loss: 0.0027 Acc: 79.5845\n",
      "validation Loss: 0.0024 Acc: 82.5952\n",
      "Epoch 64/99\n",
      "training Loss: 0.0027 Acc: 79.7006\n",
      "validation Loss: 0.0024 Acc: 82.5714\n",
      "Epoch 65/99\n",
      "training Loss: 0.0027 Acc: 79.6054\n",
      "validation Loss: 0.0024 Acc: 82.5833\n",
      "Epoch 66/99\n",
      "training Loss: 0.0027 Acc: 79.8464\n",
      "validation Loss: 0.0024 Acc: 82.6071\n",
      "Epoch 67/99\n",
      "training Loss: 0.0027 Acc: 79.7512\n",
      "validation Loss: 0.0024 Acc: 82.6310\n",
      "Epoch 68/99\n",
      "training Loss: 0.0026 Acc: 79.7304\n",
      "validation Loss: 0.0024 Acc: 82.6190\n",
      "Epoch 69/99\n",
      "training Loss: 0.0027 Acc: 79.4000\n",
      "validation Loss: 0.0024 Acc: 82.5833\n",
      "Early stopped.\n",
      "Best val acc: 82.869048\n",
      "----------\n",
      "Average best_acc across k-fold: 82.9412787205\n",
      "New configuration: {'learning_rate': 0.0007673611918774713, 'initial_nodes': 1000, 'dropout': 0.12550627122664826, 'batch_size': 197, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 82.2798\n",
      "validation Loss: 0.0019 Acc: 83.3373\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0019 Acc: 83.3393\n",
      "validation Loss: 0.0019 Acc: 83.3849\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0018 Acc: 83.5923\n",
      "validation Loss: 0.0019 Acc: 83.6825\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 83.7113\n",
      "validation Loss: 0.0019 Acc: 83.2302\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 83.7083\n",
      "validation Loss: 0.0018 Acc: 83.5039\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 83.9524\n",
      "validation Loss: 0.0019 Acc: 83.4920\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 83.9970\n",
      "validation Loss: 0.0019 Acc: 83.4682\n",
      "Epoch 7/99\n",
      "training Loss: 0.0018 Acc: 84.0268\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Epoch 8/99\n",
      "training Loss: 0.0018 Acc: 84.1250\n",
      "validation Loss: 0.0019 Acc: 83.7658\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 84.0863\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 84.1220\n",
      "validation Loss: 0.0018 Acc: 83.4801\n",
      "Epoch 11/99\n",
      "training Loss: 0.0017 Acc: 84.5387\n",
      "validation Loss: 0.0018 Acc: 83.7539\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 84.4583\n",
      "validation Loss: 0.0018 Acc: 83.8491\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 84.5506\n",
      "validation Loss: 0.0018 Acc: 83.8729\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 84.6815\n",
      "validation Loss: 0.0019 Acc: 83.5634\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 84.6488\n",
      "validation Loss: 0.0018 Acc: 83.8967\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 84.7560\n",
      "validation Loss: 0.0018 Acc: 83.9324\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0017 Acc: 84.6935\n",
      "validation Loss: 0.0019 Acc: 83.4682\n",
      "Epoch 18/99\n",
      "training Loss: 0.0017 Acc: 84.7857\n",
      "validation Loss: 0.0018 Acc: 83.6468\n",
      "Epoch 19/99\n",
      "training Loss: 0.0017 Acc: 84.9792\n",
      "validation Loss: 0.0018 Acc: 83.8372\n",
      "Epoch 20/99\n",
      "training Loss: 0.0017 Acc: 85.1429\n",
      "validation Loss: 0.0019 Acc: 83.9443\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0017 Acc: 85.1964\n",
      "validation Loss: 0.0019 Acc: 83.7182\n",
      "Epoch 22/99\n",
      "training Loss: 0.0017 Acc: 85.1488\n",
      "validation Loss: 0.0019 Acc: 83.8134\n",
      "Epoch 23/99\n",
      "training Loss: 0.0016 Acc: 85.3244\n",
      "validation Loss: 0.0019 Acc: 83.7301\n",
      "Epoch 24/99\n",
      "training Loss: 0.0016 Acc: 85.2202\n",
      "validation Loss: 0.0019 Acc: 83.7539\n",
      "Epoch 25/99\n",
      "training Loss: 0.0016 Acc: 85.4107\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Epoch 26/99\n",
      "training Loss: 0.0016 Acc: 85.4464\n",
      "validation Loss: 0.0019 Acc: 84.0395\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0016 Acc: 85.6131\n",
      "validation Loss: 0.0019 Acc: 83.7063\n",
      "Epoch 28/99\n",
      "training Loss: 0.0016 Acc: 85.7679\n",
      "validation Loss: 0.0019 Acc: 83.6706\n",
      "Epoch 29/99\n",
      "training Loss: 0.0016 Acc: 85.6607\n",
      "validation Loss: 0.0019 Acc: 83.8967\n",
      "Epoch 30/99\n",
      "training Loss: 0.0016 Acc: 85.6637\n",
      "validation Loss: 0.0019 Acc: 83.5872\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 85.7708\n",
      "validation Loss: 0.0019 Acc: 83.7539\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 85.9018\n",
      "validation Loss: 0.0019 Acc: 83.6706\n",
      "Epoch 33/99\n",
      "training Loss: 0.0016 Acc: 85.9970\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 85.9167\n",
      "validation Loss: 0.0019 Acc: 83.8253\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 85.9196\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 85.9702\n",
      "validation Loss: 0.0019 Acc: 83.5039\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 86.1012\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 85.9643\n",
      "validation Loss: 0.0019 Acc: 83.6825\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 86.1637\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 86.0506\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 86.0238\n",
      "validation Loss: 0.0019 Acc: 83.6706\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 86.1339\n",
      "validation Loss: 0.0019 Acc: 83.7063\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 86.0952\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Epoch 44/99\n",
      "training Loss: 0.0015 Acc: 86.2083\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 45/99\n",
      "training Loss: 0.0015 Acc: 86.2262\n",
      "validation Loss: 0.0019 Acc: 83.5158\n",
      "Epoch 46/99\n",
      "training Loss: 0.0015 Acc: 86.1667\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Early stopped.\n",
      "Best val acc: 84.039514\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 82.2005\n",
      "validation Loss: 0.0018 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0019 Acc: 83.1558\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Epoch 2/99\n",
      "training Loss: 0.0018 Acc: 83.5218\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 83.5694\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 83.6974\n",
      "validation Loss: 0.0018 Acc: 83.8571\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 83.9623\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 83.7837\n",
      "validation Loss: 0.0018 Acc: 84.2143\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0018 Acc: 83.9385\n",
      "validation Loss: 0.0018 Acc: 84.0595\n",
      "Epoch 8/99\n",
      "training Loss: 0.0018 Acc: 83.9266\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 84.0605\n",
      "validation Loss: 0.0018 Acc: 83.3452\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 84.2033\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 84.1527\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Epoch 12/99\n",
      "training Loss: 0.0018 Acc: 84.3105\n",
      "validation Loss: 0.0018 Acc: 84.0833\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 84.2986\n",
      "validation Loss: 0.0018 Acc: 84.1667\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 84.4831\n",
      "validation Loss: 0.0018 Acc: 83.8571\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 84.7628\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 84.8075\n",
      "validation Loss: 0.0018 Acc: 84.0357\n",
      "Epoch 17/99\n",
      "training Loss: 0.0017 Acc: 84.7866\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 18/99\n",
      "training Loss: 0.0017 Acc: 84.9354\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 19/99\n",
      "training Loss: 0.0017 Acc: 84.8848\n",
      "validation Loss: 0.0019 Acc: 83.9048\n",
      "Epoch 20/99\n",
      "training Loss: 0.0016 Acc: 85.1646\n",
      "validation Loss: 0.0018 Acc: 83.6548\n",
      "Epoch 21/99\n",
      "training Loss: 0.0016 Acc: 85.3997\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Epoch 22/99\n",
      "training Loss: 0.0016 Acc: 85.3908\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 23/99\n",
      "training Loss: 0.0016 Acc: 85.3848\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 24/99\n",
      "training Loss: 0.0016 Acc: 85.3967\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 25/99\n",
      "training Loss: 0.0016 Acc: 85.4473\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 26/99\n",
      "training Loss: 0.0016 Acc: 85.7003\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Early stopped.\n",
      "Best val acc: 84.214286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 82.3969\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0019 Acc: 83.4563\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0018 Acc: 83.5278\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 83.6260\n",
      "validation Loss: 0.0019 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 83.5843\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 83.8194\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 83.7331\n",
      "validation Loss: 0.0018 Acc: 83.6548\n",
      "Epoch 7/99\n",
      "training Loss: 0.0018 Acc: 83.9980\n",
      "validation Loss: 0.0018 Acc: 83.4524\n",
      "Epoch 8/99\n",
      "training Loss: 0.0018 Acc: 84.0813\n",
      "validation Loss: 0.0018 Acc: 83.8452\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 84.0099\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 84.0515\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 84.2122\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Epoch 12/99\n",
      "training Loss: 0.0018 Acc: 84.2450\n",
      "validation Loss: 0.0018 Acc: 83.7381\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 84.3015\n",
      "validation Loss: 0.0018 Acc: 83.8810\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 84.2539\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 84.4027\n",
      "validation Loss: 0.0018 Acc: 83.6905\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 84.4741\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 17/99\n",
      "training Loss: 0.0017 Acc: 84.7241\n",
      "validation Loss: 0.0018 Acc: 83.8214\n",
      "Epoch 18/99\n",
      "training Loss: 0.0017 Acc: 84.8194\n",
      "validation Loss: 0.0018 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0017 Acc: 84.8253\n",
      "validation Loss: 0.0018 Acc: 83.8810\n",
      "Epoch 20/99\n",
      "training Loss: 0.0017 Acc: 84.8670\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Epoch 21/99\n",
      "training Loss: 0.0017 Acc: 84.9979\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Epoch 22/99\n",
      "training Loss: 0.0017 Acc: 84.9682\n",
      "validation Loss: 0.0019 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0016 Acc: 85.2628\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0016 Acc: 85.3699\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 25/99\n",
      "training Loss: 0.0016 Acc: 85.3729\n",
      "validation Loss: 0.0019 Acc: 83.9762\n",
      "Epoch 26/99\n",
      "training Loss: 0.0016 Acc: 85.5157\n",
      "validation Loss: 0.0019 Acc: 83.9524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0016 Acc: 85.5812\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Epoch 28/99\n",
      "training Loss: 0.0016 Acc: 85.5485\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 29/99\n",
      "training Loss: 0.0016 Acc: 85.8014\n",
      "validation Loss: 0.0019 Acc: 83.9167\n",
      "Epoch 30/99\n",
      "training Loss: 0.0016 Acc: 85.7866\n",
      "validation Loss: 0.0019 Acc: 83.9167\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 85.8342\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 85.8193\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 86.0157\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 85.9830\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 86.0931\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 86.1764\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 86.2300\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 86.1913\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 39/99\n",
      "training Loss: 0.0015 Acc: 86.2181\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0015 Acc: 86.2717\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 41/99\n",
      "training Loss: 0.0015 Acc: 86.2330\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 42/99\n",
      "training Loss: 0.0015 Acc: 86.3341\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 43/99\n",
      "training Loss: 0.0015 Acc: 86.3937\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Early stopped.\n",
      "Best val acc: 84.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 82.2511\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0019 Acc: 83.2718\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0018 Acc: 83.5873\n",
      "validation Loss: 0.0018 Acc: 83.4167\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 83.6557\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 83.7778\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 83.8908\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 83.9801\n",
      "validation Loss: 0.0018 Acc: 83.5595\n",
      "Epoch 7/99\n",
      "training Loss: 0.0018 Acc: 84.0724\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Epoch 8/99\n",
      "training Loss: 0.0018 Acc: 84.2480\n",
      "validation Loss: 0.0018 Acc: 83.4405\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 84.1765\n",
      "validation Loss: 0.0018 Acc: 83.6071\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 84.2271\n",
      "validation Loss: 0.0018 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 84.3045\n",
      "validation Loss: 0.0018 Acc: 83.4524\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 84.4414\n",
      "validation Loss: 0.0018 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 84.6319\n",
      "validation Loss: 0.0018 Acc: 83.7024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 84.6795\n",
      "validation Loss: 0.0018 Acc: 83.7024\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 84.7509\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 84.7896\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 17/99\n",
      "training Loss: 0.0017 Acc: 84.7331\n",
      "validation Loss: 0.0018 Acc: 83.6429\n",
      "Epoch 18/99\n",
      "training Loss: 0.0017 Acc: 84.8104\n",
      "validation Loss: 0.0018 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0017 Acc: 84.9771\n",
      "validation Loss: 0.0018 Acc: 83.8571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0017 Acc: 85.1527\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 21/99\n",
      "training Loss: 0.0017 Acc: 85.2628\n",
      "validation Loss: 0.0018 Acc: 83.6667\n",
      "Epoch 22/99\n",
      "training Loss: 0.0016 Acc: 85.2717\n",
      "validation Loss: 0.0018 Acc: 83.9524\n",
      "Epoch 23/99\n",
      "training Loss: 0.0016 Acc: 85.3818\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 24/99\n",
      "training Loss: 0.0016 Acc: 85.4919\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Epoch 25/99\n",
      "training Loss: 0.0016 Acc: 85.4473\n",
      "validation Loss: 0.0018 Acc: 83.6429\n",
      "Epoch 26/99\n",
      "training Loss: 0.0016 Acc: 85.6794\n",
      "validation Loss: 0.0019 Acc: 83.7619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0016 Acc: 85.6437\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 28/99\n",
      "training Loss: 0.0016 Acc: 85.7687\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 29/99\n",
      "training Loss: 0.0016 Acc: 85.8401\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 30/99\n",
      "training Loss: 0.0016 Acc: 85.7360\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 85.8282\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 85.9443\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 33/99\n",
      "training Loss: 0.0016 Acc: 86.0127\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 86.1050\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 86.0544\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 86.1258\n",
      "validation Loss: 0.0019 Acc: 83.8214\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 86.1824\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 86.1169\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Early stopped.\n",
      "Best val acc: 83.976190\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0020 Acc: 82.5368\n",
      "validation Loss: 0.0019 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0019 Acc: 83.2272\n",
      "validation Loss: 0.0019 Acc: 82.7143\n",
      "Epoch 2/99\n",
      "training Loss: 0.0019 Acc: 83.4861\n",
      "validation Loss: 0.0018 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0018 Acc: 83.6349\n",
      "validation Loss: 0.0019 Acc: 83.0833\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 83.8016\n",
      "validation Loss: 0.0018 Acc: 83.2619\n",
      "Epoch 5/99\n",
      "training Loss: 0.0018 Acc: 84.0099\n",
      "validation Loss: 0.0018 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0018 Acc: 84.0069\n",
      "validation Loss: 0.0018 Acc: 83.3929\n",
      "Epoch 7/99\n",
      "training Loss: 0.0018 Acc: 84.0545\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0018 Acc: 84.0694\n",
      "validation Loss: 0.0018 Acc: 83.4762\n",
      "Epoch 9/99\n",
      "training Loss: 0.0018 Acc: 84.1884\n",
      "validation Loss: 0.0018 Acc: 83.1310\n",
      "Epoch 10/99\n",
      "training Loss: 0.0018 Acc: 84.3105\n",
      "validation Loss: 0.0018 Acc: 83.4048\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 84.1825\n",
      "validation Loss: 0.0018 Acc: 83.1786\n",
      "Epoch 12/99\n",
      "training Loss: 0.0017 Acc: 84.4503\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0017 Acc: 84.4950\n",
      "validation Loss: 0.0018 Acc: 83.6071\n",
      "Epoch 14/99\n",
      "training Loss: 0.0017 Acc: 84.7063\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 15/99\n",
      "training Loss: 0.0017 Acc: 84.6854\n",
      "validation Loss: 0.0018 Acc: 83.5595\n",
      "Epoch 16/99\n",
      "training Loss: 0.0017 Acc: 84.8223\n",
      "validation Loss: 0.0018 Acc: 83.5714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0017 Acc: 84.8342\n",
      "validation Loss: 0.0018 Acc: 83.4048\n",
      "Epoch 18/99\n",
      "training Loss: 0.0017 Acc: 84.8938\n",
      "validation Loss: 0.0018 Acc: 83.3214\n",
      "Epoch 19/99\n",
      "training Loss: 0.0017 Acc: 84.8402\n",
      "validation Loss: 0.0018 Acc: 83.4881\n",
      "Epoch 20/99\n",
      "training Loss: 0.0017 Acc: 84.9592\n",
      "validation Loss: 0.0018 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0017 Acc: 84.9949\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0016 Acc: 85.2836\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 23/99\n",
      "training Loss: 0.0016 Acc: 85.3699\n",
      "validation Loss: 0.0018 Acc: 83.3333\n",
      "Epoch 24/99\n",
      "training Loss: 0.0016 Acc: 85.5396\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 25/99\n",
      "training Loss: 0.0016 Acc: 85.5455\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 26/99\n",
      "training Loss: 0.0016 Acc: 85.4949\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 27/99\n",
      "training Loss: 0.0016 Acc: 85.5693\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 28/99\n",
      "training Loss: 0.0016 Acc: 85.8401\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 29/99\n",
      "training Loss: 0.0016 Acc: 85.8372\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 30/99\n",
      "training Loss: 0.0016 Acc: 85.8877\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 86.0008\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 85.8758\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 33/99\n",
      "training Loss: 0.0016 Acc: 86.0723\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 86.1348\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 86.2836\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 86.2359\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 86.1437\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 86.2627\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 39/99\n",
      "training Loss: 0.0015 Acc: 86.2687\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 40/99\n",
      "training Loss: 0.0015 Acc: 86.3669\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 41/99\n",
      "training Loss: 0.0015 Acc: 86.4085\n",
      "validation Loss: 0.0019 Acc: 83.0833\n",
      "Early stopped.\n",
      "Best val acc: 83.702381\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9864743088\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 971, 'dropout': 0.01, 'batch_size': 188, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 77.0595\n",
      "validation Loss: 0.0026 Acc: 80.1238\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.9851\n",
      "validation Loss: 0.0022 Acc: 81.9448\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.2798\n",
      "validation Loss: 0.0021 Acc: 82.5042\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.6845\n",
      "validation Loss: 0.0021 Acc: 82.5994\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.8571\n",
      "validation Loss: 0.0020 Acc: 82.5756\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 83.0982\n",
      "validation Loss: 0.0020 Acc: 82.7184\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 83.2440\n",
      "validation Loss: 0.0020 Acc: 82.5042\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 83.3929\n",
      "validation Loss: 0.0020 Acc: 82.7541\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 83.3601\n",
      "validation Loss: 0.0020 Acc: 82.6708\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.4732\n",
      "validation Loss: 0.0020 Acc: 82.7184\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.5268\n",
      "validation Loss: 0.0020 Acc: 82.6589\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.5625\n",
      "validation Loss: 0.0020 Acc: 82.8136\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.5268\n",
      "validation Loss: 0.0020 Acc: 82.7065\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.5655\n",
      "validation Loss: 0.0020 Acc: 82.8731\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.6429\n",
      "validation Loss: 0.0020 Acc: 82.8731\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.6994\n",
      "validation Loss: 0.0020 Acc: 82.8255\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.7054\n",
      "validation Loss: 0.0020 Acc: 82.8612\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.6756\n",
      "validation Loss: 0.0020 Acc: 82.8612\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.6786\n",
      "validation Loss: 0.0020 Acc: 82.8850\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.6696\n",
      "validation Loss: 0.0020 Acc: 82.9921\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.8036\n",
      "validation Loss: 0.0020 Acc: 82.9564\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.7946\n",
      "validation Loss: 0.0020 Acc: 82.9802\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7917\n",
      "validation Loss: 0.0020 Acc: 82.9088\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.8720\n",
      "validation Loss: 0.0020 Acc: 83.0398\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.7827\n",
      "validation Loss: 0.0020 Acc: 82.9683\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.8869\n",
      "validation Loss: 0.0020 Acc: 82.9921\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.9077\n",
      "validation Loss: 0.0020 Acc: 82.8969\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.8393\n",
      "validation Loss: 0.0020 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.9107\n",
      "validation Loss: 0.0020 Acc: 83.0874\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.9524\n",
      "validation Loss: 0.0020 Acc: 83.0398\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.9673\n",
      "validation Loss: 0.0020 Acc: 83.1231\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.9613\n",
      "validation Loss: 0.0020 Acc: 83.1350\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.9554\n",
      "validation Loss: 0.0019 Acc: 83.2302\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 84.0357\n",
      "validation Loss: 0.0019 Acc: 83.1112\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 84.1101\n",
      "validation Loss: 0.0020 Acc: 83.3254\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 84.0655\n",
      "validation Loss: 0.0020 Acc: 83.1826\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 84.1339\n",
      "validation Loss: 0.0020 Acc: 83.2064\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 84.0952\n",
      "validation Loss: 0.0020 Acc: 83.2778\n",
      "Epoch 38/99\n",
      "training Loss: 0.0019 Acc: 84.1369\n",
      "validation Loss: 0.0019 Acc: 83.2421\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 84.0774\n",
      "validation Loss: 0.0019 Acc: 83.2064\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 84.1875\n",
      "validation Loss: 0.0019 Acc: 83.3492\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 84.1369\n",
      "validation Loss: 0.0019 Acc: 83.2064\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 84.1488\n",
      "validation Loss: 0.0019 Acc: 83.3135\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 84.1518\n",
      "validation Loss: 0.0019 Acc: 83.2421\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 84.0863\n",
      "validation Loss: 0.0019 Acc: 83.2183\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 84.2589\n",
      "validation Loss: 0.0019 Acc: 83.3968\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 84.1935\n",
      "validation Loss: 0.0019 Acc: 83.2302\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 84.1190\n",
      "validation Loss: 0.0019 Acc: 83.2302\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 84.2173\n",
      "validation Loss: 0.0019 Acc: 83.5396\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 84.2083\n",
      "validation Loss: 0.0019 Acc: 83.2778\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.2530\n",
      "validation Loss: 0.0019 Acc: 83.1231\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 84.2024\n",
      "validation Loss: 0.0019 Acc: 83.3968\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 84.1429\n",
      "validation Loss: 0.0019 Acc: 83.3730\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 84.2500\n",
      "validation Loss: 0.0019 Acc: 83.4206\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 84.2143\n",
      "validation Loss: 0.0019 Acc: 83.3730\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 84.2500\n",
      "validation Loss: 0.0019 Acc: 83.3730\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.3155\n",
      "validation Loss: 0.0019 Acc: 83.4444\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 84.2768\n",
      "validation Loss: 0.0019 Acc: 83.4444\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.3393\n",
      "validation Loss: 0.0019 Acc: 83.4087\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.3036\n",
      "validation Loss: 0.0019 Acc: 83.3730\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.4077\n",
      "validation Loss: 0.0019 Acc: 83.4444\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.3065\n",
      "validation Loss: 0.0019 Acc: 83.4563\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.3810\n",
      "validation Loss: 0.0019 Acc: 83.4801\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.4315\n",
      "validation Loss: 0.0019 Acc: 83.5158\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.4583\n",
      "validation Loss: 0.0019 Acc: 83.5634\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 84.4107\n",
      "validation Loss: 0.0019 Acc: 83.5991\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.4851\n",
      "validation Loss: 0.0019 Acc: 83.4444\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.3482\n",
      "validation Loss: 0.0019 Acc: 83.4801\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.4405\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.4702\n",
      "validation Loss: 0.0019 Acc: 83.5158\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 84.3988\n",
      "validation Loss: 0.0019 Acc: 83.5039\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 84.3720\n",
      "validation Loss: 0.0019 Acc: 83.4801\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 84.4613\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 84.4435\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 84.4583\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.4226\n",
      "validation Loss: 0.0019 Acc: 83.5396\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 84.4792\n",
      "validation Loss: 0.0019 Acc: 83.5158\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 84.4821\n",
      "validation Loss: 0.0019 Acc: 83.4920\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 84.5149\n",
      "validation Loss: 0.0019 Acc: 83.4920\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 84.4940\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 84.4405\n",
      "validation Loss: 0.0019 Acc: 83.5396\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 84.4375\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 84.4762\n",
      "validation Loss: 0.0019 Acc: 83.5158\n",
      "Epoch 83/99\n",
      "training Loss: 0.0018 Acc: 84.4792\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 84/99\n",
      "training Loss: 0.0018 Acc: 84.5387\n",
      "validation Loss: 0.0019 Acc: 83.4682\n",
      "Epoch 85/99\n",
      "training Loss: 0.0018 Acc: 84.4851\n",
      "validation Loss: 0.0019 Acc: 83.4563\n",
      "Early stopped.\n",
      "Best val acc: 83.599143\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0032 Acc: 75.7366\n",
      "validation Loss: 0.0026 Acc: 79.4524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.9684\n",
      "validation Loss: 0.0022 Acc: 81.2738\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.3076\n",
      "validation Loss: 0.0021 Acc: 82.4643\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 82.8403\n",
      "validation Loss: 0.0021 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.9713\n",
      "validation Loss: 0.0020 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 83.0218\n",
      "validation Loss: 0.0020 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 83.1587\n",
      "validation Loss: 0.0020 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 83.2748\n",
      "validation Loss: 0.0020 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 83.2956\n",
      "validation Loss: 0.0020 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 83.3581\n",
      "validation Loss: 0.0020 Acc: 83.3690\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.3611\n",
      "validation Loss: 0.0020 Acc: 83.4167\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.3849\n",
      "validation Loss: 0.0020 Acc: 83.4286\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.3700\n",
      "validation Loss: 0.0020 Acc: 83.3810\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.4861\n",
      "validation Loss: 0.0020 Acc: 83.3690\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.4474\n",
      "validation Loss: 0.0020 Acc: 83.3929\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.5754\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.5248\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.5992\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.6051\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.7599\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.6736\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.7748\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7301\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.7569\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.8135\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.7986\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.7956\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.7956\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.8670\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.7778\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.9325\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.8879\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.9027\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.9325\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.9533\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 83.9861\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 84.0188\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 84.0248\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 38/99\n",
      "training Loss: 0.0019 Acc: 84.0129\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 84.1051\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 84.1498\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 84.1200\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 84.1170\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 84.0664\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 84.2242\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 84.1259\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 84.1617\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 84.1557\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 84.2361\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 84.1855\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 84.2361\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 84.2509\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 84.2122\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 84.2599\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Early stopped.\n",
      "Best val acc: 83.678571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 77.4805\n",
      "validation Loss: 0.0026 Acc: 79.5833\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.7869\n",
      "validation Loss: 0.0022 Acc: 81.9643\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.3671\n",
      "validation Loss: 0.0021 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.7867\n",
      "validation Loss: 0.0020 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.8552\n",
      "validation Loss: 0.0020 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.9593\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 83.1855\n",
      "validation Loss: 0.0020 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 83.1558\n",
      "validation Loss: 0.0020 Acc: 83.4405\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 83.2837\n",
      "validation Loss: 0.0020 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 83.3135\n",
      "validation Loss: 0.0020 Acc: 83.5833\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.2778\n",
      "validation Loss: 0.0020 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.3165\n",
      "validation Loss: 0.0020 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.4415\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.4623\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.4861\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.4831\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.5397\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.4921\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.5635\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.5069\n",
      "validation Loss: 0.0019 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.6260\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.6290\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.6379\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.6557\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.7034\n",
      "validation Loss: 0.0019 Acc: 83.8452\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.8075\n",
      "validation Loss: 0.0019 Acc: 83.8452\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.6795\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.7569\n",
      "validation Loss: 0.0019 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.7420\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.8810\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.7837\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.8819\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.9176\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.9325\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 83.9355\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 83.9504\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 83.9623\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 38/99\n",
      "training Loss: 0.0019 Acc: 83.8908\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 83.9355\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 83.9920\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 84.0248\n",
      "validation Loss: 0.0019 Acc: 83.9762\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 84.0813\n",
      "validation Loss: 0.0019 Acc: 83.9524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 83.9890\n",
      "validation Loss: 0.0019 Acc: 83.9048\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 84.1200\n",
      "validation Loss: 0.0019 Acc: 83.9048\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 84.1438\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 84.1527\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 84.0902\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 84.1349\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 84.1706\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 84.1527\n",
      "validation Loss: 0.0019 Acc: 83.8452\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 84.1289\n",
      "validation Loss: 0.0019 Acc: 83.8810\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 84.1646\n",
      "validation Loss: 0.0019 Acc: 83.9167\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 84.1646\n",
      "validation Loss: 0.0019 Acc: 83.9167\n",
      "Epoch 54/99\n",
      "training Loss: 0.0019 Acc: 84.1587\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 84.1230\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 84.2152\n",
      "validation Loss: 0.0019 Acc: 83.9048\n",
      "Epoch 57/99\n",
      "training Loss: 0.0019 Acc: 84.1378\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Epoch 58/99\n",
      "training Loss: 0.0019 Acc: 84.1378\n",
      "validation Loss: 0.0019 Acc: 83.9048\n",
      "Early stopped.\n",
      "Best val acc: 84.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 77.6501\n",
      "validation Loss: 0.0026 Acc: 80.5000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 80.9505\n",
      "validation Loss: 0.0021 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.1856\n",
      "validation Loss: 0.0021 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 82.5040\n",
      "validation Loss: 0.0020 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.7927\n",
      "validation Loss: 0.0020 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 83.0516\n",
      "validation Loss: 0.0020 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 83.0457\n",
      "validation Loss: 0.0020 Acc: 83.5476\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 83.1409\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 83.2123\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 83.3165\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0020 Acc: 83.3611\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 11/99\n",
      "training Loss: 0.0020 Acc: 83.3492\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.3849\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.4385\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.5188\n",
      "validation Loss: 0.0019 Acc: 83.8214\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.4147\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.4712\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.4504\n",
      "validation Loss: 0.0019 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.4772\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.5010\n",
      "validation Loss: 0.0019 Acc: 83.8214\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.5546\n",
      "validation Loss: 0.0019 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.4861\n",
      "validation Loss: 0.0019 Acc: 83.8929\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.6081\n",
      "validation Loss: 0.0019 Acc: 83.7976\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.5784\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.5843\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.6379\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.7004\n",
      "validation Loss: 0.0019 Acc: 83.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.7599\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.7569\n",
      "validation Loss: 0.0019 Acc: 83.9048\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.7629\n",
      "validation Loss: 0.0019 Acc: 84.0119\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.7718\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.8045\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.8045\n",
      "validation Loss: 0.0019 Acc: 83.8571\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.8522\n",
      "validation Loss: 0.0019 Acc: 83.9762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 83.8998\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 83.8075\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 83.9027\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 38/99\n",
      "training Loss: 0.0019 Acc: 83.8432\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 83.9117\n",
      "validation Loss: 0.0019 Acc: 83.9524\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 83.9146\n",
      "validation Loss: 0.0019 Acc: 84.0119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 84.0069\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 83.9593\n",
      "validation Loss: 0.0019 Acc: 83.9524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 83.9682\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 84.0218\n",
      "validation Loss: 0.0019 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 83.9325\n",
      "validation Loss: 0.0019 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 83.9712\n",
      "validation Loss: 0.0019 Acc: 83.9405\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 84.0396\n",
      "validation Loss: 0.0019 Acc: 84.0476\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 84.0069\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 83.9295\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 84.1021\n",
      "validation Loss: 0.0019 Acc: 83.9643\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 84.0158\n",
      "validation Loss: 0.0019 Acc: 83.9167\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 84.0248\n",
      "validation Loss: 0.0019 Acc: 84.1786\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 84.0754\n",
      "validation Loss: 0.0019 Acc: 83.9643\n",
      "Epoch 54/99\n",
      "training Loss: 0.0019 Acc: 84.1230\n",
      "validation Loss: 0.0019 Acc: 84.0595\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 84.1974\n",
      "validation Loss: 0.0019 Acc: 83.9762\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 84.0902\n",
      "validation Loss: 0.0019 Acc: 84.1786\n",
      "Epoch 57/99\n",
      "training Loss: 0.0019 Acc: 84.1349\n",
      "validation Loss: 0.0019 Acc: 84.0714\n",
      "Epoch 58/99\n",
      "training Loss: 0.0019 Acc: 84.1408\n",
      "validation Loss: 0.0019 Acc: 84.0476\n",
      "Epoch 59/99\n",
      "training Loss: 0.0019 Acc: 84.2033\n",
      "validation Loss: 0.0019 Acc: 83.9286\n",
      "Epoch 60/99\n",
      "training Loss: 0.0019 Acc: 84.1944\n",
      "validation Loss: 0.0019 Acc: 84.0833\n",
      "Epoch 61/99\n",
      "training Loss: 0.0019 Acc: 84.2182\n",
      "validation Loss: 0.0019 Acc: 84.0238\n",
      "Epoch 62/99\n",
      "training Loss: 0.0019 Acc: 84.1944\n",
      "validation Loss: 0.0019 Acc: 84.0595\n",
      "Epoch 63/99\n",
      "training Loss: 0.0019 Acc: 84.2242\n",
      "validation Loss: 0.0019 Acc: 83.9881\n",
      "Epoch 64/99\n",
      "training Loss: 0.0019 Acc: 84.1557\n",
      "validation Loss: 0.0019 Acc: 84.1310\n",
      "Epoch 65/99\n",
      "training Loss: 0.0019 Acc: 84.1974\n",
      "validation Loss: 0.0019 Acc: 84.0952\n",
      "Epoch 66/99\n",
      "training Loss: 0.0019 Acc: 84.1587\n",
      "validation Loss: 0.0019 Acc: 83.9881\n",
      "Epoch 67/99\n",
      "training Loss: 0.0019 Acc: 84.2033\n",
      "validation Loss: 0.0019 Acc: 84.0714\n",
      "Epoch 68/99\n",
      "training Loss: 0.0019 Acc: 84.1557\n",
      "validation Loss: 0.0019 Acc: 84.0595\n",
      "Epoch 69/99\n",
      "training Loss: 0.0019 Acc: 84.2003\n",
      "validation Loss: 0.0019 Acc: 84.1310\n",
      "Epoch 70/99\n",
      "training Loss: 0.0019 Acc: 84.2301\n",
      "validation Loss: 0.0019 Acc: 84.0000\n",
      "Epoch 71/99\n",
      "training Loss: 0.0019 Acc: 84.1765\n",
      "validation Loss: 0.0019 Acc: 84.0357\n",
      "Epoch 72/99\n",
      "training Loss: 0.0019 Acc: 84.2093\n",
      "validation Loss: 0.0019 Acc: 84.0238\n",
      "Early stopped.\n",
      "Best val acc: 84.178571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 77.7722\n",
      "validation Loss: 0.0026 Acc: 79.5595\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 81.2719\n",
      "validation Loss: 0.0022 Acc: 81.6548\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.4743\n",
      "validation Loss: 0.0021 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.8195\n",
      "validation Loss: 0.0021 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.9921\n",
      "validation Loss: 0.0021 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 83.0992\n",
      "validation Loss: 0.0020 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 83.1766\n",
      "validation Loss: 0.0020 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 83.2837\n",
      "validation Loss: 0.0020 Acc: 82.7619\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 83.3968\n",
      "validation Loss: 0.0020 Acc: 82.7024\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.4057\n",
      "validation Loss: 0.0020 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.4355\n",
      "validation Loss: 0.0020 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.5159\n",
      "validation Loss: 0.0020 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.5010\n",
      "validation Loss: 0.0020 Acc: 82.8571\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.5486\n",
      "validation Loss: 0.0020 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.6141\n",
      "validation Loss: 0.0020 Acc: 82.9286\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.5605\n",
      "validation Loss: 0.0020 Acc: 82.9048\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.5575\n",
      "validation Loss: 0.0020 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.6081\n",
      "validation Loss: 0.0020 Acc: 83.0476\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.6587\n",
      "validation Loss: 0.0020 Acc: 83.0952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.6736\n",
      "validation Loss: 0.0020 Acc: 83.0238\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.6944\n",
      "validation Loss: 0.0020 Acc: 83.0238\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.7123\n",
      "validation Loss: 0.0020 Acc: 83.1071\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7004\n",
      "validation Loss: 0.0020 Acc: 83.1786\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.7688\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.7034\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.8551\n",
      "validation Loss: 0.0020 Acc: 83.0595\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.8045\n",
      "validation Loss: 0.0020 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.8789\n",
      "validation Loss: 0.0020 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.7956\n",
      "validation Loss: 0.0020 Acc: 83.2619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.9087\n",
      "validation Loss: 0.0020 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.9206\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.9266\n",
      "validation Loss: 0.0020 Acc: 83.3929\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.8908\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.9146\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.9087\n",
      "validation Loss: 0.0020 Acc: 83.2024\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 84.0129\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 84.0010\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 84.0515\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 38/99\n",
      "training Loss: 0.0019 Acc: 84.0248\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 83.9890\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 84.0754\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 84.0248\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 83.9742\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 84.0754\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 84.1200\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 84.1527\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 84.1765\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 84.1527\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 84.1557\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 84.2063\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 84.2063\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 84.2926\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 84.2301\n",
      "validation Loss: 0.0019 Acc: 83.6071\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 84.3343\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 84.2361\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.2271\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 84.2718\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.2212\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.2718\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.3849\n",
      "validation Loss: 0.0019 Acc: 83.7619\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.3253\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.2986\n",
      "validation Loss: 0.0019 Acc: 83.6310\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.3105\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.3521\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 84.4860\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.3759\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.4206\n",
      "validation Loss: 0.0019 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.3491\n",
      "validation Loss: 0.0019 Acc: 83.8214\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.4503\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 84.4652\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 84.4474\n",
      "validation Loss: 0.0019 Acc: 83.7857\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 84.4652\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 84.4682\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 84.5277\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.4503\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 84.4741\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 84.4771\n",
      "validation Loss: 0.0019 Acc: 83.7619\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 84.4920\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 84.4771\n",
      "validation Loss: 0.0019 Acc: 83.7143\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 84.5218\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 84.4355\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 84.5188\n",
      "validation Loss: 0.0019 Acc: 83.6905\n",
      "Epoch 83/99\n",
      "training Loss: 0.0018 Acc: 84.5366\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 84/99\n",
      "training Loss: 0.0018 Acc: 84.4831\n",
      "validation Loss: 0.0019 Acc: 83.7500\n",
      "Epoch 85/99\n",
      "training Loss: 0.0018 Acc: 84.4116\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 86/99\n",
      "training Loss: 0.0018 Acc: 84.5158\n",
      "validation Loss: 0.0019 Acc: 83.7262\n",
      "Epoch 87/99\n",
      "training Loss: 0.0018 Acc: 84.5009\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Early stopped.\n",
      "Best val acc: 83.833333\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8579238503\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 1000, 'dropout': 0.2888826152186183, 'batch_size': 196, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0033 Acc: 65.0119\n",
      "validation Loss: 0.0029 Acc: 77.7910\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 79.3899\n",
      "validation Loss: 0.0022 Acc: 80.9807\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 81.6042\n",
      "validation Loss: 0.0020 Acc: 82.4328\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.0208\n",
      "validation Loss: 0.0020 Acc: 83.0398\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.4256\n",
      "validation Loss: 0.0020 Acc: 82.9683\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.4792\n",
      "validation Loss: 0.0020 Acc: 83.1231\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 82.7054\n",
      "validation Loss: 0.0020 Acc: 83.0636\n",
      "Epoch 7/99\n",
      "training Loss: 0.0019 Acc: 82.7946\n",
      "validation Loss: 0.0019 Acc: 83.2302\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 82.9256\n",
      "validation Loss: 0.0019 Acc: 83.0755\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.0565\n",
      "validation Loss: 0.0019 Acc: 83.2064\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.0565\n",
      "validation Loss: 0.0019 Acc: 83.3016\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.0268\n",
      "validation Loss: 0.0019 Acc: 83.3968\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.2589\n",
      "validation Loss: 0.0019 Acc: 83.2897\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.1845\n",
      "validation Loss: 0.0019 Acc: 83.4444\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.1607\n",
      "validation Loss: 0.0019 Acc: 83.4682\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.4226\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.3542\n",
      "validation Loss: 0.0019 Acc: 83.3730\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.3423\n",
      "validation Loss: 0.0019 Acc: 83.4087\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.4167\n",
      "validation Loss: 0.0019 Acc: 83.4801\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.3839\n",
      "validation Loss: 0.0019 Acc: 83.4325\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.3899\n",
      "validation Loss: 0.0019 Acc: 83.4563\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.4643\n",
      "validation Loss: 0.0019 Acc: 83.5277\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.4286\n",
      "validation Loss: 0.0019 Acc: 83.5634\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.5833\n",
      "validation Loss: 0.0019 Acc: 83.5396\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.4851\n",
      "validation Loss: 0.0019 Acc: 83.5039\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 83.5179\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 83.5804\n",
      "validation Loss: 0.0019 Acc: 83.4920\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.4702\n",
      "validation Loss: 0.0019 Acc: 83.5634\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 83.7024\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 83.5923\n",
      "validation Loss: 0.0019 Acc: 83.6229\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.6488\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 83.7500\n",
      "validation Loss: 0.0019 Acc: 83.5039\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 83.6429\n",
      "validation Loss: 0.0019 Acc: 83.5158\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 83.6994\n",
      "validation Loss: 0.0019 Acc: 83.7420\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.6667\n",
      "validation Loss: 0.0019 Acc: 83.6110\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.7321\n",
      "validation Loss: 0.0019 Acc: 83.5753\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.6637\n",
      "validation Loss: 0.0019 Acc: 83.7182\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.8006\n",
      "validation Loss: 0.0019 Acc: 83.7896\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.9464\n",
      "validation Loss: 0.0019 Acc: 83.6944\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.8899\n",
      "validation Loss: 0.0019 Acc: 83.6110\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.8839\n",
      "validation Loss: 0.0019 Acc: 83.7658\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.8899\n",
      "validation Loss: 0.0019 Acc: 83.5515\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.9405\n",
      "validation Loss: 0.0019 Acc: 83.6944\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.7798\n",
      "validation Loss: 0.0019 Acc: 83.7658\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.7500\n",
      "validation Loss: 0.0019 Acc: 83.8253\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 84.0208\n",
      "validation Loss: 0.0019 Acc: 83.6944\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.9762\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.8393\n",
      "validation Loss: 0.0019 Acc: 83.7420\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 83.8869\n",
      "validation Loss: 0.0019 Acc: 83.7063\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 84.0863\n",
      "validation Loss: 0.0019 Acc: 83.7301\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 83.8839\n",
      "validation Loss: 0.0019 Acc: 83.7420\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 84.1042\n",
      "validation Loss: 0.0019 Acc: 83.6706\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 84.0000\n",
      "validation Loss: 0.0019 Acc: 83.7658\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 84.0506\n",
      "validation Loss: 0.0019 Acc: 83.8134\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 83.8720\n",
      "validation Loss: 0.0019 Acc: 83.6229\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 84.0298\n",
      "validation Loss: 0.0019 Acc: 83.6110\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.0536\n",
      "validation Loss: 0.0019 Acc: 83.6587\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 83.9821\n",
      "validation Loss: 0.0019 Acc: 83.6348\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.0952\n",
      "validation Loss: 0.0019 Acc: 83.7063\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.0982\n",
      "validation Loss: 0.0019 Acc: 83.7301\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.1161\n",
      "validation Loss: 0.0019 Acc: 83.6825\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.0982\n",
      "validation Loss: 0.0019 Acc: 83.7301\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 83.9940\n",
      "validation Loss: 0.0019 Acc: 83.7539\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.0179\n",
      "validation Loss: 0.0019 Acc: 83.7777\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.1875\n",
      "validation Loss: 0.0019 Acc: 83.7539\n",
      "Early stopped.\n",
      "Best val acc: 83.825280\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0033 Acc: 65.8741\n",
      "validation Loss: 0.0029 Acc: 78.8452\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 79.9595\n",
      "validation Loss: 0.0022 Acc: 80.6429\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 81.5844\n",
      "validation Loss: 0.0021 Acc: 81.6310\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 82.2064\n",
      "validation Loss: 0.0020 Acc: 81.9048\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.6290\n",
      "validation Loss: 0.0020 Acc: 82.2024\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.8552\n",
      "validation Loss: 0.0020 Acc: 82.1786\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 82.8909\n",
      "validation Loss: 0.0020 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 82.7540\n",
      "validation Loss: 0.0019 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.1141\n",
      "validation Loss: 0.0019 Acc: 82.5833\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.0962\n",
      "validation Loss: 0.0019 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.1349\n",
      "validation Loss: 0.0019 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.1349\n",
      "validation Loss: 0.0019 Acc: 82.9048\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.2510\n",
      "validation Loss: 0.0019 Acc: 82.8333\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.2569\n",
      "validation Loss: 0.0019 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.3611\n",
      "validation Loss: 0.0019 Acc: 82.7976\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.3165\n",
      "validation Loss: 0.0019 Acc: 82.8929\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.3819\n",
      "validation Loss: 0.0019 Acc: 82.9167\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.4444\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.4474\n",
      "validation Loss: 0.0019 Acc: 82.9167\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.4325\n",
      "validation Loss: 0.0019 Acc: 82.9643\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.4296\n",
      "validation Loss: 0.0019 Acc: 83.0357\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.5218\n",
      "validation Loss: 0.0019 Acc: 83.1071\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.5754\n",
      "validation Loss: 0.0019 Acc: 83.0595\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.6081\n",
      "validation Loss: 0.0019 Acc: 83.1071\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.6557\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.6498\n",
      "validation Loss: 0.0019 Acc: 83.1310\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.5843\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.7123\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.6349\n",
      "validation Loss: 0.0019 Acc: 83.1429\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.7301\n",
      "validation Loss: 0.0019 Acc: 83.1429\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.6855\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.7837\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.6766\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.6944\n",
      "validation Loss: 0.0019 Acc: 83.1190\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.8968\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.9057\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.8492\n",
      "validation Loss: 0.0019 Acc: 83.0952\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.7242\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.8462\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.8849\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.7778\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.8760\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.9474\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.9682\n",
      "validation Loss: 0.0019 Acc: 83.2857\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.8819\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 83.8879\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.8700\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 84.0099\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 83.9623\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.0367\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 83.9890\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 84.0724\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 84.1200\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 83.9623\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.0248\n",
      "validation Loss: 0.0018 Acc: 83.3571\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 83.8760\n",
      "validation Loss: 0.0018 Acc: 83.4524\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.0813\n",
      "validation Loss: 0.0018 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.0367\n",
      "validation Loss: 0.0018 Acc: 83.6071\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.0694\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.0634\n",
      "validation Loss: 0.0018 Acc: 83.3571\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.0724\n",
      "validation Loss: 0.0018 Acc: 83.4643\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.1468\n",
      "validation Loss: 0.0018 Acc: 83.2976\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.2837\n",
      "validation Loss: 0.0018 Acc: 83.4881\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 84.0932\n",
      "validation Loss: 0.0018 Acc: 83.4643\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.1974\n",
      "validation Loss: 0.0018 Acc: 83.4643\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.2450\n",
      "validation Loss: 0.0018 Acc: 83.2262\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.1706\n",
      "validation Loss: 0.0018 Acc: 83.5714\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.2390\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 84.1051\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 84.1974\n",
      "validation Loss: 0.0018 Acc: 83.4762\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 84.1200\n",
      "validation Loss: 0.0018 Acc: 83.4405\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 84.2301\n",
      "validation Loss: 0.0018 Acc: 83.5476\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 84.1230\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0018 Acc: 83.5357\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 84.2747\n",
      "validation Loss: 0.0018 Acc: 83.4762\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 84.2242\n",
      "validation Loss: 0.0018 Acc: 83.3690\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 84.2688\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Early stopped.\n",
      "Best val acc: 83.678571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 58.3299\n",
      "validation Loss: 0.0030 Acc: 77.9524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 79.2096\n",
      "validation Loss: 0.0022 Acc: 80.9762\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.1708\n",
      "validation Loss: 0.0020 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 82.0546\n",
      "validation Loss: 0.0020 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.3403\n",
      "validation Loss: 0.0019 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.5516\n",
      "validation Loss: 0.0019 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 82.7451\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 82.7897\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 83.0635\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 82.8969\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.1409\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.1320\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.2183\n",
      "validation Loss: 0.0018 Acc: 83.5595\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.2034\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.2569\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.3343\n",
      "validation Loss: 0.0018 Acc: 83.5714\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.1706\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.2510\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.4444\n",
      "validation Loss: 0.0018 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.2808\n",
      "validation Loss: 0.0018 Acc: 83.6548\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.4266\n",
      "validation Loss: 0.0018 Acc: 83.6786\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.4355\n",
      "validation Loss: 0.0018 Acc: 83.7381\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.5754\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.4444\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.5307\n",
      "validation Loss: 0.0018 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.4801\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.4980\n",
      "validation Loss: 0.0018 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.4921\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.5307\n",
      "validation Loss: 0.0018 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.5129\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.6200\n",
      "validation Loss: 0.0018 Acc: 83.7976\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.4742\n",
      "validation Loss: 0.0018 Acc: 83.9286\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.7034\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.7004\n",
      "validation Loss: 0.0018 Acc: 83.8571\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.5248\n",
      "validation Loss: 0.0018 Acc: 83.8690\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 83.6230\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.7123\n",
      "validation Loss: 0.0018 Acc: 84.0119\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.7034\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.7331\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.7063\n",
      "validation Loss: 0.0018 Acc: 83.8095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.7301\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.8432\n",
      "validation Loss: 0.0018 Acc: 83.8690\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.6468\n",
      "validation Loss: 0.0018 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.7718\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.8373\n",
      "validation Loss: 0.0018 Acc: 83.9167\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 83.7539\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.8135\n",
      "validation Loss: 0.0018 Acc: 84.0357\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.9266\n",
      "validation Loss: 0.0018 Acc: 84.0000\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 83.9474\n",
      "validation Loss: 0.0018 Acc: 84.0714\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 83.8492\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 83.9712\n",
      "validation Loss: 0.0018 Acc: 83.8929\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 83.8998\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 83.9355\n",
      "validation Loss: 0.0018 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 83.7629\n",
      "validation Loss: 0.0018 Acc: 83.9643\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 83.9325\n",
      "validation Loss: 0.0018 Acc: 84.1310\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 83.9831\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 83.9980\n",
      "validation Loss: 0.0018 Acc: 84.3214\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 83.9474\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 83.9325\n",
      "validation Loss: 0.0018 Acc: 84.1905\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 83.9771\n",
      "validation Loss: 0.0018 Acc: 84.0357\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.0337\n",
      "validation Loss: 0.0018 Acc: 84.2738\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.0396\n",
      "validation Loss: 0.0018 Acc: 84.2024\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 83.9682\n",
      "validation Loss: 0.0018 Acc: 84.3333\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.0843\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.0396\n",
      "validation Loss: 0.0018 Acc: 84.0952\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 83.9771\n",
      "validation Loss: 0.0018 Acc: 84.0833\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.0515\n",
      "validation Loss: 0.0018 Acc: 84.1548\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.0069\n",
      "validation Loss: 0.0018 Acc: 84.0952\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.0367\n",
      "validation Loss: 0.0018 Acc: 84.1786\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.1051\n",
      "validation Loss: 0.0018 Acc: 84.0119\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 84.1498\n",
      "validation Loss: 0.0018 Acc: 83.9524\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 84.2361\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 84.0962\n",
      "validation Loss: 0.0018 Acc: 84.1310\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 84.0575\n",
      "validation Loss: 0.0018 Acc: 84.0357\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 84.1498\n",
      "validation Loss: 0.0018 Acc: 83.9167\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.1140\n",
      "validation Loss: 0.0018 Acc: 84.1071\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0018 Acc: 84.1548\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 84.2718\n",
      "validation Loss: 0.0018 Acc: 84.1071\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 84.1170\n",
      "validation Loss: 0.0018 Acc: 84.0000\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 84.1438\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 84.1170\n",
      "validation Loss: 0.0018 Acc: 83.9881\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 84.0486\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 84.2509\n",
      "validation Loss: 0.0018 Acc: 84.1190\n",
      "Early stopped.\n",
      "Best val acc: 84.333333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0033 Acc: 73.4391\n",
      "validation Loss: 0.0029 Acc: 78.4405\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 79.7810\n",
      "validation Loss: 0.0022 Acc: 80.5714\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.6112\n",
      "validation Loss: 0.0021 Acc: 81.6310\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 82.2481\n",
      "validation Loss: 0.0020 Acc: 82.2619\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.5665\n",
      "validation Loss: 0.0020 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.7183\n",
      "validation Loss: 0.0020 Acc: 82.5714\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 82.8730\n",
      "validation Loss: 0.0020 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 82.9445\n",
      "validation Loss: 0.0020 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 82.9713\n",
      "validation Loss: 0.0019 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 82.9534\n",
      "validation Loss: 0.0019 Acc: 82.9524\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.1647\n",
      "validation Loss: 0.0019 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.2212\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.2331\n",
      "validation Loss: 0.0019 Acc: 83.1905\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.2718\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.3343\n",
      "validation Loss: 0.0019 Acc: 83.2024\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.3581\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.4534\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.4801\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.4474\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.5040\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.5159\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.5694\n",
      "validation Loss: 0.0019 Acc: 83.2619\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.5248\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.4444\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.5903\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.4623\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.5843\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.7034\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.7658\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.6914\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.6944\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 83.7510\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.7153\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 83.8373\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 83.7272\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.6795\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.7123\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.8819\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.8283\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.9027\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.8938\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.9146\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.9295\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 83.7063\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.8373\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.8938\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 83.8194\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 83.9444\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.0694\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 83.9980\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 84.0664\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 83.9057\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 83.9027\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 84.0694\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 84.0218\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 84.0456\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.0456\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 84.1378\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.1289\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.1200\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.1349\n",
      "validation Loss: 0.0018 Acc: 83.5357\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.2242\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 84.0010\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 83.9771\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.0575\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.1170\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.2390\n",
      "validation Loss: 0.0018 Acc: 83.5714\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 84.2569\n",
      "validation Loss: 0.0018 Acc: 83.5595\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 84.2390\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 84.0932\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 84.2569\n",
      "validation Loss: 0.0018 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 84.0783\n",
      "validation Loss: 0.0018 Acc: 83.5476\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.2271\n",
      "validation Loss: 0.0018 Acc: 83.6071\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 84.1676\n",
      "validation Loss: 0.0018 Acc: 83.5476\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 84.2956\n",
      "validation Loss: 0.0018 Acc: 83.6548\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 84.2242\n",
      "validation Loss: 0.0018 Acc: 83.4881\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 84.2093\n",
      "validation Loss: 0.0018 Acc: 83.5357\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 84.3968\n",
      "validation Loss: 0.0018 Acc: 83.6667\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 84.3372\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 84.3194\n",
      "validation Loss: 0.0018 Acc: 83.5714\n",
      "Epoch 83/99\n",
      "training Loss: 0.0018 Acc: 84.2599\n",
      "validation Loss: 0.0018 Acc: 83.6429\n",
      "Epoch 84/99\n",
      "training Loss: 0.0018 Acc: 84.3849\n",
      "validation Loss: 0.0018 Acc: 83.5000\n",
      "Epoch 85/99\n",
      "training Loss: 0.0018 Acc: 84.2628\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 86/99\n",
      "training Loss: 0.0018 Acc: 84.2569\n",
      "validation Loss: 0.0018 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0018 Acc: 84.3491\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Epoch 88/99\n",
      "training Loss: 0.0018 Acc: 84.2807\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 89/99\n",
      "training Loss: 0.0018 Acc: 84.2599\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 90/99\n",
      "training Loss: 0.0018 Acc: 84.3491\n",
      "validation Loss: 0.0018 Acc: 83.6429\n",
      "Epoch 91/99\n",
      "training Loss: 0.0018 Acc: 84.3491\n",
      "validation Loss: 0.0018 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 92/99\n",
      "training Loss: 0.0018 Acc: 84.4265\n",
      "validation Loss: 0.0018 Acc: 83.5000\n",
      "Epoch 93/99\n",
      "training Loss: 0.0018 Acc: 84.3343\n",
      "validation Loss: 0.0018 Acc: 83.6190\n",
      "Epoch 94/99\n",
      "training Loss: 0.0018 Acc: 84.3849\n",
      "validation Loss: 0.0018 Acc: 83.6548\n",
      "Epoch 95/99\n",
      "training Loss: 0.0018 Acc: 84.3610\n",
      "validation Loss: 0.0018 Acc: 83.7024\n",
      "Epoch 96/99\n",
      "training Loss: 0.0018 Acc: 84.5396\n",
      "validation Loss: 0.0018 Acc: 83.6190\n",
      "Epoch 97/99\n",
      "training Loss: 0.0018 Acc: 84.4384\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 98/99\n",
      "training Loss: 0.0018 Acc: 84.4146\n",
      "validation Loss: 0.0018 Acc: 83.5476\n",
      "Epoch 99/99\n",
      "training Loss: 0.0018 Acc: 84.4622\n",
      "validation Loss: 0.0018 Acc: 83.7024\n",
      "Best val acc: 83.702381\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0033 Acc: 74.0254\n",
      "validation Loss: 0.0028 Acc: 79.7738\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 79.8107\n",
      "validation Loss: 0.0022 Acc: 81.7143\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.4535\n",
      "validation Loss: 0.0020 Acc: 82.5833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0021 Acc: 82.3255\n",
      "validation Loss: 0.0019 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.4445\n",
      "validation Loss: 0.0019 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.5963\n",
      "validation Loss: 0.0019 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 82.8344\n",
      "validation Loss: 0.0019 Acc: 83.0357\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 82.8165\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 82.9415\n",
      "validation Loss: 0.0019 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.0040\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 82.9683\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.0605\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.2212\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.0397\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.3938\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.3790\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.2927\n",
      "validation Loss: 0.0018 Acc: 83.4405\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.4206\n",
      "validation Loss: 0.0018 Acc: 83.3929\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.3849\n",
      "validation Loss: 0.0018 Acc: 83.5357\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.4057\n",
      "validation Loss: 0.0018 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.4950\n",
      "validation Loss: 0.0018 Acc: 83.5238\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.5307\n",
      "validation Loss: 0.0018 Acc: 83.5000\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.3968\n",
      "validation Loss: 0.0018 Acc: 83.4881\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.4891\n",
      "validation Loss: 0.0018 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.3730\n",
      "validation Loss: 0.0018 Acc: 83.4881\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 83.4891\n",
      "validation Loss: 0.0018 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.5040\n",
      "validation Loss: 0.0018 Acc: 83.7262\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.5546\n",
      "validation Loss: 0.0018 Acc: 83.6429\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.5665\n",
      "validation Loss: 0.0018 Acc: 83.7262\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.5694\n",
      "validation Loss: 0.0018 Acc: 83.6786\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 83.5516\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.6081\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 83.5962\n",
      "validation Loss: 0.0018 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.7658\n",
      "validation Loss: 0.0018 Acc: 83.7381\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 83.5932\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 83.7688\n",
      "validation Loss: 0.0018 Acc: 83.7143\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.8105\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 83.7182\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.7658\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 83.8224\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.6647\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 83.7748\n",
      "validation Loss: 0.0018 Acc: 83.4286\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 83.8700\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 83.8075\n",
      "validation Loss: 0.0018 Acc: 83.8214\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 83.8700\n",
      "validation Loss: 0.0018 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 83.7539\n",
      "validation Loss: 0.0018 Acc: 83.9167\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 83.8700\n",
      "validation Loss: 0.0018 Acc: 83.9286\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 83.9146\n",
      "validation Loss: 0.0018 Acc: 83.8095\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 83.9057\n",
      "validation Loss: 0.0018 Acc: 83.9167\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 83.8016\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0018 Acc: 83.7500\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 83.9295\n",
      "validation Loss: 0.0018 Acc: 83.9405\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 83.8700\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 53/99\n",
      "training Loss: 0.0018 Acc: 83.9444\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 54/99\n",
      "training Loss: 0.0018 Acc: 83.9920\n",
      "validation Loss: 0.0018 Acc: 83.9167\n",
      "Epoch 55/99\n",
      "training Loss: 0.0018 Acc: 83.8908\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 56/99\n",
      "training Loss: 0.0018 Acc: 83.8849\n",
      "validation Loss: 0.0018 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0018 Acc: 83.9890\n",
      "validation Loss: 0.0018 Acc: 83.8810\n",
      "Epoch 58/99\n",
      "training Loss: 0.0018 Acc: 84.1111\n",
      "validation Loss: 0.0018 Acc: 83.7976\n",
      "Epoch 59/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0018 Acc: 84.0119\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0018 Acc: 84.1825\n",
      "validation Loss: 0.0018 Acc: 83.9286\n",
      "Epoch 61/99\n",
      "training Loss: 0.0018 Acc: 84.0515\n",
      "validation Loss: 0.0018 Acc: 84.0119\n",
      "Epoch 62/99\n",
      "training Loss: 0.0018 Acc: 84.1974\n",
      "validation Loss: 0.0018 Acc: 83.9048\n",
      "Epoch 63/99\n",
      "training Loss: 0.0018 Acc: 84.1170\n",
      "validation Loss: 0.0018 Acc: 83.9405\n",
      "Epoch 64/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0018 Acc: 83.9167\n",
      "Epoch 65/99\n",
      "training Loss: 0.0018 Acc: 83.9920\n",
      "validation Loss: 0.0018 Acc: 84.0000\n",
      "Epoch 66/99\n",
      "training Loss: 0.0018 Acc: 84.1765\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Saving..\n",
      "Epoch 67/99\n",
      "training Loss: 0.0018 Acc: 84.0188\n",
      "validation Loss: 0.0018 Acc: 84.0714\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0018 Acc: 84.0515\n",
      "validation Loss: 0.0018 Acc: 83.9643\n",
      "Epoch 69/99\n",
      "training Loss: 0.0018 Acc: 84.1319\n",
      "validation Loss: 0.0018 Acc: 83.9643\n",
      "Epoch 70/99\n",
      "training Loss: 0.0018 Acc: 84.1378\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 71/99\n",
      "training Loss: 0.0018 Acc: 83.9890\n",
      "validation Loss: 0.0018 Acc: 83.9881\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 84.2777\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 73/99\n",
      "training Loss: 0.0018 Acc: 84.1349\n",
      "validation Loss: 0.0018 Acc: 83.9881\n",
      "Epoch 74/99\n",
      "training Loss: 0.0018 Acc: 84.2182\n",
      "validation Loss: 0.0018 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0018 Acc: 84.1974\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 84.1884\n",
      "validation Loss: 0.0018 Acc: 84.0952\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0018 Acc: 84.1289\n",
      "validation Loss: 0.0018 Acc: 84.0714\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 84.1706\n",
      "validation Loss: 0.0018 Acc: 83.9405\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 84.1706\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 84.2212\n",
      "validation Loss: 0.0018 Acc: 84.0952\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 84.1170\n",
      "validation Loss: 0.0018 Acc: 84.0714\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 84.2182\n",
      "validation Loss: 0.0018 Acc: 83.9643\n",
      "Epoch 83/99\n",
      "training Loss: 0.0018 Acc: 84.2212\n",
      "validation Loss: 0.0018 Acc: 84.0833\n",
      "Epoch 84/99\n",
      "training Loss: 0.0018 Acc: 84.1825\n",
      "validation Loss: 0.0018 Acc: 83.9881\n",
      "Epoch 85/99\n",
      "training Loss: 0.0018 Acc: 84.2361\n",
      "validation Loss: 0.0018 Acc: 84.0238\n",
      "Epoch 86/99\n",
      "training Loss: 0.0018 Acc: 84.1021\n",
      "validation Loss: 0.0018 Acc: 84.0952\n",
      "Epoch 87/99\n",
      "training Loss: 0.0018 Acc: 84.2777\n",
      "validation Loss: 0.0018 Acc: 84.0119\n",
      "Epoch 88/99\n",
      "training Loss: 0.0018 Acc: 84.1914\n",
      "validation Loss: 0.0018 Acc: 84.0595\n",
      "Epoch 89/99\n",
      "training Loss: 0.0018 Acc: 84.1617\n",
      "validation Loss: 0.0018 Acc: 83.9881\n",
      "Epoch 90/99\n",
      "training Loss: 0.0018 Acc: 84.0813\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 91/99\n",
      "training Loss: 0.0018 Acc: 84.2301\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 92/99\n",
      "training Loss: 0.0018 Acc: 84.1617\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 93/99\n",
      "training Loss: 0.0018 Acc: 84.2242\n",
      "validation Loss: 0.0018 Acc: 84.0476\n",
      "Epoch 94/99\n",
      "training Loss: 0.0018 Acc: 84.0932\n",
      "validation Loss: 0.0018 Acc: 84.0595\n",
      "Epoch 95/99\n",
      "training Loss: 0.0018 Acc: 84.1736\n",
      "validation Loss: 0.0018 Acc: 84.0595\n",
      "Epoch 96/99\n",
      "training Loss: 0.0018 Acc: 84.2271\n",
      "validation Loss: 0.0018 Acc: 84.0833\n",
      "Early stopped.\n",
      "Best val acc: 84.095238\n",
      "----------\n",
      "Average best_acc across k-fold: 83.926960701\n",
      "New configuration: {'learning_rate': 0.08565325206386668, 'initial_nodes': 563, 'dropout': 0.7567299835200918, 'batch_size': 197, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0131 Acc: 52.1012\n",
      "validation Loss: 0.0035 Acc: 51.2973\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0035 Acc: 51.1042\n",
      "validation Loss: 0.0034 Acc: 54.2490\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0035 Acc: 50.9077\n",
      "validation Loss: 0.0034 Acc: 55.0821\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0035 Acc: 51.1577\n",
      "validation Loss: 0.0034 Acc: 53.6896\n",
      "Epoch 4/99\n",
      "training Loss: 0.0040 Acc: 50.2411\n",
      "validation Loss: 0.0034 Acc: 53.9633\n",
      "Epoch 5/99\n",
      "training Loss: 0.0035 Acc: 50.5030\n",
      "validation Loss: 0.0034 Acc: 54.1062\n",
      "Epoch 6/99\n",
      "training Loss: 0.0035 Acc: 50.7440\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0035 Acc: 50.4940\n",
      "validation Loss: 0.0034 Acc: 54.4513\n",
      "Epoch 8/99\n",
      "training Loss: 0.0036 Acc: 50.3899\n",
      "validation Loss: 0.0034 Acc: 53.0469\n",
      "Epoch 9/99\n",
      "training Loss: 0.0035 Acc: 50.4077\n",
      "validation Loss: 0.0035 Acc: 55.1535\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 50.3006\n",
      "validation Loss: 0.0036 Acc: 55.2368\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0035 Acc: 50.2202\n",
      "validation Loss: 0.0034 Acc: 53.7729\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 50.6399\n",
      "validation Loss: 0.0034 Acc: 53.8562\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 49.9405\n",
      "validation Loss: 0.0034 Acc: 53.9633\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 49.9375\n",
      "validation Loss: 0.0034 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 49.7619\n",
      "validation Loss: 0.0034 Acc: 54.2966\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 50.5833\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 49.8988\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 49.8304\n",
      "validation Loss: 0.0035 Acc: 54.8798\n",
      "Epoch 19/99\n",
      "training Loss: 0.0035 Acc: 50.3363\n",
      "validation Loss: 0.0035 Acc: 55.1892\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 50.5685\n",
      "validation Loss: 0.0035 Acc: 55.2130\n",
      "Epoch 21/99\n",
      "training Loss: 0.0035 Acc: 50.2917\n",
      "validation Loss: 0.0035 Acc: 55.2845\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0035 Acc: 50.5625\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 23/99\n",
      "training Loss: 0.0035 Acc: 50.1875\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0035 Acc: 50.1280\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 25/99\n",
      "training Loss: 0.0035 Acc: 50.0208\n",
      "validation Loss: 0.0035 Acc: 55.4035\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0035 Acc: 50.0506\n",
      "validation Loss: 0.0035 Acc: 55.4035\n",
      "Epoch 27/99\n",
      "training Loss: 0.0035 Acc: 50.4524\n",
      "validation Loss: 0.0035 Acc: 55.4035\n",
      "Epoch 28/99\n",
      "training Loss: 0.0035 Acc: 50.4405\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 29/99\n",
      "training Loss: 0.0035 Acc: 50.3036\n",
      "validation Loss: 0.0035 Acc: 55.4392\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0035 Acc: 50.6280\n",
      "validation Loss: 0.0035 Acc: 55.4630\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0035 Acc: 50.2679\n",
      "validation Loss: 0.0035 Acc: 55.5106\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0035 Acc: 50.5833\n",
      "validation Loss: 0.0035 Acc: 55.5106\n",
      "Epoch 33/99\n",
      "training Loss: 0.0035 Acc: 50.5089\n",
      "validation Loss: 0.0035 Acc: 55.4749\n",
      "Epoch 34/99\n",
      "training Loss: 0.0035 Acc: 50.5179\n",
      "validation Loss: 0.0035 Acc: 55.4749\n",
      "Epoch 35/99\n",
      "training Loss: 0.0035 Acc: 50.5179\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 36/99\n",
      "training Loss: 0.0035 Acc: 50.7560\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 37/99\n",
      "training Loss: 0.0035 Acc: 50.3333\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 38/99\n",
      "training Loss: 0.0035 Acc: 50.6458\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 39/99\n",
      "training Loss: 0.0035 Acc: 50.4732\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 40/99\n",
      "training Loss: 0.0035 Acc: 50.5298\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 41/99\n",
      "training Loss: 0.0035 Acc: 50.4494\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 42/99\n",
      "training Loss: 0.0035 Acc: 50.5625\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 43/99\n",
      "training Loss: 0.0035 Acc: 50.7946\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 44/99\n",
      "training Loss: 0.0035 Acc: 50.8244\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 45/99\n",
      "training Loss: 0.0035 Acc: 50.8095\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 46/99\n",
      "training Loss: 0.0035 Acc: 50.7857\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 47/99\n",
      "training Loss: 0.0035 Acc: 50.8482\n",
      "validation Loss: 0.0035 Acc: 55.4868\n",
      "Epoch 48/99\n",
      "training Loss: 0.0035 Acc: 50.8125\n",
      "validation Loss: 0.0035 Acc: 55.4392\n",
      "Epoch 49/99\n",
      "training Loss: 0.0035 Acc: 50.8274\n",
      "validation Loss: 0.0035 Acc: 55.4392\n",
      "Epoch 50/99\n",
      "training Loss: 0.0035 Acc: 50.8065\n",
      "validation Loss: 0.0035 Acc: 55.4392\n",
      "Epoch 51/99\n",
      "training Loss: 0.0035 Acc: 50.8185\n",
      "validation Loss: 0.0036 Acc: 55.4392\n",
      "Early stopped.\n",
      "Best val acc: 55.510593\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0126 Acc: 59.7703\n",
      "validation Loss: 0.0035 Acc: 50.5000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0038 Acc: 51.8213\n",
      "validation Loss: 0.0035 Acc: 50.0119\n",
      "Epoch 2/99\n",
      "training Loss: 0.0041 Acc: 52.0505\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0035 Acc: 52.0832\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0035 Acc: 52.0088\n",
      "validation Loss: 0.0035 Acc: 76.6310\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 52.3421\n",
      "validation Loss: 0.0035 Acc: 79.5357\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0035 Acc: 52.5356\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0035 Acc: 51.7201\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0037 Acc: 52.0207\n",
      "validation Loss: 0.0035 Acc: 80.0714\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0035 Acc: 52.5653\n",
      "validation Loss: 0.0034 Acc: 79.6786\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 52.1695\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0035 Acc: 51.7797\n",
      "validation Loss: 0.0035 Acc: 78.1786\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 51.6606\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 51.6190\n",
      "validation Loss: 0.0035 Acc: 80.4286\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 51.1725\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 51.4077\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 52.4165\n",
      "validation Loss: 0.0035 Acc: 80.5952\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 52.6844\n",
      "validation Loss: 0.0035 Acc: 80.7262\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 52.5147\n",
      "validation Loss: 0.0034 Acc: 80.1786\n",
      "Epoch 19/99\n",
      "training Loss: 0.0035 Acc: 52.6100\n",
      "validation Loss: 0.0035 Acc: 79.0833\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 52.5207\n",
      "validation Loss: 0.0035 Acc: 80.7500\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0035 Acc: 52.7350\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 22/99\n",
      "training Loss: 0.0035 Acc: 52.9790\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 23/99\n",
      "training Loss: 0.0035 Acc: 52.8153\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0035 Acc: 53.1337\n",
      "validation Loss: 0.0035 Acc: 79.6667\n",
      "Epoch 25/99\n",
      "training Loss: 0.0035 Acc: 53.4819\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 26/99\n",
      "training Loss: 0.0035 Acc: 52.6278\n",
      "validation Loss: 0.0035 Acc: 80.1310\n",
      "Epoch 27/99\n",
      "training Loss: 0.0035 Acc: 53.0712\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 28/99\n",
      "training Loss: 0.0035 Acc: 53.4105\n",
      "validation Loss: 0.0035 Acc: 79.8571\n",
      "Epoch 29/99\n",
      "training Loss: 0.0035 Acc: 53.3569\n",
      "validation Loss: 0.0035 Acc: 79.9881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0035 Acc: 53.0802\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 31/99\n",
      "training Loss: 0.0035 Acc: 53.0117\n",
      "validation Loss: 0.0035 Acc: 79.9881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0035 Acc: 53.1576\n",
      "validation Loss: 0.0035 Acc: 79.2143\n",
      "Epoch 33/99\n",
      "training Loss: 0.0035 Acc: 53.2409\n",
      "validation Loss: 0.0035 Acc: 79.7857\n",
      "Epoch 34/99\n",
      "training Loss: 0.0035 Acc: 53.2617\n",
      "validation Loss: 0.0035 Acc: 79.7381\n",
      "Epoch 35/99\n",
      "training Loss: 0.0035 Acc: 53.0861\n",
      "validation Loss: 0.0035 Acc: 79.5952\n",
      "Epoch 36/99\n",
      "training Loss: 0.0035 Acc: 53.7081\n",
      "validation Loss: 0.0035 Acc: 79.6071\n",
      "Epoch 37/99\n",
      "training Loss: 0.0035 Acc: 52.9492\n",
      "validation Loss: 0.0035 Acc: 79.6786\n",
      "Epoch 38/99\n",
      "training Loss: 0.0035 Acc: 53.1337\n",
      "validation Loss: 0.0035 Acc: 79.7143\n",
      "Epoch 39/99\n",
      "training Loss: 0.0035 Acc: 53.1873\n",
      "validation Loss: 0.0035 Acc: 79.8095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0035 Acc: 53.2498\n",
      "validation Loss: 0.0035 Acc: 79.8333\n",
      "Early stopped.\n",
      "Best val acc: 80.750000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0105 Acc: 59.7583\n",
      "validation Loss: 0.0055 Acc: 63.3929\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0080 Acc: 50.6160\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0035 Acc: 49.6875\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0035 Acc: 50.1488\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0035 Acc: 50.3482\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0035 Acc: 49.6726\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0035 Acc: 50.4791\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0035 Acc: 50.2768\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0035 Acc: 49.8214\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0035 Acc: 49.6012\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 49.6399\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0035 Acc: 50.0417\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 50.3512\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 49.8274\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 49.6399\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 49.4584\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 50.0208\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 50.1488\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 49.7976\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0035 Acc: 50.3036\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 50.0714\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 63.392857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0126 Acc: 63.7075\n",
      "validation Loss: 0.0046 Acc: 60.1190\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0042 Acc: 54.5176\n",
      "validation Loss: 0.0040 Acc: 59.9405\n",
      "Epoch 2/99\n",
      "training Loss: 0.0040 Acc: 50.6904\n",
      "validation Loss: 0.0035 Acc: 50.5595\n",
      "Epoch 3/99\n",
      "training Loss: 0.0035 Acc: 50.3274\n",
      "validation Loss: 0.0035 Acc: 49.9881\n",
      "Epoch 4/99\n",
      "training Loss: 0.0036 Acc: 50.1756\n",
      "validation Loss: 0.0046 Acc: 58.1310\n",
      "Epoch 5/99\n",
      "training Loss: 0.0036 Acc: 50.8988\n",
      "validation Loss: 0.0052 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0035 Acc: 50.6339\n",
      "validation Loss: 0.0047 Acc: 53.7500\n",
      "Epoch 7/99\n",
      "training Loss: 0.0038 Acc: 50.3869\n",
      "validation Loss: 0.0041 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0036 Acc: 50.2440\n",
      "validation Loss: 0.0038 Acc: 51.2143\n",
      "Epoch 9/99\n",
      "training Loss: 0.0036 Acc: 50.2113\n",
      "validation Loss: 0.0038 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 50.4077\n",
      "validation Loss: 0.0039 Acc: 51.0595\n",
      "Epoch 11/99\n",
      "training Loss: 0.0035 Acc: 50.1428\n",
      "validation Loss: 0.0039 Acc: 51.2024\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 50.1905\n",
      "validation Loss: 0.0039 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 50.3720\n",
      "validation Loss: 0.0039 Acc: 51.2024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 50.2916\n",
      "validation Loss: 0.0039 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 50.2202\n",
      "validation Loss: 0.0041 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 50.2797\n",
      "validation Loss: 0.0041 Acc: 51.5952\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 50.0684\n",
      "validation Loss: 0.0041 Acc: 51.5952\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 50.2589\n",
      "validation Loss: 0.0041 Acc: 51.6310\n",
      "Epoch 19/99\n",
      "training Loss: 0.0035 Acc: 49.9673\n",
      "validation Loss: 0.0041 Acc: 51.6905\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 50.0863\n",
      "validation Loss: 0.0041 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 60.119048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 58.8864\n",
      "validation Loss: 0.0038 Acc: 60.0833\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0041 Acc: 52.2677\n",
      "validation Loss: 0.0035 Acc: 55.1429\n",
      "Epoch 2/99\n",
      "training Loss: 0.0042 Acc: 50.9255\n",
      "validation Loss: 0.0197 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0278 Acc: 50.5208\n",
      "validation Loss: 0.0035 Acc: 50.2738\n",
      "Epoch 4/99\n",
      "training Loss: 0.0035 Acc: 49.6280\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0044 Acc: 49.9316\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0035 Acc: 49.7828\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0041 Acc: 50.0179\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0039 Acc: 50.1012\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0035 Acc: 49.4078\n",
      "validation Loss: 0.0035 Acc: 50.1190\n",
      "Epoch 10/99\n",
      "training Loss: 0.0035 Acc: 50.2738\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0035 Acc: 50.0982\n",
      "validation Loss: 0.0035 Acc: 50.1190\n",
      "Epoch 12/99\n",
      "training Loss: 0.0035 Acc: 49.5536\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0035 Acc: 49.8631\n",
      "validation Loss: 0.0036 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0035 Acc: 50.0149\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0035 Acc: 49.6012\n",
      "validation Loss: 0.0035 Acc: 50.0952\n",
      "Epoch 16/99\n",
      "training Loss: 0.0035 Acc: 50.0238\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0035 Acc: 50.2321\n",
      "validation Loss: 0.0035 Acc: 50.0952\n",
      "Epoch 18/99\n",
      "training Loss: 0.0035 Acc: 50.1369\n",
      "validation Loss: 0.0035 Acc: 50.0952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0035 Acc: 49.9732\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0035 Acc: 50.3541\n",
      "validation Loss: 0.0035 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 60.083333\n",
      "----------\n",
      "Average best_acc across k-fold: 63.9711661623\n",
      "New configuration: {'learning_rate': 0.059289413748048705, 'initial_nodes': 632, 'dropout': 0.038177797493203144, 'batch_size': 263, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0177 Acc: 77.4345\n",
      "validation Loss: 0.0015 Acc: 83.0040\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 82.5536\n",
      "validation Loss: 0.0014 Acc: 82.7422\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 83.1101\n",
      "validation Loss: 0.0014 Acc: 83.1350\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.1935\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.4613\n",
      "validation Loss: 0.0014 Acc: 82.8493\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 83.7679\n",
      "validation Loss: 0.0014 Acc: 83.0040\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 83.5625\n",
      "validation Loss: 0.0014 Acc: 83.3373\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 83.6250\n",
      "validation Loss: 0.0014 Acc: 82.8136\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 83.6131\n",
      "validation Loss: 0.0014 Acc: 82.8850\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.6250\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 83.6815\n",
      "validation Loss: 0.0014 Acc: 82.9445\n",
      "Epoch 11/99\n",
      "training Loss: 0.0014 Acc: 83.8958\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 83.8393\n",
      "validation Loss: 0.0014 Acc: 83.3373\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 83.7857\n",
      "validation Loss: 0.0014 Acc: 82.9207\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 83.9792\n",
      "validation Loss: 0.0014 Acc: 83.2897\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 83.8185\n",
      "validation Loss: 0.0014 Acc: 83.5872\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.9702\n",
      "validation Loss: 0.0014 Acc: 83.3016\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 84.0774\n",
      "validation Loss: 0.0014 Acc: 83.0159\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 84.2589\n",
      "validation Loss: 0.0014 Acc: 83.2302\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 84.5714\n",
      "validation Loss: 0.0014 Acc: 83.5039\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 84.4435\n",
      "validation Loss: 0.0013 Acc: 83.5872\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 84.4077\n",
      "validation Loss: 0.0014 Acc: 83.1350\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 84.3631\n",
      "validation Loss: 0.0013 Acc: 83.6348\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 84.6518\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 84.6429\n",
      "validation Loss: 0.0014 Acc: 83.6229\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 84.4762\n",
      "validation Loss: 0.0014 Acc: 83.5991\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 84.7500\n",
      "validation Loss: 0.0014 Acc: 83.3492\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 84.6220\n",
      "validation Loss: 0.0014 Acc: 83.3849\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 84.6339\n",
      "validation Loss: 0.0013 Acc: 83.6110\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 85.0089\n",
      "validation Loss: 0.0014 Acc: 83.5515\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 85.1607\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 85.2530\n",
      "validation Loss: 0.0014 Acc: 83.2897\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 85.3214\n",
      "validation Loss: 0.0014 Acc: 83.3135\n",
      "Epoch 33/99\n",
      "training Loss: 0.0013 Acc: 85.2976\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 85.2083\n",
      "validation Loss: 0.0014 Acc: 83.3135\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 85.5804\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 85.7500\n",
      "validation Loss: 0.0014 Acc: 83.2540\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 85.7113\n",
      "validation Loss: 0.0014 Acc: 83.3849\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 85.6458\n",
      "validation Loss: 0.0014 Acc: 83.2540\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 85.8571\n",
      "validation Loss: 0.0014 Acc: 83.0993\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 85.8274\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 86.1399\n",
      "validation Loss: 0.0014 Acc: 83.3373\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 86.0893\n",
      "validation Loss: 0.0014 Acc: 83.0517\n",
      "Early stopped.\n",
      "Best val acc: 83.634849\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0104 Acc: 77.5847\n",
      "validation Loss: 0.0017 Acc: 81.4286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0017 Acc: 80.6262\n",
      "validation Loss: 0.0015 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0018 Acc: 80.4327\n",
      "validation Loss: 0.0016 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 80.9178\n",
      "validation Loss: 0.0016 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.0041\n",
      "validation Loss: 0.0015 Acc: 82.3929\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.5011\n",
      "validation Loss: 0.0015 Acc: 82.6548\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 81.1708\n",
      "validation Loss: 0.0015 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 80.9059\n",
      "validation Loss: 0.0016 Acc: 83.1310\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 77.3793\n",
      "validation Loss: 0.0021 Acc: 76.2024\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 76.7425\n",
      "validation Loss: 0.0022 Acc: 76.0476\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 75.9598\n",
      "validation Loss: 0.0021 Acc: 75.8214\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 75.2723\n",
      "validation Loss: 0.0021 Acc: 76.0595\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 76.0193\n",
      "validation Loss: 0.0020 Acc: 77.4167\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 76.2336\n",
      "validation Loss: 0.0020 Acc: 76.4762\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 76.1264\n",
      "validation Loss: 0.0020 Acc: 76.2381\n",
      "Epoch 15/99\n",
      "training Loss: 0.0022 Acc: 74.3111\n",
      "validation Loss: 0.0021 Acc: 73.6429\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 74.0968\n",
      "validation Loss: 0.0020 Acc: 74.8929\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 75.3080\n",
      "validation Loss: 0.0021 Acc: 74.4286\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 74.5610\n",
      "validation Loss: 0.0020 Acc: 75.7500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 75.7247\n",
      "validation Loss: 0.0020 Acc: 76.0714\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 75.3824\n",
      "validation Loss: 0.0020 Acc: 75.8452\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 76.0788\n",
      "validation Loss: 0.0020 Acc: 76.6429\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 76.8347\n",
      "validation Loss: 0.0020 Acc: 77.5238\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 76.8198\n",
      "validation Loss: 0.0020 Acc: 77.4167\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 76.8853\n",
      "validation Loss: 0.0020 Acc: 77.3571\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 77.1591\n",
      "validation Loss: 0.0020 Acc: 78.0357\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 77.4121\n",
      "validation Loss: 0.0020 Acc: 77.4524\n",
      "Early stopped.\n",
      "Best val acc: 83.416667\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0095 Acc: 79.2364\n",
      "validation Loss: 0.0015 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 82.9742\n",
      "validation Loss: 0.0015 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0015 Acc: 83.2153\n",
      "validation Loss: 0.0014 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.3998\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.6051\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Epoch 5/99\n",
      "training Loss: 0.0015 Acc: 83.3313\n",
      "validation Loss: 0.0015 Acc: 82.9762\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 83.2748\n",
      "validation Loss: 0.0014 Acc: 82.9762\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.8969\n",
      "validation Loss: 0.0019 Acc: 77.3333\n",
      "Epoch 8/99\n",
      "training Loss: 0.0017 Acc: 80.1917\n",
      "validation Loss: 0.0016 Acc: 82.5595\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 81.2422\n",
      "validation Loss: 0.0016 Acc: 82.6905\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 81.0279\n",
      "validation Loss: 0.0016 Acc: 80.5357\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 81.3374\n",
      "validation Loss: 0.0015 Acc: 82.7857\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 81.9177\n",
      "validation Loss: 0.0015 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 81.8552\n",
      "validation Loss: 0.0014 Acc: 83.0833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 82.0844\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 82.0933\n",
      "validation Loss: 0.0014 Acc: 83.0595\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 82.1082\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 82.5070\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 82.3403\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 82.0904\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 82.4504\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 82.3880\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 82.6588\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 82.7600\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 82.6945\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 82.8373\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 82.9445\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 82.6141\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 82.9980\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 82.9445\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.0397\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 82.9326\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.0486\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.1201\n",
      "validation Loss: 0.0014 Acc: 83.6786\n",
      "Epoch 34/99\n",
      "training Loss: 0.0014 Acc: 83.1915\n",
      "validation Loss: 0.0014 Acc: 83.6667\n",
      "Epoch 35/99\n",
      "training Loss: 0.0014 Acc: 83.1558\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 36/99\n",
      "training Loss: 0.0014 Acc: 83.0992\n",
      "validation Loss: 0.0014 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0014 Acc: 83.0516\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 38/99\n",
      "training Loss: 0.0014 Acc: 83.1766\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 83.1349\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 40/99\n",
      "training Loss: 0.0014 Acc: 83.2004\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 83.3194\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 83.2897\n",
      "validation Loss: 0.0014 Acc: 83.6548\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 83.3909\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 83.4504\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 83.2569\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 83.3254\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 83.4504\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 83.5605\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 83.6111\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 83.4177\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 83.5516\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 83.5069\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 83.4921\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 83.6170\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 83.6290\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 83.6260\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Early stopped.\n",
      "Best val acc: 83.773810\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0151 Acc: 76.0580\n",
      "validation Loss: 0.0017 Acc: 80.5476\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0017 Acc: 80.8672\n",
      "validation Loss: 0.0015 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 81.1946\n",
      "validation Loss: 0.0015 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 81.6410\n",
      "validation Loss: 0.0015 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.6826\n",
      "validation Loss: 0.0015 Acc: 83.1667\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.5071\n",
      "validation Loss: 0.0015 Acc: 83.0476\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 81.8344\n",
      "validation Loss: 0.0015 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0022 Acc: 70.4809\n",
      "validation Loss: 0.0021 Acc: 70.9881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0020 Acc: 74.3944\n",
      "validation Loss: 0.0017 Acc: 80.3452\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 78.1233\n",
      "validation Loss: 0.0019 Acc: 80.1190\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 76.9805\n",
      "validation Loss: 0.0022 Acc: 74.3452\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 76.4002\n",
      "validation Loss: 0.0021 Acc: 76.2143\n",
      "Epoch 12/99\n",
      "training Loss: 0.0020 Acc: 76.3675\n",
      "validation Loss: 0.0019 Acc: 79.5595\n",
      "Epoch 13/99\n",
      "training Loss: 0.0020 Acc: 76.2633\n",
      "validation Loss: 0.0019 Acc: 79.4881\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 77.1770\n",
      "validation Loss: 0.0019 Acc: 80.1905\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 78.3912\n",
      "validation Loss: 0.0019 Acc: 81.0357\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 77.8347\n",
      "validation Loss: 0.0019 Acc: 80.5238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 78.6620\n",
      "validation Loss: 0.0019 Acc: 79.0119\n",
      "Epoch 18/99\n",
      "training Loss: 0.0022 Acc: 77.6978\n",
      "validation Loss: 0.0021 Acc: 75.0714\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 78.2989\n",
      "validation Loss: 0.0019 Acc: 80.5000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 79.3256\n",
      "validation Loss: 0.0018 Acc: 81.8214\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 79.5429\n",
      "validation Loss: 0.0018 Acc: 81.6905\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 79.8673\n",
      "validation Loss: 0.0018 Acc: 82.0357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 78.6917\n",
      "validation Loss: 0.0018 Acc: 82.0833\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 79.0846\n",
      "validation Loss: 0.0018 Acc: 82.4167\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 79.3762\n",
      "validation Loss: 0.0018 Acc: 82.1071\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 78.7900\n",
      "validation Loss: 0.0018 Acc: 82.1071\n",
      "Early stopped.\n",
      "Best val acc: 83.309524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0056 Acc: 77.2454\n",
      "validation Loss: 0.0016 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0017 Acc: 80.5518\n",
      "validation Loss: 0.0015 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0017 Acc: 80.3256\n",
      "validation Loss: 0.0018 Acc: 77.5357\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 80.7958\n",
      "validation Loss: 0.0015 Acc: 82.2619\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 81.2243\n",
      "validation Loss: 0.0016 Acc: 80.9524\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.5160\n",
      "validation Loss: 0.0018 Acc: 78.1667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 81.1916\n",
      "validation Loss: 0.0015 Acc: 82.6310\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 81.5755\n",
      "validation Loss: 0.0015 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 81.7422\n",
      "validation Loss: 0.0015 Acc: 82.4167\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 81.5279\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 81.7273\n",
      "validation Loss: 0.0015 Acc: 82.4048\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 81.5874\n",
      "validation Loss: 0.0015 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0016 Acc: 81.5487\n",
      "validation Loss: 0.0015 Acc: 82.8452\n",
      "Epoch 13/99\n",
      "training Loss: 0.0020 Acc: 77.5519\n",
      "validation Loss: 0.0018 Acc: 81.1190\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 79.0876\n",
      "validation Loss: 0.0018 Acc: 81.8452\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 76.6859\n",
      "validation Loss: 0.0019 Acc: 79.3690\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 74.5729\n",
      "validation Loss: 0.0020 Acc: 75.9405\n",
      "Epoch 17/99\n",
      "training Loss: 0.0022 Acc: 73.0522\n",
      "validation Loss: 0.0020 Acc: 77.1548\n",
      "Epoch 18/99\n",
      "training Loss: 0.0021 Acc: 76.4359\n",
      "validation Loss: 0.0020 Acc: 78.8810\n",
      "Epoch 19/99\n",
      "training Loss: 0.0021 Acc: 76.9746\n",
      "validation Loss: 0.0019 Acc: 79.7381\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 78.0102\n",
      "validation Loss: 0.0018 Acc: 81.1786\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 78.9745\n",
      "validation Loss: 0.0018 Acc: 82.0119\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 79.8018\n",
      "validation Loss: 0.0018 Acc: 82.3690\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 79.3881\n",
      "validation Loss: 0.0018 Acc: 81.7024\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 79.5548\n",
      "validation Loss: 0.0018 Acc: 82.0000\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 79.8941\n",
      "validation Loss: 0.0018 Acc: 81.9762\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 79.8197\n",
      "validation Loss: 0.0018 Acc: 82.2143\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 79.9476\n",
      "validation Loss: 0.0018 Acc: 82.4286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 79.9149\n",
      "validation Loss: 0.0018 Acc: 82.6310\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 80.1143\n",
      "validation Loss: 0.0018 Acc: 82.7976\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 80.4059\n",
      "validation Loss: 0.0018 Acc: 82.9286\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 80.1381\n",
      "validation Loss: 0.0018 Acc: 82.9405\n",
      "Early stopped.\n",
      "Best val acc: 83.047619\n",
      "----------\n",
      "Average best_acc across k-fold: 83.4364935786\n",
      "New configuration: {'learning_rate': 0.04834700243110988, 'initial_nodes': 480, 'dropout': 0.4320258867294419, 'batch_size': 193, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0047 Acc: 77.8482\n",
      "validation Loss: 0.0022 Acc: 82.3494\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 81.5565\n",
      "validation Loss: 0.0021 Acc: 81.5996\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.6815\n",
      "validation Loss: 0.0020 Acc: 82.4328\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 81.9851\n",
      "validation Loss: 0.0022 Acc: 78.8384\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 81.3601\n",
      "validation Loss: 0.0021 Acc: 82.5399\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0023 Acc: 82.0119\n",
      "validation Loss: 0.0021 Acc: 83.4920\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 80.9970\n",
      "validation Loss: 0.0023 Acc: 82.1233\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 80.7054\n",
      "validation Loss: 0.0023 Acc: 82.1352\n",
      "Epoch 8/99\n",
      "training Loss: 0.0025 Acc: 80.7560\n",
      "validation Loss: 0.0023 Acc: 82.1352\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 81.9375\n",
      "validation Loss: 0.0023 Acc: 82.1947\n",
      "Epoch 10/99\n",
      "training Loss: 0.0025 Acc: 80.9077\n",
      "validation Loss: 0.0022 Acc: 82.8255\n",
      "Epoch 11/99\n",
      "training Loss: 0.0024 Acc: 81.7321\n",
      "validation Loss: 0.0023 Acc: 81.8496\n",
      "Epoch 12/99\n",
      "training Loss: 0.0024 Acc: 81.7708\n",
      "validation Loss: 0.0025 Acc: 82.7660\n",
      "Epoch 13/99\n",
      "training Loss: 0.0024 Acc: 81.8185\n",
      "validation Loss: 0.0023 Acc: 82.9207\n",
      "Epoch 14/99\n",
      "training Loss: 0.0024 Acc: 82.1935\n",
      "validation Loss: 0.0023 Acc: 83.1350\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 82.5119\n",
      "validation Loss: 0.0022 Acc: 83.1826\n",
      "Epoch 16/99\n",
      "training Loss: 0.0023 Acc: 82.5357\n",
      "validation Loss: 0.0022 Acc: 83.3373\n",
      "Epoch 17/99\n",
      "training Loss: 0.0023 Acc: 82.6458\n",
      "validation Loss: 0.0022 Acc: 83.1350\n",
      "Epoch 18/99\n",
      "training Loss: 0.0023 Acc: 82.5714\n",
      "validation Loss: 0.0022 Acc: 83.1112\n",
      "Epoch 19/99\n",
      "training Loss: 0.0023 Acc: 82.4048\n",
      "validation Loss: 0.0022 Acc: 83.2778\n",
      "Epoch 20/99\n",
      "training Loss: 0.0023 Acc: 82.6042\n",
      "validation Loss: 0.0022 Acc: 82.6827\n",
      "Epoch 21/99\n",
      "training Loss: 0.0023 Acc: 82.5744\n",
      "validation Loss: 0.0022 Acc: 83.1945\n",
      "Epoch 22/99\n",
      "training Loss: 0.0023 Acc: 82.8125\n",
      "validation Loss: 0.0021 Acc: 83.3254\n",
      "Epoch 23/99\n",
      "training Loss: 0.0022 Acc: 82.7470\n",
      "validation Loss: 0.0022 Acc: 83.4206\n",
      "Epoch 24/99\n",
      "training Loss: 0.0022 Acc: 82.8690\n",
      "validation Loss: 0.0021 Acc: 83.4920\n",
      "Epoch 25/99\n",
      "training Loss: 0.0022 Acc: 82.6280\n",
      "validation Loss: 0.0021 Acc: 83.4801\n",
      "Early stopped.\n",
      "Best val acc: 83.492026\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 79.2631\n",
      "validation Loss: 0.0021 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 80.0339\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 79.6976\n",
      "validation Loss: 0.0025 Acc: 82.7857\n",
      "Epoch 3/99\n",
      "training Loss: 0.0027 Acc: 78.4894\n",
      "validation Loss: 0.0026 Acc: 80.0714\n",
      "Epoch 4/99\n",
      "training Loss: 0.0027 Acc: 77.8555\n",
      "validation Loss: 0.0027 Acc: 79.9762\n",
      "Epoch 5/99\n",
      "training Loss: 0.0028 Acc: 77.7335\n",
      "validation Loss: 0.0026 Acc: 78.1190\n",
      "Epoch 6/99\n",
      "training Loss: 0.0028 Acc: 77.0371\n",
      "validation Loss: 0.0025 Acc: 81.4048\n",
      "Epoch 7/99\n",
      "training Loss: 0.0027 Acc: 79.9506\n",
      "validation Loss: 0.0025 Acc: 81.4881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0026 Acc: 80.3256\n",
      "validation Loss: 0.0025 Acc: 81.2500\n",
      "Epoch 9/99\n",
      "training Loss: 0.0027 Acc: 79.9327\n",
      "validation Loss: 0.0026 Acc: 78.0238\n",
      "Epoch 10/99\n",
      "training Loss: 0.0026 Acc: 80.1559\n",
      "validation Loss: 0.0025 Acc: 81.7619\n",
      "Epoch 11/99\n",
      "training Loss: 0.0027 Acc: 80.1143\n",
      "validation Loss: 0.0025 Acc: 81.7738\n",
      "Epoch 12/99\n",
      "training Loss: 0.0027 Acc: 80.1381\n",
      "validation Loss: 0.0025 Acc: 81.6905\n",
      "Epoch 13/99\n",
      "training Loss: 0.0026 Acc: 80.4089\n",
      "validation Loss: 0.0025 Acc: 82.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0026 Acc: 80.4952\n",
      "validation Loss: 0.0025 Acc: 82.0357\n",
      "Epoch 15/99\n",
      "training Loss: 0.0026 Acc: 80.4059\n",
      "validation Loss: 0.0025 Acc: 82.3214\n",
      "Epoch 16/99\n",
      "training Loss: 0.0026 Acc: 80.7422\n",
      "validation Loss: 0.0024 Acc: 82.5119\n",
      "Epoch 17/99\n",
      "training Loss: 0.0026 Acc: 80.6738\n",
      "validation Loss: 0.0025 Acc: 82.5000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0026 Acc: 81.0130\n",
      "validation Loss: 0.0024 Acc: 82.7381\n",
      "Epoch 19/99\n",
      "training Loss: 0.0026 Acc: 81.0041\n",
      "validation Loss: 0.0024 Acc: 82.6310\n",
      "Epoch 20/99\n",
      "training Loss: 0.0026 Acc: 81.2511\n",
      "validation Loss: 0.0024 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0026 Acc: 81.0934\n",
      "validation Loss: 0.0024 Acc: 82.8810\n",
      "Epoch 22/99\n",
      "training Loss: 0.0026 Acc: 80.9595\n",
      "validation Loss: 0.0024 Acc: 82.5952\n",
      "Epoch 23/99\n",
      "training Loss: 0.0026 Acc: 81.1975\n",
      "validation Loss: 0.0024 Acc: 82.7619\n",
      "Epoch 24/99\n",
      "training Loss: 0.0026 Acc: 81.2571\n",
      "validation Loss: 0.0024 Acc: 82.6190\n",
      "Epoch 25/99\n",
      "training Loss: 0.0026 Acc: 81.0190\n",
      "validation Loss: 0.0024 Acc: 82.7381\n",
      "Epoch 26/99\n",
      "training Loss: 0.0026 Acc: 81.3255\n",
      "validation Loss: 0.0024 Acc: 82.7738\n",
      "Epoch 27/99\n",
      "training Loss: 0.0026 Acc: 81.4624\n",
      "validation Loss: 0.0024 Acc: 82.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0025 Acc: 81.4475\n",
      "validation Loss: 0.0024 Acc: 82.7857\n",
      "Epoch 29/99\n",
      "training Loss: 0.0026 Acc: 81.3850\n",
      "validation Loss: 0.0024 Acc: 82.8571\n",
      "Epoch 30/99\n",
      "training Loss: 0.0026 Acc: 81.3850\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 31/99\n",
      "training Loss: 0.0026 Acc: 81.4416\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 32/99\n",
      "training Loss: 0.0025 Acc: 81.4535\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 33/99\n",
      "training Loss: 0.0026 Acc: 81.3374\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 34/99\n",
      "training Loss: 0.0025 Acc: 81.4951\n",
      "validation Loss: 0.0024 Acc: 82.9286\n",
      "Epoch 35/99\n",
      "training Loss: 0.0025 Acc: 81.5219\n",
      "validation Loss: 0.0024 Acc: 82.9524\n",
      "Epoch 36/99\n",
      "training Loss: 0.0025 Acc: 81.5011\n",
      "validation Loss: 0.0024 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0025 Acc: 81.3374\n",
      "validation Loss: 0.0024 Acc: 83.0119\n",
      "Epoch 38/99\n",
      "training Loss: 0.0025 Acc: 81.5695\n",
      "validation Loss: 0.0024 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0025 Acc: 81.5576\n",
      "validation Loss: 0.0024 Acc: 82.9881\n",
      "Epoch 40/99\n",
      "training Loss: 0.0025 Acc: 81.4773\n",
      "validation Loss: 0.0024 Acc: 82.9524\n",
      "Epoch 41/99\n",
      "training Loss: 0.0025 Acc: 81.5755\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 42/99\n",
      "training Loss: 0.0025 Acc: 81.5606\n",
      "validation Loss: 0.0024 Acc: 82.9167\n",
      "Epoch 43/99\n",
      "training Loss: 0.0025 Acc: 81.4832\n",
      "validation Loss: 0.0024 Acc: 82.9643\n",
      "Epoch 44/99\n",
      "training Loss: 0.0025 Acc: 81.5993\n",
      "validation Loss: 0.0024 Acc: 82.9762\n",
      "Epoch 45/99\n",
      "training Loss: 0.0025 Acc: 81.4029\n",
      "validation Loss: 0.0024 Acc: 82.9048\n",
      "Epoch 46/99\n",
      "training Loss: 0.0025 Acc: 81.6439\n",
      "validation Loss: 0.0024 Acc: 82.9286\n",
      "Epoch 47/99\n",
      "training Loss: 0.0025 Acc: 81.4862\n",
      "validation Loss: 0.0024 Acc: 82.9524\n",
      "Epoch 48/99\n",
      "training Loss: 0.0025 Acc: 81.4207\n",
      "validation Loss: 0.0024 Acc: 82.9762\n",
      "Epoch 49/99\n",
      "training Loss: 0.0025 Acc: 81.6320\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 50/99\n",
      "training Loss: 0.0025 Acc: 81.5309\n",
      "validation Loss: 0.0024 Acc: 82.9524\n",
      "Epoch 51/99\n",
      "training Loss: 0.0025 Acc: 81.6439\n",
      "validation Loss: 0.0024 Acc: 82.9524\n",
      "Epoch 52/99\n",
      "training Loss: 0.0025 Acc: 81.7005\n",
      "validation Loss: 0.0024 Acc: 82.9405\n",
      "Epoch 53/99\n",
      "training Loss: 0.0025 Acc: 81.6588\n",
      "validation Loss: 0.0024 Acc: 82.9524\n",
      "Epoch 54/99\n",
      "training Loss: 0.0025 Acc: 81.6261\n",
      "validation Loss: 0.0024 Acc: 82.9286\n",
      "Epoch 55/99\n",
      "training Loss: 0.0025 Acc: 81.5993\n",
      "validation Loss: 0.0024 Acc: 82.9167\n",
      "Epoch 56/99\n",
      "training Loss: 0.0025 Acc: 81.6975\n",
      "validation Loss: 0.0024 Acc: 82.9167\n",
      "Epoch 57/99\n",
      "training Loss: 0.0025 Acc: 81.5993\n",
      "validation Loss: 0.0024 Acc: 82.9167\n",
      "Epoch 58/99\n",
      "training Loss: 0.0025 Acc: 81.5725\n",
      "validation Loss: 0.0024 Acc: 82.9167\n",
      "Early stopped.\n",
      "Best val acc: 83.035714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0051 Acc: 76.8734\n",
      "validation Loss: 0.0020 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 81.2987\n",
      "validation Loss: 0.0021 Acc: 82.3333\n",
      "Epoch 2/99\n",
      "training Loss: 0.0022 Acc: 81.4118\n",
      "validation Loss: 0.0020 Acc: 82.6429\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 80.7720\n",
      "validation Loss: 0.0021 Acc: 81.2024\n",
      "Epoch 4/99\n",
      "training Loss: 0.0023 Acc: 81.1559\n",
      "validation Loss: 0.0022 Acc: 81.9524\n",
      "Epoch 5/99\n",
      "training Loss: 0.0023 Acc: 81.5785\n",
      "validation Loss: 0.0022 Acc: 82.7500\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 80.5577\n",
      "validation Loss: 0.0022 Acc: 81.5833\n",
      "Epoch 7/99\n",
      "training Loss: 0.0028 Acc: 79.3048\n",
      "validation Loss: 0.0027 Acc: 75.9881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0027 Acc: 79.5340\n",
      "validation Loss: 0.0026 Acc: 79.3810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0026 Acc: 80.6648\n",
      "validation Loss: 0.0025 Acc: 81.3214\n",
      "Epoch 10/99\n",
      "training Loss: 0.0026 Acc: 80.7958\n",
      "validation Loss: 0.0025 Acc: 81.3333\n",
      "Epoch 11/99\n",
      "training Loss: 0.0026 Acc: 81.0309\n",
      "validation Loss: 0.0025 Acc: 81.0714\n",
      "Epoch 12/99\n",
      "training Loss: 0.0025 Acc: 81.2630\n",
      "validation Loss: 0.0024 Acc: 82.0476\n",
      "Epoch 13/99\n",
      "training Loss: 0.0026 Acc: 80.9684\n",
      "validation Loss: 0.0025 Acc: 80.2857\n",
      "Epoch 14/99\n",
      "training Loss: 0.0026 Acc: 80.7541\n",
      "validation Loss: 0.0025 Acc: 81.5238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0025 Acc: 81.9773\n",
      "validation Loss: 0.0024 Acc: 81.9048\n",
      "Epoch 16/99\n",
      "training Loss: 0.0025 Acc: 82.0457\n",
      "validation Loss: 0.0025 Acc: 81.5833\n",
      "Epoch 17/99\n",
      "training Loss: 0.0025 Acc: 81.8672\n",
      "validation Loss: 0.0024 Acc: 81.6786\n",
      "Epoch 18/99\n",
      "training Loss: 0.0025 Acc: 82.1767\n",
      "validation Loss: 0.0024 Acc: 82.0595\n",
      "Epoch 19/99\n",
      "training Loss: 0.0025 Acc: 82.0368\n",
      "validation Loss: 0.0024 Acc: 81.7619\n",
      "Epoch 20/99\n",
      "training Loss: 0.0025 Acc: 82.3344\n",
      "validation Loss: 0.0024 Acc: 82.2381\n",
      "Early stopped.\n",
      "Best val acc: 82.761905\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0035 Acc: 80.0994\n",
      "validation Loss: 0.0020 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0022 Acc: 81.9594\n",
      "validation Loss: 0.0020 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 81.5071\n",
      "validation Loss: 0.0021 Acc: 82.9048\n",
      "Epoch 3/99\n",
      "training Loss: 0.0023 Acc: 80.9654\n",
      "validation Loss: 0.0021 Acc: 82.6548\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 80.6410\n",
      "validation Loss: 0.0022 Acc: 78.8690\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 78.8733\n",
      "validation Loss: 0.0025 Acc: 77.1667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0027 Acc: 76.3377\n",
      "validation Loss: 0.0024 Acc: 75.3571\n",
      "Epoch 7/99\n",
      "training Loss: 0.0024 Acc: 79.9804\n",
      "validation Loss: 0.0022 Acc: 80.3571\n",
      "Epoch 8/99\n",
      "training Loss: 0.0025 Acc: 80.1946\n",
      "validation Loss: 0.0022 Acc: 80.9286\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 80.9684\n",
      "validation Loss: 0.0022 Acc: 79.9524\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 80.8226\n",
      "validation Loss: 0.0022 Acc: 80.0238\n",
      "Epoch 11/99\n",
      "training Loss: 0.0029 Acc: 80.2274\n",
      "validation Loss: 0.0021 Acc: 80.7976\n",
      "Epoch 12/99\n",
      "training Loss: 0.0023 Acc: 81.4118\n",
      "validation Loss: 0.0022 Acc: 81.4167\n",
      "Epoch 13/99\n",
      "training Loss: 0.0023 Acc: 81.6261\n",
      "validation Loss: 0.0021 Acc: 81.9286\n",
      "Epoch 14/99\n",
      "training Loss: 0.0023 Acc: 81.8225\n",
      "validation Loss: 0.0020 Acc: 82.1071\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 81.9654\n",
      "validation Loss: 0.0021 Acc: 82.0357\n",
      "Epoch 16/99\n",
      "training Loss: 0.0022 Acc: 82.2630\n",
      "validation Loss: 0.0020 Acc: 82.1667\n",
      "Epoch 17/99\n",
      "training Loss: 0.0022 Acc: 82.2600\n",
      "validation Loss: 0.0020 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0022 Acc: 82.5100\n",
      "validation Loss: 0.0020 Acc: 82.6190\n",
      "Epoch 19/99\n",
      "training Loss: 0.0022 Acc: 82.6439\n",
      "validation Loss: 0.0020 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0021 Acc: 82.7600\n",
      "validation Loss: 0.0020 Acc: 83.4762\n",
      "Epoch 21/99\n",
      "training Loss: 0.0022 Acc: 82.6498\n",
      "validation Loss: 0.0020 Acc: 83.1667\n",
      "Epoch 22/99\n",
      "training Loss: 0.0021 Acc: 82.6617\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 23/99\n",
      "training Loss: 0.0021 Acc: 82.5189\n",
      "validation Loss: 0.0019 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0021 Acc: 83.0248\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 25/99\n",
      "training Loss: 0.0021 Acc: 82.7332\n",
      "validation Loss: 0.0020 Acc: 83.2738\n",
      "Epoch 26/99\n",
      "training Loss: 0.0021 Acc: 82.7540\n",
      "validation Loss: 0.0020 Acc: 83.4524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0021 Acc: 82.7808\n",
      "validation Loss: 0.0020 Acc: 83.6190\n",
      "Epoch 28/99\n",
      "training Loss: 0.0021 Acc: 82.9028\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Epoch 29/99\n",
      "training Loss: 0.0021 Acc: 82.9207\n",
      "validation Loss: 0.0020 Acc: 83.4524\n",
      "Epoch 30/99\n",
      "training Loss: 0.0021 Acc: 83.0962\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 31/99\n",
      "training Loss: 0.0021 Acc: 82.9832\n",
      "validation Loss: 0.0020 Acc: 83.4286\n",
      "Epoch 32/99\n",
      "training Loss: 0.0021 Acc: 82.9713\n",
      "validation Loss: 0.0020 Acc: 83.6071\n",
      "Epoch 33/99\n",
      "training Loss: 0.0021 Acc: 83.0218\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Epoch 34/99\n",
      "training Loss: 0.0021 Acc: 82.9326\n",
      "validation Loss: 0.0019 Acc: 83.6786\n",
      "Epoch 35/99\n",
      "training Loss: 0.0021 Acc: 82.7659\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 36/99\n",
      "training Loss: 0.0021 Acc: 83.0605\n",
      "validation Loss: 0.0019 Acc: 83.7738\n",
      "Epoch 37/99\n",
      "training Loss: 0.0021 Acc: 83.1320\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Epoch 38/99\n",
      "training Loss: 0.0021 Acc: 82.9385\n",
      "validation Loss: 0.0019 Acc: 83.6190\n",
      "Epoch 39/99\n",
      "training Loss: 0.0021 Acc: 82.9534\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 40/99\n",
      "training Loss: 0.0021 Acc: 83.1379\n",
      "validation Loss: 0.0019 Acc: 83.7024\n",
      "Epoch 41/99\n",
      "training Loss: 0.0021 Acc: 83.2123\n",
      "validation Loss: 0.0020 Acc: 83.7262\n",
      "Epoch 42/99\n",
      "training Loss: 0.0021 Acc: 82.9266\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 43/99\n",
      "training Loss: 0.0021 Acc: 82.9117\n",
      "validation Loss: 0.0019 Acc: 83.6667\n",
      "Early stopped.\n",
      "Best val acc: 83.809524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0040 Acc: 78.7840\n",
      "validation Loss: 0.0022 Acc: 78.7143\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 81.4297\n",
      "validation Loss: 0.0021 Acc: 80.2262\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0023 Acc: 81.5547\n",
      "validation Loss: 0.0021 Acc: 82.3929\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 81.3940\n",
      "validation Loss: 0.0021 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 81.0487\n",
      "validation Loss: 0.0023 Acc: 82.7024\n",
      "Epoch 5/99\n",
      "training Loss: 0.0034 Acc: 64.8860\n",
      "validation Loss: 0.0044 Acc: 50.0119\n",
      "Epoch 6/99\n",
      "training Loss: 0.0034 Acc: 62.0886\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0034 Acc: 62.9278\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0033 Acc: 62.3981\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0034 Acc: 61.9725\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0033 Acc: 63.1986\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0033 Acc: 62.4278\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0033 Acc: 63.3028\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0033 Acc: 63.1570\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0033 Acc: 63.1808\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0033 Acc: 63.4546\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0033 Acc: 63.5260\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0033 Acc: 63.4516\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0033 Acc: 62.5201\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0033 Acc: 63.0826\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0033 Acc: 62.7552\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 21/99\n",
      "training Loss: 0.0033 Acc: 63.1897\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 22/99\n",
      "training Loss: 0.0033 Acc: 62.9278\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Epoch 23/99\n",
      "training Loss: 0.0033 Acc: 62.6778\n",
      "validation Loss: 0.0037 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 82.761905\n",
      "----------\n",
      "Average best_acc across k-fold: 83.1722146654\n",
      "New configuration: {'learning_rate': 0.0002678281939064884, 'initial_nodes': 1000, 'dropout': 0.9, 'batch_size': 32, 'max_depth': 5}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0227 Acc: 50.5446\n",
      "validation Loss: 0.0217 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0209 Acc: 59.2798\n",
      "validation Loss: 0.0218 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0195 Acc: 67.9643\n",
      "validation Loss: 0.0216 Acc: 51.5473\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0188 Acc: 70.0268\n",
      "validation Loss: 0.0203 Acc: 60.2595\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0182 Acc: 71.3899\n",
      "validation Loss: 0.0190 Acc: 66.8293\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0180 Acc: 71.5893\n",
      "validation Loss: 0.0185 Acc: 68.5789\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0176 Acc: 72.3065\n",
      "validation Loss: 0.0182 Acc: 68.8646\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0175 Acc: 72.6964\n",
      "validation Loss: 0.0178 Acc: 69.9833\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0175 Acc: 72.8155\n",
      "validation Loss: 0.0173 Acc: 71.7091\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0173 Acc: 73.4792\n",
      "validation Loss: 0.0170 Acc: 72.2447\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0172 Acc: 73.5179\n",
      "validation Loss: 0.0165 Acc: 73.8872\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0172 Acc: 73.6518\n",
      "validation Loss: 0.0164 Acc: 73.7682\n",
      "Epoch 12/99\n",
      "training Loss: 0.0171 Acc: 73.9315\n",
      "validation Loss: 0.0164 Acc: 73.5182\n",
      "Epoch 13/99\n",
      "training Loss: 0.0167 Acc: 74.6845\n",
      "validation Loss: 0.0165 Acc: 72.9945\n",
      "Epoch 14/99\n",
      "training Loss: 0.0167 Acc: 74.6637\n",
      "validation Loss: 0.0162 Acc: 73.9348\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0169 Acc: 74.6161\n",
      "validation Loss: 0.0162 Acc: 73.8991\n",
      "Epoch 16/99\n",
      "training Loss: 0.0170 Acc: 74.5089\n",
      "validation Loss: 0.0164 Acc: 73.3516\n",
      "Epoch 17/99\n",
      "training Loss: 0.0166 Acc: 75.0893\n",
      "validation Loss: 0.0159 Acc: 74.8036\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0167 Acc: 75.2321\n",
      "validation Loss: 0.0158 Acc: 75.0060\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0167 Acc: 75.2470\n",
      "validation Loss: 0.0153 Acc: 76.7436\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0166 Acc: 75.2381\n",
      "validation Loss: 0.0158 Acc: 75.0893\n",
      "Epoch 21/99\n",
      "training Loss: 0.0164 Acc: 75.4345\n",
      "validation Loss: 0.0149 Acc: 77.5292\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0165 Acc: 75.5446\n",
      "validation Loss: 0.0154 Acc: 76.1961\n",
      "Epoch 23/99\n",
      "training Loss: 0.0163 Acc: 75.3571\n",
      "validation Loss: 0.0154 Acc: 75.9581\n",
      "Epoch 24/99\n",
      "training Loss: 0.0165 Acc: 75.6696\n",
      "validation Loss: 0.0151 Acc: 77.0769\n",
      "Epoch 25/99\n",
      "training Loss: 0.0166 Acc: 75.6905\n",
      "validation Loss: 0.0156 Acc: 75.5772\n",
      "Epoch 26/99\n",
      "training Loss: 0.0162 Acc: 76.0625\n",
      "validation Loss: 0.0148 Acc: 77.8505\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0162 Acc: 76.1548\n",
      "validation Loss: 0.0153 Acc: 76.0295\n",
      "Epoch 28/99\n",
      "training Loss: 0.0162 Acc: 76.1726\n",
      "validation Loss: 0.0150 Acc: 77.3030\n",
      "Epoch 29/99\n",
      "training Loss: 0.0162 Acc: 76.3631\n",
      "validation Loss: 0.0152 Acc: 76.2914\n",
      "Epoch 30/99\n",
      "training Loss: 0.0162 Acc: 76.1964\n",
      "validation Loss: 0.0152 Acc: 76.3866\n",
      "Epoch 31/99\n",
      "training Loss: 0.0162 Acc: 76.1726\n",
      "validation Loss: 0.0149 Acc: 77.2673\n",
      "Epoch 32/99\n",
      "training Loss: 0.0162 Acc: 76.1637\n",
      "validation Loss: 0.0152 Acc: 75.9581\n",
      "Epoch 33/99\n",
      "training Loss: 0.0160 Acc: 76.2321\n",
      "validation Loss: 0.0151 Acc: 76.3152\n",
      "Epoch 34/99\n",
      "training Loss: 0.0161 Acc: 76.1577\n",
      "validation Loss: 0.0149 Acc: 77.2554\n",
      "Epoch 35/99\n",
      "training Loss: 0.0161 Acc: 76.4196\n",
      "validation Loss: 0.0151 Acc: 76.4699\n",
      "Epoch 36/99\n",
      "training Loss: 0.0159 Acc: 76.5655\n",
      "validation Loss: 0.0147 Acc: 77.5054\n",
      "Epoch 37/99\n",
      "training Loss: 0.0159 Acc: 76.9881\n",
      "validation Loss: 0.0147 Acc: 77.4697\n",
      "Epoch 38/99\n",
      "training Loss: 0.0159 Acc: 76.6190\n",
      "validation Loss: 0.0145 Acc: 78.1243\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0158 Acc: 77.0208\n",
      "validation Loss: 0.0148 Acc: 77.0888\n",
      "Epoch 40/99\n",
      "training Loss: 0.0159 Acc: 76.7292\n",
      "validation Loss: 0.0147 Acc: 77.3030\n",
      "Epoch 41/99\n",
      "training Loss: 0.0157 Acc: 76.8065\n",
      "validation Loss: 0.0147 Acc: 77.4816\n",
      "Epoch 42/99\n",
      "training Loss: 0.0161 Acc: 76.4226\n",
      "validation Loss: 0.0145 Acc: 78.1243\n",
      "Epoch 43/99\n",
      "training Loss: 0.0160 Acc: 76.8065\n",
      "validation Loss: 0.0148 Acc: 77.2316\n",
      "Epoch 44/99\n",
      "training Loss: 0.0159 Acc: 76.5208\n",
      "validation Loss: 0.0147 Acc: 77.4697\n",
      "Epoch 45/99\n",
      "training Loss: 0.0158 Acc: 76.7321\n",
      "validation Loss: 0.0146 Acc: 77.7315\n",
      "Epoch 46/99\n",
      "training Loss: 0.0156 Acc: 76.9911\n",
      "validation Loss: 0.0145 Acc: 77.7077\n",
      "Epoch 47/99\n",
      "training Loss: 0.0158 Acc: 76.9702\n",
      "validation Loss: 0.0147 Acc: 77.2078\n",
      "Epoch 48/99\n",
      "training Loss: 0.0158 Acc: 77.0506\n",
      "validation Loss: 0.0145 Acc: 77.8505\n",
      "Epoch 49/99\n",
      "training Loss: 0.0157 Acc: 76.9851\n",
      "validation Loss: 0.0145 Acc: 77.9457\n",
      "Epoch 50/99\n",
      "training Loss: 0.0157 Acc: 77.1161\n",
      "validation Loss: 0.0145 Acc: 77.9219\n",
      "Epoch 51/99\n",
      "training Loss: 0.0156 Acc: 77.4435\n",
      "validation Loss: 0.0145 Acc: 77.8029\n",
      "Epoch 52/99\n",
      "training Loss: 0.0157 Acc: 77.1161\n",
      "validation Loss: 0.0146 Acc: 77.6482\n",
      "Epoch 53/99\n",
      "training Loss: 0.0157 Acc: 77.0208\n",
      "validation Loss: 0.0146 Acc: 77.6006\n",
      "Epoch 54/99\n",
      "training Loss: 0.0155 Acc: 77.4018\n",
      "validation Loss: 0.0145 Acc: 77.8862\n",
      "Epoch 55/99\n",
      "training Loss: 0.0157 Acc: 77.3571\n",
      "validation Loss: 0.0145 Acc: 77.8148\n",
      "Epoch 56/99\n",
      "training Loss: 0.0156 Acc: 77.0179\n",
      "validation Loss: 0.0144 Acc: 77.9695\n",
      "Epoch 57/99\n",
      "training Loss: 0.0157 Acc: 77.0149\n",
      "validation Loss: 0.0145 Acc: 77.8624\n",
      "Epoch 58/99\n",
      "training Loss: 0.0155 Acc: 77.2351\n",
      "validation Loss: 0.0145 Acc: 77.7196\n",
      "Early stopped.\n",
      "Best val acc: 78.124256\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0226 Acc: 51.9820\n",
      "validation Loss: 0.0217 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0207 Acc: 62.0469\n",
      "validation Loss: 0.0218 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0198 Acc: 66.9811\n",
      "validation Loss: 0.0218 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0193 Acc: 68.5554\n",
      "validation Loss: 0.0213 Acc: 50.7500\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0190 Acc: 69.5256\n",
      "validation Loss: 0.0212 Acc: 52.4524\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0187 Acc: 70.1655\n",
      "validation Loss: 0.0207 Acc: 57.8690\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0187 Acc: 70.4155\n",
      "validation Loss: 0.0203 Acc: 62.3929\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0184 Acc: 71.1624\n",
      "validation Loss: 0.0204 Acc: 60.9286\n",
      "Epoch 8/99\n",
      "training Loss: 0.0184 Acc: 71.4035\n",
      "validation Loss: 0.0202 Acc: 62.5238\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0182 Acc: 72.1148\n",
      "validation Loss: 0.0192 Acc: 69.4167\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0182 Acc: 72.1177\n",
      "validation Loss: 0.0194 Acc: 68.2976\n",
      "Epoch 11/99\n",
      "training Loss: 0.0180 Acc: 72.4719\n",
      "validation Loss: 0.0186 Acc: 71.3333\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0180 Acc: 72.9540\n",
      "validation Loss: 0.0187 Acc: 70.6429\n",
      "Epoch 13/99\n",
      "training Loss: 0.0180 Acc: 72.8350\n",
      "validation Loss: 0.0184 Acc: 71.8452\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0176 Acc: 73.5016\n",
      "validation Loss: 0.0180 Acc: 72.9762\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0177 Acc: 73.4361\n",
      "validation Loss: 0.0188 Acc: 69.7381\n",
      "Epoch 16/99\n",
      "training Loss: 0.0178 Acc: 73.2070\n",
      "validation Loss: 0.0174 Acc: 74.4405\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0177 Acc: 73.9271\n",
      "validation Loss: 0.0180 Acc: 72.5000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0176 Acc: 73.9480\n",
      "validation Loss: 0.0180 Acc: 72.2738\n",
      "Epoch 19/99\n",
      "training Loss: 0.0174 Acc: 74.2634\n",
      "validation Loss: 0.0175 Acc: 73.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0176 Acc: 73.7456\n",
      "validation Loss: 0.0180 Acc: 71.5357\n",
      "Epoch 21/99\n",
      "training Loss: 0.0175 Acc: 74.2158\n",
      "validation Loss: 0.0172 Acc: 74.3452\n",
      "Epoch 22/99\n",
      "training Loss: 0.0175 Acc: 74.4837\n",
      "validation Loss: 0.0168 Acc: 75.2619\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0173 Acc: 74.4718\n",
      "validation Loss: 0.0171 Acc: 73.7500\n",
      "Epoch 24/99\n",
      "training Loss: 0.0173 Acc: 74.5759\n",
      "validation Loss: 0.0173 Acc: 73.1429\n",
      "Epoch 25/99\n",
      "training Loss: 0.0174 Acc: 74.3289\n",
      "validation Loss: 0.0165 Acc: 75.6786\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0172 Acc: 74.8438\n",
      "validation Loss: 0.0164 Acc: 75.4643\n",
      "Epoch 27/99\n",
      "training Loss: 0.0173 Acc: 74.5283\n",
      "validation Loss: 0.0163 Acc: 76.1786\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0171 Acc: 75.2664\n",
      "validation Loss: 0.0163 Acc: 75.4048\n",
      "Epoch 29/99\n",
      "training Loss: 0.0173 Acc: 74.4182\n",
      "validation Loss: 0.0161 Acc: 76.3214\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0170 Acc: 75.4271\n",
      "validation Loss: 0.0161 Acc: 75.7500\n",
      "Epoch 31/99\n",
      "training Loss: 0.0170 Acc: 75.1681\n",
      "validation Loss: 0.0158 Acc: 75.9048\n",
      "Epoch 32/99\n",
      "training Loss: 0.0171 Acc: 74.8557\n",
      "validation Loss: 0.0158 Acc: 76.5476\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0170 Acc: 75.4390\n",
      "validation Loss: 0.0164 Acc: 74.9524\n",
      "Epoch 34/99\n",
      "training Loss: 0.0170 Acc: 75.1324\n",
      "validation Loss: 0.0158 Acc: 75.7262\n",
      "Epoch 35/99\n",
      "training Loss: 0.0170 Acc: 75.4330\n",
      "validation Loss: 0.0163 Acc: 74.7262\n",
      "Epoch 36/99\n",
      "training Loss: 0.0169 Acc: 75.1354\n",
      "validation Loss: 0.0162 Acc: 74.7024\n",
      "Epoch 37/99\n",
      "training Loss: 0.0168 Acc: 75.6979\n",
      "validation Loss: 0.0161 Acc: 74.5000\n",
      "Epoch 38/99\n",
      "training Loss: 0.0167 Acc: 75.7991\n",
      "validation Loss: 0.0159 Acc: 75.4286\n",
      "Epoch 39/99\n",
      "training Loss: 0.0169 Acc: 75.3288\n",
      "validation Loss: 0.0159 Acc: 75.4643\n",
      "Epoch 40/99\n",
      "training Loss: 0.0167 Acc: 75.9360\n",
      "validation Loss: 0.0157 Acc: 76.2738\n",
      "Epoch 41/99\n",
      "training Loss: 0.0168 Acc: 75.9389\n",
      "validation Loss: 0.0159 Acc: 75.5119\n",
      "Epoch 42/99\n",
      "training Loss: 0.0167 Acc: 76.0074\n",
      "validation Loss: 0.0154 Acc: 76.6667\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0166 Acc: 76.0818\n",
      "validation Loss: 0.0155 Acc: 76.5119\n",
      "Epoch 44/99\n",
      "training Loss: 0.0166 Acc: 76.0639\n",
      "validation Loss: 0.0156 Acc: 76.3095\n",
      "Epoch 45/99\n",
      "training Loss: 0.0167 Acc: 75.6949\n",
      "validation Loss: 0.0155 Acc: 76.7381\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0168 Acc: 75.9657\n",
      "validation Loss: 0.0157 Acc: 76.4405\n",
      "Epoch 47/99\n",
      "training Loss: 0.0165 Acc: 75.9925\n",
      "validation Loss: 0.0156 Acc: 76.1429\n",
      "Epoch 48/99\n",
      "training Loss: 0.0167 Acc: 75.7544\n",
      "validation Loss: 0.0152 Acc: 77.6548\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0167 Acc: 75.9508\n",
      "validation Loss: 0.0157 Acc: 75.5833\n",
      "Epoch 50/99\n",
      "training Loss: 0.0165 Acc: 76.2514\n",
      "validation Loss: 0.0153 Acc: 76.8810\n",
      "Epoch 51/99\n",
      "training Loss: 0.0166 Acc: 75.9836\n",
      "validation Loss: 0.0155 Acc: 76.2619\n",
      "Epoch 52/99\n",
      "training Loss: 0.0167 Acc: 76.2484\n",
      "validation Loss: 0.0154 Acc: 76.8571\n",
      "Epoch 53/99\n",
      "training Loss: 0.0165 Acc: 76.5966\n",
      "validation Loss: 0.0154 Acc: 76.6667\n",
      "Epoch 54/99\n",
      "training Loss: 0.0164 Acc: 76.5341\n",
      "validation Loss: 0.0152 Acc: 76.8571\n",
      "Epoch 55/99\n",
      "training Loss: 0.0167 Acc: 76.0014\n",
      "validation Loss: 0.0154 Acc: 76.4167\n",
      "Epoch 56/99\n",
      "training Loss: 0.0164 Acc: 76.6294\n",
      "validation Loss: 0.0152 Acc: 77.0952\n",
      "Epoch 57/99\n",
      "training Loss: 0.0163 Acc: 76.7186\n",
      "validation Loss: 0.0152 Acc: 77.0833\n",
      "Epoch 58/99\n",
      "training Loss: 0.0165 Acc: 76.3020\n",
      "validation Loss: 0.0154 Acc: 76.2738\n",
      "Epoch 59/99\n",
      "training Loss: 0.0165 Acc: 76.3109\n",
      "validation Loss: 0.0151 Acc: 77.2738\n",
      "Epoch 60/99\n",
      "training Loss: 0.0163 Acc: 76.7663\n",
      "validation Loss: 0.0150 Acc: 77.8095\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0163 Acc: 76.6829\n",
      "validation Loss: 0.0149 Acc: 78.0595\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0164 Acc: 76.7871\n",
      "validation Loss: 0.0154 Acc: 76.4524\n",
      "Epoch 63/99\n",
      "training Loss: 0.0164 Acc: 76.6442\n",
      "validation Loss: 0.0150 Acc: 77.7143\n",
      "Epoch 64/99\n",
      "training Loss: 0.0164 Acc: 76.4330\n",
      "validation Loss: 0.0150 Acc: 77.7143\n",
      "Epoch 65/99\n",
      "training Loss: 0.0164 Acc: 76.4121\n",
      "validation Loss: 0.0149 Acc: 77.8571\n",
      "Epoch 66/99\n",
      "training Loss: 0.0164 Acc: 76.3139\n",
      "validation Loss: 0.0151 Acc: 77.2024\n",
      "Epoch 67/99\n",
      "training Loss: 0.0164 Acc: 76.6859\n",
      "validation Loss: 0.0153 Acc: 76.5000\n",
      "Epoch 68/99\n",
      "training Loss: 0.0163 Acc: 76.3913\n",
      "validation Loss: 0.0151 Acc: 77.3452\n",
      "Epoch 69/99\n",
      "training Loss: 0.0163 Acc: 76.6353\n",
      "validation Loss: 0.0150 Acc: 77.5595\n",
      "Epoch 70/99\n",
      "training Loss: 0.0164 Acc: 76.4270\n",
      "validation Loss: 0.0150 Acc: 77.7381\n",
      "Epoch 71/99\n",
      "training Loss: 0.0163 Acc: 76.8585\n",
      "validation Loss: 0.0151 Acc: 77.3214\n",
      "Epoch 72/99\n",
      "training Loss: 0.0163 Acc: 76.8883\n",
      "validation Loss: 0.0149 Acc: 77.9524\n",
      "Epoch 73/99\n",
      "training Loss: 0.0163 Acc: 76.7127\n",
      "validation Loss: 0.0149 Acc: 77.8929\n",
      "Epoch 74/99\n",
      "training Loss: 0.0162 Acc: 77.1591\n",
      "validation Loss: 0.0151 Acc: 77.2500\n",
      "Epoch 75/99\n",
      "training Loss: 0.0163 Acc: 76.6681\n",
      "validation Loss: 0.0150 Acc: 77.6667\n",
      "Epoch 76/99\n",
      "training Loss: 0.0161 Acc: 77.1531\n",
      "validation Loss: 0.0149 Acc: 78.0476\n",
      "Epoch 77/99\n",
      "training Loss: 0.0163 Acc: 76.8555\n",
      "validation Loss: 0.0151 Acc: 77.5476\n",
      "Epoch 78/99\n",
      "training Loss: 0.0163 Acc: 76.4895\n",
      "validation Loss: 0.0150 Acc: 77.6310\n",
      "Epoch 79/99\n",
      "training Loss: 0.0163 Acc: 77.0787\n",
      "validation Loss: 0.0151 Acc: 77.2500\n",
      "Epoch 80/99\n",
      "training Loss: 0.0164 Acc: 76.4925\n",
      "validation Loss: 0.0151 Acc: 77.4048\n",
      "Epoch 81/99\n",
      "training Loss: 0.0160 Acc: 77.0787\n",
      "validation Loss: 0.0148 Acc: 78.1429\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0162 Acc: 76.5193\n",
      "validation Loss: 0.0149 Acc: 77.9405\n",
      "Epoch 83/99\n",
      "training Loss: 0.0162 Acc: 77.0668\n",
      "validation Loss: 0.0149 Acc: 77.9643\n",
      "Epoch 84/99\n",
      "training Loss: 0.0162 Acc: 76.8050\n",
      "validation Loss: 0.0148 Acc: 78.0952\n",
      "Epoch 85/99\n",
      "training Loss: 0.0161 Acc: 77.0877\n",
      "validation Loss: 0.0149 Acc: 77.8214\n",
      "Epoch 86/99\n",
      "training Loss: 0.0163 Acc: 76.7663\n",
      "validation Loss: 0.0150 Acc: 77.7619\n",
      "Epoch 87/99\n",
      "training Loss: 0.0163 Acc: 76.7990\n",
      "validation Loss: 0.0150 Acc: 77.5238\n",
      "Epoch 88/99\n",
      "training Loss: 0.0161 Acc: 76.7811\n",
      "validation Loss: 0.0149 Acc: 77.7500\n",
      "Epoch 89/99\n",
      "training Loss: 0.0161 Acc: 77.0936\n",
      "validation Loss: 0.0149 Acc: 77.8929\n",
      "Epoch 90/99\n",
      "training Loss: 0.0163 Acc: 76.6740\n",
      "validation Loss: 0.0149 Acc: 77.6905\n",
      "Epoch 91/99\n",
      "training Loss: 0.0161 Acc: 76.8526\n",
      "validation Loss: 0.0149 Acc: 77.8095\n",
      "Epoch 92/99\n",
      "training Loss: 0.0161 Acc: 77.1680\n",
      "validation Loss: 0.0149 Acc: 77.8810\n",
      "Epoch 93/99\n",
      "training Loss: 0.0161 Acc: 77.1174\n",
      "validation Loss: 0.0149 Acc: 77.7619\n",
      "Epoch 94/99\n",
      "training Loss: 0.0161 Acc: 77.0520\n",
      "validation Loss: 0.0149 Acc: 77.8452\n",
      "Epoch 95/99\n",
      "training Loss: 0.0160 Acc: 77.0698\n",
      "validation Loss: 0.0148 Acc: 77.9762\n",
      "Epoch 96/99\n",
      "training Loss: 0.0161 Acc: 77.2127\n",
      "validation Loss: 0.0148 Acc: 77.9524\n",
      "Epoch 97/99\n",
      "training Loss: 0.0160 Acc: 77.0996\n",
      "validation Loss: 0.0148 Acc: 78.0119\n",
      "Epoch 98/99\n",
      "training Loss: 0.0161 Acc: 77.1323\n",
      "validation Loss: 0.0149 Acc: 77.8929\n",
      "Epoch 99/99\n",
      "training Loss: 0.0163 Acc: 76.8347\n",
      "validation Loss: 0.0149 Acc: 77.7024\n",
      "Best val acc: 78.142857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0220 Acc: 56.9907\n",
      "validation Loss: 0.0217 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0199 Acc: 67.2103\n",
      "validation Loss: 0.0218 Acc: 50.6071\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0189 Acc: 69.5465\n",
      "validation Loss: 0.0209 Acc: 57.1905\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0184 Acc: 71.0434\n",
      "validation Loss: 0.0196 Acc: 63.6548\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0183 Acc: 71.2428\n",
      "validation Loss: 0.0186 Acc: 67.5714\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0177 Acc: 72.3945\n",
      "validation Loss: 0.0176 Acc: 70.7381\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0177 Acc: 72.4689\n",
      "validation Loss: 0.0180 Acc: 68.1310\n",
      "Epoch 7/99\n",
      "training Loss: 0.0176 Acc: 72.4481\n",
      "validation Loss: 0.0173 Acc: 70.9048\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0174 Acc: 73.3974\n",
      "validation Loss: 0.0171 Acc: 71.2619\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0171 Acc: 73.4778\n",
      "validation Loss: 0.0162 Acc: 74.7024\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0174 Acc: 73.6028\n",
      "validation Loss: 0.0164 Acc: 73.8810\n",
      "Epoch 11/99\n",
      "training Loss: 0.0169 Acc: 74.1265\n",
      "validation Loss: 0.0162 Acc: 73.9643\n",
      "Epoch 12/99\n",
      "training Loss: 0.0172 Acc: 74.0462\n",
      "validation Loss: 0.0163 Acc: 73.3571\n",
      "Epoch 13/99\n",
      "training Loss: 0.0169 Acc: 74.3825\n",
      "validation Loss: 0.0160 Acc: 74.4048\n",
      "Epoch 14/99\n",
      "training Loss: 0.0167 Acc: 74.4509\n",
      "validation Loss: 0.0156 Acc: 75.8810\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0169 Acc: 74.4777\n",
      "validation Loss: 0.0156 Acc: 75.8095\n",
      "Epoch 16/99\n",
      "training Loss: 0.0168 Acc: 74.7604\n",
      "validation Loss: 0.0153 Acc: 76.8690\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0167 Acc: 75.0074\n",
      "validation Loss: 0.0155 Acc: 76.0119\n",
      "Epoch 18/99\n",
      "training Loss: 0.0165 Acc: 75.0878\n",
      "validation Loss: 0.0154 Acc: 76.0595\n",
      "Epoch 19/99\n",
      "training Loss: 0.0167 Acc: 75.1771\n",
      "validation Loss: 0.0153 Acc: 76.2381\n",
      "Epoch 20/99\n",
      "training Loss: 0.0165 Acc: 75.3437\n",
      "validation Loss: 0.0151 Acc: 76.8810\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0164 Acc: 75.3765\n",
      "validation Loss: 0.0152 Acc: 76.5476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0165 Acc: 75.4866\n",
      "validation Loss: 0.0148 Acc: 77.8333\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0162 Acc: 75.6592\n",
      "validation Loss: 0.0147 Acc: 78.0595\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0165 Acc: 75.5521\n",
      "validation Loss: 0.0151 Acc: 76.6667\n",
      "Epoch 25/99\n",
      "training Loss: 0.0165 Acc: 75.2783\n",
      "validation Loss: 0.0147 Acc: 78.4286\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0165 Acc: 75.5759\n",
      "validation Loss: 0.0150 Acc: 77.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0164 Acc: 75.9389\n",
      "validation Loss: 0.0148 Acc: 77.8333\n",
      "Epoch 28/99\n",
      "training Loss: 0.0162 Acc: 75.7485\n",
      "validation Loss: 0.0145 Acc: 78.8810\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0161 Acc: 75.8199\n",
      "validation Loss: 0.0144 Acc: 78.9048\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0161 Acc: 76.1562\n",
      "validation Loss: 0.0144 Acc: 78.8571\n",
      "Epoch 31/99\n",
      "training Loss: 0.0160 Acc: 75.9241\n",
      "validation Loss: 0.0144 Acc: 78.8690\n",
      "Epoch 32/99\n",
      "training Loss: 0.0162 Acc: 75.9806\n",
      "validation Loss: 0.0145 Acc: 78.4405\n",
      "Epoch 33/99\n",
      "training Loss: 0.0161 Acc: 76.1978\n",
      "validation Loss: 0.0148 Acc: 77.2857\n",
      "Epoch 34/99\n",
      "training Loss: 0.0160 Acc: 75.9985\n",
      "validation Loss: 0.0147 Acc: 77.8810\n",
      "Epoch 35/99\n",
      "training Loss: 0.0160 Acc: 76.1413\n",
      "validation Loss: 0.0149 Acc: 77.0238\n",
      "Epoch 36/99\n",
      "training Loss: 0.0160 Acc: 76.1413\n",
      "validation Loss: 0.0144 Acc: 78.4405\n",
      "Epoch 37/99\n",
      "training Loss: 0.0158 Acc: 76.3943\n",
      "validation Loss: 0.0143 Acc: 78.7738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0161 Acc: 76.0996\n",
      "validation Loss: 0.0148 Acc: 77.0238\n",
      "Epoch 39/99\n",
      "training Loss: 0.0157 Acc: 76.6919\n",
      "validation Loss: 0.0144 Acc: 78.2262\n",
      "Epoch 40/99\n",
      "training Loss: 0.0159 Acc: 76.6472\n",
      "validation Loss: 0.0144 Acc: 78.4167\n",
      "Epoch 41/99\n",
      "training Loss: 0.0158 Acc: 76.6264\n",
      "validation Loss: 0.0143 Acc: 79.0595\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0159 Acc: 76.6085\n",
      "validation Loss: 0.0145 Acc: 77.7738\n",
      "Epoch 43/99\n",
      "training Loss: 0.0158 Acc: 76.5163\n",
      "validation Loss: 0.0145 Acc: 77.8929\n",
      "Epoch 44/99\n",
      "training Loss: 0.0160 Acc: 76.6681\n",
      "validation Loss: 0.0145 Acc: 77.8690\n",
      "Epoch 45/99\n",
      "training Loss: 0.0161 Acc: 75.9479\n",
      "validation Loss: 0.0144 Acc: 78.5000\n",
      "Epoch 46/99\n",
      "training Loss: 0.0161 Acc: 76.2574\n",
      "validation Loss: 0.0144 Acc: 78.3690\n",
      "Epoch 47/99\n",
      "training Loss: 0.0161 Acc: 76.2693\n",
      "validation Loss: 0.0145 Acc: 77.9524\n",
      "Epoch 48/99\n",
      "training Loss: 0.0158 Acc: 76.7038\n",
      "validation Loss: 0.0144 Acc: 78.5595\n",
      "Epoch 49/99\n",
      "training Loss: 0.0159 Acc: 76.4508\n",
      "validation Loss: 0.0144 Acc: 78.3571\n",
      "Epoch 50/99\n",
      "training Loss: 0.0157 Acc: 76.6056\n",
      "validation Loss: 0.0143 Acc: 78.7619\n",
      "Epoch 51/99\n",
      "training Loss: 0.0158 Acc: 76.7930\n",
      "validation Loss: 0.0142 Acc: 79.1667\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0157 Acc: 76.8317\n",
      "validation Loss: 0.0143 Acc: 78.9524\n",
      "Epoch 53/99\n",
      "training Loss: 0.0156 Acc: 76.7841\n",
      "validation Loss: 0.0142 Acc: 79.1310\n",
      "Epoch 54/99\n",
      "training Loss: 0.0157 Acc: 77.0222\n",
      "validation Loss: 0.0143 Acc: 78.6429\n",
      "Epoch 55/99\n",
      "training Loss: 0.0156 Acc: 77.0282\n",
      "validation Loss: 0.0142 Acc: 78.9048\n",
      "Epoch 56/99\n",
      "training Loss: 0.0157 Acc: 76.9180\n",
      "validation Loss: 0.0143 Acc: 78.6905\n",
      "Epoch 57/99\n",
      "training Loss: 0.0156 Acc: 76.9865\n",
      "validation Loss: 0.0141 Acc: 79.1905\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0158 Acc: 76.6740\n",
      "validation Loss: 0.0143 Acc: 78.5952\n",
      "Epoch 59/99\n",
      "training Loss: 0.0158 Acc: 76.5550\n",
      "validation Loss: 0.0142 Acc: 79.1548\n",
      "Epoch 60/99\n",
      "training Loss: 0.0156 Acc: 76.9151\n",
      "validation Loss: 0.0141 Acc: 79.4762\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0156 Acc: 77.0014\n",
      "validation Loss: 0.0141 Acc: 79.4167\n",
      "Epoch 62/99\n",
      "training Loss: 0.0157 Acc: 77.0430\n",
      "validation Loss: 0.0143 Acc: 78.7143\n",
      "Epoch 63/99\n",
      "training Loss: 0.0157 Acc: 76.8288\n",
      "validation Loss: 0.0142 Acc: 79.0833\n",
      "Epoch 64/99\n",
      "training Loss: 0.0156 Acc: 76.9329\n",
      "validation Loss: 0.0141 Acc: 79.2857\n",
      "Epoch 65/99\n",
      "training Loss: 0.0160 Acc: 77.0490\n",
      "validation Loss: 0.0144 Acc: 78.2024\n",
      "Epoch 66/99\n",
      "training Loss: 0.0158 Acc: 76.6502\n",
      "validation Loss: 0.0142 Acc: 78.9286\n",
      "Epoch 67/99\n",
      "training Loss: 0.0157 Acc: 76.6204\n",
      "validation Loss: 0.0141 Acc: 79.1786\n",
      "Epoch 68/99\n",
      "training Loss: 0.0157 Acc: 76.9567\n",
      "validation Loss: 0.0142 Acc: 78.7381\n",
      "Epoch 69/99\n",
      "training Loss: 0.0156 Acc: 76.7752\n",
      "validation Loss: 0.0141 Acc: 79.1786\n",
      "Epoch 70/99\n",
      "training Loss: 0.0155 Acc: 77.1293\n",
      "validation Loss: 0.0140 Acc: 79.5833\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0156 Acc: 77.2008\n",
      "validation Loss: 0.0141 Acc: 79.1548\n",
      "Epoch 72/99\n",
      "training Loss: 0.0156 Acc: 77.2930\n",
      "validation Loss: 0.0141 Acc: 79.2500\n",
      "Epoch 73/99\n",
      "training Loss: 0.0157 Acc: 77.3109\n",
      "validation Loss: 0.0141 Acc: 79.1548\n",
      "Epoch 74/99\n",
      "training Loss: 0.0156 Acc: 77.2722\n",
      "validation Loss: 0.0142 Acc: 78.6071\n",
      "Epoch 75/99\n",
      "training Loss: 0.0156 Acc: 76.8139\n",
      "validation Loss: 0.0142 Acc: 78.8095\n",
      "Epoch 76/99\n",
      "training Loss: 0.0155 Acc: 77.0758\n",
      "validation Loss: 0.0140 Acc: 79.4643\n",
      "Epoch 77/99\n",
      "training Loss: 0.0155 Acc: 77.1799\n",
      "validation Loss: 0.0141 Acc: 79.2619\n",
      "Epoch 78/99\n",
      "training Loss: 0.0155 Acc: 77.1710\n",
      "validation Loss: 0.0141 Acc: 79.2976\n",
      "Epoch 79/99\n",
      "training Loss: 0.0157 Acc: 76.7514\n",
      "validation Loss: 0.0141 Acc: 79.2857\n",
      "Epoch 80/99\n",
      "training Loss: 0.0155 Acc: 77.1502\n",
      "validation Loss: 0.0140 Acc: 79.4405\n",
      "Epoch 81/99\n",
      "training Loss: 0.0154 Acc: 77.3883\n",
      "validation Loss: 0.0140 Acc: 79.6548\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0156 Acc: 77.2514\n",
      "validation Loss: 0.0140 Acc: 79.4643\n",
      "Epoch 83/99\n",
      "training Loss: 0.0155 Acc: 77.1770\n",
      "validation Loss: 0.0140 Acc: 79.5833\n",
      "Epoch 84/99\n",
      "training Loss: 0.0156 Acc: 77.0787\n",
      "validation Loss: 0.0140 Acc: 79.5595\n",
      "Epoch 85/99\n",
      "training Loss: 0.0158 Acc: 76.7901\n",
      "validation Loss: 0.0141 Acc: 79.3929\n",
      "Epoch 86/99\n",
      "training Loss: 0.0154 Acc: 77.2008\n",
      "validation Loss: 0.0140 Acc: 79.6905\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0157 Acc: 77.1948\n",
      "validation Loss: 0.0141 Acc: 79.3333\n",
      "Epoch 88/99\n",
      "training Loss: 0.0157 Acc: 77.1145\n",
      "validation Loss: 0.0141 Acc: 79.2619\n",
      "Epoch 89/99\n",
      "training Loss: 0.0156 Acc: 77.4091\n",
      "validation Loss: 0.0140 Acc: 79.5238\n",
      "Epoch 90/99\n",
      "training Loss: 0.0156 Acc: 77.0252\n",
      "validation Loss: 0.0140 Acc: 79.4643\n",
      "Epoch 91/99\n",
      "training Loss: 0.0155 Acc: 77.1353\n",
      "validation Loss: 0.0140 Acc: 79.4405\n",
      "Epoch 92/99\n",
      "training Loss: 0.0156 Acc: 77.1353\n",
      "validation Loss: 0.0140 Acc: 79.4286\n",
      "Epoch 93/99\n",
      "training Loss: 0.0156 Acc: 77.0728\n",
      "validation Loss: 0.0140 Acc: 79.3929\n",
      "Epoch 94/99\n",
      "training Loss: 0.0158 Acc: 77.2216\n",
      "validation Loss: 0.0141 Acc: 79.3452\n",
      "Epoch 95/99\n",
      "training Loss: 0.0156 Acc: 77.0966\n",
      "validation Loss: 0.0141 Acc: 79.3095\n",
      "Epoch 96/99\n",
      "training Loss: 0.0156 Acc: 77.0371\n",
      "validation Loss: 0.0140 Acc: 79.2976\n",
      "Epoch 97/99\n",
      "training Loss: 0.0156 Acc: 77.0490\n",
      "validation Loss: 0.0141 Acc: 79.2857\n",
      "Epoch 98/99\n",
      "training Loss: 0.0155 Acc: 77.1859\n",
      "validation Loss: 0.0140 Acc: 79.3333\n",
      "Epoch 99/99\n",
      "training Loss: 0.0157 Acc: 77.0311\n",
      "validation Loss: 0.0141 Acc: 79.3214\n",
      "Best val acc: 79.690476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0222 Acc: 54.3182\n",
      "validation Loss: 0.0217 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0201 Acc: 66.2728\n",
      "validation Loss: 0.0216 Acc: 51.5952\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0189 Acc: 69.5970\n",
      "validation Loss: 0.0203 Acc: 61.0476\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0183 Acc: 71.0583\n",
      "validation Loss: 0.0193 Acc: 65.3810\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0180 Acc: 71.2934\n",
      "validation Loss: 0.0182 Acc: 69.4167\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0175 Acc: 72.5433\n",
      "validation Loss: 0.0175 Acc: 71.1071\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0174 Acc: 72.7725\n",
      "validation Loss: 0.0171 Acc: 72.3333\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0172 Acc: 73.3111\n",
      "validation Loss: 0.0171 Acc: 71.4167\n",
      "Epoch 8/99\n",
      "training Loss: 0.0171 Acc: 73.4897\n",
      "validation Loss: 0.0167 Acc: 73.1190\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0170 Acc: 73.7545\n",
      "validation Loss: 0.0166 Acc: 73.3095\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0167 Acc: 74.3527\n",
      "validation Loss: 0.0165 Acc: 73.5952\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0168 Acc: 73.9896\n",
      "validation Loss: 0.0161 Acc: 74.7500\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0167 Acc: 74.4926\n",
      "validation Loss: 0.0166 Acc: 72.6667\n",
      "Epoch 13/99\n",
      "training Loss: 0.0166 Acc: 74.5610\n",
      "validation Loss: 0.0159 Acc: 75.4643\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0164 Acc: 75.2098\n",
      "validation Loss: 0.0161 Acc: 74.4167\n",
      "Epoch 15/99\n",
      "training Loss: 0.0163 Acc: 75.1771\n",
      "validation Loss: 0.0156 Acc: 76.3571\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0166 Acc: 74.9747\n",
      "validation Loss: 0.0157 Acc: 76.1190\n",
      "Epoch 17/99\n",
      "training Loss: 0.0164 Acc: 75.0670\n",
      "validation Loss: 0.0155 Acc: 76.9167\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0163 Acc: 75.2723\n",
      "validation Loss: 0.0158 Acc: 75.6905\n",
      "Epoch 19/99\n",
      "training Loss: 0.0165 Acc: 75.2098\n",
      "validation Loss: 0.0158 Acc: 75.4881\n",
      "Epoch 20/99\n",
      "training Loss: 0.0164 Acc: 75.3586\n",
      "validation Loss: 0.0154 Acc: 77.4762\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0165 Acc: 75.3408\n",
      "validation Loss: 0.0157 Acc: 75.8214\n",
      "Epoch 22/99\n",
      "training Loss: 0.0165 Acc: 75.2931\n",
      "validation Loss: 0.0155 Acc: 77.0714\n",
      "Epoch 23/99\n",
      "training Loss: 0.0162 Acc: 75.8288\n",
      "validation Loss: 0.0157 Acc: 75.9524\n",
      "Epoch 24/99\n",
      "training Loss: 0.0162 Acc: 75.9925\n",
      "validation Loss: 0.0155 Acc: 76.6786\n",
      "Epoch 25/99\n",
      "training Loss: 0.0159 Acc: 75.9895\n",
      "validation Loss: 0.0156 Acc: 76.8571\n",
      "Epoch 26/99\n",
      "training Loss: 0.0161 Acc: 75.9002\n",
      "validation Loss: 0.0157 Acc: 76.3452\n",
      "Epoch 27/99\n",
      "training Loss: 0.0161 Acc: 75.8675\n",
      "validation Loss: 0.0156 Acc: 76.6071\n",
      "Epoch 28/99\n",
      "training Loss: 0.0160 Acc: 76.5669\n",
      "validation Loss: 0.0154 Acc: 77.3214\n",
      "Epoch 29/99\n",
      "training Loss: 0.0159 Acc: 76.3526\n",
      "validation Loss: 0.0154 Acc: 77.2738\n",
      "Epoch 30/99\n",
      "training Loss: 0.0158 Acc: 76.3675\n",
      "validation Loss: 0.0152 Acc: 78.0119\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0158 Acc: 76.3050\n",
      "validation Loss: 0.0150 Acc: 78.7500\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0157 Acc: 76.6651\n",
      "validation Loss: 0.0151 Acc: 78.1190\n",
      "Epoch 33/99\n",
      "training Loss: 0.0158 Acc: 76.5788\n",
      "validation Loss: 0.0153 Acc: 77.3214\n",
      "Epoch 34/99\n",
      "training Loss: 0.0159 Acc: 76.3943\n",
      "validation Loss: 0.0152 Acc: 77.6429\n",
      "Epoch 35/99\n",
      "training Loss: 0.0161 Acc: 76.3228\n",
      "validation Loss: 0.0152 Acc: 77.6548\n",
      "Epoch 36/99\n",
      "training Loss: 0.0158 Acc: 76.2871\n",
      "validation Loss: 0.0152 Acc: 77.7976\n",
      "Epoch 37/99\n",
      "training Loss: 0.0158 Acc: 76.3437\n",
      "validation Loss: 0.0152 Acc: 78.0952\n",
      "Epoch 38/99\n",
      "training Loss: 0.0159 Acc: 76.6829\n",
      "validation Loss: 0.0153 Acc: 77.5000\n",
      "Epoch 39/99\n",
      "training Loss: 0.0156 Acc: 76.7306\n",
      "validation Loss: 0.0153 Acc: 77.7024\n",
      "Epoch 40/99\n",
      "training Loss: 0.0156 Acc: 76.7633\n",
      "validation Loss: 0.0153 Acc: 77.3571\n",
      "Epoch 41/99\n",
      "training Loss: 0.0157 Acc: 76.4716\n",
      "validation Loss: 0.0153 Acc: 77.6548\n",
      "Epoch 42/99\n",
      "training Loss: 0.0157 Acc: 76.7127\n",
      "validation Loss: 0.0153 Acc: 77.8214\n",
      "Epoch 43/99\n",
      "training Loss: 0.0158 Acc: 76.7603\n",
      "validation Loss: 0.0154 Acc: 77.0476\n",
      "Epoch 44/99\n",
      "training Loss: 0.0159 Acc: 76.7573\n",
      "validation Loss: 0.0153 Acc: 77.4881\n",
      "Epoch 45/99\n",
      "training Loss: 0.0158 Acc: 76.5669\n",
      "validation Loss: 0.0152 Acc: 77.7262\n",
      "Epoch 46/99\n",
      "training Loss: 0.0159 Acc: 76.8466\n",
      "validation Loss: 0.0152 Acc: 77.9048\n",
      "Epoch 47/99\n",
      "training Loss: 0.0157 Acc: 76.9032\n",
      "validation Loss: 0.0152 Acc: 77.6905\n",
      "Epoch 48/99\n",
      "training Loss: 0.0156 Acc: 76.7097\n",
      "validation Loss: 0.0152 Acc: 77.8810\n",
      "Epoch 49/99\n",
      "training Loss: 0.0156 Acc: 76.8109\n",
      "validation Loss: 0.0152 Acc: 77.7738\n",
      "Epoch 50/99\n",
      "training Loss: 0.0157 Acc: 76.4865\n",
      "validation Loss: 0.0152 Acc: 77.7976\n",
      "Epoch 51/99\n",
      "training Loss: 0.0157 Acc: 76.5639\n",
      "validation Loss: 0.0152 Acc: 77.8452\n",
      "Early stopped.\n",
      "Best val acc: 78.750000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0227 Acc: 50.6726\n",
      "validation Loss: 0.0217 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0211 Acc: 58.9548\n",
      "validation Loss: 0.0217 Acc: 50.5357\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0198 Acc: 67.0674\n",
      "validation Loss: 0.0212 Acc: 55.1548\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0187 Acc: 69.8411\n",
      "validation Loss: 0.0198 Acc: 63.7857\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0184 Acc: 70.0881\n",
      "validation Loss: 0.0188 Acc: 67.0714\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0180 Acc: 71.6505\n",
      "validation Loss: 0.0181 Acc: 69.2976\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0179 Acc: 71.7814\n",
      "validation Loss: 0.0178 Acc: 70.1548\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0177 Acc: 72.1951\n",
      "validation Loss: 0.0176 Acc: 70.1429\n",
      "Epoch 8/99\n",
      "training Loss: 0.0174 Acc: 72.7427\n",
      "validation Loss: 0.0171 Acc: 71.9762\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0173 Acc: 72.9302\n",
      "validation Loss: 0.0170 Acc: 71.5833\n",
      "Epoch 10/99\n",
      "training Loss: 0.0173 Acc: 73.3200\n",
      "validation Loss: 0.0165 Acc: 73.2381\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0171 Acc: 73.6593\n",
      "validation Loss: 0.0167 Acc: 71.8810\n",
      "Epoch 12/99\n",
      "training Loss: 0.0170 Acc: 73.8706\n",
      "validation Loss: 0.0164 Acc: 73.1429\n",
      "Epoch 13/99\n",
      "training Loss: 0.0172 Acc: 74.1652\n",
      "validation Loss: 0.0166 Acc: 72.1310\n",
      "Epoch 14/99\n",
      "training Loss: 0.0171 Acc: 73.7754\n",
      "validation Loss: 0.0160 Acc: 74.4762\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0167 Acc: 74.6741\n",
      "validation Loss: 0.0158 Acc: 74.7738\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0168 Acc: 74.4450\n",
      "validation Loss: 0.0156 Acc: 75.4167\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0167 Acc: 74.8319\n",
      "validation Loss: 0.0160 Acc: 74.1429\n",
      "Epoch 18/99\n",
      "training Loss: 0.0164 Acc: 75.0283\n",
      "validation Loss: 0.0157 Acc: 74.8214\n",
      "Epoch 19/99\n",
      "training Loss: 0.0169 Acc: 74.6116\n",
      "validation Loss: 0.0156 Acc: 75.1667\n",
      "Epoch 20/99\n",
      "training Loss: 0.0168 Acc: 74.9182\n",
      "validation Loss: 0.0154 Acc: 75.8333\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0167 Acc: 75.1652\n",
      "validation Loss: 0.0155 Acc: 75.4167\n",
      "Epoch 22/99\n",
      "training Loss: 0.0164 Acc: 75.3378\n",
      "validation Loss: 0.0152 Acc: 76.0595\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0165 Acc: 75.7009\n",
      "validation Loss: 0.0152 Acc: 76.1667\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0165 Acc: 75.1116\n",
      "validation Loss: 0.0152 Acc: 76.2976\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0164 Acc: 75.8526\n",
      "validation Loss: 0.0151 Acc: 76.8452\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0162 Acc: 75.9776\n",
      "validation Loss: 0.0150 Acc: 76.7262\n",
      "Epoch 27/99\n",
      "training Loss: 0.0165 Acc: 75.7961\n",
      "validation Loss: 0.0154 Acc: 75.4048\n",
      "Epoch 28/99\n",
      "training Loss: 0.0163 Acc: 75.7872\n",
      "validation Loss: 0.0151 Acc: 76.4405\n",
      "Epoch 29/99\n",
      "training Loss: 0.0164 Acc: 75.5193\n",
      "validation Loss: 0.0150 Acc: 76.6905\n",
      "Epoch 30/99\n",
      "training Loss: 0.0162 Acc: 75.7336\n",
      "validation Loss: 0.0152 Acc: 75.7024\n",
      "Epoch 31/99\n",
      "training Loss: 0.0161 Acc: 76.1978\n",
      "validation Loss: 0.0154 Acc: 75.0714\n",
      "Epoch 32/99\n",
      "training Loss: 0.0162 Acc: 75.8467\n",
      "validation Loss: 0.0152 Acc: 75.6548\n",
      "Epoch 33/99\n",
      "training Loss: 0.0160 Acc: 76.2871\n",
      "validation Loss: 0.0149 Acc: 76.5476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0160 Acc: 76.2514\n",
      "validation Loss: 0.0151 Acc: 75.7738\n",
      "Epoch 35/99\n",
      "training Loss: 0.0161 Acc: 76.2425\n",
      "validation Loss: 0.0153 Acc: 75.2619\n",
      "Epoch 36/99\n",
      "training Loss: 0.0161 Acc: 76.1353\n",
      "validation Loss: 0.0150 Acc: 76.2143\n",
      "Epoch 37/99\n",
      "training Loss: 0.0161 Acc: 76.3437\n",
      "validation Loss: 0.0151 Acc: 75.9286\n",
      "Epoch 38/99\n",
      "training Loss: 0.0158 Acc: 76.5282\n",
      "validation Loss: 0.0149 Acc: 76.4286\n",
      "Epoch 39/99\n",
      "training Loss: 0.0159 Acc: 76.3347\n",
      "validation Loss: 0.0149 Acc: 76.6429\n",
      "Epoch 40/99\n",
      "training Loss: 0.0157 Acc: 76.7692\n",
      "validation Loss: 0.0149 Acc: 76.4167\n",
      "Epoch 41/99\n",
      "training Loss: 0.0160 Acc: 76.5401\n",
      "validation Loss: 0.0148 Acc: 76.6905\n",
      "Epoch 42/99\n",
      "training Loss: 0.0159 Acc: 76.5728\n",
      "validation Loss: 0.0150 Acc: 76.0595\n",
      "Epoch 43/99\n",
      "training Loss: 0.0158 Acc: 76.6056\n",
      "validation Loss: 0.0147 Acc: 77.2738\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0159 Acc: 76.4478\n",
      "validation Loss: 0.0147 Acc: 77.3690\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0159 Acc: 76.5341\n",
      "validation Loss: 0.0149 Acc: 76.4167\n",
      "Epoch 46/99\n",
      "training Loss: 0.0159 Acc: 76.6948\n",
      "validation Loss: 0.0148 Acc: 76.8214\n",
      "Epoch 47/99\n",
      "training Loss: 0.0159 Acc: 76.5639\n",
      "validation Loss: 0.0147 Acc: 77.0000\n",
      "Epoch 48/99\n",
      "training Loss: 0.0158 Acc: 76.7246\n",
      "validation Loss: 0.0150 Acc: 75.7857\n",
      "Epoch 49/99\n",
      "training Loss: 0.0158 Acc: 76.7365\n",
      "validation Loss: 0.0149 Acc: 76.3810\n",
      "Epoch 50/99\n",
      "training Loss: 0.0160 Acc: 76.4270\n",
      "validation Loss: 0.0149 Acc: 76.1786\n",
      "Epoch 51/99\n",
      "training Loss: 0.0157 Acc: 76.3556\n",
      "validation Loss: 0.0147 Acc: 76.8929\n",
      "Epoch 52/99\n",
      "training Loss: 0.0157 Acc: 76.9121\n",
      "validation Loss: 0.0146 Acc: 77.1310\n",
      "Epoch 53/99\n",
      "training Loss: 0.0158 Acc: 76.6859\n",
      "validation Loss: 0.0148 Acc: 76.5119\n",
      "Epoch 54/99\n",
      "training Loss: 0.0155 Acc: 77.0192\n",
      "validation Loss: 0.0147 Acc: 77.0238\n",
      "Epoch 55/99\n",
      "training Loss: 0.0157 Acc: 76.9389\n",
      "validation Loss: 0.0147 Acc: 76.8214\n",
      "Epoch 56/99\n",
      "training Loss: 0.0156 Acc: 77.0014\n",
      "validation Loss: 0.0148 Acc: 76.5000\n",
      "Epoch 57/99\n",
      "training Loss: 0.0155 Acc: 77.2662\n",
      "validation Loss: 0.0148 Acc: 76.4643\n",
      "Epoch 58/99\n",
      "training Loss: 0.0157 Acc: 76.9121\n",
      "validation Loss: 0.0147 Acc: 76.6548\n",
      "Epoch 59/99\n",
      "training Loss: 0.0156 Acc: 77.2841\n",
      "validation Loss: 0.0147 Acc: 76.7143\n",
      "Epoch 60/99\n",
      "training Loss: 0.0156 Acc: 77.1383\n",
      "validation Loss: 0.0147 Acc: 76.8929\n",
      "Epoch 61/99\n",
      "training Loss: 0.0156 Acc: 77.0311\n",
      "validation Loss: 0.0147 Acc: 76.7857\n",
      "Epoch 62/99\n",
      "training Loss: 0.0155 Acc: 76.9984\n",
      "validation Loss: 0.0147 Acc: 76.9048\n",
      "Epoch 63/99\n",
      "training Loss: 0.0156 Acc: 77.0192\n",
      "validation Loss: 0.0146 Acc: 77.1548\n",
      "Epoch 64/99\n",
      "training Loss: 0.0156 Acc: 77.1948\n",
      "validation Loss: 0.0147 Acc: 76.9405\n",
      "Early stopped.\n",
      "Best val acc: 77.369048\n",
      "----------\n",
      "Average best_acc across k-fold: 78.4153274164\n",
      "New configuration: {'learning_rate': 0.0540298647429419, 'initial_nodes': 578, 'dropout': 0.01, 'batch_size': 271, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0045 Acc: 79.6875\n",
      "validation Loss: 0.0014 Acc: 82.4804\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 82.6964\n",
      "validation Loss: 0.0014 Acc: 83.8134\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 82.8780\n",
      "validation Loss: 0.0013 Acc: 83.7420\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.2440\n",
      "validation Loss: 0.0014 Acc: 83.3611\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 82.7887\n",
      "validation Loss: 0.0014 Acc: 82.9921\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 83.2768\n",
      "validation Loss: 0.0013 Acc: 84.3014\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 83.3601\n",
      "validation Loss: 0.0013 Acc: 83.9443\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 83.4940\n",
      "validation Loss: 0.0013 Acc: 83.7539\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 83.4524\n",
      "validation Loss: 0.0013 Acc: 83.6825\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 83.5149\n",
      "validation Loss: 0.0014 Acc: 83.9681\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 83.6815\n",
      "validation Loss: 0.0013 Acc: 83.6587\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 83.5804\n",
      "validation Loss: 0.0013 Acc: 83.9562\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 83.6280\n",
      "validation Loss: 0.0014 Acc: 84.4442\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.9077\n",
      "validation Loss: 0.0013 Acc: 84.1942\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 84.0179\n",
      "validation Loss: 0.0014 Acc: 84.0038\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 83.9256\n",
      "validation Loss: 0.0017 Acc: 84.1942\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 84.0298\n",
      "validation Loss: 0.0013 Acc: 84.3490\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 84.0952\n",
      "validation Loss: 0.0015 Acc: 83.9800\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 84.0714\n",
      "validation Loss: 0.0013 Acc: 84.2657\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 84.0863\n",
      "validation Loss: 0.0013 Acc: 84.3490\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 84.2083\n",
      "validation Loss: 0.0014 Acc: 84.4561\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 84.3571\n",
      "validation Loss: 0.0013 Acc: 84.1109\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 84.3185\n",
      "validation Loss: 0.0013 Acc: 84.4085\n",
      "Epoch 23/99\n",
      "training Loss: 0.0012 Acc: 84.3095\n",
      "validation Loss: 0.0015 Acc: 84.1109\n",
      "Epoch 24/99\n",
      "training Loss: 0.0012 Acc: 84.4048\n",
      "validation Loss: 0.0013 Acc: 84.0990\n",
      "Epoch 25/99\n",
      "training Loss: 0.0012 Acc: 84.5000\n",
      "validation Loss: 0.0014 Acc: 84.3133\n",
      "Epoch 26/99\n",
      "training Loss: 0.0012 Acc: 84.6875\n",
      "validation Loss: 0.0013 Acc: 84.2537\n",
      "Epoch 27/99\n",
      "training Loss: 0.0012 Acc: 84.7054\n",
      "validation Loss: 0.0013 Acc: 84.1347\n",
      "Epoch 28/99\n",
      "training Loss: 0.0012 Acc: 84.7411\n",
      "validation Loss: 0.0013 Acc: 83.9086\n",
      "Epoch 29/99\n",
      "training Loss: 0.0012 Acc: 84.6964\n",
      "validation Loss: 0.0013 Acc: 83.9324\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 84.7649\n",
      "validation Loss: 0.0013 Acc: 83.8729\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 84.8125\n",
      "validation Loss: 0.0013 Acc: 83.8967\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 84.9762\n",
      "validation Loss: 0.0014 Acc: 83.9205\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 84.9583\n",
      "validation Loss: 0.0014 Acc: 83.9681\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 84.9911\n",
      "validation Loss: 0.0014 Acc: 83.8253\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 85.1012\n",
      "validation Loss: 0.0014 Acc: 83.8967\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 85.0238\n",
      "validation Loss: 0.0014 Acc: 83.8610\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 85.1577\n",
      "validation Loss: 0.0014 Acc: 83.8015\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 85.1786\n",
      "validation Loss: 0.0014 Acc: 83.8134\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 85.2292\n",
      "validation Loss: 0.0014 Acc: 83.6229\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 85.3155\n",
      "validation Loss: 0.0015 Acc: 83.7539\n",
      "Early stopped.\n",
      "Best val acc: 84.456082\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0044 Acc: 79.4477\n",
      "validation Loss: 0.0014 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 83.0308\n",
      "validation Loss: 0.0014 Acc: 82.9881\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 82.9028\n",
      "validation Loss: 0.0014 Acc: 82.5595\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.1706\n",
      "validation Loss: 0.0013 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0013 Acc: 83.4385\n",
      "validation Loss: 0.0013 Acc: 83.1786\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 83.3165\n",
      "validation Loss: 0.0013 Acc: 83.0714\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 83.3165\n",
      "validation Loss: 0.0013 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 83.4206\n",
      "validation Loss: 0.0013 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 83.6319\n",
      "validation Loss: 0.0013 Acc: 82.8810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 83.4325\n",
      "validation Loss: 0.0013 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 83.6795\n",
      "validation Loss: 0.0013 Acc: 84.0000\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 83.7034\n",
      "validation Loss: 0.0013 Acc: 83.8929\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 83.6200\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.6319\n",
      "validation Loss: 0.0013 Acc: 83.5119\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 83.2450\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 81.9386\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 82.6975\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.1677\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 83.0665\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.0873\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.3700\n",
      "validation Loss: 0.0013 Acc: 83.7976\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.4831\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 83.6170\n",
      "validation Loss: 0.0013 Acc: 83.4286\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 83.7182\n",
      "validation Loss: 0.0013 Acc: 83.7738\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 83.8283\n",
      "validation Loss: 0.0016 Acc: 83.7857\n",
      "Epoch 25/99\n",
      "training Loss: 0.0025 Acc: 83.3194\n",
      "validation Loss: 0.0014 Acc: 83.0595\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 83.8343\n",
      "validation Loss: 0.0013 Acc: 83.4286\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 83.8224\n",
      "validation Loss: 0.0013 Acc: 83.8333\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 83.7658\n",
      "validation Loss: 0.0013 Acc: 83.7619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 83.9027\n",
      "validation Loss: 0.0013 Acc: 83.6429\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 83.9742\n",
      "validation Loss: 0.0013 Acc: 83.7024\n",
      "Early stopped.\n",
      "Best val acc: 84.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 79.7423\n",
      "validation Loss: 0.0014 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 83.2361\n",
      "validation Loss: 0.0014 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 83.1587\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.3016\n",
      "validation Loss: 0.0013 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.2837\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 83.2034\n",
      "validation Loss: 0.0013 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 83.3819\n",
      "validation Loss: 0.0013 Acc: 83.3571\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 83.3105\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 83.3284\n",
      "validation Loss: 0.0013 Acc: 82.8690\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.2927\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 83.0754\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.7451\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 83.3254\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 83.2480\n",
      "validation Loss: 0.0014 Acc: 83.5595\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 83.4236\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 83.3611\n",
      "validation Loss: 0.0013 Acc: 83.5476\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.4534\n",
      "validation Loss: 0.0013 Acc: 83.4524\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.4057\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.6706\n",
      "validation Loss: 0.0013 Acc: 83.5952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.5843\n",
      "validation Loss: 0.0013 Acc: 83.6667\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.7004\n",
      "validation Loss: 0.0013 Acc: 83.6905\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.7361\n",
      "validation Loss: 0.0013 Acc: 83.2857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.8224\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 83.8522\n",
      "validation Loss: 0.0013 Acc: 83.4405\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 83.8789\n",
      "validation Loss: 0.0013 Acc: 83.6429\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 83.8343\n",
      "validation Loss: 0.0013 Acc: 83.5952\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 83.9474\n",
      "validation Loss: 0.0013 Acc: 83.7262\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 83.9831\n",
      "validation Loss: 0.0013 Acc: 83.6310\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 84.0099\n",
      "validation Loss: 0.0013 Acc: 83.7381\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 83.9057\n",
      "validation Loss: 0.0013 Acc: 83.5833\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 84.0426\n",
      "validation Loss: 0.0013 Acc: 83.5476\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 83.9474\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 83.9682\n",
      "validation Loss: 0.0013 Acc: 83.7500\n",
      "Early stopped.\n",
      "Best val acc: 83.750000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0058 Acc: 79.2959\n",
      "validation Loss: 0.0015 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 82.9951\n",
      "validation Loss: 0.0014 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 83.3135\n",
      "validation Loss: 0.0014 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0013 Acc: 83.5486\n",
      "validation Loss: 0.0014 Acc: 82.7024\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.5307\n",
      "validation Loss: 0.0014 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0013 Acc: 83.5665\n",
      "validation Loss: 0.0014 Acc: 82.7976\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 83.6409\n",
      "validation Loss: 0.0014 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 83.7093\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 83.5516\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 83.7182\n",
      "validation Loss: 0.0014 Acc: 83.1548\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 83.6766\n",
      "validation Loss: 0.0014 Acc: 83.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 83.8819\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 83.5813\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.6290\n",
      "validation Loss: 0.0014 Acc: 83.1548\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 83.8819\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 84.0367\n",
      "validation Loss: 0.0013 Acc: 83.3333\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 84.2271\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 84.3700\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 84.3075\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 84.2807\n",
      "validation Loss: 0.0013 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 84.3105\n",
      "validation Loss: 0.0013 Acc: 83.5833\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 84.4890\n",
      "validation Loss: 0.0013 Acc: 83.3810\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 84.4146\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 84.5485\n",
      "validation Loss: 0.0013 Acc: 83.6190\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 84.4474\n",
      "validation Loss: 0.0013 Acc: 83.3690\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 84.3908\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 84.4890\n",
      "validation Loss: 0.0013 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 84.5575\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 84.4563\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 84.4355\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 84.7450\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 84.8729\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 84.8521\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 85.0842\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 84.9146\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 84.9116\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 85.1675\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 85.2658\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 85.2777\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 85.3848\n",
      "validation Loss: 0.0014 Acc: 83.5833\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 85.3818\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 85.3759\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 85.5901\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 85.6229\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 85.6318\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 45/99\n",
      "training Loss: 0.0012 Acc: 85.6169\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 46/99\n",
      "training Loss: 0.0012 Acc: 85.6973\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Early stopped.\n",
      "Best val acc: 83.750000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0069 Acc: 78.0638\n",
      "validation Loss: 0.0014 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 82.7391\n",
      "validation Loss: 0.0014 Acc: 82.0595\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 83.0129\n",
      "validation Loss: 0.0014 Acc: 82.8690\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 82.6826\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.0814\n",
      "validation Loss: 0.0014 Acc: 82.6548\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 82.7421\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 82.7570\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 83.0070\n",
      "validation Loss: 0.0014 Acc: 83.1190\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 82.7719\n",
      "validation Loss: 0.0014 Acc: 82.7381\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.2927\n",
      "validation Loss: 0.0014 Acc: 82.9405\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 83.2004\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Epoch 11/99\n",
      "training Loss: 0.0014 Acc: 83.1617\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 82.9861\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.3343\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 83.4950\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 83.5040\n",
      "validation Loss: 0.0014 Acc: 83.6310\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 83.3075\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 83.4177\n",
      "validation Loss: 0.0013 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 83.4028\n",
      "validation Loss: 0.0014 Acc: 83.6905\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 83.3760\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 83.4236\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 83.4980\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 83.4921\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 83.5069\n",
      "validation Loss: 0.0013 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 83.4147\n",
      "validation Loss: 0.0014 Acc: 83.7024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 83.4534\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 83.4057\n",
      "validation Loss: 0.0013 Acc: 83.4286\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 83.5843\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 83.5694\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 83.8016\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 83.8194\n",
      "validation Loss: 0.0013 Acc: 83.8333\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 83.7956\n",
      "validation Loss: 0.0013 Acc: 84.0000\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 83.8730\n",
      "validation Loss: 0.0013 Acc: 83.6310\n",
      "Epoch 33/99\n",
      "training Loss: 0.0013 Acc: 83.8016\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 83.8879\n",
      "validation Loss: 0.0014 Acc: 83.7143\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 83.9950\n",
      "validation Loss: 0.0014 Acc: 83.8333\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 83.8402\n",
      "validation Loss: 0.0013 Acc: 83.6786\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 84.0277\n",
      "validation Loss: 0.0013 Acc: 83.8810\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 83.9920\n",
      "validation Loss: 0.0013 Acc: 83.7857\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 84.1170\n",
      "validation Loss: 0.0013 Acc: 83.7976\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 84.1408\n",
      "validation Loss: 0.0013 Acc: 84.1548\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 84.0664\n",
      "validation Loss: 0.0013 Acc: 83.5357\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 84.1914\n",
      "validation Loss: 0.0013 Acc: 84.0952\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 84.1468\n",
      "validation Loss: 0.0013 Acc: 83.8810\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 83.8402\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 84.0277\n",
      "validation Loss: 0.0013 Acc: 83.8452\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 84.1021\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 84.1259\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 84.1408\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 84.3849\n",
      "validation Loss: 0.0013 Acc: 83.7143\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 84.5188\n",
      "validation Loss: 0.0013 Acc: 83.9405\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 84.4146\n",
      "validation Loss: 0.0013 Acc: 83.8690\n",
      "Epoch 52/99\n",
      "training Loss: 0.0013 Acc: 84.3521\n",
      "validation Loss: 0.0013 Acc: 83.7500\n",
      "Epoch 53/99\n",
      "training Loss: 0.0013 Acc: 84.4503\n",
      "validation Loss: 0.0014 Acc: 83.9048\n",
      "Epoch 54/99\n",
      "training Loss: 0.0013 Acc: 84.4414\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 55/99\n",
      "training Loss: 0.0013 Acc: 84.6765\n",
      "validation Loss: 0.0013 Acc: 83.8452\n",
      "Epoch 56/99\n",
      "training Loss: 0.0012 Acc: 84.6348\n",
      "validation Loss: 0.0013 Acc: 83.6429\n",
      "Epoch 57/99\n",
      "training Loss: 0.0012 Acc: 84.7836\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Epoch 58/99\n",
      "training Loss: 0.0012 Acc: 84.7152\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 59/99\n",
      "training Loss: 0.0012 Acc: 84.6944\n",
      "validation Loss: 0.0014 Acc: 83.7619\n",
      "Epoch 60/99\n",
      "training Loss: 0.0012 Acc: 84.7331\n",
      "validation Loss: 0.0014 Acc: 83.7500\n",
      "Early stopped.\n",
      "Best val acc: 84.154762\n",
      "----------\n",
      "Average best_acc across k-fold: 84.022168758\n",
      "New configuration: {'learning_rate': 1.0142821856708841e-05, 'initial_nodes': 771, 'dropout': 0.01, 'batch_size': 151, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 76.9375\n",
      "validation Loss: 0.0032 Acc: 79.8143\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0029 Acc: 80.6488\n",
      "validation Loss: 0.0027 Acc: 81.9329\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0026 Acc: 82.2798\n",
      "validation Loss: 0.0026 Acc: 82.5637\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 82.6280\n",
      "validation Loss: 0.0025 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 82.8720\n",
      "validation Loss: 0.0025 Acc: 83.2064\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 83.0327\n",
      "validation Loss: 0.0025 Acc: 83.4087\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 83.0774\n",
      "validation Loss: 0.0025 Acc: 83.4682\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 83.2143\n",
      "validation Loss: 0.0024 Acc: 83.5515\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0025 Acc: 83.2351\n",
      "validation Loss: 0.0024 Acc: 83.5634\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 83.2500\n",
      "validation Loss: 0.0024 Acc: 83.6110\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 83.2827\n",
      "validation Loss: 0.0024 Acc: 83.6825\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0024 Acc: 83.3631\n",
      "validation Loss: 0.0024 Acc: 83.6706\n",
      "Epoch 12/99\n",
      "training Loss: 0.0024 Acc: 83.3690\n",
      "validation Loss: 0.0024 Acc: 83.6348\n",
      "Epoch 13/99\n",
      "training Loss: 0.0024 Acc: 83.3571\n",
      "validation Loss: 0.0024 Acc: 83.8015\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0024 Acc: 83.3690\n",
      "validation Loss: 0.0024 Acc: 83.6348\n",
      "Epoch 15/99\n",
      "training Loss: 0.0024 Acc: 83.4226\n",
      "validation Loss: 0.0024 Acc: 83.9086\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0024 Acc: 83.4077\n",
      "validation Loss: 0.0024 Acc: 83.6587\n",
      "Epoch 17/99\n",
      "training Loss: 0.0024 Acc: 83.4792\n",
      "validation Loss: 0.0024 Acc: 83.9205\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0024 Acc: 83.5000\n",
      "validation Loss: 0.0024 Acc: 83.9800\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0024 Acc: 83.4018\n",
      "validation Loss: 0.0024 Acc: 83.9205\n",
      "Epoch 20/99\n",
      "training Loss: 0.0024 Acc: 83.4762\n",
      "validation Loss: 0.0024 Acc: 83.8610\n",
      "Epoch 21/99\n",
      "training Loss: 0.0024 Acc: 83.5476\n",
      "validation Loss: 0.0024 Acc: 83.9086\n",
      "Epoch 22/99\n",
      "training Loss: 0.0024 Acc: 83.5149\n",
      "validation Loss: 0.0024 Acc: 84.0157\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0024 Acc: 83.5685\n",
      "validation Loss: 0.0024 Acc: 84.1109\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0024 Acc: 83.5327\n",
      "validation Loss: 0.0024 Acc: 84.0752\n",
      "Epoch 25/99\n",
      "training Loss: 0.0024 Acc: 83.6518\n",
      "validation Loss: 0.0024 Acc: 83.9324\n",
      "Epoch 26/99\n",
      "training Loss: 0.0024 Acc: 83.6012\n",
      "validation Loss: 0.0024 Acc: 84.0276\n",
      "Epoch 27/99\n",
      "training Loss: 0.0024 Acc: 83.6220\n",
      "validation Loss: 0.0024 Acc: 84.0514\n",
      "Epoch 28/99\n",
      "training Loss: 0.0024 Acc: 83.6756\n",
      "validation Loss: 0.0024 Acc: 83.8729\n",
      "Epoch 29/99\n",
      "training Loss: 0.0024 Acc: 83.7470\n",
      "validation Loss: 0.0024 Acc: 84.1466\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0024 Acc: 83.7143\n",
      "validation Loss: 0.0024 Acc: 84.1823\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0024 Acc: 83.6875\n",
      "validation Loss: 0.0024 Acc: 84.2418\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0024 Acc: 83.7351\n",
      "validation Loss: 0.0024 Acc: 84.0633\n",
      "Epoch 33/99\n",
      "training Loss: 0.0024 Acc: 83.7083\n",
      "validation Loss: 0.0024 Acc: 84.0752\n",
      "Epoch 34/99\n",
      "training Loss: 0.0024 Acc: 83.7857\n",
      "validation Loss: 0.0024 Acc: 83.9919\n",
      "Epoch 35/99\n",
      "training Loss: 0.0024 Acc: 83.7619\n",
      "validation Loss: 0.0024 Acc: 84.0276\n",
      "Epoch 36/99\n",
      "training Loss: 0.0023 Acc: 83.7887\n",
      "validation Loss: 0.0024 Acc: 84.0633\n",
      "Epoch 37/99\n",
      "training Loss: 0.0023 Acc: 83.8274\n",
      "validation Loss: 0.0024 Acc: 84.1466\n",
      "Epoch 38/99\n",
      "training Loss: 0.0023 Acc: 83.8185\n",
      "validation Loss: 0.0024 Acc: 84.2299\n",
      "Epoch 39/99\n",
      "training Loss: 0.0023 Acc: 83.8185\n",
      "validation Loss: 0.0024 Acc: 84.2657\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0023 Acc: 83.8571\n",
      "validation Loss: 0.0024 Acc: 84.1347\n",
      "Epoch 41/99\n",
      "training Loss: 0.0023 Acc: 83.8720\n",
      "validation Loss: 0.0024 Acc: 84.1228\n",
      "Epoch 42/99\n",
      "training Loss: 0.0023 Acc: 83.8839\n",
      "validation Loss: 0.0024 Acc: 84.2180\n",
      "Epoch 43/99\n",
      "training Loss: 0.0023 Acc: 83.8988\n",
      "validation Loss: 0.0024 Acc: 84.1823\n",
      "Epoch 44/99\n",
      "training Loss: 0.0023 Acc: 83.9286\n",
      "validation Loss: 0.0024 Acc: 84.1585\n",
      "Epoch 45/99\n",
      "training Loss: 0.0023 Acc: 83.8839\n",
      "validation Loss: 0.0024 Acc: 84.1585\n",
      "Epoch 46/99\n",
      "training Loss: 0.0023 Acc: 83.9048\n",
      "validation Loss: 0.0024 Acc: 84.0871\n",
      "Epoch 47/99\n",
      "training Loss: 0.0023 Acc: 83.9286\n",
      "validation Loss: 0.0024 Acc: 84.2895\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0023 Acc: 83.9167\n",
      "validation Loss: 0.0024 Acc: 84.2418\n",
      "Epoch 49/99\n",
      "training Loss: 0.0023 Acc: 84.0208\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 50/99\n",
      "training Loss: 0.0023 Acc: 83.9970\n",
      "validation Loss: 0.0024 Acc: 84.3014\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0023 Acc: 84.0208\n",
      "validation Loss: 0.0024 Acc: 84.3014\n",
      "Epoch 52/99\n",
      "training Loss: 0.0023 Acc: 84.0298\n",
      "validation Loss: 0.0024 Acc: 84.2061\n",
      "Epoch 53/99\n",
      "training Loss: 0.0023 Acc: 84.0565\n",
      "validation Loss: 0.0024 Acc: 84.2299\n",
      "Epoch 54/99\n",
      "training Loss: 0.0023 Acc: 84.0149\n",
      "validation Loss: 0.0024 Acc: 84.0752\n",
      "Epoch 55/99\n",
      "training Loss: 0.0023 Acc: 84.1429\n",
      "validation Loss: 0.0024 Acc: 84.1228\n",
      "Epoch 56/99\n",
      "training Loss: 0.0023 Acc: 84.0417\n",
      "validation Loss: 0.0024 Acc: 84.2657\n",
      "Epoch 57/99\n",
      "training Loss: 0.0023 Acc: 84.0417\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 58/99\n",
      "training Loss: 0.0023 Acc: 84.1071\n",
      "validation Loss: 0.0024 Acc: 84.1704\n",
      "Epoch 59/99\n",
      "training Loss: 0.0023 Acc: 84.0923\n",
      "validation Loss: 0.0024 Acc: 84.4085\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0023 Acc: 84.0655\n",
      "validation Loss: 0.0024 Acc: 84.3371\n",
      "Epoch 61/99\n",
      "training Loss: 0.0023 Acc: 84.1012\n",
      "validation Loss: 0.0024 Acc: 84.2895\n",
      "Epoch 62/99\n",
      "training Loss: 0.0023 Acc: 84.1488\n",
      "validation Loss: 0.0024 Acc: 84.2061\n",
      "Epoch 63/99\n",
      "training Loss: 0.0023 Acc: 84.1131\n",
      "validation Loss: 0.0024 Acc: 84.2537\n",
      "Epoch 64/99\n",
      "training Loss: 0.0023 Acc: 84.2262\n",
      "validation Loss: 0.0024 Acc: 84.2895\n",
      "Epoch 65/99\n",
      "training Loss: 0.0023 Acc: 84.1815\n",
      "validation Loss: 0.0024 Acc: 84.2895\n",
      "Epoch 66/99\n",
      "training Loss: 0.0023 Acc: 84.2083\n",
      "validation Loss: 0.0024 Acc: 84.3490\n",
      "Epoch 67/99\n",
      "training Loss: 0.0023 Acc: 84.1667\n",
      "validation Loss: 0.0024 Acc: 84.3490\n",
      "Epoch 68/99\n",
      "training Loss: 0.0023 Acc: 84.1131\n",
      "validation Loss: 0.0024 Acc: 84.3133\n",
      "Epoch 69/99\n",
      "training Loss: 0.0023 Acc: 84.1994\n",
      "validation Loss: 0.0024 Acc: 84.2657\n",
      "Epoch 70/99\n",
      "training Loss: 0.0023 Acc: 84.2768\n",
      "validation Loss: 0.0024 Acc: 84.3133\n",
      "Epoch 71/99\n",
      "training Loss: 0.0023 Acc: 84.2083\n",
      "validation Loss: 0.0024 Acc: 84.4204\n",
      "Saving..\n",
      "Epoch 72/99\n",
      "training Loss: 0.0023 Acc: 84.2173\n",
      "validation Loss: 0.0024 Acc: 84.2061\n",
      "Epoch 73/99\n",
      "training Loss: 0.0023 Acc: 84.3006\n",
      "validation Loss: 0.0024 Acc: 84.2418\n",
      "Epoch 74/99\n",
      "training Loss: 0.0023 Acc: 84.2143\n",
      "validation Loss: 0.0024 Acc: 84.3847\n",
      "Epoch 75/99\n",
      "training Loss: 0.0023 Acc: 84.2262\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 76/99\n",
      "training Loss: 0.0023 Acc: 84.2619\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 77/99\n",
      "training Loss: 0.0023 Acc: 84.2738\n",
      "validation Loss: 0.0024 Acc: 84.2657\n",
      "Epoch 78/99\n",
      "training Loss: 0.0023 Acc: 84.2411\n",
      "validation Loss: 0.0024 Acc: 84.2537\n",
      "Epoch 79/99\n",
      "training Loss: 0.0023 Acc: 84.2857\n",
      "validation Loss: 0.0024 Acc: 84.2180\n",
      "Epoch 80/99\n",
      "training Loss: 0.0023 Acc: 84.2589\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 81/99\n",
      "training Loss: 0.0023 Acc: 84.3214\n",
      "validation Loss: 0.0024 Acc: 84.3252\n",
      "Epoch 82/99\n",
      "training Loss: 0.0023 Acc: 84.2768\n",
      "validation Loss: 0.0024 Acc: 84.3371\n",
      "Epoch 83/99\n",
      "training Loss: 0.0023 Acc: 84.2946\n",
      "validation Loss: 0.0024 Acc: 84.3490\n",
      "Epoch 84/99\n",
      "training Loss: 0.0023 Acc: 84.2708\n",
      "validation Loss: 0.0024 Acc: 84.2418\n",
      "Epoch 85/99\n",
      "training Loss: 0.0023 Acc: 84.3601\n",
      "validation Loss: 0.0024 Acc: 84.2657\n",
      "Epoch 86/99\n",
      "training Loss: 0.0023 Acc: 84.2619\n",
      "validation Loss: 0.0024 Acc: 84.3490\n",
      "Epoch 87/99\n",
      "training Loss: 0.0023 Acc: 84.3125\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 88/99\n",
      "training Loss: 0.0023 Acc: 84.4048\n",
      "validation Loss: 0.0024 Acc: 84.2776\n",
      "Epoch 89/99\n",
      "training Loss: 0.0023 Acc: 84.3185\n",
      "validation Loss: 0.0024 Acc: 84.3133\n",
      "Epoch 90/99\n",
      "training Loss: 0.0023 Acc: 84.4167\n",
      "validation Loss: 0.0024 Acc: 84.3014\n",
      "Epoch 91/99\n",
      "training Loss: 0.0023 Acc: 84.2827\n",
      "validation Loss: 0.0024 Acc: 84.3252\n",
      "Early stopped.\n",
      "Best val acc: 84.420376\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0040 Acc: 74.3557\n",
      "validation Loss: 0.0034 Acc: 79.0238\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 80.4416\n",
      "validation Loss: 0.0028 Acc: 81.4286\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0027 Acc: 82.2124\n",
      "validation Loss: 0.0027 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 82.6439\n",
      "validation Loss: 0.0026 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 82.9147\n",
      "validation Loss: 0.0026 Acc: 82.4405\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 83.0843\n",
      "validation Loss: 0.0025 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 83.1945\n",
      "validation Loss: 0.0025 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 83.2897\n",
      "validation Loss: 0.0025 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0024 Acc: 83.3254\n",
      "validation Loss: 0.0025 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 83.3462\n",
      "validation Loss: 0.0025 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 83.3254\n",
      "validation Loss: 0.0025 Acc: 82.9048\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0024 Acc: 83.4891\n",
      "validation Loss: 0.0025 Acc: 82.8690\n",
      "Epoch 12/99\n",
      "training Loss: 0.0024 Acc: 83.5010\n",
      "validation Loss: 0.0025 Acc: 82.8452\n",
      "Epoch 13/99\n",
      "training Loss: 0.0024 Acc: 83.5754\n",
      "validation Loss: 0.0025 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0024 Acc: 83.4474\n",
      "validation Loss: 0.0025 Acc: 82.9167\n",
      "Epoch 15/99\n",
      "training Loss: 0.0024 Acc: 83.5456\n",
      "validation Loss: 0.0025 Acc: 82.9762\n",
      "Epoch 16/99\n",
      "training Loss: 0.0024 Acc: 83.5605\n",
      "validation Loss: 0.0025 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0024 Acc: 83.5932\n",
      "validation Loss: 0.0025 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0024 Acc: 83.6587\n",
      "validation Loss: 0.0025 Acc: 83.0714\n",
      "Epoch 19/99\n",
      "training Loss: 0.0024 Acc: 83.6409\n",
      "validation Loss: 0.0025 Acc: 82.9881\n",
      "Epoch 20/99\n",
      "training Loss: 0.0024 Acc: 83.6498\n",
      "validation Loss: 0.0025 Acc: 83.0714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0024 Acc: 83.6528\n",
      "validation Loss: 0.0025 Acc: 83.0476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0024 Acc: 83.6468\n",
      "validation Loss: 0.0025 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0024 Acc: 83.7480\n",
      "validation Loss: 0.0025 Acc: 83.0833\n",
      "Epoch 24/99\n",
      "training Loss: 0.0024 Acc: 83.7688\n",
      "validation Loss: 0.0025 Acc: 83.1071\n",
      "Epoch 25/99\n",
      "training Loss: 0.0024 Acc: 83.7361\n",
      "validation Loss: 0.0025 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0024 Acc: 83.7867\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0024 Acc: 83.7629\n",
      "validation Loss: 0.0025 Acc: 83.0595\n",
      "Epoch 28/99\n",
      "training Loss: 0.0024 Acc: 83.7599\n",
      "validation Loss: 0.0025 Acc: 83.0357\n",
      "Epoch 29/99\n",
      "training Loss: 0.0024 Acc: 83.8522\n",
      "validation Loss: 0.0025 Acc: 83.0595\n",
      "Epoch 30/99\n",
      "training Loss: 0.0024 Acc: 83.8343\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0023 Acc: 83.8998\n",
      "validation Loss: 0.0024 Acc: 83.1429\n",
      "Epoch 32/99\n",
      "training Loss: 0.0023 Acc: 83.8462\n",
      "validation Loss: 0.0024 Acc: 83.1310\n",
      "Epoch 33/99\n",
      "training Loss: 0.0023 Acc: 83.8522\n",
      "validation Loss: 0.0024 Acc: 83.1429\n",
      "Epoch 34/99\n",
      "training Loss: 0.0023 Acc: 83.9801\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Epoch 35/99\n",
      "training Loss: 0.0023 Acc: 83.9771\n",
      "validation Loss: 0.0024 Acc: 83.0714\n",
      "Epoch 36/99\n",
      "training Loss: 0.0023 Acc: 83.9563\n",
      "validation Loss: 0.0024 Acc: 83.1429\n",
      "Epoch 37/99\n",
      "training Loss: 0.0023 Acc: 83.9027\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Epoch 38/99\n",
      "training Loss: 0.0023 Acc: 83.9801\n",
      "validation Loss: 0.0024 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0023 Acc: 83.9623\n",
      "validation Loss: 0.0024 Acc: 83.2738\n",
      "Epoch 40/99\n",
      "training Loss: 0.0023 Acc: 84.0099\n",
      "validation Loss: 0.0024 Acc: 83.2857\n",
      "Epoch 41/99\n",
      "training Loss: 0.0023 Acc: 84.0992\n",
      "validation Loss: 0.0024 Acc: 83.2262\n",
      "Epoch 42/99\n",
      "training Loss: 0.0023 Acc: 84.0545\n",
      "validation Loss: 0.0024 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0023 Acc: 83.9771\n",
      "validation Loss: 0.0024 Acc: 83.3095\n",
      "Epoch 44/99\n",
      "training Loss: 0.0023 Acc: 84.1021\n",
      "validation Loss: 0.0024 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0023 Acc: 84.0634\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Epoch 46/99\n",
      "training Loss: 0.0023 Acc: 84.0724\n",
      "validation Loss: 0.0024 Acc: 83.1905\n",
      "Epoch 47/99\n",
      "training Loss: 0.0023 Acc: 84.1319\n",
      "validation Loss: 0.0024 Acc: 83.3452\n",
      "Epoch 48/99\n",
      "training Loss: 0.0023 Acc: 84.0754\n",
      "validation Loss: 0.0024 Acc: 83.2976\n",
      "Epoch 49/99\n",
      "training Loss: 0.0023 Acc: 84.1527\n",
      "validation Loss: 0.0024 Acc: 83.1786\n",
      "Epoch 50/99\n",
      "training Loss: 0.0023 Acc: 84.1259\n",
      "validation Loss: 0.0024 Acc: 83.2024\n",
      "Epoch 51/99\n",
      "training Loss: 0.0023 Acc: 84.1468\n",
      "validation Loss: 0.0024 Acc: 83.4524\n",
      "Epoch 52/99\n",
      "training Loss: 0.0023 Acc: 84.1974\n",
      "validation Loss: 0.0024 Acc: 83.3095\n",
      "Epoch 53/99\n",
      "training Loss: 0.0023 Acc: 84.0843\n",
      "validation Loss: 0.0024 Acc: 83.3690\n",
      "Epoch 54/99\n",
      "training Loss: 0.0023 Acc: 84.2182\n",
      "validation Loss: 0.0024 Acc: 83.3214\n",
      "Epoch 55/99\n",
      "training Loss: 0.0023 Acc: 84.2331\n",
      "validation Loss: 0.0024 Acc: 83.4167\n",
      "Epoch 56/99\n",
      "training Loss: 0.0023 Acc: 84.2212\n",
      "validation Loss: 0.0024 Acc: 83.3690\n",
      "Epoch 57/99\n",
      "training Loss: 0.0023 Acc: 84.2688\n",
      "validation Loss: 0.0024 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0023 Acc: 84.2777\n",
      "validation Loss: 0.0024 Acc: 83.1905\n",
      "Epoch 59/99\n",
      "training Loss: 0.0023 Acc: 84.2182\n",
      "validation Loss: 0.0024 Acc: 83.4167\n",
      "Epoch 60/99\n",
      "training Loss: 0.0023 Acc: 84.3164\n",
      "validation Loss: 0.0024 Acc: 83.3571\n",
      "Epoch 61/99\n",
      "training Loss: 0.0023 Acc: 84.2926\n",
      "validation Loss: 0.0024 Acc: 83.1905\n",
      "Epoch 62/99\n",
      "training Loss: 0.0023 Acc: 84.2688\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 63/99\n",
      "training Loss: 0.0023 Acc: 84.3640\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 64/99\n",
      "training Loss: 0.0023 Acc: 84.3045\n",
      "validation Loss: 0.0024 Acc: 83.3214\n",
      "Epoch 65/99\n",
      "training Loss: 0.0023 Acc: 84.3313\n",
      "validation Loss: 0.0024 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0023 Acc: 84.3640\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 67/99\n",
      "training Loss: 0.0023 Acc: 84.3402\n",
      "validation Loss: 0.0024 Acc: 83.4643\n",
      "Epoch 68/99\n",
      "training Loss: 0.0023 Acc: 84.3313\n",
      "validation Loss: 0.0024 Acc: 83.3929\n",
      "Epoch 69/99\n",
      "training Loss: 0.0023 Acc: 84.3283\n",
      "validation Loss: 0.0024 Acc: 83.4643\n",
      "Epoch 70/99\n",
      "training Loss: 0.0023 Acc: 84.3581\n",
      "validation Loss: 0.0024 Acc: 83.3452\n",
      "Epoch 71/99\n",
      "training Loss: 0.0023 Acc: 84.3700\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 72/99\n",
      "training Loss: 0.0023 Acc: 84.3551\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 73/99\n",
      "training Loss: 0.0023 Acc: 84.4771\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 74/99\n",
      "training Loss: 0.0023 Acc: 84.3491\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 75/99\n",
      "training Loss: 0.0023 Acc: 84.4444\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 76/99\n",
      "training Loss: 0.0023 Acc: 84.4325\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 77/99\n",
      "training Loss: 0.0023 Acc: 84.3283\n",
      "validation Loss: 0.0024 Acc: 83.4762\n",
      "Epoch 78/99\n",
      "training Loss: 0.0023 Acc: 84.4176\n",
      "validation Loss: 0.0024 Acc: 83.4524\n",
      "Epoch 79/99\n",
      "training Loss: 0.0023 Acc: 84.3938\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 80/99\n",
      "training Loss: 0.0023 Acc: 84.4444\n",
      "validation Loss: 0.0024 Acc: 83.4524\n",
      "Epoch 81/99\n",
      "training Loss: 0.0023 Acc: 84.3640\n",
      "validation Loss: 0.0024 Acc: 83.3810\n",
      "Epoch 82/99\n",
      "training Loss: 0.0023 Acc: 84.4474\n",
      "validation Loss: 0.0024 Acc: 83.4881\n",
      "Epoch 83/99\n",
      "training Loss: 0.0023 Acc: 84.4444\n",
      "validation Loss: 0.0024 Acc: 83.3810\n",
      "Epoch 84/99\n",
      "training Loss: 0.0023 Acc: 84.4057\n",
      "validation Loss: 0.0024 Acc: 83.3810\n",
      "Epoch 85/99\n",
      "training Loss: 0.0023 Acc: 84.4235\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Early stopped.\n",
      "Best val acc: 83.488095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 77.1710\n",
      "validation Loss: 0.0033 Acc: 79.6905\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0029 Acc: 80.7244\n",
      "validation Loss: 0.0027 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0027 Acc: 82.1409\n",
      "validation Loss: 0.0026 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 82.5516\n",
      "validation Loss: 0.0025 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 82.6677\n",
      "validation Loss: 0.0025 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 82.9236\n",
      "validation Loss: 0.0025 Acc: 83.2976\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 83.0427\n",
      "validation Loss: 0.0025 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 83.1468\n",
      "validation Loss: 0.0024 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0025 Acc: 83.2153\n",
      "validation Loss: 0.0024 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 83.2331\n",
      "validation Loss: 0.0024 Acc: 83.6667\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 83.2748\n",
      "validation Loss: 0.0024 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0024 Acc: 83.3343\n",
      "validation Loss: 0.0024 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0024 Acc: 83.3373\n",
      "validation Loss: 0.0024 Acc: 83.7143\n",
      "Epoch 13/99\n",
      "training Loss: 0.0024 Acc: 83.4712\n",
      "validation Loss: 0.0024 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0024 Acc: 83.3760\n",
      "validation Loss: 0.0024 Acc: 83.7143\n",
      "Epoch 15/99\n",
      "training Loss: 0.0024 Acc: 83.4593\n",
      "validation Loss: 0.0024 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0024 Acc: 83.4861\n",
      "validation Loss: 0.0024 Acc: 83.6786\n",
      "Epoch 17/99\n",
      "training Loss: 0.0024 Acc: 83.4921\n",
      "validation Loss: 0.0024 Acc: 83.6310\n",
      "Epoch 18/99\n",
      "training Loss: 0.0024 Acc: 83.5010\n",
      "validation Loss: 0.0024 Acc: 83.6667\n",
      "Epoch 19/99\n",
      "training Loss: 0.0024 Acc: 83.5784\n",
      "validation Loss: 0.0024 Acc: 83.6786\n",
      "Epoch 20/99\n",
      "training Loss: 0.0024 Acc: 83.5843\n",
      "validation Loss: 0.0024 Acc: 83.7381\n",
      "Epoch 21/99\n",
      "training Loss: 0.0024 Acc: 83.5754\n",
      "validation Loss: 0.0024 Acc: 83.7738\n",
      "Epoch 22/99\n",
      "training Loss: 0.0024 Acc: 83.5516\n",
      "validation Loss: 0.0024 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0024 Acc: 83.6944\n",
      "validation Loss: 0.0024 Acc: 83.7500\n",
      "Epoch 24/99\n",
      "training Loss: 0.0024 Acc: 83.6557\n",
      "validation Loss: 0.0024 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0024 Acc: 83.6885\n",
      "validation Loss: 0.0024 Acc: 83.8452\n",
      "Epoch 26/99\n",
      "training Loss: 0.0024 Acc: 83.7480\n",
      "validation Loss: 0.0024 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0024 Acc: 83.7510\n",
      "validation Loss: 0.0024 Acc: 83.8333\n",
      "Epoch 28/99\n",
      "training Loss: 0.0024 Acc: 83.7867\n",
      "validation Loss: 0.0024 Acc: 83.8810\n",
      "Epoch 29/99\n",
      "training Loss: 0.0024 Acc: 83.7212\n",
      "validation Loss: 0.0024 Acc: 83.9524\n",
      "Epoch 30/99\n",
      "training Loss: 0.0024 Acc: 83.6795\n",
      "validation Loss: 0.0024 Acc: 83.8929\n",
      "Epoch 31/99\n",
      "training Loss: 0.0024 Acc: 83.7778\n",
      "validation Loss: 0.0024 Acc: 83.9167\n",
      "Epoch 32/99\n",
      "training Loss: 0.0024 Acc: 83.7093\n",
      "validation Loss: 0.0024 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0024 Acc: 83.8164\n",
      "validation Loss: 0.0024 Acc: 83.8929\n",
      "Epoch 34/99\n",
      "training Loss: 0.0023 Acc: 83.8016\n",
      "validation Loss: 0.0024 Acc: 83.9762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0023 Acc: 83.8254\n",
      "validation Loss: 0.0024 Acc: 83.8690\n",
      "Epoch 36/99\n",
      "training Loss: 0.0023 Acc: 83.7718\n",
      "validation Loss: 0.0024 Acc: 84.0238\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0023 Acc: 83.8402\n",
      "validation Loss: 0.0024 Acc: 83.9524\n",
      "Epoch 38/99\n",
      "training Loss: 0.0023 Acc: 83.8016\n",
      "validation Loss: 0.0024 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0023 Acc: 83.7420\n",
      "validation Loss: 0.0024 Acc: 83.9048\n",
      "Epoch 40/99\n",
      "training Loss: 0.0023 Acc: 83.7391\n",
      "validation Loss: 0.0024 Acc: 84.0119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0023 Acc: 83.7688\n",
      "validation Loss: 0.0024 Acc: 83.8810\n",
      "Epoch 42/99\n",
      "training Loss: 0.0023 Acc: 83.7748\n",
      "validation Loss: 0.0024 Acc: 84.0000\n",
      "Epoch 43/99\n",
      "training Loss: 0.0023 Acc: 83.8194\n",
      "validation Loss: 0.0024 Acc: 83.9524\n",
      "Epoch 44/99\n",
      "training Loss: 0.0023 Acc: 83.9087\n",
      "validation Loss: 0.0024 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0023 Acc: 83.8313\n",
      "validation Loss: 0.0024 Acc: 84.0476\n",
      "Epoch 46/99\n",
      "training Loss: 0.0023 Acc: 83.8670\n",
      "validation Loss: 0.0024 Acc: 84.0357\n",
      "Epoch 47/99\n",
      "training Loss: 0.0023 Acc: 83.7986\n",
      "validation Loss: 0.0024 Acc: 84.0000\n",
      "Epoch 48/99\n",
      "training Loss: 0.0023 Acc: 83.8224\n",
      "validation Loss: 0.0024 Acc: 84.1071\n",
      "Epoch 49/99\n",
      "training Loss: 0.0023 Acc: 83.9117\n",
      "validation Loss: 0.0024 Acc: 84.0476\n",
      "Epoch 50/99\n",
      "training Loss: 0.0023 Acc: 83.8402\n",
      "validation Loss: 0.0024 Acc: 84.0119\n",
      "Epoch 51/99\n",
      "training Loss: 0.0023 Acc: 83.8135\n",
      "validation Loss: 0.0024 Acc: 84.0952\n",
      "Epoch 52/99\n",
      "training Loss: 0.0023 Acc: 83.8670\n",
      "validation Loss: 0.0024 Acc: 84.0357\n",
      "Epoch 53/99\n",
      "training Loss: 0.0023 Acc: 83.9236\n",
      "validation Loss: 0.0024 Acc: 84.0595\n",
      "Epoch 54/99\n",
      "training Loss: 0.0023 Acc: 83.8402\n",
      "validation Loss: 0.0024 Acc: 84.0476\n",
      "Epoch 55/99\n",
      "training Loss: 0.0023 Acc: 83.8581\n",
      "validation Loss: 0.0024 Acc: 84.0476\n",
      "Epoch 56/99\n",
      "training Loss: 0.0023 Acc: 83.8998\n",
      "validation Loss: 0.0024 Acc: 84.0476\n",
      "Epoch 57/99\n",
      "training Loss: 0.0023 Acc: 83.8879\n",
      "validation Loss: 0.0024 Acc: 84.0357\n",
      "Epoch 58/99\n",
      "training Loss: 0.0023 Acc: 83.9027\n",
      "validation Loss: 0.0024 Acc: 84.0238\n",
      "Epoch 59/99\n",
      "training Loss: 0.0023 Acc: 83.8373\n",
      "validation Loss: 0.0024 Acc: 84.0119\n",
      "Epoch 60/99\n",
      "training Loss: 0.0023 Acc: 83.8432\n",
      "validation Loss: 0.0024 Acc: 84.0595\n",
      "Epoch 61/99\n",
      "training Loss: 0.0023 Acc: 83.8819\n",
      "validation Loss: 0.0024 Acc: 84.0238\n",
      "Epoch 62/99\n",
      "training Loss: 0.0023 Acc: 83.8581\n",
      "validation Loss: 0.0024 Acc: 84.0238\n",
      "Epoch 63/99\n",
      "training Loss: 0.0023 Acc: 83.8402\n",
      "validation Loss: 0.0024 Acc: 84.0238\n",
      "Epoch 64/99\n",
      "training Loss: 0.0023 Acc: 83.8968\n",
      "validation Loss: 0.0024 Acc: 84.0595\n",
      "Early stopped.\n",
      "Best val acc: 84.107143\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 73.5551\n",
      "validation Loss: 0.0032 Acc: 79.5714\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0029 Acc: 81.1291\n",
      "validation Loss: 0.0028 Acc: 81.6548\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0026 Acc: 82.2719\n",
      "validation Loss: 0.0026 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 82.7272\n",
      "validation Loss: 0.0026 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 82.9653\n",
      "validation Loss: 0.0026 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 83.0992\n",
      "validation Loss: 0.0025 Acc: 82.6667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 83.1825\n",
      "validation Loss: 0.0025 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 83.3075\n",
      "validation Loss: 0.0025 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0025 Acc: 83.3165\n",
      "validation Loss: 0.0025 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 83.4087\n",
      "validation Loss: 0.0025 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 83.4236\n",
      "validation Loss: 0.0025 Acc: 82.9048\n",
      "Epoch 11/99\n",
      "training Loss: 0.0024 Acc: 83.4206\n",
      "validation Loss: 0.0025 Acc: 82.8929\n",
      "Epoch 12/99\n",
      "training Loss: 0.0024 Acc: 83.5367\n",
      "validation Loss: 0.0025 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0024 Acc: 83.4772\n",
      "validation Loss: 0.0025 Acc: 82.9762\n",
      "Epoch 14/99\n",
      "training Loss: 0.0024 Acc: 83.5962\n",
      "validation Loss: 0.0024 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0024 Acc: 83.5843\n",
      "validation Loss: 0.0024 Acc: 82.9881\n",
      "Epoch 16/99\n",
      "training Loss: 0.0024 Acc: 83.6647\n",
      "validation Loss: 0.0024 Acc: 83.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0024 Acc: 83.5843\n",
      "validation Loss: 0.0024 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0024 Acc: 83.6260\n",
      "validation Loss: 0.0024 Acc: 83.0357\n",
      "Epoch 19/99\n",
      "training Loss: 0.0024 Acc: 83.7153\n",
      "validation Loss: 0.0025 Acc: 83.0119\n",
      "Epoch 20/99\n",
      "training Loss: 0.0024 Acc: 83.7153\n",
      "validation Loss: 0.0024 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0024 Acc: 83.6855\n",
      "validation Loss: 0.0024 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0024 Acc: 83.7272\n",
      "validation Loss: 0.0024 Acc: 83.0833\n",
      "Epoch 23/99\n",
      "training Loss: 0.0024 Acc: 83.8194\n",
      "validation Loss: 0.0024 Acc: 83.1310\n",
      "Epoch 24/99\n",
      "training Loss: 0.0024 Acc: 83.8432\n",
      "validation Loss: 0.0024 Acc: 83.0833\n",
      "Epoch 25/99\n",
      "training Loss: 0.0024 Acc: 83.7629\n",
      "validation Loss: 0.0024 Acc: 82.9643\n",
      "Epoch 26/99\n",
      "training Loss: 0.0024 Acc: 83.8641\n",
      "validation Loss: 0.0024 Acc: 83.0714\n",
      "Epoch 27/99\n",
      "training Loss: 0.0024 Acc: 83.8254\n",
      "validation Loss: 0.0024 Acc: 83.0714\n",
      "Epoch 28/99\n",
      "training Loss: 0.0024 Acc: 83.8432\n",
      "validation Loss: 0.0024 Acc: 83.0357\n",
      "Epoch 29/99\n",
      "training Loss: 0.0024 Acc: 83.8849\n",
      "validation Loss: 0.0024 Acc: 83.0238\n",
      "Epoch 30/99\n",
      "training Loss: 0.0024 Acc: 83.9504\n",
      "validation Loss: 0.0024 Acc: 83.0952\n",
      "Epoch 31/99\n",
      "training Loss: 0.0024 Acc: 83.8938\n",
      "validation Loss: 0.0024 Acc: 83.0595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0023 Acc: 83.9266\n",
      "validation Loss: 0.0024 Acc: 83.0952\n",
      "Epoch 33/99\n",
      "training Loss: 0.0023 Acc: 83.9504\n",
      "validation Loss: 0.0024 Acc: 83.0476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0023 Acc: 83.8908\n",
      "validation Loss: 0.0024 Acc: 83.0952\n",
      "Epoch 35/99\n",
      "training Loss: 0.0023 Acc: 83.9295\n",
      "validation Loss: 0.0024 Acc: 83.1310\n",
      "Epoch 36/99\n",
      "training Loss: 0.0023 Acc: 83.9980\n",
      "validation Loss: 0.0024 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0023 Acc: 84.0069\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0023 Acc: 83.9593\n",
      "validation Loss: 0.0024 Acc: 83.1429\n",
      "Epoch 39/99\n",
      "training Loss: 0.0023 Acc: 83.9771\n",
      "validation Loss: 0.0024 Acc: 83.1429\n",
      "Epoch 40/99\n",
      "training Loss: 0.0023 Acc: 84.0575\n",
      "validation Loss: 0.0024 Acc: 83.1786\n",
      "Epoch 41/99\n",
      "training Loss: 0.0023 Acc: 84.0129\n",
      "validation Loss: 0.0024 Acc: 83.1905\n",
      "Epoch 42/99\n",
      "training Loss: 0.0023 Acc: 84.0605\n",
      "validation Loss: 0.0024 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0023 Acc: 84.0724\n",
      "validation Loss: 0.0024 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0023 Acc: 84.1140\n",
      "validation Loss: 0.0024 Acc: 83.2143\n",
      "Epoch 45/99\n",
      "training Loss: 0.0023 Acc: 84.2003\n",
      "validation Loss: 0.0024 Acc: 83.2262\n",
      "Epoch 46/99\n",
      "training Loss: 0.0023 Acc: 84.0277\n",
      "validation Loss: 0.0024 Acc: 83.2381\n",
      "Epoch 47/99\n",
      "training Loss: 0.0023 Acc: 84.1557\n",
      "validation Loss: 0.0024 Acc: 83.2381\n",
      "Epoch 48/99\n",
      "training Loss: 0.0023 Acc: 84.0873\n",
      "validation Loss: 0.0024 Acc: 83.2381\n",
      "Epoch 49/99\n",
      "training Loss: 0.0023 Acc: 84.0992\n",
      "validation Loss: 0.0024 Acc: 83.2381\n",
      "Epoch 50/99\n",
      "training Loss: 0.0023 Acc: 84.1021\n",
      "validation Loss: 0.0024 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0023 Acc: 84.1259\n",
      "validation Loss: 0.0024 Acc: 83.2381\n",
      "Epoch 52/99\n",
      "training Loss: 0.0023 Acc: 84.1349\n",
      "validation Loss: 0.0024 Acc: 83.3095\n",
      "Epoch 53/99\n",
      "training Loss: 0.0023 Acc: 84.1617\n",
      "validation Loss: 0.0024 Acc: 83.2262\n",
      "Epoch 54/99\n",
      "training Loss: 0.0023 Acc: 84.1498\n",
      "validation Loss: 0.0024 Acc: 83.3214\n",
      "Epoch 55/99\n",
      "training Loss: 0.0023 Acc: 84.1319\n",
      "validation Loss: 0.0024 Acc: 83.2619\n",
      "Epoch 56/99\n",
      "training Loss: 0.0023 Acc: 84.1914\n",
      "validation Loss: 0.0024 Acc: 83.3452\n",
      "Epoch 57/99\n",
      "training Loss: 0.0023 Acc: 84.0962\n",
      "validation Loss: 0.0024 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0023 Acc: 84.2420\n",
      "validation Loss: 0.0024 Acc: 83.3571\n",
      "Epoch 59/99\n",
      "training Loss: 0.0023 Acc: 84.2361\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0023 Acc: 84.2271\n",
      "validation Loss: 0.0024 Acc: 83.2976\n",
      "Epoch 61/99\n",
      "training Loss: 0.0023 Acc: 84.2896\n",
      "validation Loss: 0.0024 Acc: 83.3095\n",
      "Epoch 62/99\n",
      "training Loss: 0.0023 Acc: 84.2361\n",
      "validation Loss: 0.0024 Acc: 83.3929\n",
      "Epoch 63/99\n",
      "training Loss: 0.0023 Acc: 84.2986\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0023 Acc: 84.3402\n",
      "validation Loss: 0.0024 Acc: 83.3810\n",
      "Epoch 65/99\n",
      "training Loss: 0.0023 Acc: 84.2747\n",
      "validation Loss: 0.0024 Acc: 83.2857\n",
      "Epoch 66/99\n",
      "training Loss: 0.0023 Acc: 84.3015\n",
      "validation Loss: 0.0024 Acc: 83.3690\n",
      "Epoch 67/99\n",
      "training Loss: 0.0023 Acc: 84.3134\n",
      "validation Loss: 0.0024 Acc: 83.3690\n",
      "Epoch 68/99\n",
      "training Loss: 0.0023 Acc: 84.3730\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 69/99\n",
      "training Loss: 0.0023 Acc: 84.2807\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 70/99\n",
      "training Loss: 0.0023 Acc: 84.3968\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 71/99\n",
      "training Loss: 0.0023 Acc: 84.3253\n",
      "validation Loss: 0.0024 Acc: 83.4167\n",
      "Epoch 72/99\n",
      "training Loss: 0.0023 Acc: 84.3372\n",
      "validation Loss: 0.0024 Acc: 83.2976\n",
      "Epoch 73/99\n",
      "training Loss: 0.0023 Acc: 84.3253\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 74/99\n",
      "training Loss: 0.0023 Acc: 84.3283\n",
      "validation Loss: 0.0024 Acc: 83.3333\n",
      "Epoch 75/99\n",
      "training Loss: 0.0023 Acc: 84.3164\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 76/99\n",
      "training Loss: 0.0023 Acc: 84.3551\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 77/99\n",
      "training Loss: 0.0023 Acc: 84.3224\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 78/99\n",
      "training Loss: 0.0023 Acc: 84.3819\n",
      "validation Loss: 0.0024 Acc: 83.3929\n",
      "Epoch 79/99\n",
      "training Loss: 0.0023 Acc: 84.4027\n",
      "validation Loss: 0.0024 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 80/99\n",
      "training Loss: 0.0023 Acc: 84.3521\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 81/99\n",
      "training Loss: 0.0023 Acc: 84.3640\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 82/99\n",
      "training Loss: 0.0023 Acc: 84.3789\n",
      "validation Loss: 0.0024 Acc: 83.4167\n",
      "Epoch 83/99\n",
      "training Loss: 0.0023 Acc: 84.3700\n",
      "validation Loss: 0.0024 Acc: 83.5238\n",
      "Epoch 84/99\n",
      "training Loss: 0.0023 Acc: 84.4235\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 85/99\n",
      "training Loss: 0.0023 Acc: 84.3759\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 86/99\n",
      "training Loss: 0.0023 Acc: 84.3372\n",
      "validation Loss: 0.0024 Acc: 83.3810\n",
      "Epoch 87/99\n",
      "training Loss: 0.0023 Acc: 84.3670\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 88/99\n",
      "training Loss: 0.0023 Acc: 84.3700\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 89/99\n",
      "training Loss: 0.0023 Acc: 84.3968\n",
      "validation Loss: 0.0024 Acc: 83.4762\n",
      "Epoch 90/99\n",
      "training Loss: 0.0023 Acc: 84.4116\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 91/99\n",
      "training Loss: 0.0023 Acc: 84.4355\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 92/99\n",
      "training Loss: 0.0023 Acc: 84.4444\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 93/99\n",
      "training Loss: 0.0023 Acc: 84.4087\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 94/99\n",
      "training Loss: 0.0023 Acc: 84.3610\n",
      "validation Loss: 0.0024 Acc: 83.4167\n",
      "Epoch 95/99\n",
      "training Loss: 0.0023 Acc: 84.3700\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 96/99\n",
      "training Loss: 0.0023 Acc: 84.4146\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 97/99\n",
      "training Loss: 0.0023 Acc: 84.4235\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 98/99\n",
      "training Loss: 0.0023 Acc: 84.3938\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Epoch 99/99\n",
      "training Loss: 0.0023 Acc: 84.4146\n",
      "validation Loss: 0.0024 Acc: 83.4286\n",
      "Best val acc: 83.523810\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 78.0817\n",
      "validation Loss: 0.0032 Acc: 79.3452\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0029 Acc: 80.6946\n",
      "validation Loss: 0.0028 Acc: 81.4167\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0027 Acc: 82.0487\n",
      "validation Loss: 0.0026 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 82.5754\n",
      "validation Loss: 0.0026 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 82.8849\n",
      "validation Loss: 0.0025 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 82.9832\n",
      "validation Loss: 0.0025 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 83.1825\n",
      "validation Loss: 0.0025 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 83.3046\n",
      "validation Loss: 0.0025 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0025 Acc: 83.3373\n",
      "validation Loss: 0.0025 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 83.2689\n",
      "validation Loss: 0.0025 Acc: 83.0595\n",
      "Epoch 10/99\n",
      "training Loss: 0.0024 Acc: 83.4355\n",
      "validation Loss: 0.0024 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0024 Acc: 83.4177\n",
      "validation Loss: 0.0024 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0024 Acc: 83.4474\n",
      "validation Loss: 0.0024 Acc: 83.2024\n",
      "Epoch 13/99\n",
      "training Loss: 0.0024 Acc: 83.4742\n",
      "validation Loss: 0.0024 Acc: 83.2262\n",
      "Epoch 14/99\n",
      "training Loss: 0.0024 Acc: 83.5546\n",
      "validation Loss: 0.0024 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0024 Acc: 83.4891\n",
      "validation Loss: 0.0024 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0024 Acc: 83.5605\n",
      "validation Loss: 0.0024 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0024 Acc: 83.5456\n",
      "validation Loss: 0.0024 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0024 Acc: 83.6528\n",
      "validation Loss: 0.0024 Acc: 83.2619\n",
      "Epoch 19/99\n",
      "training Loss: 0.0024 Acc: 83.6676\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0024 Acc: 83.6170\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 21/99\n",
      "training Loss: 0.0024 Acc: 83.6736\n",
      "validation Loss: 0.0024 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0024 Acc: 83.6498\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 23/99\n",
      "training Loss: 0.0024 Acc: 83.7153\n",
      "validation Loss: 0.0024 Acc: 83.3571\n",
      "Epoch 24/99\n",
      "training Loss: 0.0024 Acc: 83.8194\n",
      "validation Loss: 0.0024 Acc: 83.4643\n",
      "Epoch 25/99\n",
      "training Loss: 0.0024 Acc: 83.8730\n",
      "validation Loss: 0.0024 Acc: 83.4405\n",
      "Epoch 26/99\n",
      "training Loss: 0.0024 Acc: 83.7867\n",
      "validation Loss: 0.0024 Acc: 83.4524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0024 Acc: 83.8670\n",
      "validation Loss: 0.0024 Acc: 83.3571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0024 Acc: 83.8522\n",
      "validation Loss: 0.0024 Acc: 83.3095\n",
      "Epoch 29/99\n",
      "training Loss: 0.0024 Acc: 83.8968\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 30/99\n",
      "training Loss: 0.0024 Acc: 83.8373\n",
      "validation Loss: 0.0024 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0024 Acc: 83.8551\n",
      "validation Loss: 0.0024 Acc: 83.4881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0024 Acc: 83.8879\n",
      "validation Loss: 0.0024 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0024 Acc: 83.8998\n",
      "validation Loss: 0.0024 Acc: 83.5476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0024 Acc: 83.9712\n",
      "validation Loss: 0.0024 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0024 Acc: 83.9385\n",
      "validation Loss: 0.0024 Acc: 83.6310\n",
      "Epoch 36/99\n",
      "training Loss: 0.0023 Acc: 83.9682\n",
      "validation Loss: 0.0024 Acc: 83.4048\n",
      "Epoch 37/99\n",
      "training Loss: 0.0023 Acc: 83.9206\n",
      "validation Loss: 0.0024 Acc: 83.5833\n",
      "Epoch 38/99\n",
      "training Loss: 0.0023 Acc: 83.9295\n",
      "validation Loss: 0.0024 Acc: 83.5595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0023 Acc: 83.9950\n",
      "validation Loss: 0.0024 Acc: 83.4881\n",
      "Epoch 40/99\n",
      "training Loss: 0.0023 Acc: 84.0188\n",
      "validation Loss: 0.0024 Acc: 83.5357\n",
      "Epoch 41/99\n",
      "training Loss: 0.0023 Acc: 84.1111\n",
      "validation Loss: 0.0024 Acc: 83.4762\n",
      "Epoch 42/99\n",
      "training Loss: 0.0023 Acc: 84.0932\n",
      "validation Loss: 0.0024 Acc: 83.4881\n",
      "Epoch 43/99\n",
      "training Loss: 0.0023 Acc: 84.0992\n",
      "validation Loss: 0.0024 Acc: 83.5833\n",
      "Epoch 44/99\n",
      "training Loss: 0.0023 Acc: 84.0932\n",
      "validation Loss: 0.0024 Acc: 83.4762\n",
      "Epoch 45/99\n",
      "training Loss: 0.0023 Acc: 84.1051\n",
      "validation Loss: 0.0024 Acc: 83.6310\n",
      "Epoch 46/99\n",
      "training Loss: 0.0023 Acc: 84.1140\n",
      "validation Loss: 0.0024 Acc: 83.6310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0023 Acc: 84.1765\n",
      "validation Loss: 0.0024 Acc: 83.5952\n",
      "Epoch 48/99\n",
      "training Loss: 0.0023 Acc: 84.1676\n",
      "validation Loss: 0.0024 Acc: 83.5238\n",
      "Epoch 49/99\n",
      "training Loss: 0.0023 Acc: 84.1021\n",
      "validation Loss: 0.0024 Acc: 83.5833\n",
      "Epoch 50/99\n",
      "training Loss: 0.0023 Acc: 84.1617\n",
      "validation Loss: 0.0024 Acc: 83.5952\n",
      "Epoch 51/99\n",
      "training Loss: 0.0023 Acc: 84.2093\n",
      "validation Loss: 0.0024 Acc: 83.5952\n",
      "Epoch 52/99\n",
      "training Loss: 0.0023 Acc: 84.1676\n",
      "validation Loss: 0.0024 Acc: 83.6071\n",
      "Epoch 53/99\n",
      "training Loss: 0.0023 Acc: 84.2033\n",
      "validation Loss: 0.0024 Acc: 83.4881\n",
      "Epoch 54/99\n",
      "training Loss: 0.0023 Acc: 84.2420\n",
      "validation Loss: 0.0024 Acc: 83.5595\n",
      "Early stopped.\n",
      "Best val acc: 83.630952\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8340752202\n",
      "New configuration: {'learning_rate': 1.0011031116873583e-05, 'initial_nodes': 200, 'dropout': 0.6637472287179998, 'batch_size': 80, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0084 Acc: 60.6131\n",
      "validation Loss: 0.0079 Acc: 77.6839\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0074 Acc: 75.3423\n",
      "validation Loss: 0.0070 Acc: 78.4813\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0067 Acc: 77.9315\n",
      "validation Loss: 0.0064 Acc: 78.7432\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0062 Acc: 78.7827\n",
      "validation Loss: 0.0060 Acc: 79.1716\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0059 Acc: 79.4821\n",
      "validation Loss: 0.0058 Acc: 79.4573\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0057 Acc: 79.9702\n",
      "validation Loss: 0.0057 Acc: 79.8500\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0055 Acc: 80.4107\n",
      "validation Loss: 0.0055 Acc: 80.1119\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0055 Acc: 80.6696\n",
      "validation Loss: 0.0054 Acc: 80.4451\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 80.9792\n",
      "validation Loss: 0.0055 Acc: 80.6832\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0053 Acc: 80.9464\n",
      "validation Loss: 0.0054 Acc: 80.8141\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 81.5685\n",
      "validation Loss: 0.0052 Acc: 80.9569\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0052 Acc: 81.6696\n",
      "validation Loss: 0.0052 Acc: 81.0521\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0052 Acc: 81.8095\n",
      "validation Loss: 0.0051 Acc: 81.1950\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0052 Acc: 81.6637\n",
      "validation Loss: 0.0051 Acc: 81.2783\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0051 Acc: 81.9821\n",
      "validation Loss: 0.0051 Acc: 81.3854\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0051 Acc: 82.0149\n",
      "validation Loss: 0.0051 Acc: 81.5282\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0051 Acc: 82.0030\n",
      "validation Loss: 0.0051 Acc: 81.6234\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0050 Acc: 82.2976\n",
      "validation Loss: 0.0050 Acc: 81.6234\n",
      "Epoch 18/99\n",
      "training Loss: 0.0050 Acc: 82.1815\n",
      "validation Loss: 0.0050 Acc: 81.6948\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0050 Acc: 82.2619\n",
      "validation Loss: 0.0050 Acc: 81.8139\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0050 Acc: 82.2917\n",
      "validation Loss: 0.0050 Acc: 81.8853\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0050 Acc: 82.4167\n",
      "validation Loss: 0.0050 Acc: 82.0400\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0049 Acc: 82.3780\n",
      "validation Loss: 0.0049 Acc: 82.0519\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0049 Acc: 82.5595\n",
      "validation Loss: 0.0050 Acc: 82.1590\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0049 Acc: 82.5000\n",
      "validation Loss: 0.0050 Acc: 82.0876\n",
      "Epoch 25/99\n",
      "training Loss: 0.0049 Acc: 82.6161\n",
      "validation Loss: 0.0049 Acc: 82.2066\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0049 Acc: 82.5327\n",
      "validation Loss: 0.0051 Acc: 82.2423\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0049 Acc: 82.6994\n",
      "validation Loss: 0.0049 Acc: 82.2185\n",
      "Epoch 28/99\n",
      "training Loss: 0.0049 Acc: 82.7649\n",
      "validation Loss: 0.0049 Acc: 82.2780\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0049 Acc: 82.7113\n",
      "validation Loss: 0.0049 Acc: 82.2185\n",
      "Epoch 30/99\n",
      "training Loss: 0.0049 Acc: 82.6726\n",
      "validation Loss: 0.0049 Acc: 82.3018\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0048 Acc: 82.7292\n",
      "validation Loss: 0.0050 Acc: 82.2661\n",
      "Epoch 32/99\n",
      "training Loss: 0.0048 Acc: 82.7917\n",
      "validation Loss: 0.0049 Acc: 82.3137\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0048 Acc: 82.8452\n",
      "validation Loss: 0.0048 Acc: 82.3256\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0048 Acc: 82.7768\n",
      "validation Loss: 0.0048 Acc: 82.2304\n",
      "Epoch 35/99\n",
      "training Loss: 0.0048 Acc: 82.8929\n",
      "validation Loss: 0.0049 Acc: 82.2423\n",
      "Epoch 36/99\n",
      "training Loss: 0.0048 Acc: 82.8363\n",
      "validation Loss: 0.0048 Acc: 82.2899\n",
      "Epoch 37/99\n",
      "training Loss: 0.0048 Acc: 82.9464\n",
      "validation Loss: 0.0048 Acc: 82.2780\n",
      "Epoch 38/99\n",
      "training Loss: 0.0048 Acc: 83.0595\n",
      "validation Loss: 0.0049 Acc: 82.3137\n",
      "Epoch 39/99\n",
      "training Loss: 0.0048 Acc: 82.9613\n",
      "validation Loss: 0.0048 Acc: 82.3018\n",
      "Epoch 40/99\n",
      "training Loss: 0.0048 Acc: 82.9851\n",
      "validation Loss: 0.0048 Acc: 82.3732\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0048 Acc: 83.0119\n",
      "validation Loss: 0.0048 Acc: 82.4090\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0048 Acc: 83.0774\n",
      "validation Loss: 0.0048 Acc: 82.4566\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0048 Acc: 82.9643\n",
      "validation Loss: 0.0048 Acc: 82.3851\n",
      "Epoch 44/99\n",
      "training Loss: 0.0048 Acc: 83.0060\n",
      "validation Loss: 0.0049 Acc: 82.3851\n",
      "Epoch 45/99\n",
      "training Loss: 0.0048 Acc: 83.1399\n",
      "validation Loss: 0.0048 Acc: 82.4447\n",
      "Epoch 46/99\n",
      "training Loss: 0.0047 Acc: 83.0625\n",
      "validation Loss: 0.0048 Acc: 82.4685\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0048 Acc: 83.1071\n",
      "validation Loss: 0.0048 Acc: 82.4447\n",
      "Epoch 48/99\n",
      "training Loss: 0.0047 Acc: 83.0060\n",
      "validation Loss: 0.0048 Acc: 82.4804\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0047 Acc: 83.0685\n",
      "validation Loss: 0.0049 Acc: 82.5161\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0047 Acc: 83.1220\n",
      "validation Loss: 0.0048 Acc: 82.5280\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0047 Acc: 83.1458\n",
      "validation Loss: 0.0048 Acc: 82.5042\n",
      "Epoch 52/99\n",
      "training Loss: 0.0047 Acc: 83.1607\n",
      "validation Loss: 0.0049 Acc: 82.5161\n",
      "Epoch 53/99\n",
      "training Loss: 0.0047 Acc: 83.0685\n",
      "validation Loss: 0.0048 Acc: 82.4447\n",
      "Epoch 54/99\n",
      "training Loss: 0.0047 Acc: 83.1458\n",
      "validation Loss: 0.0048 Acc: 82.4804\n",
      "Epoch 55/99\n",
      "training Loss: 0.0047 Acc: 83.1548\n",
      "validation Loss: 0.0048 Acc: 82.5042\n",
      "Epoch 56/99\n",
      "training Loss: 0.0047 Acc: 83.1131\n",
      "validation Loss: 0.0048 Acc: 82.5042\n",
      "Epoch 57/99\n",
      "training Loss: 0.0047 Acc: 83.1786\n",
      "validation Loss: 0.0048 Acc: 82.4685\n",
      "Epoch 58/99\n",
      "training Loss: 0.0047 Acc: 83.1161\n",
      "validation Loss: 0.0048 Acc: 82.5042\n",
      "Epoch 59/99\n",
      "training Loss: 0.0047 Acc: 83.0952\n",
      "validation Loss: 0.0048 Acc: 82.4804\n",
      "Epoch 60/99\n",
      "training Loss: 0.0047 Acc: 83.1905\n",
      "validation Loss: 0.0048 Acc: 82.4804\n",
      "Epoch 61/99\n",
      "training Loss: 0.0047 Acc: 83.2143\n",
      "validation Loss: 0.0048 Acc: 82.4804\n",
      "Epoch 62/99\n",
      "training Loss: 0.0047 Acc: 83.2589\n",
      "validation Loss: 0.0049 Acc: 82.4804\n",
      "Epoch 63/99\n",
      "training Loss: 0.0047 Acc: 83.1250\n",
      "validation Loss: 0.0048 Acc: 82.4685\n",
      "Epoch 64/99\n",
      "training Loss: 0.0047 Acc: 83.1190\n",
      "validation Loss: 0.0047 Acc: 82.4328\n",
      "Epoch 65/99\n",
      "training Loss: 0.0047 Acc: 83.1220\n",
      "validation Loss: 0.0048 Acc: 82.4447\n",
      "Epoch 66/99\n",
      "training Loss: 0.0047 Acc: 83.1756\n",
      "validation Loss: 0.0047 Acc: 82.4566\n",
      "Epoch 67/99\n",
      "training Loss: 0.0047 Acc: 83.1012\n",
      "validation Loss: 0.0049 Acc: 82.4566\n",
      "Epoch 68/99\n",
      "training Loss: 0.0047 Acc: 83.1756\n",
      "validation Loss: 0.0047 Acc: 82.4923\n",
      "Epoch 69/99\n",
      "training Loss: 0.0047 Acc: 83.1429\n",
      "validation Loss: 0.0048 Acc: 82.5042\n",
      "Epoch 70/99\n",
      "training Loss: 0.0047 Acc: 83.2857\n",
      "validation Loss: 0.0047 Acc: 82.4804\n",
      "Early stopped.\n",
      "Best val acc: 82.527970\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0084 Acc: 57.9698\n",
      "validation Loss: 0.0078 Acc: 75.4167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0075 Acc: 73.7218\n",
      "validation Loss: 0.0070 Acc: 78.6786\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0068 Acc: 77.5668\n",
      "validation Loss: 0.0063 Acc: 79.1190\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0063 Acc: 78.7394\n",
      "validation Loss: 0.0059 Acc: 79.4762\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0059 Acc: 79.3584\n",
      "validation Loss: 0.0056 Acc: 79.8690\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0057 Acc: 79.7453\n",
      "validation Loss: 0.0055 Acc: 80.2381\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0056 Acc: 80.1262\n",
      "validation Loss: 0.0053 Acc: 80.6667\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0055 Acc: 80.6023\n",
      "validation Loss: 0.0052 Acc: 81.2500\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 80.8553\n",
      "validation Loss: 0.0052 Acc: 81.3929\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0053 Acc: 81.0368\n",
      "validation Loss: 0.0051 Acc: 81.6071\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 81.3583\n",
      "validation Loss: 0.0050 Acc: 81.8095\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0052 Acc: 81.4475\n",
      "validation Loss: 0.0050 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0052 Acc: 81.4803\n",
      "validation Loss: 0.0050 Acc: 82.0833\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0052 Acc: 81.5636\n",
      "validation Loss: 0.0049 Acc: 82.2024\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0051 Acc: 81.6707\n",
      "validation Loss: 0.0049 Acc: 82.2619\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0051 Acc: 81.9237\n",
      "validation Loss: 0.0049 Acc: 82.3333\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0051 Acc: 81.9535\n",
      "validation Loss: 0.0049 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0051 Acc: 82.0487\n",
      "validation Loss: 0.0048 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0051 Acc: 82.1231\n",
      "validation Loss: 0.0048 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0050 Acc: 81.9713\n",
      "validation Loss: 0.0048 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0050 Acc: 82.4237\n",
      "validation Loss: 0.0048 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0050 Acc: 82.1499\n",
      "validation Loss: 0.0048 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0050 Acc: 82.2897\n",
      "validation Loss: 0.0048 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0050 Acc: 82.3969\n",
      "validation Loss: 0.0048 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0050 Acc: 82.2897\n",
      "validation Loss: 0.0047 Acc: 82.8452\n",
      "Epoch 25/99\n",
      "training Loss: 0.0050 Acc: 82.3641\n",
      "validation Loss: 0.0047 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0050 Acc: 82.3136\n",
      "validation Loss: 0.0047 Acc: 82.9048\n",
      "Epoch 27/99\n",
      "training Loss: 0.0049 Acc: 82.4385\n",
      "validation Loss: 0.0047 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0049 Acc: 82.5457\n",
      "validation Loss: 0.0047 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0050 Acc: 82.4802\n",
      "validation Loss: 0.0047 Acc: 82.9643\n",
      "Epoch 30/99\n",
      "training Loss: 0.0049 Acc: 82.6260\n",
      "validation Loss: 0.0047 Acc: 82.9524\n",
      "Epoch 31/99\n",
      "training Loss: 0.0049 Acc: 82.4564\n",
      "validation Loss: 0.0047 Acc: 82.9643\n",
      "Epoch 32/99\n",
      "training Loss: 0.0049 Acc: 82.5903\n",
      "validation Loss: 0.0047 Acc: 83.0000\n",
      "Epoch 33/99\n",
      "training Loss: 0.0049 Acc: 82.4713\n",
      "validation Loss: 0.0047 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0049 Acc: 82.6498\n",
      "validation Loss: 0.0047 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0049 Acc: 82.7302\n",
      "validation Loss: 0.0047 Acc: 83.0714\n",
      "Epoch 36/99\n",
      "training Loss: 0.0049 Acc: 82.7748\n",
      "validation Loss: 0.0047 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0048 Acc: 82.8016\n",
      "validation Loss: 0.0047 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0048 Acc: 82.8314\n",
      "validation Loss: 0.0047 Acc: 83.1190\n",
      "Epoch 39/99\n",
      "training Loss: 0.0049 Acc: 82.7986\n",
      "validation Loss: 0.0046 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0048 Acc: 82.8076\n",
      "validation Loss: 0.0046 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0048 Acc: 82.7123\n",
      "validation Loss: 0.0046 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0048 Acc: 82.6915\n",
      "validation Loss: 0.0046 Acc: 83.1667\n",
      "Epoch 43/99\n",
      "training Loss: 0.0048 Acc: 82.7481\n",
      "validation Loss: 0.0046 Acc: 83.1905\n",
      "Epoch 44/99\n",
      "training Loss: 0.0048 Acc: 82.9504\n",
      "validation Loss: 0.0046 Acc: 83.1905\n",
      "Epoch 45/99\n",
      "training Loss: 0.0048 Acc: 83.0308\n",
      "validation Loss: 0.0046 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0048 Acc: 82.9742\n",
      "validation Loss: 0.0046 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0048 Acc: 82.9355\n",
      "validation Loss: 0.0046 Acc: 83.2143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0048 Acc: 82.8760\n",
      "validation Loss: 0.0046 Acc: 83.2143\n",
      "Epoch 49/99\n",
      "training Loss: 0.0048 Acc: 83.0516\n",
      "validation Loss: 0.0046 Acc: 83.2381\n",
      "Epoch 50/99\n",
      "training Loss: 0.0048 Acc: 82.8909\n",
      "validation Loss: 0.0046 Acc: 83.2381\n",
      "Epoch 51/99\n",
      "training Loss: 0.0048 Acc: 83.0129\n",
      "validation Loss: 0.0046 Acc: 83.2262\n",
      "Epoch 52/99\n",
      "training Loss: 0.0048 Acc: 82.7867\n",
      "validation Loss: 0.0046 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0048 Acc: 83.1766\n",
      "validation Loss: 0.0046 Acc: 83.2619\n",
      "Epoch 54/99\n",
      "training Loss: 0.0048 Acc: 82.9177\n",
      "validation Loss: 0.0046 Acc: 83.2619\n",
      "Epoch 55/99\n",
      "training Loss: 0.0048 Acc: 83.1320\n",
      "validation Loss: 0.0046 Acc: 83.2500\n",
      "Epoch 56/99\n",
      "training Loss: 0.0048 Acc: 83.0427\n",
      "validation Loss: 0.0046 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0048 Acc: 83.1409\n",
      "validation Loss: 0.0046 Acc: 83.2738\n",
      "Epoch 58/99\n",
      "training Loss: 0.0048 Acc: 83.0992\n",
      "validation Loss: 0.0046 Acc: 83.2738\n",
      "Epoch 59/99\n",
      "training Loss: 0.0048 Acc: 83.1111\n",
      "validation Loss: 0.0046 Acc: 83.2857\n",
      "Epoch 60/99\n",
      "training Loss: 0.0048 Acc: 83.1796\n",
      "validation Loss: 0.0046 Acc: 83.3095\n",
      "Epoch 61/99\n",
      "training Loss: 0.0048 Acc: 83.1141\n",
      "validation Loss: 0.0046 Acc: 83.2976\n",
      "Epoch 62/99\n",
      "training Loss: 0.0048 Acc: 83.1230\n",
      "validation Loss: 0.0046 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0048 Acc: 83.0308\n",
      "validation Loss: 0.0046 Acc: 83.3214\n",
      "Epoch 64/99\n",
      "training Loss: 0.0048 Acc: 83.1081\n",
      "validation Loss: 0.0046 Acc: 83.2976\n",
      "Epoch 65/99\n",
      "training Loss: 0.0047 Acc: 83.1260\n",
      "validation Loss: 0.0046 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0048 Acc: 83.0903\n",
      "validation Loss: 0.0046 Acc: 83.3214\n",
      "Epoch 67/99\n",
      "training Loss: 0.0047 Acc: 83.1230\n",
      "validation Loss: 0.0046 Acc: 83.3333\n",
      "Epoch 68/99\n",
      "training Loss: 0.0047 Acc: 83.2421\n",
      "validation Loss: 0.0046 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0048 Acc: 83.2004\n",
      "validation Loss: 0.0046 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0047 Acc: 83.1409\n",
      "validation Loss: 0.0046 Acc: 83.3571\n",
      "Epoch 71/99\n",
      "training Loss: 0.0047 Acc: 83.2183\n",
      "validation Loss: 0.0046 Acc: 83.3810\n",
      "Epoch 72/99\n",
      "training Loss: 0.0048 Acc: 83.2391\n",
      "validation Loss: 0.0045 Acc: 83.3929\n",
      "Epoch 73/99\n",
      "training Loss: 0.0047 Acc: 83.1022\n",
      "validation Loss: 0.0045 Acc: 83.3571\n",
      "Epoch 74/99\n",
      "training Loss: 0.0048 Acc: 83.3581\n",
      "validation Loss: 0.0045 Acc: 83.3929\n",
      "Epoch 75/99\n",
      "training Loss: 0.0047 Acc: 83.1439\n",
      "validation Loss: 0.0045 Acc: 83.3810\n",
      "Epoch 76/99\n",
      "training Loss: 0.0047 Acc: 83.1677\n",
      "validation Loss: 0.0045 Acc: 83.3690\n",
      "Epoch 77/99\n",
      "training Loss: 0.0047 Acc: 83.1766\n",
      "validation Loss: 0.0045 Acc: 83.3571\n",
      "Epoch 78/99\n",
      "training Loss: 0.0047 Acc: 83.3909\n",
      "validation Loss: 0.0045 Acc: 83.3571\n",
      "Epoch 79/99\n",
      "training Loss: 0.0047 Acc: 83.3105\n",
      "validation Loss: 0.0045 Acc: 83.3214\n",
      "Epoch 80/99\n",
      "training Loss: 0.0047 Acc: 83.2927\n",
      "validation Loss: 0.0045 Acc: 83.3810\n",
      "Epoch 81/99\n",
      "training Loss: 0.0047 Acc: 83.1945\n",
      "validation Loss: 0.0045 Acc: 83.3214\n",
      "Epoch 82/99\n",
      "training Loss: 0.0047 Acc: 83.2956\n",
      "validation Loss: 0.0045 Acc: 83.2976\n",
      "Epoch 83/99\n",
      "training Loss: 0.0047 Acc: 83.3433\n",
      "validation Loss: 0.0045 Acc: 83.3333\n",
      "Epoch 84/99\n",
      "training Loss: 0.0047 Acc: 83.2450\n",
      "validation Loss: 0.0045 Acc: 83.3333\n",
      "Epoch 85/99\n",
      "training Loss: 0.0047 Acc: 83.3552\n",
      "validation Loss: 0.0045 Acc: 83.2976\n",
      "Epoch 86/99\n",
      "training Loss: 0.0047 Acc: 83.3373\n",
      "validation Loss: 0.0045 Acc: 83.3214\n",
      "Epoch 87/99\n",
      "training Loss: 0.0047 Acc: 83.5010\n",
      "validation Loss: 0.0045 Acc: 83.3452\n",
      "Epoch 88/99\n",
      "training Loss: 0.0047 Acc: 83.3462\n",
      "validation Loss: 0.0045 Acc: 83.4048\n",
      "Epoch 89/99\n",
      "training Loss: 0.0047 Acc: 83.3849\n",
      "validation Loss: 0.0045 Acc: 83.3214\n",
      "Early stopped.\n",
      "Best val acc: 83.416667\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0083 Acc: 60.2107\n",
      "validation Loss: 0.0078 Acc: 73.7738\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0074 Acc: 74.0908\n",
      "validation Loss: 0.0069 Acc: 78.6190\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0067 Acc: 78.1590\n",
      "validation Loss: 0.0063 Acc: 79.4167\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0061 Acc: 79.2929\n",
      "validation Loss: 0.0058 Acc: 80.1429\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0058 Acc: 79.7780\n",
      "validation Loss: 0.0056 Acc: 80.5714\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0056 Acc: 80.2244\n",
      "validation Loss: 0.0054 Acc: 80.9048\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0055 Acc: 80.4714\n",
      "validation Loss: 0.0053 Acc: 81.0833\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 80.9922\n",
      "validation Loss: 0.0052 Acc: 81.3095\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0053 Acc: 81.0845\n",
      "validation Loss: 0.0051 Acc: 81.5714\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0053 Acc: 81.3315\n",
      "validation Loss: 0.0051 Acc: 81.7262\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0052 Acc: 81.3344\n",
      "validation Loss: 0.0050 Acc: 81.8452\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0052 Acc: 81.5517\n",
      "validation Loss: 0.0050 Acc: 81.8095\n",
      "Epoch 12/99\n",
      "training Loss: 0.0051 Acc: 81.6350\n",
      "validation Loss: 0.0050 Acc: 81.9405\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0051 Acc: 81.8017\n",
      "validation Loss: 0.0050 Acc: 82.0357\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0051 Acc: 81.9535\n",
      "validation Loss: 0.0049 Acc: 82.1429\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0051 Acc: 81.9832\n",
      "validation Loss: 0.0049 Acc: 82.1667\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0050 Acc: 82.0427\n",
      "validation Loss: 0.0049 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0050 Acc: 81.9118\n",
      "validation Loss: 0.0049 Acc: 82.2143\n",
      "Epoch 18/99\n",
      "training Loss: 0.0050 Acc: 82.2124\n",
      "validation Loss: 0.0049 Acc: 82.2857\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0050 Acc: 82.0517\n",
      "validation Loss: 0.0049 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0050 Acc: 82.0993\n",
      "validation Loss: 0.0048 Acc: 82.3690\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0049 Acc: 82.2332\n",
      "validation Loss: 0.0048 Acc: 82.3452\n",
      "Epoch 22/99\n",
      "training Loss: 0.0050 Acc: 82.3374\n",
      "validation Loss: 0.0048 Acc: 82.3095\n",
      "Epoch 23/99\n",
      "training Loss: 0.0049 Acc: 82.1320\n",
      "validation Loss: 0.0048 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0049 Acc: 82.3463\n",
      "validation Loss: 0.0048 Acc: 82.3929\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0049 Acc: 82.4356\n",
      "validation Loss: 0.0048 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0049 Acc: 82.3225\n",
      "validation Loss: 0.0048 Acc: 82.4405\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0049 Acc: 82.3790\n",
      "validation Loss: 0.0048 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0049 Acc: 82.3165\n",
      "validation Loss: 0.0048 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0049 Acc: 82.4385\n",
      "validation Loss: 0.0048 Acc: 82.4881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0049 Acc: 82.4624\n",
      "validation Loss: 0.0048 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0049 Acc: 82.4415\n",
      "validation Loss: 0.0048 Acc: 82.5833\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0049 Acc: 82.6112\n",
      "validation Loss: 0.0048 Acc: 82.5714\n",
      "Epoch 33/99\n",
      "training Loss: 0.0049 Acc: 82.6439\n",
      "validation Loss: 0.0048 Acc: 82.5833\n",
      "Epoch 34/99\n",
      "training Loss: 0.0049 Acc: 82.5993\n",
      "validation Loss: 0.0048 Acc: 82.5714\n",
      "Epoch 35/99\n",
      "training Loss: 0.0048 Acc: 82.6320\n",
      "validation Loss: 0.0047 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0048 Acc: 82.6588\n",
      "validation Loss: 0.0047 Acc: 82.6190\n",
      "Epoch 37/99\n",
      "training Loss: 0.0048 Acc: 82.7778\n",
      "validation Loss: 0.0047 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0048 Acc: 82.8195\n",
      "validation Loss: 0.0047 Acc: 82.7143\n",
      "Epoch 39/99\n",
      "training Loss: 0.0048 Acc: 82.6975\n",
      "validation Loss: 0.0047 Acc: 82.7024\n",
      "Epoch 40/99\n",
      "training Loss: 0.0048 Acc: 82.7004\n",
      "validation Loss: 0.0047 Acc: 82.6905\n",
      "Epoch 41/99\n",
      "training Loss: 0.0048 Acc: 82.6082\n",
      "validation Loss: 0.0047 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0048 Acc: 82.7689\n",
      "validation Loss: 0.0047 Acc: 82.7262\n",
      "Epoch 43/99\n",
      "training Loss: 0.0048 Acc: 82.8463\n",
      "validation Loss: 0.0047 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0048 Acc: 82.7897\n",
      "validation Loss: 0.0047 Acc: 82.7738\n",
      "Epoch 45/99\n",
      "training Loss: 0.0048 Acc: 82.7867\n",
      "validation Loss: 0.0047 Acc: 82.7381\n",
      "Epoch 46/99\n",
      "training Loss: 0.0048 Acc: 82.7123\n",
      "validation Loss: 0.0047 Acc: 82.7738\n",
      "Epoch 47/99\n",
      "training Loss: 0.0048 Acc: 82.7986\n",
      "validation Loss: 0.0047 Acc: 82.7857\n",
      "Epoch 48/99\n",
      "training Loss: 0.0048 Acc: 82.8135\n",
      "validation Loss: 0.0047 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0048 Acc: 82.8879\n",
      "validation Loss: 0.0047 Acc: 82.8095\n",
      "Epoch 50/99\n",
      "training Loss: 0.0048 Acc: 82.9385\n",
      "validation Loss: 0.0047 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0048 Acc: 82.9147\n",
      "validation Loss: 0.0047 Acc: 82.7857\n",
      "Epoch 52/99\n",
      "training Loss: 0.0048 Acc: 82.8611\n",
      "validation Loss: 0.0047 Acc: 82.8214\n",
      "Epoch 53/99\n",
      "training Loss: 0.0048 Acc: 82.8998\n",
      "validation Loss: 0.0047 Acc: 82.8333\n",
      "Epoch 54/99\n",
      "training Loss: 0.0047 Acc: 83.1022\n",
      "validation Loss: 0.0047 Acc: 82.8095\n",
      "Epoch 55/99\n",
      "training Loss: 0.0048 Acc: 82.9564\n",
      "validation Loss: 0.0047 Acc: 82.8095\n",
      "Epoch 56/99\n",
      "training Loss: 0.0048 Acc: 82.8909\n",
      "validation Loss: 0.0047 Acc: 82.7857\n",
      "Epoch 57/99\n",
      "training Loss: 0.0048 Acc: 82.9593\n",
      "validation Loss: 0.0047 Acc: 82.7500\n",
      "Epoch 58/99\n",
      "training Loss: 0.0047 Acc: 83.0189\n",
      "validation Loss: 0.0047 Acc: 82.8333\n",
      "Epoch 59/99\n",
      "training Loss: 0.0048 Acc: 82.9534\n",
      "validation Loss: 0.0047 Acc: 82.8452\n",
      "Epoch 60/99\n",
      "training Loss: 0.0048 Acc: 83.0397\n",
      "validation Loss: 0.0047 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0047 Acc: 83.0040\n",
      "validation Loss: 0.0047 Acc: 82.8690\n",
      "Epoch 62/99\n",
      "training Loss: 0.0047 Acc: 82.9355\n",
      "validation Loss: 0.0047 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0048 Acc: 82.9921\n",
      "validation Loss: 0.0047 Acc: 82.8810\n",
      "Epoch 64/99\n",
      "training Loss: 0.0047 Acc: 83.0754\n",
      "validation Loss: 0.0047 Acc: 82.8452\n",
      "Epoch 65/99\n",
      "training Loss: 0.0048 Acc: 82.9355\n",
      "validation Loss: 0.0047 Acc: 82.8690\n",
      "Epoch 66/99\n",
      "training Loss: 0.0047 Acc: 83.0605\n",
      "validation Loss: 0.0047 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 67/99\n",
      "training Loss: 0.0047 Acc: 83.1171\n",
      "validation Loss: 0.0047 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0047 Acc: 83.0873\n",
      "validation Loss: 0.0047 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0047 Acc: 83.0933\n",
      "validation Loss: 0.0047 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0047 Acc: 83.0903\n",
      "validation Loss: 0.0047 Acc: 82.9167\n",
      "Epoch 71/99\n",
      "training Loss: 0.0047 Acc: 83.1468\n",
      "validation Loss: 0.0047 Acc: 82.9286\n",
      "Epoch 72/99\n",
      "training Loss: 0.0047 Acc: 83.1558\n",
      "validation Loss: 0.0047 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 73/99\n",
      "training Loss: 0.0047 Acc: 83.0248\n",
      "validation Loss: 0.0047 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0047 Acc: 83.1230\n",
      "validation Loss: 0.0046 Acc: 82.9286\n",
      "Epoch 75/99\n",
      "training Loss: 0.0047 Acc: 83.1320\n",
      "validation Loss: 0.0046 Acc: 82.9048\n",
      "Epoch 76/99\n",
      "training Loss: 0.0047 Acc: 83.1915\n",
      "validation Loss: 0.0046 Acc: 82.9286\n",
      "Epoch 77/99\n",
      "training Loss: 0.0047 Acc: 83.1945\n",
      "validation Loss: 0.0046 Acc: 82.9643\n",
      "Epoch 78/99\n",
      "training Loss: 0.0047 Acc: 83.1945\n",
      "validation Loss: 0.0046 Acc: 82.9524\n",
      "Epoch 79/99\n",
      "training Loss: 0.0047 Acc: 83.1587\n",
      "validation Loss: 0.0046 Acc: 82.9881\n",
      "Epoch 80/99\n",
      "training Loss: 0.0047 Acc: 83.0695\n",
      "validation Loss: 0.0046 Acc: 82.9524\n",
      "Epoch 81/99\n",
      "training Loss: 0.0047 Acc: 83.2986\n",
      "validation Loss: 0.0046 Acc: 82.9405\n",
      "Epoch 82/99\n",
      "training Loss: 0.0047 Acc: 83.2004\n",
      "validation Loss: 0.0046 Acc: 82.9167\n",
      "Epoch 83/99\n",
      "training Loss: 0.0047 Acc: 83.1171\n",
      "validation Loss: 0.0046 Acc: 82.9524\n",
      "Epoch 84/99\n",
      "training Loss: 0.0047 Acc: 83.1528\n",
      "validation Loss: 0.0046 Acc: 82.9048\n",
      "Epoch 85/99\n",
      "training Loss: 0.0047 Acc: 83.1617\n",
      "validation Loss: 0.0046 Acc: 82.9643\n",
      "Epoch 86/99\n",
      "training Loss: 0.0047 Acc: 83.0784\n",
      "validation Loss: 0.0046 Acc: 82.9762\n",
      "Epoch 87/99\n",
      "training Loss: 0.0047 Acc: 83.1677\n",
      "validation Loss: 0.0046 Acc: 82.9286\n",
      "Epoch 88/99\n",
      "training Loss: 0.0047 Acc: 83.1974\n",
      "validation Loss: 0.0046 Acc: 82.9405\n",
      "Epoch 89/99\n",
      "training Loss: 0.0047 Acc: 83.2510\n",
      "validation Loss: 0.0046 Acc: 82.9286\n",
      "Epoch 90/99\n",
      "training Loss: 0.0047 Acc: 83.2331\n",
      "validation Loss: 0.0046 Acc: 82.9286\n",
      "Epoch 91/99\n",
      "training Loss: 0.0047 Acc: 83.2183\n",
      "validation Loss: 0.0046 Acc: 82.9286\n",
      "Epoch 92/99\n",
      "training Loss: 0.0047 Acc: 83.1677\n",
      "validation Loss: 0.0046 Acc: 82.9881\n",
      "Epoch 93/99\n",
      "training Loss: 0.0047 Acc: 83.2302\n",
      "validation Loss: 0.0046 Acc: 82.9643\n",
      "Early stopped.\n",
      "Best val acc: 82.988095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0084 Acc: 58.7048\n",
      "validation Loss: 0.0078 Acc: 75.8810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0076 Acc: 73.3438\n",
      "validation Loss: 0.0070 Acc: 78.5357\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0069 Acc: 76.6413\n",
      "validation Loss: 0.0064 Acc: 79.4524\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0064 Acc: 78.3525\n",
      "validation Loss: 0.0060 Acc: 80.0119\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0060 Acc: 78.9477\n",
      "validation Loss: 0.0057 Acc: 80.4167\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0058 Acc: 79.3792\n",
      "validation Loss: 0.0055 Acc: 80.8333\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0057 Acc: 79.8732\n",
      "validation Loss: 0.0053 Acc: 81.3690\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0056 Acc: 80.3910\n",
      "validation Loss: 0.0052 Acc: 81.6310\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0055 Acc: 80.7273\n",
      "validation Loss: 0.0051 Acc: 81.8452\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0054 Acc: 80.7690\n",
      "validation Loss: 0.0051 Acc: 82.0119\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 80.9565\n",
      "validation Loss: 0.0050 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0053 Acc: 81.3017\n",
      "validation Loss: 0.0050 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0052 Acc: 81.3999\n",
      "validation Loss: 0.0049 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0052 Acc: 81.6856\n",
      "validation Loss: 0.0049 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0051 Acc: 81.7094\n",
      "validation Loss: 0.0049 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0051 Acc: 81.7868\n",
      "validation Loss: 0.0048 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0051 Acc: 81.7749\n",
      "validation Loss: 0.0048 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0051 Acc: 81.9356\n",
      "validation Loss: 0.0048 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0051 Acc: 82.1499\n",
      "validation Loss: 0.0048 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0050 Acc: 82.1469\n",
      "validation Loss: 0.0048 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0050 Acc: 82.1350\n",
      "validation Loss: 0.0047 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0050 Acc: 82.1826\n",
      "validation Loss: 0.0047 Acc: 83.1190\n",
      "Epoch 22/99\n",
      "training Loss: 0.0050 Acc: 82.3760\n",
      "validation Loss: 0.0047 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0050 Acc: 82.3195\n",
      "validation Loss: 0.0047 Acc: 83.1905\n",
      "Epoch 24/99\n",
      "training Loss: 0.0050 Acc: 82.2540\n",
      "validation Loss: 0.0047 Acc: 83.0833\n",
      "Epoch 25/99\n",
      "training Loss: 0.0049 Acc: 82.3909\n",
      "validation Loss: 0.0047 Acc: 83.1429\n",
      "Epoch 26/99\n",
      "training Loss: 0.0050 Acc: 82.4237\n",
      "validation Loss: 0.0047 Acc: 83.1905\n",
      "Epoch 27/99\n",
      "training Loss: 0.0049 Acc: 82.4891\n",
      "validation Loss: 0.0047 Acc: 83.1667\n",
      "Epoch 28/99\n",
      "training Loss: 0.0049 Acc: 82.5754\n",
      "validation Loss: 0.0047 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0049 Acc: 82.4921\n",
      "validation Loss: 0.0047 Acc: 83.2262\n",
      "Epoch 30/99\n",
      "training Loss: 0.0049 Acc: 82.4713\n",
      "validation Loss: 0.0047 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0049 Acc: 82.6112\n",
      "validation Loss: 0.0046 Acc: 83.2143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0049 Acc: 82.6052\n",
      "validation Loss: 0.0046 Acc: 83.2143\n",
      "Epoch 33/99\n",
      "training Loss: 0.0049 Acc: 82.7867\n",
      "validation Loss: 0.0046 Acc: 83.2262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0049 Acc: 82.6588\n",
      "validation Loss: 0.0046 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0049 Acc: 82.8433\n",
      "validation Loss: 0.0046 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0049 Acc: 82.8225\n",
      "validation Loss: 0.0046 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0049 Acc: 82.7927\n",
      "validation Loss: 0.0046 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0048 Acc: 82.7867\n",
      "validation Loss: 0.0046 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0049 Acc: 82.6290\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Epoch 40/99\n",
      "training Loss: 0.0048 Acc: 82.7719\n",
      "validation Loss: 0.0046 Acc: 83.4286\n",
      "Epoch 41/99\n",
      "training Loss: 0.0048 Acc: 82.7183\n",
      "validation Loss: 0.0046 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0049 Acc: 82.8254\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Epoch 43/99\n",
      "training Loss: 0.0048 Acc: 82.9028\n",
      "validation Loss: 0.0046 Acc: 83.4643\n",
      "Epoch 44/99\n",
      "training Loss: 0.0048 Acc: 82.8314\n",
      "validation Loss: 0.0046 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0048 Acc: 82.8165\n",
      "validation Loss: 0.0046 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0048 Acc: 82.9564\n",
      "validation Loss: 0.0046 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0048 Acc: 83.0367\n",
      "validation Loss: 0.0046 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0048 Acc: 82.8641\n",
      "validation Loss: 0.0046 Acc: 83.5476\n",
      "Epoch 49/99\n",
      "training Loss: 0.0048 Acc: 82.8730\n",
      "validation Loss: 0.0046 Acc: 83.5595\n",
      "Epoch 50/99\n",
      "training Loss: 0.0048 Acc: 82.9653\n",
      "validation Loss: 0.0046 Acc: 83.5119\n",
      "Epoch 51/99\n",
      "training Loss: 0.0048 Acc: 82.8195\n",
      "validation Loss: 0.0046 Acc: 83.5000\n",
      "Epoch 52/99\n",
      "training Loss: 0.0048 Acc: 82.9266\n",
      "validation Loss: 0.0046 Acc: 83.5238\n",
      "Epoch 53/99\n",
      "training Loss: 0.0048 Acc: 83.0040\n",
      "validation Loss: 0.0046 Acc: 83.4881\n",
      "Epoch 54/99\n",
      "training Loss: 0.0048 Acc: 83.0665\n",
      "validation Loss: 0.0046 Acc: 83.5476\n",
      "Epoch 55/99\n",
      "training Loss: 0.0048 Acc: 83.0516\n",
      "validation Loss: 0.0046 Acc: 83.5357\n",
      "Epoch 56/99\n",
      "training Loss: 0.0048 Acc: 83.0040\n",
      "validation Loss: 0.0046 Acc: 83.5595\n",
      "Epoch 57/99\n",
      "training Loss: 0.0048 Acc: 83.1974\n",
      "validation Loss: 0.0046 Acc: 83.5595\n",
      "Epoch 58/99\n",
      "training Loss: 0.0048 Acc: 83.0278\n",
      "validation Loss: 0.0046 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0048 Acc: 82.9861\n",
      "validation Loss: 0.0045 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0048 Acc: 82.9802\n",
      "validation Loss: 0.0045 Acc: 83.5833\n",
      "Epoch 61/99\n",
      "training Loss: 0.0048 Acc: 83.0635\n",
      "validation Loss: 0.0045 Acc: 83.5833\n",
      "Epoch 62/99\n",
      "training Loss: 0.0048 Acc: 82.9832\n",
      "validation Loss: 0.0045 Acc: 83.5952\n",
      "Epoch 63/99\n",
      "training Loss: 0.0048 Acc: 82.9683\n",
      "validation Loss: 0.0045 Acc: 83.5476\n",
      "Epoch 64/99\n",
      "training Loss: 0.0048 Acc: 83.0962\n",
      "validation Loss: 0.0045 Acc: 83.5714\n",
      "Epoch 65/99\n",
      "training Loss: 0.0047 Acc: 83.0754\n",
      "validation Loss: 0.0045 Acc: 83.5714\n",
      "Epoch 66/99\n",
      "training Loss: 0.0048 Acc: 83.0486\n",
      "validation Loss: 0.0045 Acc: 83.5476\n",
      "Epoch 67/99\n",
      "training Loss: 0.0047 Acc: 83.0099\n",
      "validation Loss: 0.0045 Acc: 83.5714\n",
      "Epoch 68/99\n",
      "training Loss: 0.0047 Acc: 83.1855\n",
      "validation Loss: 0.0045 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0048 Acc: 83.0784\n",
      "validation Loss: 0.0045 Acc: 83.5952\n",
      "Epoch 70/99\n",
      "training Loss: 0.0047 Acc: 83.1111\n",
      "validation Loss: 0.0045 Acc: 83.6310\n",
      "Epoch 71/99\n",
      "training Loss: 0.0047 Acc: 83.1498\n",
      "validation Loss: 0.0045 Acc: 83.6190\n",
      "Epoch 72/99\n",
      "training Loss: 0.0048 Acc: 83.1052\n",
      "validation Loss: 0.0045 Acc: 83.6310\n",
      "Epoch 73/99\n",
      "training Loss: 0.0048 Acc: 83.0903\n",
      "validation Loss: 0.0045 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0047 Acc: 83.1796\n",
      "validation Loss: 0.0045 Acc: 83.5833\n",
      "Epoch 75/99\n",
      "training Loss: 0.0047 Acc: 83.0486\n",
      "validation Loss: 0.0045 Acc: 83.6310\n",
      "Epoch 76/99\n",
      "training Loss: 0.0047 Acc: 83.0695\n",
      "validation Loss: 0.0045 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0048 Acc: 83.0576\n",
      "validation Loss: 0.0045 Acc: 83.6667\n",
      "Epoch 78/99\n",
      "training Loss: 0.0048 Acc: 83.0635\n",
      "validation Loss: 0.0045 Acc: 83.6548\n",
      "Epoch 79/99\n",
      "training Loss: 0.0047 Acc: 83.2123\n",
      "validation Loss: 0.0045 Acc: 83.6071\n",
      "Epoch 80/99\n",
      "training Loss: 0.0047 Acc: 83.2242\n",
      "validation Loss: 0.0045 Acc: 83.6667\n",
      "Epoch 81/99\n",
      "training Loss: 0.0047 Acc: 83.1201\n",
      "validation Loss: 0.0045 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0047 Acc: 83.2569\n",
      "validation Loss: 0.0045 Acc: 83.6667\n",
      "Epoch 83/99\n",
      "training Loss: 0.0047 Acc: 83.0903\n",
      "validation Loss: 0.0045 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 84/99\n",
      "training Loss: 0.0047 Acc: 83.1915\n",
      "validation Loss: 0.0045 Acc: 83.6548\n",
      "Epoch 85/99\n",
      "training Loss: 0.0047 Acc: 83.1468\n",
      "validation Loss: 0.0045 Acc: 83.6429\n",
      "Epoch 86/99\n",
      "training Loss: 0.0047 Acc: 83.2480\n",
      "validation Loss: 0.0045 Acc: 83.6905\n",
      "Epoch 87/99\n",
      "training Loss: 0.0047 Acc: 83.2897\n",
      "validation Loss: 0.0045 Acc: 83.6548\n",
      "Epoch 88/99\n",
      "training Loss: 0.0047 Acc: 83.1528\n",
      "validation Loss: 0.0045 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 89/99\n",
      "training Loss: 0.0047 Acc: 83.2480\n",
      "validation Loss: 0.0045 Acc: 83.6905\n",
      "Epoch 90/99\n",
      "training Loss: 0.0047 Acc: 82.9891\n",
      "validation Loss: 0.0045 Acc: 83.6786\n",
      "Epoch 91/99\n",
      "training Loss: 0.0047 Acc: 83.2629\n",
      "validation Loss: 0.0045 Acc: 83.7024\n",
      "Epoch 92/99\n",
      "training Loss: 0.0047 Acc: 83.1796\n",
      "validation Loss: 0.0045 Acc: 83.6905\n",
      "Epoch 93/99\n",
      "training Loss: 0.0047 Acc: 83.2391\n",
      "validation Loss: 0.0045 Acc: 83.7024\n",
      "Epoch 94/99\n",
      "training Loss: 0.0047 Acc: 83.3879\n",
      "validation Loss: 0.0045 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0047 Acc: 83.1855\n",
      "validation Loss: 0.0045 Acc: 83.6667\n",
      "Epoch 96/99\n",
      "training Loss: 0.0047 Acc: 83.2123\n",
      "validation Loss: 0.0045 Acc: 83.6905\n",
      "Epoch 97/99\n",
      "training Loss: 0.0047 Acc: 83.1766\n",
      "validation Loss: 0.0045 Acc: 83.7143\n",
      "Epoch 98/99\n",
      "training Loss: 0.0047 Acc: 83.2927\n",
      "validation Loss: 0.0045 Acc: 83.7262\n",
      "Epoch 99/99\n",
      "training Loss: 0.0047 Acc: 83.1945\n",
      "validation Loss: 0.0045 Acc: 83.7381\n",
      "Saving..\n",
      "Best val acc: 83.738095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0082 Acc: 64.8592\n",
      "validation Loss: 0.0076 Acc: 78.6905\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0073 Acc: 76.2633\n",
      "validation Loss: 0.0067 Acc: 79.4048\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0066 Acc: 78.4328\n",
      "validation Loss: 0.0061 Acc: 79.9643\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0061 Acc: 79.0667\n",
      "validation Loss: 0.0058 Acc: 80.5119\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0058 Acc: 79.8583\n",
      "validation Loss: 0.0055 Acc: 80.8690\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0057 Acc: 80.0548\n",
      "validation Loss: 0.0054 Acc: 81.1310\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0055 Acc: 80.4327\n",
      "validation Loss: 0.0053 Acc: 81.4167\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 80.8226\n",
      "validation Loss: 0.0052 Acc: 81.5357\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 81.0041\n",
      "validation Loss: 0.0051 Acc: 81.7024\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0053 Acc: 81.2452\n",
      "validation Loss: 0.0051 Acc: 81.9405\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 81.4356\n",
      "validation Loss: 0.0050 Acc: 82.1667\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0052 Acc: 81.6767\n",
      "validation Loss: 0.0050 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0052 Acc: 81.6678\n",
      "validation Loss: 0.0049 Acc: 82.3333\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0051 Acc: 81.8672\n",
      "validation Loss: 0.0049 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0051 Acc: 81.9862\n",
      "validation Loss: 0.0049 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0051 Acc: 81.9475\n",
      "validation Loss: 0.0049 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0050 Acc: 81.9505\n",
      "validation Loss: 0.0049 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0050 Acc: 82.0308\n",
      "validation Loss: 0.0048 Acc: 82.6548\n",
      "Epoch 18/99\n",
      "training Loss: 0.0050 Acc: 82.1618\n",
      "validation Loss: 0.0048 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0050 Acc: 82.2897\n",
      "validation Loss: 0.0048 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0050 Acc: 82.3671\n",
      "validation Loss: 0.0048 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0050 Acc: 82.2243\n",
      "validation Loss: 0.0048 Acc: 82.9286\n",
      "Epoch 22/99\n",
      "training Loss: 0.0049 Acc: 82.3493\n",
      "validation Loss: 0.0048 Acc: 82.9405\n",
      "Epoch 23/99\n",
      "training Loss: 0.0049 Acc: 82.3195\n",
      "validation Loss: 0.0048 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0050 Acc: 82.4118\n",
      "validation Loss: 0.0048 Acc: 82.9048\n",
      "Epoch 25/99\n",
      "training Loss: 0.0049 Acc: 82.4207\n",
      "validation Loss: 0.0047 Acc: 83.0000\n",
      "Epoch 26/99\n",
      "training Loss: 0.0049 Acc: 82.4802\n",
      "validation Loss: 0.0047 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0049 Acc: 82.5695\n",
      "validation Loss: 0.0047 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0049 Acc: 82.5784\n",
      "validation Loss: 0.0047 Acc: 83.0714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0049 Acc: 82.4862\n",
      "validation Loss: 0.0047 Acc: 83.0952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0049 Acc: 82.6409\n",
      "validation Loss: 0.0047 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0049 Acc: 82.6409\n",
      "validation Loss: 0.0047 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0049 Acc: 82.7153\n",
      "validation Loss: 0.0047 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0049 Acc: 82.5665\n",
      "validation Loss: 0.0047 Acc: 83.1786\n",
      "Epoch 34/99\n",
      "training Loss: 0.0048 Acc: 82.8344\n",
      "validation Loss: 0.0047 Acc: 83.2024\n",
      "Epoch 35/99\n",
      "training Loss: 0.0048 Acc: 82.7510\n",
      "validation Loss: 0.0047 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0048 Acc: 82.6975\n",
      "validation Loss: 0.0047 Acc: 83.1905\n",
      "Epoch 37/99\n",
      "training Loss: 0.0048 Acc: 82.6409\n",
      "validation Loss: 0.0047 Acc: 83.1667\n",
      "Epoch 38/99\n",
      "training Loss: 0.0048 Acc: 82.8016\n",
      "validation Loss: 0.0047 Acc: 83.1310\n",
      "Epoch 39/99\n",
      "training Loss: 0.0048 Acc: 82.8998\n",
      "validation Loss: 0.0047 Acc: 83.1190\n",
      "Epoch 40/99\n",
      "training Loss: 0.0048 Acc: 82.8105\n",
      "validation Loss: 0.0047 Acc: 83.1667\n",
      "Epoch 41/99\n",
      "training Loss: 0.0048 Acc: 82.8849\n",
      "validation Loss: 0.0047 Acc: 83.1786\n",
      "Epoch 42/99\n",
      "training Loss: 0.0048 Acc: 82.7957\n",
      "validation Loss: 0.0047 Acc: 83.2143\n",
      "Epoch 43/99\n",
      "training Loss: 0.0048 Acc: 82.8969\n",
      "validation Loss: 0.0047 Acc: 83.1310\n",
      "Epoch 44/99\n",
      "training Loss: 0.0048 Acc: 82.8701\n",
      "validation Loss: 0.0047 Acc: 83.2143\n",
      "Epoch 45/99\n",
      "training Loss: 0.0048 Acc: 82.8790\n",
      "validation Loss: 0.0047 Acc: 83.2143\n",
      "Epoch 46/99\n",
      "training Loss: 0.0048 Acc: 82.8195\n",
      "validation Loss: 0.0047 Acc: 83.1310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0048 Acc: 82.9921\n",
      "validation Loss: 0.0046 Acc: 83.1667\n",
      "Epoch 48/99\n",
      "training Loss: 0.0048 Acc: 82.8522\n",
      "validation Loss: 0.0046 Acc: 83.1905\n",
      "Epoch 49/99\n",
      "training Loss: 0.0048 Acc: 82.8939\n",
      "validation Loss: 0.0046 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0048 Acc: 82.9355\n",
      "validation Loss: 0.0046 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0048 Acc: 82.9951\n",
      "validation Loss: 0.0046 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0048 Acc: 82.9921\n",
      "validation Loss: 0.0046 Acc: 83.3214\n",
      "Epoch 53/99\n",
      "training Loss: 0.0048 Acc: 82.9742\n",
      "validation Loss: 0.0046 Acc: 83.2500\n",
      "Epoch 54/99\n",
      "training Loss: 0.0048 Acc: 82.9802\n",
      "validation Loss: 0.0046 Acc: 83.2738\n",
      "Epoch 55/99\n",
      "training Loss: 0.0048 Acc: 83.0427\n",
      "validation Loss: 0.0046 Acc: 83.2857\n",
      "Epoch 56/99\n",
      "training Loss: 0.0047 Acc: 82.9355\n",
      "validation Loss: 0.0046 Acc: 83.2619\n",
      "Epoch 57/99\n",
      "training Loss: 0.0047 Acc: 82.9802\n",
      "validation Loss: 0.0046 Acc: 83.2500\n",
      "Epoch 58/99\n",
      "training Loss: 0.0048 Acc: 83.1111\n",
      "validation Loss: 0.0046 Acc: 83.2738\n",
      "Epoch 59/99\n",
      "training Loss: 0.0047 Acc: 83.0248\n",
      "validation Loss: 0.0046 Acc: 83.2976\n",
      "Epoch 60/99\n",
      "training Loss: 0.0047 Acc: 82.9772\n",
      "validation Loss: 0.0046 Acc: 83.3095\n",
      "Epoch 61/99\n",
      "training Loss: 0.0047 Acc: 83.1379\n",
      "validation Loss: 0.0046 Acc: 83.2619\n",
      "Epoch 62/99\n",
      "training Loss: 0.0047 Acc: 83.1052\n",
      "validation Loss: 0.0046 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0047 Acc: 83.0248\n",
      "validation Loss: 0.0046 Acc: 83.2976\n",
      "Epoch 64/99\n",
      "training Loss: 0.0047 Acc: 83.1201\n",
      "validation Loss: 0.0046 Acc: 83.3095\n",
      "Epoch 65/99\n",
      "training Loss: 0.0048 Acc: 83.1230\n",
      "validation Loss: 0.0046 Acc: 83.3095\n",
      "Epoch 66/99\n",
      "training Loss: 0.0047 Acc: 83.1081\n",
      "validation Loss: 0.0046 Acc: 83.3452\n",
      "Epoch 67/99\n",
      "training Loss: 0.0047 Acc: 83.2004\n",
      "validation Loss: 0.0046 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0047 Acc: 83.0754\n",
      "validation Loss: 0.0046 Acc: 83.3214\n",
      "Epoch 69/99\n",
      "training Loss: 0.0048 Acc: 83.1290\n",
      "validation Loss: 0.0046 Acc: 83.3333\n",
      "Epoch 70/99\n",
      "training Loss: 0.0047 Acc: 83.1706\n",
      "validation Loss: 0.0046 Acc: 83.3452\n",
      "Epoch 71/99\n",
      "training Loss: 0.0047 Acc: 83.0843\n",
      "validation Loss: 0.0046 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 72/99\n",
      "training Loss: 0.0047 Acc: 83.1290\n",
      "validation Loss: 0.0046 Acc: 83.3452\n",
      "Epoch 73/99\n",
      "training Loss: 0.0048 Acc: 83.1230\n",
      "validation Loss: 0.0046 Acc: 83.3690\n",
      "Epoch 74/99\n",
      "training Loss: 0.0047 Acc: 83.1855\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0047 Acc: 83.0814\n",
      "validation Loss: 0.0046 Acc: 83.3929\n",
      "Epoch 76/99\n",
      "training Loss: 0.0047 Acc: 83.2659\n",
      "validation Loss: 0.0046 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0047 Acc: 83.1260\n",
      "validation Loss: 0.0046 Acc: 83.3571\n",
      "Epoch 78/99\n",
      "training Loss: 0.0047 Acc: 83.2391\n",
      "validation Loss: 0.0046 Acc: 83.3690\n",
      "Epoch 79/99\n",
      "training Loss: 0.0047 Acc: 83.2510\n",
      "validation Loss: 0.0046 Acc: 83.3810\n",
      "Epoch 80/99\n",
      "training Loss: 0.0047 Acc: 83.0903\n",
      "validation Loss: 0.0046 Acc: 83.3810\n",
      "Epoch 81/99\n",
      "training Loss: 0.0047 Acc: 83.2718\n",
      "validation Loss: 0.0046 Acc: 83.3452\n",
      "Epoch 82/99\n",
      "training Loss: 0.0047 Acc: 83.2450\n",
      "validation Loss: 0.0046 Acc: 83.3690\n",
      "Epoch 83/99\n",
      "training Loss: 0.0047 Acc: 83.1052\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Epoch 84/99\n",
      "training Loss: 0.0047 Acc: 83.1766\n",
      "validation Loss: 0.0046 Acc: 83.3810\n",
      "Epoch 85/99\n",
      "training Loss: 0.0047 Acc: 83.2569\n",
      "validation Loss: 0.0046 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 86/99\n",
      "training Loss: 0.0047 Acc: 83.2242\n",
      "validation Loss: 0.0046 Acc: 83.4643\n",
      "Epoch 87/99\n",
      "training Loss: 0.0047 Acc: 83.2093\n",
      "validation Loss: 0.0046 Acc: 83.3690\n",
      "Epoch 88/99\n",
      "training Loss: 0.0047 Acc: 83.3671\n",
      "validation Loss: 0.0046 Acc: 83.3929\n",
      "Epoch 89/99\n",
      "training Loss: 0.0047 Acc: 83.1468\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Epoch 90/99\n",
      "training Loss: 0.0047 Acc: 83.2837\n",
      "validation Loss: 0.0046 Acc: 83.4524\n",
      "Epoch 91/99\n",
      "training Loss: 0.0047 Acc: 83.2808\n",
      "validation Loss: 0.0046 Acc: 83.4762\n",
      "Epoch 92/99\n",
      "training Loss: 0.0047 Acc: 83.2927\n",
      "validation Loss: 0.0046 Acc: 83.4643\n",
      "Epoch 93/99\n",
      "training Loss: 0.0047 Acc: 83.2689\n",
      "validation Loss: 0.0046 Acc: 83.3929\n",
      "Epoch 94/99\n",
      "training Loss: 0.0047 Acc: 83.3105\n",
      "validation Loss: 0.0046 Acc: 83.4286\n",
      "Epoch 95/99\n",
      "training Loss: 0.0047 Acc: 83.3046\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Epoch 96/99\n",
      "training Loss: 0.0047 Acc: 83.3671\n",
      "validation Loss: 0.0046 Acc: 83.4286\n",
      "Epoch 97/99\n",
      "training Loss: 0.0047 Acc: 83.2689\n",
      "validation Loss: 0.0046 Acc: 83.3690\n",
      "Epoch 98/99\n",
      "training Loss: 0.0047 Acc: 83.1766\n",
      "validation Loss: 0.0046 Acc: 83.4048\n",
      "Epoch 99/99\n",
      "training Loss: 0.0047 Acc: 83.3819\n",
      "validation Loss: 0.0046 Acc: 83.4524\n",
      "Best val acc: 83.488095\n",
      "----------\n",
      "Average best_acc across k-fold: 83.2317843824\n",
      "New configuration: {'learning_rate': 0.053515404731576156, 'initial_nodes': 570, 'dropout': 0.013237689897851138, 'batch_size': 263, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0096 Acc: 78.1726\n",
      "validation Loss: 0.0015 Acc: 82.9445\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 82.5089\n",
      "validation Loss: 0.0015 Acc: 82.6232\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 82.6071\n",
      "validation Loss: 0.0014 Acc: 83.0993\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 82.6637\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 82.7440\n",
      "validation Loss: 0.0014 Acc: 82.9564\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 82.8274\n",
      "validation Loss: 0.0014 Acc: 82.6589\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 82.8363\n",
      "validation Loss: 0.0014 Acc: 82.8731\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 83.1012\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 83.1875\n",
      "validation Loss: 0.0014 Acc: 83.1350\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.1756\n",
      "validation Loss: 0.0014 Acc: 82.6946\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 83.3155\n",
      "validation Loss: 0.0014 Acc: 83.2897\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0014 Acc: 83.3661\n",
      "validation Loss: 0.0014 Acc: 83.1945\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 83.5833\n",
      "validation Loss: 0.0014 Acc: 83.2778\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 83.6458\n",
      "validation Loss: 0.0014 Acc: 83.1231\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 83.5744\n",
      "validation Loss: 0.0014 Acc: 83.1945\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 83.5506\n",
      "validation Loss: 0.0014 Acc: 83.3849\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.8006\n",
      "validation Loss: 0.0014 Acc: 83.1112\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 83.7857\n",
      "validation Loss: 0.0014 Acc: 83.2540\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.8095\n",
      "validation Loss: 0.0014 Acc: 83.2540\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 84.0714\n",
      "validation Loss: 0.0014 Acc: 83.4206\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.8036\n",
      "validation Loss: 0.0014 Acc: 83.2540\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 84.0565\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 84.1905\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 84.2173\n",
      "validation Loss: 0.0014 Acc: 83.3968\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 84.3482\n",
      "validation Loss: 0.0014 Acc: 83.4325\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 84.3750\n",
      "validation Loss: 0.0014 Acc: 83.4325\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 84.4286\n",
      "validation Loss: 0.0014 Acc: 83.4087\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 84.4286\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 84.6726\n",
      "validation Loss: 0.0014 Acc: 83.2302\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 84.8095\n",
      "validation Loss: 0.0014 Acc: 83.1826\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 84.8125\n",
      "validation Loss: 0.0014 Acc: 83.2659\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 84.9107\n",
      "validation Loss: 0.0014 Acc: 83.3373\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 84.7054\n",
      "validation Loss: 0.0014 Acc: 83.1469\n",
      "Epoch 33/99\n",
      "training Loss: 0.0013 Acc: 84.7976\n",
      "validation Loss: 0.0014 Acc: 83.1588\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 84.8542\n",
      "validation Loss: 0.0014 Acc: 83.2421\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 85.1280\n",
      "validation Loss: 0.0014 Acc: 83.0159\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 85.2470\n",
      "validation Loss: 0.0014 Acc: 83.1945\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 85.1756\n",
      "validation Loss: 0.0014 Acc: 83.2064\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 85.2560\n",
      "validation Loss: 0.0014 Acc: 83.0398\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 85.1964\n",
      "validation Loss: 0.0014 Acc: 83.3849\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 85.3185\n",
      "validation Loss: 0.0014 Acc: 83.1826\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 85.3244\n",
      "validation Loss: 0.0014 Acc: 83.2302\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 85.4524\n",
      "validation Loss: 0.0014 Acc: 83.2778\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 85.4762\n",
      "validation Loss: 0.0014 Acc: 83.0159\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 85.5804\n",
      "validation Loss: 0.0014 Acc: 83.0993\n",
      "Early stopped.\n",
      "Best val acc: 83.432516\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0052 Acc: 78.9268\n",
      "validation Loss: 0.0015 Acc: 82.2500\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 82.4266\n",
      "validation Loss: 0.0014 Acc: 81.9643\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 82.4772\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0039 Acc: 78.4507\n",
      "validation Loss: 0.0021 Acc: 74.8214\n",
      "Epoch 4/99\n",
      "training Loss: 0.0017 Acc: 80.4297\n",
      "validation Loss: 0.0016 Acc: 81.7262\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.5904\n",
      "validation Loss: 0.0015 Acc: 82.3690\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 81.6767\n",
      "validation Loss: 0.0015 Acc: 82.8690\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 81.5309\n",
      "validation Loss: 0.0015 Acc: 82.4048\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 82.1796\n",
      "validation Loss: 0.0015 Acc: 83.0119\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 82.2302\n",
      "validation Loss: 0.0015 Acc: 82.8452\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.2094\n",
      "validation Loss: 0.0015 Acc: 82.7500\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 81.8433\n",
      "validation Loss: 0.0015 Acc: 82.8333\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 82.2659\n",
      "validation Loss: 0.0015 Acc: 82.9405\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 82.4951\n",
      "validation Loss: 0.0015 Acc: 82.8690\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 82.0606\n",
      "validation Loss: 0.0016 Acc: 81.4881\n",
      "Epoch 15/99\n",
      "training Loss: 0.0016 Acc: 81.6797\n",
      "validation Loss: 0.0015 Acc: 83.0238\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 81.9296\n",
      "validation Loss: 0.0015 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 82.1558\n",
      "validation Loss: 0.0015 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 82.3255\n",
      "validation Loss: 0.0015 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 82.1648\n",
      "validation Loss: 0.0015 Acc: 82.8452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 82.1618\n",
      "validation Loss: 0.0015 Acc: 83.0714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 82.3671\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 82.4356\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 82.3374\n",
      "validation Loss: 0.0014 Acc: 83.0000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 82.3552\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 82.4713\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 82.3671\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 82.5070\n",
      "validation Loss: 0.0014 Acc: 83.1071\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 82.5487\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 82.7004\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 82.5814\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 31/99\n",
      "training Loss: 0.0015 Acc: 82.6022\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 82.6588\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 82.5487\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 82.6617\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 82.4891\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 82.6112\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 82.6469\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 82.4921\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Early stopped.\n",
      "Best val acc: 83.511905\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0050 Acc: 79.6084\n",
      "validation Loss: 0.0015 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 82.6945\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 82.9236\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.2569\n",
      "validation Loss: 0.0014 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.2212\n",
      "validation Loss: 0.0014 Acc: 83.5238\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 83.3700\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 83.4236\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 83.3641\n",
      "validation Loss: 0.0014 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 83.4891\n",
      "validation Loss: 0.0014 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.5040\n",
      "validation Loss: 0.0014 Acc: 82.7976\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 83.7807\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 83.9176\n",
      "validation Loss: 0.0014 Acc: 83.6190\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 83.9414\n",
      "validation Loss: 0.0014 Acc: 84.1310\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.9236\n",
      "validation Loss: 0.0014 Acc: 83.8333\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 83.7272\n",
      "validation Loss: 0.0013 Acc: 83.8095\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 83.9652\n",
      "validation Loss: 0.0014 Acc: 84.0357\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 84.0396\n",
      "validation Loss: 0.0013 Acc: 84.2024\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 84.1111\n",
      "validation Loss: 0.0014 Acc: 83.9286\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 83.9890\n",
      "validation Loss: 0.0013 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 84.0873\n",
      "validation Loss: 0.0014 Acc: 84.2143\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 84.1855\n",
      "validation Loss: 0.0014 Acc: 83.9524\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 84.2480\n",
      "validation Loss: 0.0014 Acc: 83.9762\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 84.4116\n",
      "validation Loss: 0.0014 Acc: 84.0357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 84.5664\n",
      "validation Loss: 0.0013 Acc: 83.9762\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 84.4295\n",
      "validation Loss: 0.0014 Acc: 83.9405\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 84.5337\n",
      "validation Loss: 0.0014 Acc: 84.0238\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 84.7063\n",
      "validation Loss: 0.0014 Acc: 84.1071\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 84.6021\n",
      "validation Loss: 0.0014 Acc: 84.1548\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 84.7271\n",
      "validation Loss: 0.0014 Acc: 83.9881\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 84.8491\n",
      "validation Loss: 0.0014 Acc: 84.0833\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 84.9146\n",
      "validation Loss: 0.0014 Acc: 83.8690\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 84.9146\n",
      "validation Loss: 0.0014 Acc: 83.7262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 85.0515\n",
      "validation Loss: 0.0014 Acc: 83.6548\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 85.0693\n",
      "validation Loss: 0.0014 Acc: 84.0119\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 85.2092\n",
      "validation Loss: 0.0014 Acc: 84.0119\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 85.3640\n",
      "validation Loss: 0.0014 Acc: 83.9167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 85.2509\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 85.4800\n",
      "validation Loss: 0.0014 Acc: 83.7857\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 85.3908\n",
      "validation Loss: 0.0014 Acc: 83.9167\n",
      "Early stopped.\n",
      "Best val acc: 84.238095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0134 Acc: 76.2663\n",
      "validation Loss: 0.0016 Acc: 81.2619\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 82.3433\n",
      "validation Loss: 0.0015 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0015 Acc: 82.5516\n",
      "validation Loss: 0.0014 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0015 Acc: 82.7570\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0018 Acc: 81.3225\n",
      "validation Loss: 0.0017 Acc: 79.8571\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 81.7303\n",
      "validation Loss: 0.0015 Acc: 82.5595\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 82.7302\n",
      "validation Loss: 0.0016 Acc: 82.9167\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 82.8730\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 83.0248\n",
      "validation Loss: 0.0015 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.2242\n",
      "validation Loss: 0.0015 Acc: 82.6905\n",
      "Epoch 10/99\n",
      "training Loss: 0.0015 Acc: 82.9355\n",
      "validation Loss: 0.0015 Acc: 82.0595\n",
      "Epoch 11/99\n",
      "training Loss: 0.0015 Acc: 82.7957\n",
      "validation Loss: 0.0015 Acc: 82.4405\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 82.8611\n",
      "validation Loss: 0.0014 Acc: 83.1190\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 83.0486\n",
      "validation Loss: 0.0015 Acc: 83.0833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 82.9296\n",
      "validation Loss: 0.0015 Acc: 83.0357\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 82.7153\n",
      "validation Loss: 0.0016 Acc: 82.9762\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 83.0665\n",
      "validation Loss: 0.0015 Acc: 83.2619\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 82.6558\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 83.1409\n",
      "validation Loss: 0.0015 Acc: 83.2500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 83.4355\n",
      "validation Loss: 0.0015 Acc: 83.0238\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 83.1558\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 83.5307\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 83.5218\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 83.5426\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 83.5129\n",
      "validation Loss: 0.0014 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 83.3760\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 83.4921\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 83.7182\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 83.7629\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 29/99\n",
      "training Loss: 0.0014 Acc: 83.9027\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 30/99\n",
      "training Loss: 0.0014 Acc: 83.7688\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Epoch 31/99\n",
      "training Loss: 0.0014 Acc: 83.8016\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Epoch 32/99\n",
      "training Loss: 0.0014 Acc: 83.8492\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 33/99\n",
      "training Loss: 0.0014 Acc: 83.8879\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 83.8164\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 83.8819\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 83.9950\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 83.9831\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 83.9117\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 83.9742\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 83.9712\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 84.0813\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 84.1081\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 84.1587\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 83.9623\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Early stopped.\n",
      "Best val acc: 83.571429\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0068 Acc: 79.5518\n",
      "validation Loss: 0.0015 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 83.0427\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 83.2689\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 83.4742\n",
      "validation Loss: 0.0014 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 83.4623\n",
      "validation Loss: 0.0014 Acc: 82.7143\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 83.5278\n",
      "validation Loss: 0.0014 Acc: 82.8095\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 83.5069\n",
      "validation Loss: 0.0015 Acc: 83.0476\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 83.6111\n",
      "validation Loss: 0.0014 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 83.6647\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 83.8700\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 83.7212\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 11/99\n",
      "training Loss: 0.0014 Acc: 83.7986\n",
      "validation Loss: 0.0014 Acc: 83.5119\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 83.7272\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 83.8283\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 84.1200\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 84.2212\n",
      "validation Loss: 0.0014 Acc: 83.1548\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 84.2033\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 84.2331\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 84.1736\n",
      "validation Loss: 0.0014 Acc: 83.1190\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 84.2152\n",
      "validation Loss: 0.0014 Acc: 82.9643\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 84.0813\n",
      "validation Loss: 0.0014 Acc: 83.2024\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 84.4712\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 84.4890\n",
      "validation Loss: 0.0014 Acc: 82.9048\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 84.5396\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 84.5634\n",
      "validation Loss: 0.0014 Acc: 83.1190\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 84.6884\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 84.6319\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 84.9235\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Early stopped.\n",
      "Best val acc: 83.595238\n",
      "----------\n",
      "Average best_acc across k-fold: 83.6698365469\n",
      "New configuration: {'learning_rate': 3.0282434066023172e-05, 'initial_nodes': 1000, 'dropout': 0.7795280637762906, 'batch_size': 512, 'max_depth': 5}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 49.9375\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 50.0000\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 50.5565\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 50.7113\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 50.7292\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 51.4375\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 52.0238\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 53.0298\n",
      "validation Loss: 0.0014 Acc: 50.0357\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 54.5893\n",
      "validation Loss: 0.0014 Acc: 50.2618\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 56.9107\n",
      "validation Loss: 0.0014 Acc: 55.4273\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 59.8899\n",
      "validation Loss: 0.0014 Acc: 74.3275\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 63.2708\n",
      "validation Loss: 0.0013 Acc: 81.0283\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 65.9821\n",
      "validation Loss: 0.0013 Acc: 81.5401\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0012 Acc: 69.4315\n",
      "validation Loss: 0.0012 Acc: 82.0162\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0012 Acc: 71.4435\n",
      "validation Loss: 0.0012 Acc: 82.3375\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0012 Acc: 74.0208\n",
      "validation Loss: 0.0012 Acc: 82.5637\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0012 Acc: 75.5446\n",
      "validation Loss: 0.0011 Acc: 82.5994\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 76.7560\n",
      "validation Loss: 0.0011 Acc: 82.7779\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 77.4315\n",
      "validation Loss: 0.0011 Acc: 82.7065\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 77.9732\n",
      "validation Loss: 0.0011 Acc: 82.7898\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 78.6101\n",
      "validation Loss: 0.0011 Acc: 82.8374\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 79.0268\n",
      "validation Loss: 0.0010 Acc: 82.8493\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 79.3185\n",
      "validation Loss: 0.0010 Acc: 82.8493\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 79.2946\n",
      "validation Loss: 0.0010 Acc: 82.8493\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 79.5030\n",
      "validation Loss: 0.0010 Acc: 82.8017\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 79.5804\n",
      "validation Loss: 0.0010 Acc: 82.5161\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 79.7530\n",
      "validation Loss: 0.0010 Acc: 82.4566\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 79.8363\n",
      "validation Loss: 0.0010 Acc: 82.5518\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 79.8006\n",
      "validation Loss: 0.0010 Acc: 82.3256\n",
      "Epoch 29/99\n",
      "training Loss: 0.0010 Acc: 79.8750\n",
      "validation Loss: 0.0010 Acc: 82.3494\n",
      "Epoch 30/99\n",
      "training Loss: 0.0010 Acc: 79.9762\n",
      "validation Loss: 0.0010 Acc: 82.6232\n",
      "Epoch 31/99\n",
      "training Loss: 0.0010 Acc: 79.9732\n",
      "validation Loss: 0.0010 Acc: 82.7660\n",
      "Epoch 32/99\n",
      "training Loss: 0.0010 Acc: 79.8006\n",
      "validation Loss: 0.0010 Acc: 82.6351\n",
      "Epoch 33/99\n",
      "training Loss: 0.0010 Acc: 79.8214\n",
      "validation Loss: 0.0010 Acc: 82.5518\n",
      "Epoch 34/99\n",
      "training Loss: 0.0010 Acc: 80.1042\n",
      "validation Loss: 0.0010 Acc: 82.6946\n",
      "Epoch 35/99\n",
      "training Loss: 0.0010 Acc: 80.1220\n",
      "validation Loss: 0.0010 Acc: 82.7065\n",
      "Epoch 36/99\n",
      "training Loss: 0.0010 Acc: 80.3988\n",
      "validation Loss: 0.0010 Acc: 82.7779\n",
      "Epoch 37/99\n",
      "training Loss: 0.0010 Acc: 80.2768\n",
      "validation Loss: 0.0010 Acc: 82.7660\n",
      "Epoch 38/99\n",
      "training Loss: 0.0010 Acc: 80.3452\n",
      "validation Loss: 0.0010 Acc: 82.8255\n",
      "Epoch 39/99\n",
      "training Loss: 0.0010 Acc: 80.5030\n",
      "validation Loss: 0.0010 Acc: 82.8136\n",
      "Epoch 40/99\n",
      "training Loss: 0.0010 Acc: 80.5655\n",
      "validation Loss: 0.0010 Acc: 82.6946\n",
      "Epoch 41/99\n",
      "training Loss: 0.0010 Acc: 80.4792\n",
      "validation Loss: 0.0010 Acc: 82.6708\n",
      "Early stopped.\n",
      "Best val acc: 82.849322\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 50.7857\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 51.2112\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 52.0237\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 52.8272\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 54.3212\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0013 Acc: 56.4788\n",
      "validation Loss: 0.0014 Acc: 50.9762\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 59.1036\n",
      "validation Loss: 0.0014 Acc: 74.6548\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 61.6809\n",
      "validation Loss: 0.0013 Acc: 77.3690\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 63.9694\n",
      "validation Loss: 0.0013 Acc: 78.1786\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 65.5705\n",
      "validation Loss: 0.0013 Acc: 78.8333\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0012 Acc: 67.8442\n",
      "validation Loss: 0.0013 Acc: 79.0595\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0012 Acc: 69.7369\n",
      "validation Loss: 0.0012 Acc: 79.4524\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0012 Acc: 71.4928\n",
      "validation Loss: 0.0012 Acc: 79.7024\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0012 Acc: 73.1445\n",
      "validation Loss: 0.0012 Acc: 79.9048\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0012 Acc: 74.3259\n",
      "validation Loss: 0.0012 Acc: 80.2262\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0012 Acc: 75.4092\n",
      "validation Loss: 0.0012 Acc: 80.3214\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0012 Acc: 75.8764\n",
      "validation Loss: 0.0012 Acc: 80.3929\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0012 Acc: 76.6472\n",
      "validation Loss: 0.0011 Acc: 80.6071\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 77.1918\n",
      "validation Loss: 0.0011 Acc: 80.7738\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 77.6739\n",
      "validation Loss: 0.0011 Acc: 80.8333\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 78.2096\n",
      "validation Loss: 0.0011 Acc: 80.7500\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 78.1144\n",
      "validation Loss: 0.0011 Acc: 80.8929\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 78.5608\n",
      "validation Loss: 0.0011 Acc: 80.7976\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 78.6828\n",
      "validation Loss: 0.0011 Acc: 80.9643\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 78.9030\n",
      "validation Loss: 0.0011 Acc: 80.8333\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 79.0965\n",
      "validation Loss: 0.0011 Acc: 80.9286\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 79.1709\n",
      "validation Loss: 0.0011 Acc: 81.0238\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 79.4000\n",
      "validation Loss: 0.0011 Acc: 81.0357\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 79.4685\n",
      "validation Loss: 0.0011 Acc: 81.2738\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 79.7572\n",
      "validation Loss: 0.0011 Acc: 81.0952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 79.6470\n",
      "validation Loss: 0.0010 Acc: 81.1071\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 79.6917\n",
      "validation Loss: 0.0010 Acc: 80.9881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 79.5875\n",
      "validation Loss: 0.0010 Acc: 81.5000\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 79.7988\n",
      "validation Loss: 0.0010 Acc: 81.3929\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 79.9476\n",
      "validation Loss: 0.0010 Acc: 81.6786\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0011 Acc: 79.8762\n",
      "validation Loss: 0.0010 Acc: 81.6310\n",
      "Epoch 36/99\n",
      "training Loss: 0.0011 Acc: 79.8732\n",
      "validation Loss: 0.0010 Acc: 81.7024\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0011 Acc: 79.7631\n",
      "validation Loss: 0.0010 Acc: 81.5714\n",
      "Epoch 38/99\n",
      "training Loss: 0.0011 Acc: 79.8970\n",
      "validation Loss: 0.0010 Acc: 81.7619\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0011 Acc: 79.6322\n",
      "validation Loss: 0.0010 Acc: 81.6786\n",
      "Epoch 40/99\n",
      "training Loss: 0.0010 Acc: 79.9417\n",
      "validation Loss: 0.0010 Acc: 81.7619\n",
      "Epoch 41/99\n",
      "training Loss: 0.0010 Acc: 80.1113\n",
      "validation Loss: 0.0010 Acc: 81.7024\n",
      "Epoch 42/99\n",
      "training Loss: 0.0010 Acc: 79.8732\n",
      "validation Loss: 0.0010 Acc: 81.6548\n",
      "Epoch 43/99\n",
      "training Loss: 0.0010 Acc: 80.0458\n",
      "validation Loss: 0.0010 Acc: 81.7976\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0010 Acc: 80.1321\n",
      "validation Loss: 0.0010 Acc: 81.7500\n",
      "Epoch 45/99\n",
      "training Loss: 0.0010 Acc: 80.1738\n",
      "validation Loss: 0.0010 Acc: 81.4405\n",
      "Epoch 46/99\n",
      "training Loss: 0.0010 Acc: 80.1440\n",
      "validation Loss: 0.0010 Acc: 81.7976\n",
      "Epoch 47/99\n",
      "training Loss: 0.0010 Acc: 80.4059\n",
      "validation Loss: 0.0010 Acc: 82.0595\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0010 Acc: 80.4654\n",
      "validation Loss: 0.0010 Acc: 81.7381\n",
      "Epoch 49/99\n",
      "training Loss: 0.0010 Acc: 80.2393\n",
      "validation Loss: 0.0010 Acc: 82.0714\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0010 Acc: 80.3018\n",
      "validation Loss: 0.0010 Acc: 82.3333\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0010 Acc: 80.2661\n",
      "validation Loss: 0.0010 Acc: 82.2738\n",
      "Epoch 52/99\n",
      "training Loss: 0.0010 Acc: 80.3107\n",
      "validation Loss: 0.0010 Acc: 82.0357\n",
      "Epoch 53/99\n",
      "training Loss: 0.0010 Acc: 80.8702\n",
      "validation Loss: 0.0010 Acc: 82.2976\n",
      "Epoch 54/99\n",
      "training Loss: 0.0010 Acc: 80.6886\n",
      "validation Loss: 0.0009 Acc: 82.2381\n",
      "Epoch 55/99\n",
      "training Loss: 0.0010 Acc: 80.6559\n",
      "validation Loss: 0.0009 Acc: 82.1190\n",
      "Epoch 56/99\n",
      "training Loss: 0.0010 Acc: 80.5994\n",
      "validation Loss: 0.0009 Acc: 82.2738\n",
      "Epoch 57/99\n",
      "training Loss: 0.0010 Acc: 80.7244\n",
      "validation Loss: 0.0009 Acc: 82.2738\n",
      "Epoch 58/99\n",
      "training Loss: 0.0010 Acc: 80.3315\n",
      "validation Loss: 0.0009 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0010 Acc: 80.5815\n",
      "validation Loss: 0.0009 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0010 Acc: 80.3375\n",
      "validation Loss: 0.0009 Acc: 82.4524\n",
      "Epoch 61/99\n",
      "training Loss: 0.0010 Acc: 80.6381\n",
      "validation Loss: 0.0009 Acc: 82.3929\n",
      "Epoch 62/99\n",
      "training Loss: 0.0010 Acc: 80.4059\n",
      "validation Loss: 0.0009 Acc: 82.3929\n",
      "Epoch 63/99\n",
      "training Loss: 0.0010 Acc: 80.5518\n",
      "validation Loss: 0.0009 Acc: 82.3690\n",
      "Epoch 64/99\n",
      "training Loss: 0.0010 Acc: 80.7750\n",
      "validation Loss: 0.0009 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0010 Acc: 80.8077\n",
      "validation Loss: 0.0009 Acc: 82.4524\n",
      "Epoch 66/99\n",
      "training Loss: 0.0010 Acc: 80.6678\n",
      "validation Loss: 0.0009 Acc: 82.4048\n",
      "Epoch 67/99\n",
      "training Loss: 0.0010 Acc: 80.6410\n",
      "validation Loss: 0.0009 Acc: 82.5476\n",
      "Epoch 68/99\n",
      "training Loss: 0.0010 Acc: 80.7839\n",
      "validation Loss: 0.0009 Acc: 82.5238\n",
      "Epoch 69/99\n",
      "training Loss: 0.0010 Acc: 80.9863\n",
      "validation Loss: 0.0009 Acc: 82.5714\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0010 Acc: 80.7750\n",
      "validation Loss: 0.0009 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0009 Acc: 80.7363\n",
      "validation Loss: 0.0009 Acc: 82.6310\n",
      "Epoch 72/99\n",
      "training Loss: 0.0010 Acc: 80.8732\n",
      "validation Loss: 0.0009 Acc: 82.6071\n",
      "Epoch 73/99\n",
      "training Loss: 0.0010 Acc: 80.7988\n",
      "validation Loss: 0.0009 Acc: 82.6071\n",
      "Epoch 74/99\n",
      "training Loss: 0.0009 Acc: 80.7928\n",
      "validation Loss: 0.0009 Acc: 82.6905\n",
      "Epoch 75/99\n",
      "training Loss: 0.0009 Acc: 80.6827\n",
      "validation Loss: 0.0009 Acc: 82.5952\n",
      "Epoch 76/99\n",
      "training Loss: 0.0009 Acc: 80.7125\n",
      "validation Loss: 0.0009 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0009 Acc: 80.6857\n",
      "validation Loss: 0.0009 Acc: 82.6429\n",
      "Epoch 78/99\n",
      "training Loss: 0.0009 Acc: 81.0487\n",
      "validation Loss: 0.0009 Acc: 82.6667\n",
      "Epoch 79/99\n",
      "training Loss: 0.0009 Acc: 80.9624\n",
      "validation Loss: 0.0009 Acc: 82.6310\n",
      "Epoch 80/99\n",
      "training Loss: 0.0009 Acc: 81.0220\n",
      "validation Loss: 0.0009 Acc: 82.6071\n",
      "Epoch 81/99\n",
      "training Loss: 0.0009 Acc: 81.0487\n",
      "validation Loss: 0.0009 Acc: 82.6667\n",
      "Epoch 82/99\n",
      "training Loss: 0.0009 Acc: 80.8553\n",
      "validation Loss: 0.0009 Acc: 82.6667\n",
      "Epoch 83/99\n",
      "training Loss: 0.0009 Acc: 81.1737\n",
      "validation Loss: 0.0009 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 84/99\n",
      "training Loss: 0.0009 Acc: 81.1112\n",
      "validation Loss: 0.0009 Acc: 82.7143\n",
      "Epoch 85/99\n",
      "training Loss: 0.0009 Acc: 80.9922\n",
      "validation Loss: 0.0009 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 86/99\n",
      "training Loss: 0.0009 Acc: 81.0071\n",
      "validation Loss: 0.0009 Acc: 82.7262\n",
      "Epoch 87/99\n",
      "training Loss: 0.0009 Acc: 81.4773\n",
      "validation Loss: 0.0009 Acc: 82.6905\n",
      "Epoch 88/99\n",
      "training Loss: 0.0009 Acc: 81.0904\n",
      "validation Loss: 0.0009 Acc: 82.7857\n",
      "Epoch 89/99\n",
      "training Loss: 0.0009 Acc: 81.2422\n",
      "validation Loss: 0.0009 Acc: 82.7381\n",
      "Epoch 90/99\n",
      "training Loss: 0.0009 Acc: 81.1470\n",
      "validation Loss: 0.0009 Acc: 82.8690\n",
      "Epoch 91/99\n",
      "training Loss: 0.0009 Acc: 81.1797\n",
      "validation Loss: 0.0009 Acc: 82.8452\n",
      "Epoch 92/99\n",
      "training Loss: 0.0009 Acc: 81.1231\n",
      "validation Loss: 0.0008 Acc: 82.8571\n",
      "Epoch 93/99\n",
      "training Loss: 0.0009 Acc: 81.2749\n",
      "validation Loss: 0.0008 Acc: 82.8452\n",
      "Epoch 94/99\n",
      "training Loss: 0.0009 Acc: 81.1589\n",
      "validation Loss: 0.0008 Acc: 82.7381\n",
      "Epoch 95/99\n",
      "training Loss: 0.0009 Acc: 81.2035\n",
      "validation Loss: 0.0008 Acc: 82.7619\n",
      "Epoch 96/99\n",
      "training Loss: 0.0009 Acc: 81.0636\n",
      "validation Loss: 0.0008 Acc: 82.8333\n",
      "Epoch 97/99\n",
      "training Loss: 0.0009 Acc: 81.2481\n",
      "validation Loss: 0.0008 Acc: 82.8214\n",
      "Epoch 98/99\n",
      "training Loss: 0.0009 Acc: 81.2928\n",
      "validation Loss: 0.0008 Acc: 82.8333\n",
      "Epoch 99/99\n",
      "training Loss: 0.0009 Acc: 81.0547\n",
      "validation Loss: 0.0008 Acc: 82.7619\n",
      "Best val acc: 82.892857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 50.3780\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 50.1637\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 50.5476\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 50.4583\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0013 Acc: 51.1458\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0013 Acc: 51.4463\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 53.5266\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 54.0682\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 55.6663\n",
      "validation Loss: 0.0013 Acc: 50.0952\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 57.8001\n",
      "validation Loss: 0.0013 Acc: 77.8929\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0012 Acc: 59.4191\n",
      "validation Loss: 0.0013 Acc: 79.1905\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0012 Acc: 61.8683\n",
      "validation Loss: 0.0013 Acc: 79.6667\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0012 Acc: 64.1271\n",
      "validation Loss: 0.0012 Acc: 79.9762\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0012 Acc: 66.1240\n",
      "validation Loss: 0.0012 Acc: 80.2262\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0012 Acc: 68.3650\n",
      "validation Loss: 0.0012 Acc: 80.3571\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0012 Acc: 70.3559\n",
      "validation Loss: 0.0012 Acc: 80.5476\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0012 Acc: 72.2100\n",
      "validation Loss: 0.0012 Acc: 80.7976\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0012 Acc: 73.6414\n",
      "validation Loss: 0.0012 Acc: 81.0476\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0012 Acc: 75.0074\n",
      "validation Loss: 0.0012 Acc: 81.1190\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0012 Acc: 76.1532\n",
      "validation Loss: 0.0011 Acc: 81.1310\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 77.1472\n",
      "validation Loss: 0.0011 Acc: 81.2262\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 77.9715\n",
      "validation Loss: 0.0011 Acc: 81.2024\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 78.7096\n",
      "validation Loss: 0.0011 Acc: 81.2857\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 78.8941\n",
      "validation Loss: 0.0011 Acc: 81.2500\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 78.9834\n",
      "validation Loss: 0.0011 Acc: 81.2619\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 78.9298\n",
      "validation Loss: 0.0011 Acc: 81.2738\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 79.6828\n",
      "validation Loss: 0.0011 Acc: 81.3333\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 79.6203\n",
      "validation Loss: 0.0011 Acc: 81.3452\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 79.9179\n",
      "validation Loss: 0.0011 Acc: 81.3810\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 79.8435\n",
      "validation Loss: 0.0011 Acc: 81.4405\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 79.7750\n",
      "validation Loss: 0.0011 Acc: 81.5357\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 79.8167\n",
      "validation Loss: 0.0011 Acc: 81.5952\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 79.9566\n",
      "validation Loss: 0.0010 Acc: 81.6190\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 80.0310\n",
      "validation Loss: 0.0010 Acc: 81.6548\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 80.3464\n",
      "validation Loss: 0.0010 Acc: 81.6786\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0011 Acc: 80.4565\n",
      "validation Loss: 0.0010 Acc: 81.6548\n",
      "Epoch 36/99\n",
      "training Loss: 0.0011 Acc: 80.1946\n",
      "validation Loss: 0.0010 Acc: 81.9167\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0011 Acc: 80.5190\n",
      "validation Loss: 0.0010 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0011 Acc: 80.4000\n",
      "validation Loss: 0.0010 Acc: 81.9881\n",
      "Epoch 39/99\n",
      "training Loss: 0.0011 Acc: 80.2601\n",
      "validation Loss: 0.0010 Acc: 82.0714\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0011 Acc: 80.4000\n",
      "validation Loss: 0.0010 Acc: 82.0952\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0011 Acc: 80.3762\n",
      "validation Loss: 0.0010 Acc: 82.1667\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0010 Acc: 80.3464\n",
      "validation Loss: 0.0010 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0010 Acc: 80.3821\n",
      "validation Loss: 0.0010 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0010 Acc: 80.2750\n",
      "validation Loss: 0.0010 Acc: 82.1548\n",
      "Epoch 45/99\n",
      "training Loss: 0.0010 Acc: 80.4476\n",
      "validation Loss: 0.0010 Acc: 82.2143\n",
      "Epoch 46/99\n",
      "training Loss: 0.0010 Acc: 80.2274\n",
      "validation Loss: 0.0010 Acc: 82.1310\n",
      "Epoch 47/99\n",
      "training Loss: 0.0010 Acc: 80.4119\n",
      "validation Loss: 0.0010 Acc: 82.2024\n",
      "Epoch 48/99\n",
      "training Loss: 0.0010 Acc: 80.5160\n",
      "validation Loss: 0.0010 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0010 Acc: 80.3643\n",
      "validation Loss: 0.0010 Acc: 82.2619\n",
      "Epoch 50/99\n",
      "training Loss: 0.0010 Acc: 80.5696\n",
      "validation Loss: 0.0010 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0010 Acc: 80.7452\n",
      "validation Loss: 0.0010 Acc: 82.3452\n",
      "Epoch 52/99\n",
      "training Loss: 0.0010 Acc: 80.8166\n",
      "validation Loss: 0.0010 Acc: 82.4167\n",
      "Epoch 53/99\n",
      "training Loss: 0.0010 Acc: 80.7035\n",
      "validation Loss: 0.0010 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0010 Acc: 80.8672\n",
      "validation Loss: 0.0010 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0010 Acc: 80.6648\n",
      "validation Loss: 0.0010 Acc: 82.5000\n",
      "Epoch 56/99\n",
      "training Loss: 0.0010 Acc: 80.7065\n",
      "validation Loss: 0.0009 Acc: 82.5357\n",
      "Epoch 57/99\n",
      "training Loss: 0.0010 Acc: 80.6113\n",
      "validation Loss: 0.0009 Acc: 82.6429\n",
      "Epoch 58/99\n",
      "training Loss: 0.0010 Acc: 80.8910\n",
      "validation Loss: 0.0009 Acc: 82.5476\n",
      "Epoch 59/99\n",
      "training Loss: 0.0010 Acc: 80.7244\n",
      "validation Loss: 0.0009 Acc: 82.3571\n",
      "Epoch 60/99\n",
      "training Loss: 0.0010 Acc: 80.9476\n",
      "validation Loss: 0.0009 Acc: 82.5833\n",
      "Epoch 61/99\n",
      "training Loss: 0.0010 Acc: 80.8791\n",
      "validation Loss: 0.0009 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0010 Acc: 80.8196\n",
      "validation Loss: 0.0009 Acc: 82.5833\n",
      "Epoch 63/99\n",
      "training Loss: 0.0010 Acc: 80.8613\n",
      "validation Loss: 0.0009 Acc: 82.5595\n",
      "Epoch 64/99\n",
      "training Loss: 0.0010 Acc: 80.8315\n",
      "validation Loss: 0.0009 Acc: 82.5476\n",
      "Epoch 65/99\n",
      "training Loss: 0.0010 Acc: 80.7779\n",
      "validation Loss: 0.0009 Acc: 82.7619\n",
      "Epoch 66/99\n",
      "training Loss: 0.0010 Acc: 80.7511\n",
      "validation Loss: 0.0009 Acc: 82.7500\n",
      "Epoch 67/99\n",
      "training Loss: 0.0010 Acc: 81.1291\n",
      "validation Loss: 0.0009 Acc: 82.6071\n",
      "Epoch 68/99\n",
      "training Loss: 0.0010 Acc: 80.9119\n",
      "validation Loss: 0.0009 Acc: 82.7500\n",
      "Epoch 69/99\n",
      "training Loss: 0.0010 Acc: 81.1053\n",
      "validation Loss: 0.0009 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0010 Acc: 81.0220\n",
      "validation Loss: 0.0009 Acc: 82.7738\n",
      "Epoch 71/99\n",
      "training Loss: 0.0010 Acc: 80.7511\n",
      "validation Loss: 0.0009 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 72/99\n",
      "training Loss: 0.0010 Acc: 81.0845\n",
      "validation Loss: 0.0009 Acc: 82.7381\n",
      "Epoch 73/99\n",
      "training Loss: 0.0010 Acc: 80.9297\n",
      "validation Loss: 0.0009 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0010 Acc: 81.1678\n",
      "validation Loss: 0.0009 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 75/99\n",
      "training Loss: 0.0010 Acc: 81.0398\n",
      "validation Loss: 0.0009 Acc: 82.8571\n",
      "Epoch 76/99\n",
      "training Loss: 0.0010 Acc: 81.2333\n",
      "validation Loss: 0.0009 Acc: 82.9286\n",
      "Epoch 77/99\n",
      "training Loss: 0.0009 Acc: 81.3761\n",
      "validation Loss: 0.0009 Acc: 82.8214\n",
      "Epoch 78/99\n",
      "training Loss: 0.0009 Acc: 81.0160\n",
      "validation Loss: 0.0009 Acc: 82.9167\n",
      "Epoch 79/99\n",
      "training Loss: 0.0009 Acc: 81.2095\n",
      "validation Loss: 0.0009 Acc: 82.6905\n",
      "Epoch 80/99\n",
      "training Loss: 0.0009 Acc: 81.1737\n",
      "validation Loss: 0.0009 Acc: 82.9643\n",
      "Epoch 81/99\n",
      "training Loss: 0.0009 Acc: 81.0785\n",
      "validation Loss: 0.0009 Acc: 82.9405\n",
      "Epoch 82/99\n",
      "training Loss: 0.0009 Acc: 81.2154\n",
      "validation Loss: 0.0009 Acc: 82.9048\n",
      "Epoch 83/99\n",
      "training Loss: 0.0009 Acc: 81.4148\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Epoch 84/99\n",
      "training Loss: 0.0009 Acc: 81.2541\n",
      "validation Loss: 0.0009 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 85/99\n",
      "training Loss: 0.0009 Acc: 81.3374\n",
      "validation Loss: 0.0009 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 86/99\n",
      "training Loss: 0.0009 Acc: 81.2481\n",
      "validation Loss: 0.0009 Acc: 82.8810\n",
      "Epoch 87/99\n",
      "training Loss: 0.0009 Acc: 81.2065\n",
      "validation Loss: 0.0009 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 88/99\n",
      "training Loss: 0.0009 Acc: 81.4356\n",
      "validation Loss: 0.0009 Acc: 83.0238\n",
      "Epoch 89/99\n",
      "training Loss: 0.0009 Acc: 81.2124\n",
      "validation Loss: 0.0009 Acc: 83.0833\n",
      "Epoch 90/99\n",
      "training Loss: 0.0009 Acc: 81.4773\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 91/99\n",
      "training Loss: 0.0009 Acc: 81.4178\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Epoch 92/99\n",
      "training Loss: 0.0009 Acc: 81.1648\n",
      "validation Loss: 0.0009 Acc: 83.0119\n",
      "Epoch 93/99\n",
      "training Loss: 0.0009 Acc: 81.3791\n",
      "validation Loss: 0.0009 Acc: 82.9643\n",
      "Epoch 94/99\n",
      "training Loss: 0.0009 Acc: 81.4178\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0009 Acc: 81.2868\n",
      "validation Loss: 0.0008 Acc: 83.1548\n",
      "Epoch 96/99\n",
      "training Loss: 0.0009 Acc: 81.2065\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Epoch 97/99\n",
      "training Loss: 0.0009 Acc: 81.2809\n",
      "validation Loss: 0.0008 Acc: 83.0119\n",
      "Epoch 98/99\n",
      "training Loss: 0.0009 Acc: 81.3553\n",
      "validation Loss: 0.0008 Acc: 83.1310\n",
      "Epoch 99/99\n",
      "training Loss: 0.0009 Acc: 81.5874\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Best val acc: 83.178571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 50.0030\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 50.2559\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 49.8869\n",
      "validation Loss: 0.0014 Acc: 50.0595\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 50.2530\n",
      "validation Loss: 0.0014 Acc: 50.3214\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 50.3422\n",
      "validation Loss: 0.0014 Acc: 51.6548\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 50.3155\n",
      "validation Loss: 0.0014 Acc: 61.3452\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 51.0297\n",
      "validation Loss: 0.0014 Acc: 72.6190\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0014 Acc: 51.0357\n",
      "validation Loss: 0.0014 Acc: 77.3452\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0014 Acc: 51.2380\n",
      "validation Loss: 0.0014 Acc: 77.2143\n",
      "Epoch 9/99\n",
      "training Loss: 0.0014 Acc: 52.6814\n",
      "validation Loss: 0.0014 Acc: 77.0952\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 52.9463\n",
      "validation Loss: 0.0014 Acc: 76.4286\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 54.8688\n",
      "validation Loss: 0.0014 Acc: 76.1786\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 56.2824\n",
      "validation Loss: 0.0013 Acc: 76.7024\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 58.4846\n",
      "validation Loss: 0.0013 Acc: 78.1548\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 60.8684\n",
      "validation Loss: 0.0013 Acc: 79.1310\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0012 Acc: 63.7313\n",
      "validation Loss: 0.0012 Acc: 79.8571\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0012 Acc: 66.2937\n",
      "validation Loss: 0.0012 Acc: 80.5238\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0012 Acc: 68.3977\n",
      "validation Loss: 0.0012 Acc: 80.9643\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0012 Acc: 70.6238\n",
      "validation Loss: 0.0012 Acc: 81.1786\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0012 Acc: 71.8767\n",
      "validation Loss: 0.0011 Acc: 81.3929\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0012 Acc: 74.1503\n",
      "validation Loss: 0.0011 Acc: 81.6548\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0012 Acc: 75.1711\n",
      "validation Loss: 0.0011 Acc: 81.7381\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0012 Acc: 76.4032\n",
      "validation Loss: 0.0011 Acc: 81.7857\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 76.8377\n",
      "validation Loss: 0.0011 Acc: 81.8095\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 77.4924\n",
      "validation Loss: 0.0011 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 77.9061\n",
      "validation Loss: 0.0011 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 78.6590\n",
      "validation Loss: 0.0011 Acc: 82.0595\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 78.7661\n",
      "validation Loss: 0.0011 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 78.8971\n",
      "validation Loss: 0.0011 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 78.8941\n",
      "validation Loss: 0.0011 Acc: 82.0952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 78.9536\n",
      "validation Loss: 0.0011 Acc: 82.0476\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 79.1739\n",
      "validation Loss: 0.0010 Acc: 82.0357\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 78.7364\n",
      "validation Loss: 0.0010 Acc: 82.0238\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 79.3971\n",
      "validation Loss: 0.0010 Acc: 82.2857\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 79.1024\n",
      "validation Loss: 0.0010 Acc: 82.1071\n",
      "Epoch 35/99\n",
      "training Loss: 0.0011 Acc: 79.3435\n",
      "validation Loss: 0.0010 Acc: 81.9643\n",
      "Epoch 36/99\n",
      "training Loss: 0.0011 Acc: 79.4119\n",
      "validation Loss: 0.0010 Acc: 82.1905\n",
      "Epoch 37/99\n",
      "training Loss: 0.0011 Acc: 79.5935\n",
      "validation Loss: 0.0010 Acc: 82.2024\n",
      "Epoch 38/99\n",
      "training Loss: 0.0011 Acc: 79.6351\n",
      "validation Loss: 0.0010 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0011 Acc: 79.3643\n",
      "validation Loss: 0.0010 Acc: 82.2976\n",
      "Epoch 40/99\n",
      "training Loss: 0.0011 Acc: 79.6828\n",
      "validation Loss: 0.0010 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0011 Acc: 79.5667\n",
      "validation Loss: 0.0010 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0011 Acc: 79.6232\n",
      "validation Loss: 0.0010 Acc: 82.4167\n",
      "Epoch 43/99\n",
      "training Loss: 0.0011 Acc: 79.5369\n",
      "validation Loss: 0.0010 Acc: 82.3929\n",
      "Epoch 44/99\n",
      "training Loss: 0.0011 Acc: 79.7750\n",
      "validation Loss: 0.0010 Acc: 82.4643\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0010 Acc: 79.9952\n",
      "validation Loss: 0.0010 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0011 Acc: 80.0161\n",
      "validation Loss: 0.0010 Acc: 82.3214\n",
      "Epoch 47/99\n",
      "training Loss: 0.0010 Acc: 79.9000\n",
      "validation Loss: 0.0010 Acc: 82.4881\n",
      "Epoch 48/99\n",
      "training Loss: 0.0010 Acc: 80.0220\n",
      "validation Loss: 0.0010 Acc: 82.4048\n",
      "Epoch 49/99\n",
      "training Loss: 0.0010 Acc: 79.8583\n",
      "validation Loss: 0.0010 Acc: 82.5000\n",
      "Epoch 50/99\n",
      "training Loss: 0.0010 Acc: 79.8732\n",
      "validation Loss: 0.0010 Acc: 82.3810\n",
      "Epoch 51/99\n",
      "training Loss: 0.0010 Acc: 79.7810\n",
      "validation Loss: 0.0010 Acc: 82.3929\n",
      "Epoch 52/99\n",
      "training Loss: 0.0010 Acc: 80.2006\n",
      "validation Loss: 0.0010 Acc: 82.3452\n",
      "Epoch 53/99\n",
      "training Loss: 0.0010 Acc: 79.9625\n",
      "validation Loss: 0.0010 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0010 Acc: 80.1708\n",
      "validation Loss: 0.0010 Acc: 82.5714\n",
      "Epoch 55/99\n",
      "training Loss: 0.0010 Acc: 80.0220\n",
      "validation Loss: 0.0009 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0010 Acc: 80.3970\n",
      "validation Loss: 0.0010 Acc: 82.4881\n",
      "Epoch 57/99\n",
      "training Loss: 0.0010 Acc: 80.0934\n",
      "validation Loss: 0.0009 Acc: 82.5952\n",
      "Epoch 58/99\n",
      "training Loss: 0.0010 Acc: 79.9833\n",
      "validation Loss: 0.0009 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0010 Acc: 80.2155\n",
      "validation Loss: 0.0009 Acc: 82.6429\n",
      "Epoch 60/99\n",
      "training Loss: 0.0010 Acc: 80.2750\n",
      "validation Loss: 0.0009 Acc: 82.4762\n",
      "Epoch 61/99\n",
      "training Loss: 0.0010 Acc: 80.2512\n",
      "validation Loss: 0.0009 Acc: 82.7143\n",
      "Epoch 62/99\n",
      "training Loss: 0.0010 Acc: 80.3464\n",
      "validation Loss: 0.0009 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0010 Acc: 80.1708\n",
      "validation Loss: 0.0009 Acc: 82.7738\n",
      "Epoch 64/99\n",
      "training Loss: 0.0010 Acc: 80.1292\n",
      "validation Loss: 0.0009 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0010 Acc: 80.5309\n",
      "validation Loss: 0.0009 Acc: 82.8452\n",
      "Epoch 66/99\n",
      "training Loss: 0.0010 Acc: 80.0071\n",
      "validation Loss: 0.0009 Acc: 82.7500\n",
      "Epoch 67/99\n",
      "training Loss: 0.0010 Acc: 80.1738\n",
      "validation Loss: 0.0009 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0010 Acc: 80.5994\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0010 Acc: 80.2899\n",
      "validation Loss: 0.0009 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0010 Acc: 80.3613\n",
      "validation Loss: 0.0009 Acc: 83.0357\n",
      "Epoch 71/99\n",
      "training Loss: 0.0010 Acc: 80.4030\n",
      "validation Loss: 0.0009 Acc: 82.9405\n",
      "Epoch 72/99\n",
      "training Loss: 0.0010 Acc: 80.0756\n",
      "validation Loss: 0.0009 Acc: 82.9405\n",
      "Epoch 73/99\n",
      "training Loss: 0.0010 Acc: 80.3791\n",
      "validation Loss: 0.0009 Acc: 83.0000\n",
      "Epoch 74/99\n",
      "training Loss: 0.0010 Acc: 80.5220\n",
      "validation Loss: 0.0009 Acc: 83.0119\n",
      "Epoch 75/99\n",
      "training Loss: 0.0010 Acc: 80.5458\n",
      "validation Loss: 0.0009 Acc: 83.0238\n",
      "Epoch 76/99\n",
      "training Loss: 0.0010 Acc: 80.5547\n",
      "validation Loss: 0.0009 Acc: 83.0238\n",
      "Epoch 77/99\n",
      "training Loss: 0.0010 Acc: 80.4565\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Epoch 78/99\n",
      "training Loss: 0.0010 Acc: 80.7006\n",
      "validation Loss: 0.0009 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0010 Acc: 80.3345\n",
      "validation Loss: 0.0009 Acc: 83.0357\n",
      "Epoch 80/99\n",
      "training Loss: 0.0010 Acc: 80.7363\n",
      "validation Loss: 0.0009 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 81/99\n",
      "training Loss: 0.0010 Acc: 80.9267\n",
      "validation Loss: 0.0009 Acc: 83.0595\n",
      "Epoch 82/99\n",
      "training Loss: 0.0010 Acc: 80.6440\n",
      "validation Loss: 0.0009 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0010 Acc: 80.5428\n",
      "validation Loss: 0.0009 Acc: 83.0357\n",
      "Epoch 84/99\n",
      "training Loss: 0.0010 Acc: 80.6916\n",
      "validation Loss: 0.0009 Acc: 82.9762\n",
      "Epoch 85/99\n",
      "training Loss: 0.0010 Acc: 80.6589\n",
      "validation Loss: 0.0009 Acc: 83.0714\n",
      "Epoch 86/99\n",
      "training Loss: 0.0010 Acc: 80.8970\n",
      "validation Loss: 0.0009 Acc: 83.0119\n",
      "Epoch 87/99\n",
      "training Loss: 0.0010 Acc: 80.6321\n",
      "validation Loss: 0.0009 Acc: 83.0714\n",
      "Epoch 88/99\n",
      "training Loss: 0.0009 Acc: 80.4595\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Epoch 89/99\n",
      "training Loss: 0.0010 Acc: 80.8285\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Epoch 90/99\n",
      "training Loss: 0.0009 Acc: 80.7660\n",
      "validation Loss: 0.0009 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 91/99\n",
      "training Loss: 0.0009 Acc: 80.8017\n",
      "validation Loss: 0.0009 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 92/99\n",
      "training Loss: 0.0009 Acc: 80.6440\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Epoch 93/99\n",
      "training Loss: 0.0010 Acc: 80.6232\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Epoch 94/99\n",
      "training Loss: 0.0009 Acc: 80.9386\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 95/99\n",
      "training Loss: 0.0009 Acc: 80.8017\n",
      "validation Loss: 0.0008 Acc: 83.2024\n",
      "Epoch 96/99\n",
      "training Loss: 0.0009 Acc: 80.7690\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 97/99\n",
      "training Loss: 0.0009 Acc: 81.0845\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 98/99\n",
      "training Loss: 0.0009 Acc: 81.0101\n",
      "validation Loss: 0.0008 Acc: 83.2143\n",
      "Epoch 99/99\n",
      "training Loss: 0.0009 Acc: 80.6500\n",
      "validation Loss: 0.0008 Acc: 83.1190\n",
      "Best val acc: 83.488095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 49.6101\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 49.3096\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 49.3840\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0014 Acc: 49.1459\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0014 Acc: 49.0358\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0014 Acc: 48.6965\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0014 Acc: 48.9643\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 48.5953\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 48.3781\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 48.6608\n",
      "validation Loss: 0.0014 Acc: 50.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 48.9584\n",
      "validation Loss: 0.0013 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 49.3393\n",
      "validation Loss: 0.0013 Acc: 50.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 50.0000\n",
      "validation Loss: 0.0013 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0012 Acc: 50.5297\n",
      "validation Loss: 0.0012 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0012 Acc: 51.6070\n",
      "validation Loss: 0.0012 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0012 Acc: 52.2915\n",
      "validation Loss: 0.0012 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0012 Acc: 53.1576\n",
      "validation Loss: 0.0012 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0012 Acc: 54.3658\n",
      "validation Loss: 0.0012 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0012 Acc: 54.9729\n",
      "validation Loss: 0.0012 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0012 Acc: 56.3389\n",
      "validation Loss: 0.0011 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0012 Acc: 57.3150\n",
      "validation Loss: 0.0011 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 50.000000\n",
      "----------\n",
      "Average best_acc across k-fold: 76.4817690799\n",
      "New configuration: {'learning_rate': 0.05214450230845288, 'initial_nodes': 564, 'dropout': 0.01834733121142005, 'batch_size': 333, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 79.2619\n",
      "validation Loss: 0.0012 Acc: 83.0993\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0011 Acc: 83.1220\n",
      "validation Loss: 0.0012 Acc: 83.8134\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0011 Acc: 83.1964\n",
      "validation Loss: 0.0012 Acc: 83.9800\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0011 Acc: 83.3899\n",
      "validation Loss: 0.0012 Acc: 83.5277\n",
      "Epoch 4/99\n",
      "training Loss: 0.0011 Acc: 83.3690\n",
      "validation Loss: 0.0012 Acc: 83.7539\n",
      "Epoch 5/99\n",
      "training Loss: 0.0011 Acc: 83.5208\n",
      "validation Loss: 0.0011 Acc: 83.9086\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.5238\n",
      "validation Loss: 0.0011 Acc: 83.4087\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.4732\n",
      "validation Loss: 0.0012 Acc: 83.4087\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.5982\n",
      "validation Loss: 0.0011 Acc: 84.0395\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.7351\n",
      "validation Loss: 0.0011 Acc: 84.1823\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.7649\n",
      "validation Loss: 0.0011 Acc: 83.7301\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.7232\n",
      "validation Loss: 0.0011 Acc: 84.4323\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.6399\n",
      "validation Loss: 0.0018 Acc: 83.5634\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.4911\n",
      "validation Loss: 0.0011 Acc: 83.6110\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.5506\n",
      "validation Loss: 0.0012 Acc: 84.3014\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.6101\n",
      "validation Loss: 0.0011 Acc: 83.9919\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.6280\n",
      "validation Loss: 0.0012 Acc: 83.8253\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.7381\n",
      "validation Loss: 0.0012 Acc: 83.8848\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 84.0863\n",
      "validation Loss: 0.0011 Acc: 83.9324\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.3185\n",
      "validation Loss: 0.0011 Acc: 84.1347\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.2679\n",
      "validation Loss: 0.0011 Acc: 83.9205\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.4405\n",
      "validation Loss: 0.0011 Acc: 83.9205\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.3571\n",
      "validation Loss: 0.0011 Acc: 83.9681\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.5387\n",
      "validation Loss: 0.0012 Acc: 83.8372\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.8036\n",
      "validation Loss: 0.0011 Acc: 83.6587\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.9077\n",
      "validation Loss: 0.0011 Acc: 83.7420\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 84.9435\n",
      "validation Loss: 0.0011 Acc: 83.8729\n",
      "Epoch 27/99\n",
      "training Loss: 0.0010 Acc: 85.0565\n",
      "validation Loss: 0.0012 Acc: 83.5158\n",
      "Epoch 28/99\n",
      "training Loss: 0.0010 Acc: 85.0506\n",
      "validation Loss: 0.0011 Acc: 83.6944\n",
      "Epoch 29/99\n",
      "training Loss: 0.0010 Acc: 85.2083\n",
      "validation Loss: 0.0011 Acc: 84.0276\n",
      "Epoch 30/99\n",
      "training Loss: 0.0010 Acc: 85.4077\n",
      "validation Loss: 0.0012 Acc: 83.9443\n",
      "Epoch 31/99\n",
      "training Loss: 0.0010 Acc: 85.5952\n",
      "validation Loss: 0.0012 Acc: 83.8134\n",
      "Early stopped.\n",
      "Best val acc: 84.432278\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0033 Acc: 78.7721\n",
      "validation Loss: 0.0012 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0011 Acc: 82.7986\n",
      "validation Loss: 0.0011 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0011 Acc: 83.3165\n",
      "validation Loss: 0.0011 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0011 Acc: 83.2629\n",
      "validation Loss: 0.0011 Acc: 83.4881\n",
      "Epoch 4/99\n",
      "training Loss: 0.0011 Acc: 83.2927\n",
      "validation Loss: 0.0011 Acc: 83.6071\n",
      "Epoch 5/99\n",
      "training Loss: 0.0011 Acc: 83.2331\n",
      "validation Loss: 0.0011 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.4623\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.4534\n",
      "validation Loss: 0.0011 Acc: 84.3690\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.5099\n",
      "validation Loss: 0.0011 Acc: 83.7262\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.6379\n",
      "validation Loss: 0.0011 Acc: 83.9405\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.4593\n",
      "validation Loss: 0.0011 Acc: 84.1905\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.6022\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.4980\n",
      "validation Loss: 0.0011 Acc: 83.9048\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.6706\n",
      "validation Loss: 0.0011 Acc: 84.2738\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.6647\n",
      "validation Loss: 0.0011 Acc: 84.0238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.8135\n",
      "validation Loss: 0.0011 Acc: 84.2738\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.8968\n",
      "validation Loss: 0.0011 Acc: 84.2381\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 84.1795\n",
      "validation Loss: 0.0011 Acc: 84.2857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 84.1378\n",
      "validation Loss: 0.0011 Acc: 84.2857\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.2569\n",
      "validation Loss: 0.0011 Acc: 84.2381\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.2658\n",
      "validation Loss: 0.0011 Acc: 84.0714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.1527\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.1557\n",
      "validation Loss: 0.0011 Acc: 84.0833\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.5396\n",
      "validation Loss: 0.0011 Acc: 84.1310\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.7569\n",
      "validation Loss: 0.0011 Acc: 84.3452\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.6021\n",
      "validation Loss: 0.0011 Acc: 84.0357\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 84.7271\n",
      "validation Loss: 0.0011 Acc: 83.9167\n",
      "Epoch 27/99\n",
      "training Loss: 0.0010 Acc: 84.7301\n",
      "validation Loss: 0.0011 Acc: 84.0595\n",
      "Early stopped.\n",
      "Best val acc: 84.369048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 78.8495\n",
      "validation Loss: 0.0012 Acc: 81.4286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0011 Acc: 83.1974\n",
      "validation Loss: 0.0012 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0011 Acc: 83.5843\n",
      "validation Loss: 0.0011 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0011 Acc: 83.5337\n",
      "validation Loss: 0.0012 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0011 Acc: 83.6319\n",
      "validation Loss: 0.0012 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0011 Acc: 83.7658\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.6498\n",
      "validation Loss: 0.0012 Acc: 82.6429\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.8641\n",
      "validation Loss: 0.0012 Acc: 81.9405\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.6885\n",
      "validation Loss: 0.0012 Acc: 82.7976\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.6230\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.9861\n",
      "validation Loss: 0.0012 Acc: 83.1548\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.6557\n",
      "validation Loss: 0.0011 Acc: 82.9762\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.8641\n",
      "validation Loss: 0.0012 Acc: 83.1667\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.9146\n",
      "validation Loss: 0.0011 Acc: 83.3095\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.8373\n",
      "validation Loss: 0.0011 Acc: 83.2738\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.9861\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.8075\n",
      "validation Loss: 0.0011 Acc: 83.0952\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 84.2688\n",
      "validation Loss: 0.0011 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 83.7897\n",
      "validation Loss: 0.0011 Acc: 82.6429\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 84.1825\n",
      "validation Loss: 0.0012 Acc: 83.1310\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 83.9117\n",
      "validation Loss: 0.0012 Acc: 83.2500\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.4563\n",
      "validation Loss: 0.0011 Acc: 83.0119\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.4890\n",
      "validation Loss: 0.0011 Acc: 83.3810\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.5694\n",
      "validation Loss: 0.0011 Acc: 82.8333\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.6765\n",
      "validation Loss: 0.0012 Acc: 83.0000\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.5337\n",
      "validation Loss: 0.0011 Acc: 83.1071\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 84.7985\n",
      "validation Loss: 0.0011 Acc: 83.1548\n",
      "Epoch 27/99\n",
      "training Loss: 0.0010 Acc: 84.8759\n",
      "validation Loss: 0.0012 Acc: 83.0238\n",
      "Epoch 28/99\n",
      "training Loss: 0.0010 Acc: 84.7747\n",
      "validation Loss: 0.0011 Acc: 82.7619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0010 Acc: 84.9949\n",
      "validation Loss: 0.0012 Acc: 83.3571\n",
      "Epoch 30/99\n",
      "training Loss: 0.0010 Acc: 84.7955\n",
      "validation Loss: 0.0012 Acc: 83.2024\n",
      "Epoch 31/99\n",
      "training Loss: 0.0010 Acc: 84.9622\n",
      "validation Loss: 0.0012 Acc: 83.0238\n",
      "Epoch 32/99\n",
      "training Loss: 0.0010 Acc: 85.2062\n",
      "validation Loss: 0.0011 Acc: 83.1190\n",
      "Epoch 33/99\n",
      "training Loss: 0.0010 Acc: 85.3431\n",
      "validation Loss: 0.0012 Acc: 83.0714\n",
      "Epoch 34/99\n",
      "training Loss: 0.0010 Acc: 85.4324\n",
      "validation Loss: 0.0012 Acc: 83.0833\n",
      "Epoch 35/99\n",
      "training Loss: 0.0010 Acc: 85.4473\n",
      "validation Loss: 0.0012 Acc: 83.1310\n",
      "Epoch 36/99\n",
      "training Loss: 0.0010 Acc: 85.5544\n",
      "validation Loss: 0.0012 Acc: 82.9643\n",
      "Epoch 37/99\n",
      "training Loss: 0.0010 Acc: 85.5991\n",
      "validation Loss: 0.0012 Acc: 83.0238\n",
      "Early stopped.\n",
      "Best val acc: 83.428571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 78.2602\n",
      "validation Loss: 0.0012 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0011 Acc: 82.8701\n",
      "validation Loss: 0.0012 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0011 Acc: 83.4266\n",
      "validation Loss: 0.0012 Acc: 83.2857\n",
      "Epoch 3/99\n",
      "training Loss: 0.0011 Acc: 83.4177\n",
      "validation Loss: 0.0011 Acc: 83.2143\n",
      "Epoch 4/99\n",
      "training Loss: 0.0011 Acc: 83.6081\n",
      "validation Loss: 0.0011 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0011 Acc: 83.7212\n",
      "validation Loss: 0.0011 Acc: 83.2976\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.6944\n",
      "validation Loss: 0.0011 Acc: 83.3571\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.7153\n",
      "validation Loss: 0.0012 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.5486\n",
      "validation Loss: 0.0012 Acc: 82.7381\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.4504\n",
      "validation Loss: 0.0012 Acc: 83.1190\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.6825\n",
      "validation Loss: 0.0012 Acc: 83.0476\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.6290\n",
      "validation Loss: 0.0011 Acc: 83.0714\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.6409\n",
      "validation Loss: 0.0012 Acc: 83.2738\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.7361\n",
      "validation Loss: 0.0011 Acc: 83.7619\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.8224\n",
      "validation Loss: 0.0012 Acc: 83.4881\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.7480\n",
      "validation Loss: 0.0012 Acc: 82.8571\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.7420\n",
      "validation Loss: 0.0012 Acc: 83.3571\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.7123\n",
      "validation Loss: 0.0012 Acc: 83.2738\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 84.2063\n",
      "validation Loss: 0.0011 Acc: 83.5238\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.3908\n",
      "validation Loss: 0.0011 Acc: 83.3452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.3938\n",
      "validation Loss: 0.0012 Acc: 83.6071\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.4355\n",
      "validation Loss: 0.0012 Acc: 83.1786\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.5426\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.6438\n",
      "validation Loss: 0.0012 Acc: 83.5476\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.8640\n",
      "validation Loss: 0.0011 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.9146\n",
      "validation Loss: 0.0011 Acc: 83.5357\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 85.0783\n",
      "validation Loss: 0.0012 Acc: 83.2024\n",
      "Epoch 27/99\n",
      "training Loss: 0.0010 Acc: 85.0217\n",
      "validation Loss: 0.0012 Acc: 83.3452\n",
      "Epoch 28/99\n",
      "training Loss: 0.0010 Acc: 85.1943\n",
      "validation Loss: 0.0012 Acc: 83.4881\n",
      "Epoch 29/99\n",
      "training Loss: 0.0010 Acc: 85.1705\n",
      "validation Loss: 0.0011 Acc: 83.5952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0010 Acc: 85.3967\n",
      "validation Loss: 0.0012 Acc: 83.6667\n",
      "Epoch 31/99\n",
      "training Loss: 0.0010 Acc: 85.5366\n",
      "validation Loss: 0.0012 Acc: 83.4881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0010 Acc: 85.6229\n",
      "validation Loss: 0.0012 Acc: 83.4881\n",
      "Epoch 33/99\n",
      "training Loss: 0.0010 Acc: 85.5455\n",
      "validation Loss: 0.0012 Acc: 83.4167\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 85.6259\n",
      "validation Loss: 0.0012 Acc: 83.2738\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 85.5455\n",
      "validation Loss: 0.0012 Acc: 83.4167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.8461\n",
      "validation Loss: 0.0012 Acc: 83.3929\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.9235\n",
      "validation Loss: 0.0012 Acc: 83.2857\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 86.0246\n",
      "validation Loss: 0.0012 Acc: 83.3095\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 86.1020\n",
      "validation Loss: 0.0012 Acc: 83.5357\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 86.0633\n",
      "validation Loss: 0.0012 Acc: 83.5833\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 86.0157\n",
      "validation Loss: 0.0012 Acc: 83.4286\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 86.2449\n",
      "validation Loss: 0.0012 Acc: 83.5595\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 86.2062\n",
      "validation Loss: 0.0012 Acc: 83.5357\n",
      "Epoch 44/99\n",
      "training Loss: 0.0009 Acc: 86.3639\n",
      "validation Loss: 0.0012 Acc: 83.4524\n",
      "Early stopped.\n",
      "Best val acc: 83.785714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0034 Acc: 79.6500\n",
      "validation Loss: 0.0013 Acc: 81.9167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 82.7302\n",
      "validation Loss: 0.0012 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0011 Acc: 83.2510\n",
      "validation Loss: 0.0012 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0011 Acc: 83.5932\n",
      "validation Loss: 0.0012 Acc: 82.3571\n",
      "Epoch 4/99\n",
      "training Loss: 0.0011 Acc: 83.3284\n",
      "validation Loss: 0.0012 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0011 Acc: 83.5843\n",
      "validation Loss: 0.0011 Acc: 83.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.5129\n",
      "validation Loss: 0.0012 Acc: 82.7857\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.7123\n",
      "validation Loss: 0.0012 Acc: 81.9881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.6914\n",
      "validation Loss: 0.0012 Acc: 83.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.6795\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.7242\n",
      "validation Loss: 0.0011 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.7718\n",
      "validation Loss: 0.0011 Acc: 82.7857\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 84.0426\n",
      "validation Loss: 0.0011 Acc: 83.2500\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.9474\n",
      "validation Loss: 0.0012 Acc: 82.8333\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.8551\n",
      "validation Loss: 0.0012 Acc: 82.6190\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.9266\n",
      "validation Loss: 0.0012 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.8373\n",
      "validation Loss: 0.0012 Acc: 83.0476\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 84.2866\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 84.3253\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.5366\n",
      "validation Loss: 0.0011 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.5396\n",
      "validation Loss: 0.0011 Acc: 83.1310\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.6259\n",
      "validation Loss: 0.0011 Acc: 83.3571\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.6259\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.6973\n",
      "validation Loss: 0.0011 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.6140\n",
      "validation Loss: 0.0011 Acc: 83.0833\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 85.0128\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 85.1675\n",
      "validation Loss: 0.0011 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0010 Acc: 85.0902\n",
      "validation Loss: 0.0011 Acc: 83.0714\n",
      "Epoch 28/99\n",
      "training Loss: 0.0010 Acc: 85.2658\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 29/99\n",
      "training Loss: 0.0010 Acc: 85.3878\n",
      "validation Loss: 0.0011 Acc: 83.1429\n",
      "Epoch 30/99\n",
      "training Loss: 0.0010 Acc: 85.2777\n",
      "validation Loss: 0.0011 Acc: 83.1548\n",
      "Epoch 31/99\n",
      "training Loss: 0.0010 Acc: 85.2092\n",
      "validation Loss: 0.0012 Acc: 82.7500\n",
      "Epoch 32/99\n",
      "training Loss: 0.0010 Acc: 85.5187\n",
      "validation Loss: 0.0012 Acc: 83.2381\n",
      "Epoch 33/99\n",
      "training Loss: 0.0010 Acc: 85.5931\n",
      "validation Loss: 0.0012 Acc: 83.2738\n",
      "Epoch 34/99\n",
      "training Loss: 0.0010 Acc: 85.7568\n",
      "validation Loss: 0.0012 Acc: 83.0000\n",
      "Epoch 35/99\n",
      "training Loss: 0.0010 Acc: 85.8729\n",
      "validation Loss: 0.0011 Acc: 83.0714\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 85.8758\n",
      "validation Loss: 0.0011 Acc: 83.2381\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 85.8223\n",
      "validation Loss: 0.0012 Acc: 83.4167\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 86.0663\n",
      "validation Loss: 0.0012 Acc: 83.3929\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 86.1734\n",
      "validation Loss: 0.0012 Acc: 83.1667\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 86.1169\n",
      "validation Loss: 0.0012 Acc: 83.2976\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 86.1615\n",
      "validation Loss: 0.0012 Acc: 83.1310\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 86.2984\n",
      "validation Loss: 0.0012 Acc: 83.2738\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 86.1437\n",
      "validation Loss: 0.0012 Acc: 83.0952\n",
      "Epoch 44/99\n",
      "training Loss: 0.0009 Acc: 86.3609\n",
      "validation Loss: 0.0012 Acc: 83.2976\n",
      "Epoch 45/99\n",
      "training Loss: 0.0009 Acc: 86.3044\n",
      "validation Loss: 0.0012 Acc: 83.2024\n",
      "Epoch 46/99\n",
      "training Loss: 0.0009 Acc: 86.4472\n",
      "validation Loss: 0.0012 Acc: 83.2738\n",
      "Early stopped.\n",
      "Best val acc: 83.500000\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9031222725\n",
      "New configuration: {'learning_rate': 3.191434517494956e-05, 'initial_nodes': 857, 'dropout': 0.8974887520619912, 'batch_size': 273, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0027 Acc: 49.9315\n",
      "validation Loss: 0.0025 Acc: 75.0536\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 51.0119\n",
      "validation Loss: 0.0025 Acc: 76.3271\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0026 Acc: 52.3333\n",
      "validation Loss: 0.0025 Acc: 76.7793\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0025 Acc: 53.6250\n",
      "validation Loss: 0.0025 Acc: 77.6601\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 54.7679\n",
      "validation Loss: 0.0025 Acc: 78.1600\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 56.9375\n",
      "validation Loss: 0.0024 Acc: 78.4218\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0024 Acc: 59.3839\n",
      "validation Loss: 0.0024 Acc: 78.7670\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0024 Acc: 62.8958\n",
      "validation Loss: 0.0023 Acc: 79.1121\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 67.0149\n",
      "validation Loss: 0.0023 Acc: 79.4692\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0023 Acc: 70.4702\n",
      "validation Loss: 0.0022 Acc: 79.9333\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0022 Acc: 73.6667\n",
      "validation Loss: 0.0022 Acc: 79.9453\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0022 Acc: 76.1131\n",
      "validation Loss: 0.0021 Acc: 80.2190\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0022 Acc: 77.5357\n",
      "validation Loss: 0.0021 Acc: 80.4927\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 78.1042\n",
      "validation Loss: 0.0021 Acc: 80.6594\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 79.1369\n",
      "validation Loss: 0.0020 Acc: 80.6951\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 79.2411\n",
      "validation Loss: 0.0020 Acc: 80.8022\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0021 Acc: 79.3452\n",
      "validation Loss: 0.0020 Acc: 80.7546\n",
      "Epoch 17/99\n",
      "training Loss: 0.0021 Acc: 79.4732\n",
      "validation Loss: 0.0020 Acc: 80.7903\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 79.7946\n",
      "validation Loss: 0.0020 Acc: 80.7665\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 79.6756\n",
      "validation Loss: 0.0020 Acc: 80.7308\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 80.2887\n",
      "validation Loss: 0.0019 Acc: 80.8141\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 80.1310\n",
      "validation Loss: 0.0019 Acc: 80.7308\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 80.2440\n",
      "validation Loss: 0.0019 Acc: 80.7784\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 80.3988\n",
      "validation Loss: 0.0019 Acc: 80.5403\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 80.1756\n",
      "validation Loss: 0.0019 Acc: 80.5046\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 80.0060\n",
      "validation Loss: 0.0019 Acc: 80.7308\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 80.3214\n",
      "validation Loss: 0.0019 Acc: 80.8260\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 80.6250\n",
      "validation Loss: 0.0018 Acc: 80.8855\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 80.3780\n",
      "validation Loss: 0.0018 Acc: 81.0878\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 80.5565\n",
      "validation Loss: 0.0018 Acc: 81.0878\n",
      "Epoch 30/99\n",
      "training Loss: 0.0019 Acc: 80.4494\n",
      "validation Loss: 0.0018 Acc: 80.9926\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 80.5893\n",
      "validation Loss: 0.0018 Acc: 81.1473\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0019 Acc: 80.8452\n",
      "validation Loss: 0.0018 Acc: 81.2545\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 80.6012\n",
      "validation Loss: 0.0018 Acc: 81.0640\n",
      "Epoch 34/99\n",
      "training Loss: 0.0019 Acc: 80.7470\n",
      "validation Loss: 0.0018 Acc: 81.2664\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0019 Acc: 80.5833\n",
      "validation Loss: 0.0018 Acc: 81.1950\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 80.5208\n",
      "validation Loss: 0.0017 Acc: 81.5044\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 80.4345\n",
      "validation Loss: 0.0017 Acc: 81.3973\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 80.5357\n",
      "validation Loss: 0.0017 Acc: 81.4925\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 80.6994\n",
      "validation Loss: 0.0017 Acc: 81.6591\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 80.8690\n",
      "validation Loss: 0.0017 Acc: 81.6710\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 81.0298\n",
      "validation Loss: 0.0017 Acc: 81.6353\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 80.8423\n",
      "validation Loss: 0.0017 Acc: 81.8615\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 80.8006\n",
      "validation Loss: 0.0017 Acc: 81.8258\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 80.7619\n",
      "validation Loss: 0.0017 Acc: 81.8258\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 81.0625\n",
      "validation Loss: 0.0017 Acc: 81.8615\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 81.0804\n",
      "validation Loss: 0.0016 Acc: 82.0281\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 80.7768\n",
      "validation Loss: 0.0016 Acc: 82.1828\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 81.3601\n",
      "validation Loss: 0.0016 Acc: 82.2423\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 81.0089\n",
      "validation Loss: 0.0016 Acc: 82.1709\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 81.2411\n",
      "validation Loss: 0.0016 Acc: 82.2780\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 81.1042\n",
      "validation Loss: 0.0016 Acc: 82.3256\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0018 Acc: 81.0952\n",
      "validation Loss: 0.0016 Acc: 82.3851\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0017 Acc: 81.1637\n",
      "validation Loss: 0.0016 Acc: 82.3375\n",
      "Epoch 54/99\n",
      "training Loss: 0.0017 Acc: 81.3839\n",
      "validation Loss: 0.0016 Acc: 82.3256\n",
      "Epoch 55/99\n",
      "training Loss: 0.0017 Acc: 81.4762\n",
      "validation Loss: 0.0016 Acc: 82.3018\n",
      "Epoch 56/99\n",
      "training Loss: 0.0017 Acc: 81.4018\n",
      "validation Loss: 0.0016 Acc: 82.3732\n",
      "Epoch 57/99\n",
      "training Loss: 0.0017 Acc: 81.0060\n",
      "validation Loss: 0.0016 Acc: 82.3613\n",
      "Epoch 58/99\n",
      "training Loss: 0.0017 Acc: 81.0625\n",
      "validation Loss: 0.0016 Acc: 82.4566\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0017 Acc: 81.0982\n",
      "validation Loss: 0.0016 Acc: 82.4566\n",
      "Epoch 60/99\n",
      "training Loss: 0.0017 Acc: 81.4196\n",
      "validation Loss: 0.0016 Acc: 82.5518\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0017 Acc: 81.4315\n",
      "validation Loss: 0.0016 Acc: 82.7303\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0017 Acc: 81.2530\n",
      "validation Loss: 0.0016 Acc: 82.6708\n",
      "Epoch 63/99\n",
      "training Loss: 0.0017 Acc: 81.2113\n",
      "validation Loss: 0.0016 Acc: 82.5994\n",
      "Epoch 64/99\n",
      "training Loss: 0.0017 Acc: 81.1875\n",
      "validation Loss: 0.0016 Acc: 82.5875\n",
      "Epoch 65/99\n",
      "training Loss: 0.0017 Acc: 81.3304\n",
      "validation Loss: 0.0016 Acc: 82.6470\n",
      "Epoch 66/99\n",
      "training Loss: 0.0017 Acc: 81.6667\n",
      "validation Loss: 0.0015 Acc: 82.6351\n",
      "Epoch 67/99\n",
      "training Loss: 0.0017 Acc: 81.3720\n",
      "validation Loss: 0.0015 Acc: 82.8255\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0017 Acc: 81.5417\n",
      "validation Loss: 0.0015 Acc: 82.7541\n",
      "Epoch 69/99\n",
      "training Loss: 0.0017 Acc: 81.4851\n",
      "validation Loss: 0.0015 Acc: 82.8850\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0017 Acc: 81.5417\n",
      "validation Loss: 0.0015 Acc: 82.7065\n",
      "Epoch 71/99\n",
      "training Loss: 0.0017 Acc: 81.3452\n",
      "validation Loss: 0.0015 Acc: 82.8136\n",
      "Epoch 72/99\n",
      "training Loss: 0.0017 Acc: 81.3155\n",
      "validation Loss: 0.0015 Acc: 82.7184\n",
      "Epoch 73/99\n",
      "training Loss: 0.0017 Acc: 81.3720\n",
      "validation Loss: 0.0015 Acc: 82.7184\n",
      "Epoch 74/99\n",
      "training Loss: 0.0017 Acc: 81.3839\n",
      "validation Loss: 0.0015 Acc: 82.7779\n",
      "Epoch 75/99\n",
      "training Loss: 0.0017 Acc: 81.5595\n",
      "validation Loss: 0.0015 Acc: 82.7779\n",
      "Epoch 76/99\n",
      "training Loss: 0.0017 Acc: 81.4851\n",
      "validation Loss: 0.0015 Acc: 82.8969\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0017 Acc: 81.4494\n",
      "validation Loss: 0.0015 Acc: 82.8493\n",
      "Epoch 78/99\n",
      "training Loss: 0.0017 Acc: 81.5685\n",
      "validation Loss: 0.0015 Acc: 82.8731\n",
      "Epoch 79/99\n",
      "training Loss: 0.0017 Acc: 81.5536\n",
      "validation Loss: 0.0015 Acc: 83.0040\n",
      "Saving..\n",
      "Epoch 80/99\n",
      "training Loss: 0.0017 Acc: 81.4940\n",
      "validation Loss: 0.0015 Acc: 82.9207\n",
      "Epoch 81/99\n",
      "training Loss: 0.0017 Acc: 81.7232\n",
      "validation Loss: 0.0015 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0017 Acc: 81.3512\n",
      "validation Loss: 0.0015 Acc: 83.0636\n",
      "Epoch 83/99\n",
      "training Loss: 0.0017 Acc: 81.6875\n",
      "validation Loss: 0.0015 Acc: 82.9088\n",
      "Epoch 84/99\n",
      "training Loss: 0.0017 Acc: 81.7768\n",
      "validation Loss: 0.0015 Acc: 82.8850\n",
      "Epoch 85/99\n",
      "training Loss: 0.0016 Acc: 81.9077\n",
      "validation Loss: 0.0015 Acc: 83.0159\n",
      "Epoch 86/99\n",
      "training Loss: 0.0017 Acc: 81.6994\n",
      "validation Loss: 0.0015 Acc: 82.9326\n",
      "Epoch 87/99\n",
      "training Loss: 0.0017 Acc: 81.5804\n",
      "validation Loss: 0.0015 Acc: 82.9088\n",
      "Epoch 88/99\n",
      "training Loss: 0.0016 Acc: 81.8214\n",
      "validation Loss: 0.0015 Acc: 82.9564\n",
      "Epoch 89/99\n",
      "training Loss: 0.0017 Acc: 81.4762\n",
      "validation Loss: 0.0015 Acc: 82.9326\n",
      "Epoch 90/99\n",
      "training Loss: 0.0016 Acc: 81.6131\n",
      "validation Loss: 0.0015 Acc: 82.9683\n",
      "Epoch 91/99\n",
      "training Loss: 0.0016 Acc: 81.7321\n",
      "validation Loss: 0.0015 Acc: 83.0993\n",
      "Saving..\n",
      "Epoch 92/99\n",
      "training Loss: 0.0017 Acc: 81.5208\n",
      "validation Loss: 0.0015 Acc: 83.0755\n",
      "Epoch 93/99\n",
      "training Loss: 0.0016 Acc: 81.6190\n",
      "validation Loss: 0.0015 Acc: 82.9564\n",
      "Epoch 94/99\n",
      "training Loss: 0.0016 Acc: 81.6756\n",
      "validation Loss: 0.0015 Acc: 82.9564\n",
      "Epoch 95/99\n",
      "training Loss: 0.0016 Acc: 81.7857\n",
      "validation Loss: 0.0015 Acc: 83.0279\n",
      "Epoch 96/99\n",
      "training Loss: 0.0016 Acc: 81.7946\n",
      "validation Loss: 0.0015 Acc: 83.0279\n",
      "Epoch 97/99\n",
      "training Loss: 0.0016 Acc: 81.8899\n",
      "validation Loss: 0.0015 Acc: 83.0398\n",
      "Epoch 98/99\n",
      "training Loss: 0.0016 Acc: 81.5565\n",
      "validation Loss: 0.0015 Acc: 82.9802\n",
      "Epoch 99/99\n",
      "training Loss: 0.0016 Acc: 81.7381\n",
      "validation Loss: 0.0015 Acc: 83.0993\n",
      "Best val acc: 83.099262\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0027 Acc: 50.0744\n",
      "validation Loss: 0.0025 Acc: 53.3571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 51.1755\n",
      "validation Loss: 0.0025 Acc: 60.4524\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0026 Acc: 51.9910\n",
      "validation Loss: 0.0025 Acc: 69.3571\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 52.7111\n",
      "validation Loss: 0.0025 Acc: 75.6905\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 53.9194\n",
      "validation Loss: 0.0025 Acc: 77.9643\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 55.3658\n",
      "validation Loss: 0.0025 Acc: 78.7738\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 57.0710\n",
      "validation Loss: 0.0024 Acc: 79.4405\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0024 Acc: 59.0114\n",
      "validation Loss: 0.0024 Acc: 79.7619\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0024 Acc: 60.4845\n",
      "validation Loss: 0.0023 Acc: 79.9643\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 62.5528\n",
      "validation Loss: 0.0022 Acc: 80.0833\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 64.9515\n",
      "validation Loss: 0.0022 Acc: 80.3929\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0023 Acc: 66.3562\n",
      "validation Loss: 0.0021 Acc: 80.6548\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0022 Acc: 68.1715\n",
      "validation Loss: 0.0020 Acc: 80.8571\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0022 Acc: 69.8470\n",
      "validation Loss: 0.0019 Acc: 80.8929\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 71.3678\n",
      "validation Loss: 0.0018 Acc: 81.2262\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 72.9272\n",
      "validation Loss: 0.0017 Acc: 81.5833\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 73.7278\n",
      "validation Loss: 0.0017 Acc: 81.9286\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 75.0283\n",
      "validation Loss: 0.0016 Acc: 82.1310\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 75.8883\n",
      "validation Loss: 0.0016 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 76.5609\n",
      "validation Loss: 0.0015 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 76.6770\n",
      "validation Loss: 0.0015 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 77.2127\n",
      "validation Loss: 0.0015 Acc: 82.2143\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 78.2007\n",
      "validation Loss: 0.0015 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 78.3793\n",
      "validation Loss: 0.0015 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 78.7066\n",
      "validation Loss: 0.0015 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0017 Acc: 79.0578\n",
      "validation Loss: 0.0014 Acc: 82.4286\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 79.1173\n",
      "validation Loss: 0.0015 Acc: 82.4643\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 79.3197\n",
      "validation Loss: 0.0014 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 79.7720\n",
      "validation Loss: 0.0014 Acc: 82.6190\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 79.9030\n",
      "validation Loss: 0.0014 Acc: 82.6190\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 79.8613\n",
      "validation Loss: 0.0014 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 80.1321\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 80.2244\n",
      "validation Loss: 0.0014 Acc: 82.7976\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 80.3375\n",
      "validation Loss: 0.0014 Acc: 82.8214\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 80.6172\n",
      "validation Loss: 0.0014 Acc: 82.9048\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 80.8047\n",
      "validation Loss: 0.0014 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 81.0130\n",
      "validation Loss: 0.0014 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 80.7958\n",
      "validation Loss: 0.0014 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 80.9535\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 80.9208\n",
      "validation Loss: 0.0014 Acc: 82.9762\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 81.2362\n",
      "validation Loss: 0.0014 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 81.5041\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 81.2630\n",
      "validation Loss: 0.0014 Acc: 83.0238\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 81.2809\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 81.5636\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 81.6856\n",
      "validation Loss: 0.0014 Acc: 83.0595\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 81.7243\n",
      "validation Loss: 0.0014 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 81.7124\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 81.6975\n",
      "validation Loss: 0.0014 Acc: 83.1190\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 81.6767\n",
      "validation Loss: 0.0014 Acc: 83.0714\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 81.7035\n",
      "validation Loss: 0.0014 Acc: 83.0595\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 81.8493\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 52/99\n",
      "training Loss: 0.0016 Acc: 81.6320\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Epoch 53/99\n",
      "training Loss: 0.0016 Acc: 82.0398\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 81.7868\n",
      "validation Loss: 0.0014 Acc: 83.1548\n",
      "Epoch 55/99\n",
      "training Loss: 0.0016 Acc: 81.5934\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 56/99\n",
      "training Loss: 0.0016 Acc: 81.8642\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0016 Acc: 81.9832\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0016 Acc: 81.8910\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Epoch 59/99\n",
      "training Loss: 0.0016 Acc: 82.2005\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0016 Acc: 81.7838\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 61/99\n",
      "training Loss: 0.0016 Acc: 82.1528\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 62/99\n",
      "training Loss: 0.0016 Acc: 81.8672\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 63/99\n",
      "training Loss: 0.0015 Acc: 82.2392\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 64/99\n",
      "training Loss: 0.0016 Acc: 82.1082\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 65/99\n",
      "training Loss: 0.0015 Acc: 82.2630\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 66/99\n",
      "training Loss: 0.0015 Acc: 82.3493\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 67/99\n",
      "training Loss: 0.0016 Acc: 82.2481\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 68/99\n",
      "training Loss: 0.0015 Acc: 82.3701\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 69/99\n",
      "training Loss: 0.0015 Acc: 82.3612\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0015 Acc: 82.2064\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 71/99\n",
      "training Loss: 0.0016 Acc: 82.2362\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 72/99\n",
      "training Loss: 0.0015 Acc: 82.3731\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 73/99\n",
      "training Loss: 0.0015 Acc: 82.2094\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 74/99\n",
      "training Loss: 0.0015 Acc: 82.2243\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 75/99\n",
      "training Loss: 0.0015 Acc: 82.4058\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 76/99\n",
      "training Loss: 0.0015 Acc: 82.2302\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 77/99\n",
      "training Loss: 0.0015 Acc: 82.2808\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 78/99\n",
      "training Loss: 0.0015 Acc: 82.2868\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 79/99\n",
      "training Loss: 0.0015 Acc: 82.4475\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 80/99\n",
      "training Loss: 0.0015 Acc: 82.5665\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 81/99\n",
      "training Loss: 0.0015 Acc: 82.3284\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 82/99\n",
      "training Loss: 0.0015 Acc: 82.4266\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0015 Acc: 82.2838\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 84/99\n",
      "training Loss: 0.0015 Acc: 82.6856\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Epoch 85/99\n",
      "training Loss: 0.0015 Acc: 82.5457\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 86/99\n",
      "training Loss: 0.0015 Acc: 82.5754\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 87/99\n",
      "training Loss: 0.0015 Acc: 82.7094\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 88/99\n",
      "training Loss: 0.0015 Acc: 82.4147\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 89/99\n",
      "training Loss: 0.0015 Acc: 82.6201\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 90/99\n",
      "training Loss: 0.0015 Acc: 82.8969\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 91/99\n",
      "training Loss: 0.0015 Acc: 82.5516\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 92/99\n",
      "training Loss: 0.0015 Acc: 82.7302\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 93/99\n",
      "training Loss: 0.0015 Acc: 82.6796\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 94/99\n",
      "training Loss: 0.0015 Acc: 82.5903\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 95/99\n",
      "training Loss: 0.0015 Acc: 82.6647\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 96/99\n",
      "training Loss: 0.0015 Acc: 82.6826\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 97/99\n",
      "training Loss: 0.0015 Acc: 82.7986\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 98/99\n",
      "training Loss: 0.0015 Acc: 82.7808\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 99/99\n",
      "training Loss: 0.0015 Acc: 82.6647\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Saving..\n",
      "Best val acc: 83.500000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0027 Acc: 50.3393\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 50.0476\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0026 Acc: 50.4881\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0025 Acc: 51.1458\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 51.5088\n",
      "validation Loss: 0.0025 Acc: 50.0119\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 52.8927\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0024 Acc: 53.6545\n",
      "validation Loss: 0.0024 Acc: 50.0714\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0024 Acc: 56.3955\n",
      "validation Loss: 0.0023 Acc: 76.8929\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 60.2107\n",
      "validation Loss: 0.0022 Acc: 80.3571\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 63.9754\n",
      "validation Loss: 0.0022 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0022 Acc: 68.3412\n",
      "validation Loss: 0.0021 Acc: 82.1429\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 72.0136\n",
      "validation Loss: 0.0021 Acc: 82.2500\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 75.1681\n",
      "validation Loss: 0.0020 Acc: 82.2381\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 77.3347\n",
      "validation Loss: 0.0020 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 78.8167\n",
      "validation Loss: 0.0020 Acc: 82.0238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 79.6322\n",
      "validation Loss: 0.0019 Acc: 81.9048\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 80.2601\n",
      "validation Loss: 0.0019 Acc: 81.9167\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 80.3762\n",
      "validation Loss: 0.0019 Acc: 81.8929\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 80.5904\n",
      "validation Loss: 0.0019 Acc: 81.8690\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 80.8047\n",
      "validation Loss: 0.0018 Acc: 81.8571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 80.9743\n",
      "validation Loss: 0.0018 Acc: 81.7857\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 81.0547\n",
      "validation Loss: 0.0018 Acc: 81.8333\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 81.0368\n",
      "validation Loss: 0.0018 Acc: 81.7619\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 80.9476\n",
      "validation Loss: 0.0018 Acc: 81.7262\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 80.6738\n",
      "validation Loss: 0.0018 Acc: 81.7500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 80.9803\n",
      "validation Loss: 0.0017 Acc: 81.8214\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 80.8940\n",
      "validation Loss: 0.0017 Acc: 81.8095\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 80.8851\n",
      "validation Loss: 0.0017 Acc: 81.7738\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 80.9148\n",
      "validation Loss: 0.0017 Acc: 81.7500\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 80.8523\n",
      "validation Loss: 0.0017 Acc: 81.8214\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 81.0607\n",
      "validation Loss: 0.0017 Acc: 81.8333\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 81.0607\n",
      "validation Loss: 0.0017 Acc: 81.8214\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 81.1380\n",
      "validation Loss: 0.0016 Acc: 81.9286\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 81.0190\n",
      "validation Loss: 0.0016 Acc: 82.0476\n",
      "Early stopped.\n",
      "Best val acc: 82.357143\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0027 Acc: 50.4910\n",
      "validation Loss: 0.0025 Acc: 68.9048\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 50.8630\n",
      "validation Loss: 0.0025 Acc: 78.0952\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0026 Acc: 51.3571\n",
      "validation Loss: 0.0025 Acc: 79.1667\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0026 Acc: 52.2588\n",
      "validation Loss: 0.0025 Acc: 79.2738\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 53.3837\n",
      "validation Loss: 0.0025 Acc: 79.1786\n",
      "Epoch 5/99\n",
      "training Loss: 0.0025 Acc: 54.7348\n",
      "validation Loss: 0.0025 Acc: 79.6071\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0025 Acc: 56.1693\n",
      "validation Loss: 0.0024 Acc: 79.6071\n",
      "Epoch 7/99\n",
      "training Loss: 0.0025 Acc: 57.3210\n",
      "validation Loss: 0.0024 Acc: 79.6786\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0024 Acc: 59.7673\n",
      "validation Loss: 0.0023 Acc: 79.7738\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0024 Acc: 61.7791\n",
      "validation Loss: 0.0022 Acc: 80.0714\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 63.6093\n",
      "validation Loss: 0.0021 Acc: 80.4048\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0022 Acc: 66.9543\n",
      "validation Loss: 0.0020 Acc: 80.6310\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0022 Acc: 68.7221\n",
      "validation Loss: 0.0019 Acc: 80.9881\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 70.9184\n",
      "validation Loss: 0.0018 Acc: 81.1071\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 72.3201\n",
      "validation Loss: 0.0017 Acc: 81.3929\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 73.8676\n",
      "validation Loss: 0.0017 Acc: 81.5714\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 75.1890\n",
      "validation Loss: 0.0016 Acc: 81.8333\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 75.7068\n",
      "validation Loss: 0.0016 Acc: 81.8571\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 76.5490\n",
      "validation Loss: 0.0016 Acc: 82.0119\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 77.0906\n",
      "validation Loss: 0.0015 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 77.9358\n",
      "validation Loss: 0.0015 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 78.3436\n",
      "validation Loss: 0.0015 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 78.5935\n",
      "validation Loss: 0.0015 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0017 Acc: 78.9804\n",
      "validation Loss: 0.0015 Acc: 82.5000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0017 Acc: 79.1441\n",
      "validation Loss: 0.0015 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0017 Acc: 79.4655\n",
      "validation Loss: 0.0015 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 79.6530\n",
      "validation Loss: 0.0015 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 80.0458\n",
      "validation Loss: 0.0014 Acc: 82.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 80.2780\n",
      "validation Loss: 0.0014 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 80.2393\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 80.1887\n",
      "validation Loss: 0.0014 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 80.3226\n",
      "validation Loss: 0.0014 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 80.5488\n",
      "validation Loss: 0.0014 Acc: 83.0238\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 80.6113\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 81.0874\n",
      "validation Loss: 0.0014 Acc: 82.9762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 80.8494\n",
      "validation Loss: 0.0014 Acc: 83.0476\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 81.1529\n",
      "validation Loss: 0.0014 Acc: 83.0357\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 81.1916\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 81.1678\n",
      "validation Loss: 0.0014 Acc: 83.0952\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 81.2095\n",
      "validation Loss: 0.0014 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 81.1708\n",
      "validation Loss: 0.0014 Acc: 83.0238\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 81.3463\n",
      "validation Loss: 0.0014 Acc: 83.0714\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 81.0636\n",
      "validation Loss: 0.0014 Acc: 83.1429\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 81.5428\n",
      "validation Loss: 0.0014 Acc: 83.1190\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 81.3463\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 81.5695\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 81.8761\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 81.5874\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 81.5755\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 81.6588\n",
      "validation Loss: 0.0014 Acc: 83.3333\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 81.6082\n",
      "validation Loss: 0.0014 Acc: 83.2381\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 81.8910\n",
      "validation Loss: 0.0014 Acc: 83.2262\n",
      "Epoch 52/99\n",
      "training Loss: 0.0016 Acc: 81.7749\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Epoch 53/99\n",
      "training Loss: 0.0016 Acc: 81.8195\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 54/99\n",
      "training Loss: 0.0016 Acc: 81.7779\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 55/99\n",
      "training Loss: 0.0016 Acc: 81.7749\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0016 Acc: 81.6559\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 57/99\n",
      "training Loss: 0.0016 Acc: 81.7838\n",
      "validation Loss: 0.0014 Acc: 83.1786\n",
      "Epoch 58/99\n",
      "training Loss: 0.0016 Acc: 82.0011\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 59/99\n",
      "training Loss: 0.0016 Acc: 82.1142\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 60/99\n",
      "training Loss: 0.0016 Acc: 81.8969\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 61/99\n",
      "training Loss: 0.0015 Acc: 81.9713\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Epoch 62/99\n",
      "training Loss: 0.0015 Acc: 82.1737\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 63/99\n",
      "training Loss: 0.0016 Acc: 82.0546\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 64/99\n",
      "training Loss: 0.0015 Acc: 82.1648\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0015 Acc: 82.0814\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 66/99\n",
      "training Loss: 0.0015 Acc: 82.0457\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 67/99\n",
      "training Loss: 0.0015 Acc: 82.3880\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 68/99\n",
      "training Loss: 0.0015 Acc: 82.1737\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 69/99\n",
      "training Loss: 0.0016 Acc: 82.3582\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 70/99\n",
      "training Loss: 0.0016 Acc: 82.2213\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 71/99\n",
      "training Loss: 0.0015 Acc: 82.2511\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 72/99\n",
      "training Loss: 0.0015 Acc: 82.1677\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 73/99\n",
      "training Loss: 0.0016 Acc: 82.0874\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0015 Acc: 82.2153\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 75/99\n",
      "training Loss: 0.0016 Acc: 82.2659\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0015 Acc: 82.1023\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0015 Acc: 82.5278\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Epoch 78/99\n",
      "training Loss: 0.0015 Acc: 82.2927\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 79/99\n",
      "training Loss: 0.0015 Acc: 82.4326\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 80/99\n",
      "training Loss: 0.0015 Acc: 82.3463\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 81/99\n",
      "training Loss: 0.0015 Acc: 82.1261\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 82/99\n",
      "training Loss: 0.0015 Acc: 82.5308\n",
      "validation Loss: 0.0014 Acc: 83.4643\n",
      "Epoch 83/99\n",
      "training Loss: 0.0015 Acc: 82.5576\n",
      "validation Loss: 0.0014 Acc: 83.3690\n",
      "Epoch 84/99\n",
      "training Loss: 0.0015 Acc: 82.3016\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 85/99\n",
      "training Loss: 0.0015 Acc: 82.4385\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 86/99\n",
      "training Loss: 0.0015 Acc: 82.5308\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 87/99\n",
      "training Loss: 0.0015 Acc: 82.2719\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 88/99\n",
      "training Loss: 0.0015 Acc: 82.4088\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Epoch 89/99\n",
      "training Loss: 0.0015 Acc: 82.4088\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 90/99\n",
      "training Loss: 0.0015 Acc: 82.5606\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 91/99\n",
      "training Loss: 0.0015 Acc: 82.3880\n",
      "validation Loss: 0.0014 Acc: 83.5357\n",
      "Epoch 92/99\n",
      "training Loss: 0.0015 Acc: 82.6052\n",
      "validation Loss: 0.0014 Acc: 83.5476\n",
      "Epoch 93/99\n",
      "training Loss: 0.0015 Acc: 82.3284\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 94/99\n",
      "training Loss: 0.0015 Acc: 82.2600\n",
      "validation Loss: 0.0014 Acc: 83.4762\n",
      "Epoch 95/99\n",
      "training Loss: 0.0015 Acc: 82.3641\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 96/99\n",
      "training Loss: 0.0015 Acc: 82.5219\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 97/99\n",
      "training Loss: 0.0015 Acc: 82.4921\n",
      "validation Loss: 0.0014 Acc: 83.4167\n",
      "Epoch 98/99\n",
      "training Loss: 0.0015 Acc: 82.5963\n",
      "validation Loss: 0.0014 Acc: 83.4524\n",
      "Epoch 99/99\n",
      "training Loss: 0.0015 Acc: 82.3165\n",
      "validation Loss: 0.0014 Acc: 83.4048\n",
      "Best val acc: 83.547619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0027 Acc: 49.9286\n",
      "validation Loss: 0.0026 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 49.6964\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0025 Acc: 49.2947\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0025 Acc: 49.6994\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0025 Acc: 51.2350\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 52.7677\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0024 Acc: 55.3955\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 58.4995\n",
      "validation Loss: 0.0023 Acc: 80.6429\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0022 Acc: 61.8683\n",
      "validation Loss: 0.0022 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 66.5347\n",
      "validation Loss: 0.0022 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0022 Acc: 70.9214\n",
      "validation Loss: 0.0021 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 74.3974\n",
      "validation Loss: 0.0021 Acc: 82.8810\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 77.3436\n",
      "validation Loss: 0.0020 Acc: 82.8095\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 78.7036\n",
      "validation Loss: 0.0020 Acc: 82.7857\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 79.8970\n",
      "validation Loss: 0.0020 Acc: 82.5119\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 80.2065\n",
      "validation Loss: 0.0020 Acc: 82.4524\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 80.6440\n",
      "validation Loss: 0.0019 Acc: 82.3690\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 80.6113\n",
      "validation Loss: 0.0019 Acc: 82.4167\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 80.8196\n",
      "validation Loss: 0.0019 Acc: 82.4524\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 80.9327\n",
      "validation Loss: 0.0019 Acc: 82.3452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 80.9029\n",
      "validation Loss: 0.0019 Acc: 82.5119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 81.0101\n",
      "validation Loss: 0.0018 Acc: 82.0476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 81.0398\n",
      "validation Loss: 0.0018 Acc: 82.0238\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 80.8375\n",
      "validation Loss: 0.0018 Acc: 81.9881\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 81.1529\n",
      "validation Loss: 0.0018 Acc: 82.1667\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 80.8761\n",
      "validation Loss: 0.0018 Acc: 81.9286\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 80.9327\n",
      "validation Loss: 0.0018 Acc: 81.8929\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 80.9982\n",
      "validation Loss: 0.0018 Acc: 81.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 80.8434\n",
      "validation Loss: 0.0018 Acc: 81.6548\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 80.9714\n",
      "validation Loss: 0.0017 Acc: 81.8929\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 81.0874\n",
      "validation Loss: 0.0017 Acc: 81.9405\n",
      "Early stopped.\n",
      "Best val acc: 83.047619\n",
      "----------\n",
      "Average best_acc across k-fold: 83.1103286066\n",
      "New configuration: {'learning_rate': 0.052716088795413424, 'initial_nodes': 684, 'dropout': 0.13299065720254657, 'batch_size': 512, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0043 Acc: 77.4464\n",
      "validation Loss: 0.0009 Acc: 83.3254\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 83.0030\n",
      "validation Loss: 0.0008 Acc: 83.7182\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.1935\n",
      "validation Loss: 0.0007 Acc: 83.4920\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.3958\n",
      "validation Loss: 0.0007 Acc: 83.6110\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.4494\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.4762\n",
      "validation Loss: 0.0008 Acc: 83.4682\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.6042\n",
      "validation Loss: 0.0008 Acc: 83.6944\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.4970\n",
      "validation Loss: 0.0007 Acc: 84.1466\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.6548\n",
      "validation Loss: 0.0007 Acc: 83.7658\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.6726\n",
      "validation Loss: 0.0007 Acc: 83.7896\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.7708\n",
      "validation Loss: 0.0007 Acc: 83.8372\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.6696\n",
      "validation Loss: 0.0007 Acc: 83.9324\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.6071\n",
      "validation Loss: 0.0007 Acc: 83.9562\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.8393\n",
      "validation Loss: 0.0007 Acc: 84.0871\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.7262\n",
      "validation Loss: 0.0008 Acc: 83.3730\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.0030\n",
      "validation Loss: 0.0008 Acc: 83.6110\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.7798\n",
      "validation Loss: 0.0007 Acc: 83.9086\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.7470\n",
      "validation Loss: 0.0007 Acc: 84.0752\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.8631\n",
      "validation Loss: 0.0007 Acc: 84.1585\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.8720\n",
      "validation Loss: 0.0007 Acc: 83.6587\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.9018\n",
      "validation Loss: 0.0007 Acc: 84.1109\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.0952\n",
      "validation Loss: 0.0007 Acc: 84.0276\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.1399\n",
      "validation Loss: 0.0008 Acc: 83.6706\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.0000\n",
      "validation Loss: 0.0007 Acc: 83.8491\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.0774\n",
      "validation Loss: 0.0007 Acc: 84.0633\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.0268\n",
      "validation Loss: 0.0007 Acc: 83.6229\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.0149\n",
      "validation Loss: 0.0007 Acc: 84.0633\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.0149\n",
      "validation Loss: 0.0007 Acc: 83.5872\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.1696\n",
      "validation Loss: 0.0007 Acc: 83.7420\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.2113\n",
      "validation Loss: 0.0007 Acc: 83.7063\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.3036\n",
      "validation Loss: 0.0007 Acc: 84.1347\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.2440\n",
      "validation Loss: 0.0007 Acc: 84.2418\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.2411\n",
      "validation Loss: 0.0007 Acc: 84.0276\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.3304\n",
      "validation Loss: 0.0007 Acc: 84.0276\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.2887\n",
      "validation Loss: 0.0008 Acc: 83.3254\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.2827\n",
      "validation Loss: 0.0007 Acc: 84.0038\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.9107\n",
      "validation Loss: 0.0007 Acc: 83.9919\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.7738\n",
      "validation Loss: 0.0007 Acc: 83.9086\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 84.9405\n",
      "validation Loss: 0.0007 Acc: 83.7301\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 84.8631\n",
      "validation Loss: 0.0007 Acc: 83.7182\n",
      "Epoch 40/99\n",
      "training Loss: 0.0006 Acc: 85.1637\n",
      "validation Loss: 0.0007 Acc: 83.7539\n",
      "Epoch 41/99\n",
      "training Loss: 0.0006 Acc: 85.3274\n",
      "validation Loss: 0.0007 Acc: 83.7301\n",
      "Epoch 42/99\n",
      "training Loss: 0.0006 Acc: 85.4762\n",
      "validation Loss: 0.0007 Acc: 83.8015\n",
      "Epoch 43/99\n",
      "training Loss: 0.0006 Acc: 85.5952\n",
      "validation Loss: 0.0008 Acc: 83.6587\n",
      "Epoch 44/99\n",
      "training Loss: 0.0006 Acc: 85.6786\n",
      "validation Loss: 0.0008 Acc: 83.6468\n",
      "Epoch 45/99\n",
      "training Loss: 0.0006 Acc: 85.5923\n",
      "validation Loss: 0.0008 Acc: 83.5396\n",
      "Epoch 46/99\n",
      "training Loss: 0.0006 Acc: 85.7054\n",
      "validation Loss: 0.0008 Acc: 83.6348\n",
      "Epoch 47/99\n",
      "training Loss: 0.0006 Acc: 85.9018\n",
      "validation Loss: 0.0008 Acc: 83.5872\n",
      "Epoch 48/99\n",
      "training Loss: 0.0006 Acc: 86.0089\n",
      "validation Loss: 0.0008 Acc: 83.5634\n",
      "Epoch 49/99\n",
      "training Loss: 0.0006 Acc: 86.1815\n",
      "validation Loss: 0.0008 Acc: 83.5158\n",
      "Epoch 50/99\n",
      "training Loss: 0.0006 Acc: 86.2738\n",
      "validation Loss: 0.0008 Acc: 83.6587\n",
      "Epoch 51/99\n",
      "training Loss: 0.0006 Acc: 86.2649\n",
      "validation Loss: 0.0008 Acc: 83.3016\n",
      "Early stopped.\n",
      "Best val acc: 84.241847\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0036 Acc: 77.3347\n",
      "validation Loss: 0.0008 Acc: 82.2024\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.1260\n",
      "validation Loss: 0.0008 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.5218\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.7093\n",
      "validation Loss: 0.0008 Acc: 82.6786\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.5456\n",
      "validation Loss: 0.0008 Acc: 83.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.7450\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.8581\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.8075\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.7956\n",
      "validation Loss: 0.0008 Acc: 83.1667\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.8908\n",
      "validation Loss: 0.0008 Acc: 83.1667\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.9593\n",
      "validation Loss: 0.0008 Acc: 83.0238\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 84.1795\n",
      "validation Loss: 0.0008 Acc: 83.0595\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.8611\n",
      "validation Loss: 0.0007 Acc: 83.0357\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.8670\n",
      "validation Loss: 0.0007 Acc: 83.0714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.9593\n",
      "validation Loss: 0.0008 Acc: 83.1548\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.8760\n",
      "validation Loss: 0.0008 Acc: 82.9286\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.0634\n",
      "validation Loss: 0.0008 Acc: 83.2143\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.1051\n",
      "validation Loss: 0.0008 Acc: 83.0119\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.0218\n",
      "validation Loss: 0.0008 Acc: 82.7976\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.3343\n",
      "validation Loss: 0.0007 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.5813\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.4087\n",
      "validation Loss: 0.0007 Acc: 83.2857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.7628\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.5069\n",
      "validation Loss: 0.0007 Acc: 83.1905\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.5515\n",
      "validation Loss: 0.0008 Acc: 83.1071\n",
      "Epoch 25/99\n",
      "training Loss: 0.0006 Acc: 85.0455\n",
      "validation Loss: 0.0007 Acc: 83.2143\n",
      "Epoch 26/99\n",
      "training Loss: 0.0006 Acc: 84.9682\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 27/99\n",
      "training Loss: 0.0006 Acc: 85.0128\n",
      "validation Loss: 0.0007 Acc: 83.0714\n",
      "Epoch 28/99\n",
      "training Loss: 0.0006 Acc: 84.9652\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 29/99\n",
      "training Loss: 0.0006 Acc: 85.1824\n",
      "validation Loss: 0.0007 Acc: 83.2381\n",
      "Epoch 30/99\n",
      "training Loss: 0.0006 Acc: 85.1467\n",
      "validation Loss: 0.0007 Acc: 83.2500\n",
      "Epoch 31/99\n",
      "training Loss: 0.0006 Acc: 85.2628\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Epoch 32/99\n",
      "training Loss: 0.0006 Acc: 85.3283\n",
      "validation Loss: 0.0008 Acc: 83.0595\n",
      "Epoch 33/99\n",
      "training Loss: 0.0006 Acc: 85.4235\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 34/99\n",
      "training Loss: 0.0006 Acc: 85.3759\n",
      "validation Loss: 0.0008 Acc: 83.1905\n",
      "Epoch 35/99\n",
      "training Loss: 0.0006 Acc: 85.5276\n",
      "validation Loss: 0.0008 Acc: 83.1905\n",
      "Epoch 36/99\n",
      "training Loss: 0.0006 Acc: 85.4919\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 37/99\n",
      "training Loss: 0.0006 Acc: 85.7092\n",
      "validation Loss: 0.0008 Acc: 83.1905\n",
      "Epoch 38/99\n",
      "training Loss: 0.0006 Acc: 85.7479\n",
      "validation Loss: 0.0008 Acc: 83.2262\n",
      "Epoch 39/99\n",
      "training Loss: 0.0006 Acc: 85.6705\n",
      "validation Loss: 0.0008 Acc: 83.2143\n",
      "Epoch 40/99\n",
      "training Loss: 0.0006 Acc: 85.7062\n",
      "validation Loss: 0.0008 Acc: 83.0000\n",
      "Epoch 41/99\n",
      "training Loss: 0.0006 Acc: 85.8848\n",
      "validation Loss: 0.0008 Acc: 83.1548\n",
      "Epoch 42/99\n",
      "training Loss: 0.0006 Acc: 85.9235\n",
      "validation Loss: 0.0008 Acc: 83.1786\n",
      "Early stopped.\n",
      "Best val acc: 83.476190\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 78.4447\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.0814\n",
      "validation Loss: 0.0007 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.4147\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.4623\n",
      "validation Loss: 0.0007 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.4296\n",
      "validation Loss: 0.0007 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.5694\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.6795\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6587\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.5575\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.6587\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.6617\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.9087\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.9801\n",
      "validation Loss: 0.0007 Acc: 83.8929\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.2242\n",
      "validation Loss: 0.0007 Acc: 84.1190\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.1795\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.0694\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.1170\n",
      "validation Loss: 0.0007 Acc: 84.0714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.2926\n",
      "validation Loss: 0.0007 Acc: 84.2976\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.5932\n",
      "validation Loss: 0.0007 Acc: 84.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.4712\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.5515\n",
      "validation Loss: 0.0007 Acc: 84.2976\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.5753\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.7509\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.5872\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 24/99\n",
      "training Loss: 0.0006 Acc: 84.9860\n",
      "validation Loss: 0.0007 Acc: 84.2262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0006 Acc: 84.7955\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 26/99\n",
      "training Loss: 0.0006 Acc: 85.0187\n",
      "validation Loss: 0.0007 Acc: 84.0476\n",
      "Epoch 27/99\n",
      "training Loss: 0.0006 Acc: 84.9949\n",
      "validation Loss: 0.0007 Acc: 84.1786\n",
      "Epoch 28/99\n",
      "training Loss: 0.0006 Acc: 85.1497\n",
      "validation Loss: 0.0007 Acc: 84.0357\n",
      "Epoch 29/99\n",
      "training Loss: 0.0006 Acc: 85.0098\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Epoch 30/99\n",
      "training Loss: 0.0006 Acc: 85.2717\n",
      "validation Loss: 0.0007 Acc: 83.9643\n",
      "Epoch 31/99\n",
      "training Loss: 0.0006 Acc: 85.3312\n",
      "validation Loss: 0.0007 Acc: 84.1786\n",
      "Epoch 32/99\n",
      "training Loss: 0.0006 Acc: 85.3312\n",
      "validation Loss: 0.0007 Acc: 83.9643\n",
      "Epoch 33/99\n",
      "training Loss: 0.0006 Acc: 85.4027\n",
      "validation Loss: 0.0007 Acc: 84.0476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0006 Acc: 85.3402\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0006 Acc: 85.4265\n",
      "validation Loss: 0.0007 Acc: 83.9405\n",
      "Epoch 36/99\n",
      "training Loss: 0.0006 Acc: 85.6050\n",
      "validation Loss: 0.0008 Acc: 84.0476\n",
      "Epoch 37/99\n",
      "training Loss: 0.0006 Acc: 85.6259\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Early stopped.\n",
      "Best val acc: 84.297619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0047 Acc: 76.4300\n",
      "validation Loss: 0.0008 Acc: 81.7738\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.6617\n",
      "validation Loss: 0.0008 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.3343\n",
      "validation Loss: 0.0008 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.3819\n",
      "validation Loss: 0.0008 Acc: 82.7143\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.5635\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.4355\n",
      "validation Loss: 0.0007 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.7153\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0008 Acc: 82.9405\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.7182\n",
      "validation Loss: 0.0008 Acc: 82.7976\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.8135\n",
      "validation Loss: 0.0007 Acc: 83.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.6736\n",
      "validation Loss: 0.0008 Acc: 83.0714\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.8224\n",
      "validation Loss: 0.0007 Acc: 83.1310\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.9533\n",
      "validation Loss: 0.0008 Acc: 82.9881\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.7986\n",
      "validation Loss: 0.0008 Acc: 82.6548\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.8432\n",
      "validation Loss: 0.0008 Acc: 83.1429\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.9027\n",
      "validation Loss: 0.0008 Acc: 83.0119\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.7272\n",
      "validation Loss: 0.0008 Acc: 79.3929\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 82.8076\n",
      "validation Loss: 0.0008 Acc: 82.2738\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.3700\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.6081\n",
      "validation Loss: 0.0008 Acc: 82.9405\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.7986\n",
      "validation Loss: 0.0008 Acc: 83.0714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.9176\n",
      "validation Loss: 0.0008 Acc: 82.7500\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.9801\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.9444\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.2807\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.3938\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.3849\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.4890\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.4176\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.3938\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.5783\n",
      "validation Loss: 0.0007 Acc: 83.2143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.6259\n",
      "validation Loss: 0.0008 Acc: 83.4167\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.6884\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.7509\n",
      "validation Loss: 0.0008 Acc: 83.2262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.7182\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 35/99\n",
      "training Loss: 0.0006 Acc: 84.6676\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Epoch 36/99\n",
      "training Loss: 0.0006 Acc: 84.8878\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Epoch 37/99\n",
      "training Loss: 0.0006 Acc: 85.0128\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Epoch 38/99\n",
      "training Loss: 0.0006 Acc: 84.8997\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Epoch 39/99\n",
      "training Loss: 0.0006 Acc: 85.0693\n",
      "validation Loss: 0.0008 Acc: 83.4643\n",
      "Epoch 40/99\n",
      "training Loss: 0.0006 Acc: 84.9354\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Epoch 41/99\n",
      "training Loss: 0.0006 Acc: 85.0307\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 42/99\n",
      "training Loss: 0.0006 Acc: 85.0664\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Epoch 43/99\n",
      "training Loss: 0.0006 Acc: 85.2092\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Epoch 44/99\n",
      "training Loss: 0.0006 Acc: 85.2747\n",
      "validation Loss: 0.0008 Acc: 83.4762\n",
      "Epoch 45/99\n",
      "training Loss: 0.0006 Acc: 85.1646\n",
      "validation Loss: 0.0008 Acc: 83.4643\n",
      "Epoch 46/99\n",
      "training Loss: 0.0006 Acc: 85.1943\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Epoch 47/99\n",
      "training Loss: 0.0006 Acc: 85.2122\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 48/99\n",
      "training Loss: 0.0006 Acc: 85.2598\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Epoch 49/99\n",
      "training Loss: 0.0006 Acc: 85.4592\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Epoch 50/99\n",
      "training Loss: 0.0006 Acc: 85.2806\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 51/99\n",
      "training Loss: 0.0006 Acc: 85.3223\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Epoch 52/99\n",
      "training Loss: 0.0006 Acc: 85.3967\n",
      "validation Loss: 0.0008 Acc: 83.2024\n",
      "Early stopped.\n",
      "Best val acc: 83.523810\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0036 Acc: 77.1323\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.0278\n",
      "validation Loss: 0.0007 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.1885\n",
      "validation Loss: 0.0007 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.4563\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.4980\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.6409\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.6319\n",
      "validation Loss: 0.0007 Acc: 83.4524\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6498\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.7420\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.9176\n",
      "validation Loss: 0.0007 Acc: 83.5595\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.7599\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.8224\n",
      "validation Loss: 0.0007 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.8313\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.9771\n",
      "validation Loss: 0.0007 Acc: 83.5714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.8670\n",
      "validation Loss: 0.0007 Acc: 83.1190\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.6170\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.8402\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.0962\n",
      "validation Loss: 0.0007 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.0337\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.8522\n",
      "validation Loss: 0.0007 Acc: 83.6905\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.9266\n",
      "validation Loss: 0.0008 Acc: 83.5714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.1974\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.0754\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.1795\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.1527\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.1825\n",
      "validation Loss: 0.0008 Acc: 82.5119\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.5366\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.6110\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.6497\n",
      "validation Loss: 0.0007 Acc: 83.5714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.7896\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.5991\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.8432\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.9771\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 33/99\n",
      "training Loss: 0.0006 Acc: 85.1437\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 34/99\n",
      "training Loss: 0.0006 Acc: 85.1914\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 35/99\n",
      "training Loss: 0.0006 Acc: 85.3015\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 36/99\n",
      "training Loss: 0.0006 Acc: 85.2122\n",
      "validation Loss: 0.0007 Acc: 83.3452\n",
      "Epoch 37/99\n",
      "training Loss: 0.0006 Acc: 85.1348\n",
      "validation Loss: 0.0008 Acc: 83.2619\n",
      "Early stopped.\n",
      "Best val acc: 83.916667\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8912265787\n",
      "New configuration: {'learning_rate': 3.180237597007714e-05, 'initial_nodes': 855, 'dropout': 0.9, 'batch_size': 226, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 56.7470\n",
      "validation Loss: 0.0028 Acc: 77.8148\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0027 Acc: 67.5476\n",
      "validation Loss: 0.0024 Acc: 79.5644\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 73.7857\n",
      "validation Loss: 0.0021 Acc: 80.5403\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 76.8363\n",
      "validation Loss: 0.0020 Acc: 81.1235\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0021 Acc: 78.7500\n",
      "validation Loss: 0.0019 Acc: 81.8139\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 79.9494\n",
      "validation Loss: 0.0018 Acc: 82.2185\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 80.6101\n",
      "validation Loss: 0.0018 Acc: 82.4923\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 80.9732\n",
      "validation Loss: 0.0018 Acc: 82.5518\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 81.5833\n",
      "validation Loss: 0.0017 Acc: 82.8850\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 81.5923\n",
      "validation Loss: 0.0018 Acc: 82.9564\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 81.9167\n",
      "validation Loss: 0.0017 Acc: 83.0040\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0018 Acc: 82.1161\n",
      "validation Loss: 0.0017 Acc: 83.0279\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0018 Acc: 81.9583\n",
      "validation Loss: 0.0017 Acc: 82.9921\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 82.4762\n",
      "validation Loss: 0.0017 Acc: 83.0279\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 82.5030\n",
      "validation Loss: 0.0017 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 82.3780\n",
      "validation Loss: 0.0017 Acc: 83.1350\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 82.4554\n",
      "validation Loss: 0.0017 Acc: 83.2540\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 82.6161\n",
      "validation Loss: 0.0017 Acc: 83.2540\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 82.4613\n",
      "validation Loss: 0.0017 Acc: 83.2897\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 82.7857\n",
      "validation Loss: 0.0017 Acc: 83.3611\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 82.6905\n",
      "validation Loss: 0.0017 Acc: 83.4444\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 82.7619\n",
      "validation Loss: 0.0017 Acc: 83.4087\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 82.8423\n",
      "validation Loss: 0.0017 Acc: 83.3968\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 82.8006\n",
      "validation Loss: 0.0017 Acc: 83.4920\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 82.7798\n",
      "validation Loss: 0.0017 Acc: 83.5515\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 82.9494\n",
      "validation Loss: 0.0017 Acc: 83.5396\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 82.8512\n",
      "validation Loss: 0.0017 Acc: 83.4682\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 82.8512\n",
      "validation Loss: 0.0017 Acc: 83.4682\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 82.9702\n",
      "validation Loss: 0.0017 Acc: 83.3849\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 83.0893\n",
      "validation Loss: 0.0017 Acc: 83.7301\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 82.8690\n",
      "validation Loss: 0.0017 Acc: 83.5277\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 83.0923\n",
      "validation Loss: 0.0017 Acc: 83.6348\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 83.0625\n",
      "validation Loss: 0.0017 Acc: 83.6468\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 83.0565\n",
      "validation Loss: 0.0017 Acc: 83.4563\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 83.2054\n",
      "validation Loss: 0.0017 Acc: 83.6468\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 83.0804\n",
      "validation Loss: 0.0017 Acc: 83.8134\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 82.9464\n",
      "validation Loss: 0.0017 Acc: 83.6587\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 83.2411\n",
      "validation Loss: 0.0017 Acc: 83.6110\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 83.1399\n",
      "validation Loss: 0.0017 Acc: 83.5396\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 83.1369\n",
      "validation Loss: 0.0017 Acc: 83.6706\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 83.0833\n",
      "validation Loss: 0.0017 Acc: 83.5753\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 83.1905\n",
      "validation Loss: 0.0017 Acc: 83.6348\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 83.1250\n",
      "validation Loss: 0.0017 Acc: 83.7063\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 83.1935\n",
      "validation Loss: 0.0017 Acc: 83.5396\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 83.1101\n",
      "validation Loss: 0.0017 Acc: 83.6468\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 83.4137\n",
      "validation Loss: 0.0017 Acc: 83.5634\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 83.4494\n",
      "validation Loss: 0.0017 Acc: 83.6944\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 83.2292\n",
      "validation Loss: 0.0017 Acc: 83.6110\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 83.4286\n",
      "validation Loss: 0.0017 Acc: 83.6348\n",
      "Epoch 49/99\n",
      "training Loss: 0.0017 Acc: 83.3661\n",
      "validation Loss: 0.0017 Acc: 83.5753\n",
      "Epoch 50/99\n",
      "training Loss: 0.0017 Acc: 83.2649\n",
      "validation Loss: 0.0017 Acc: 83.4920\n",
      "Epoch 51/99\n",
      "training Loss: 0.0017 Acc: 83.3006\n",
      "validation Loss: 0.0017 Acc: 83.5277\n",
      "Epoch 52/99\n",
      "training Loss: 0.0017 Acc: 83.2262\n",
      "validation Loss: 0.0017 Acc: 83.5039\n",
      "Epoch 53/99\n",
      "training Loss: 0.0017 Acc: 83.3065\n",
      "validation Loss: 0.0017 Acc: 83.5872\n",
      "Epoch 54/99\n",
      "training Loss: 0.0017 Acc: 83.4494\n",
      "validation Loss: 0.0017 Acc: 83.6587\n",
      "Epoch 55/99\n",
      "training Loss: 0.0017 Acc: 83.3750\n",
      "validation Loss: 0.0017 Acc: 83.5634\n",
      "Early stopped.\n",
      "Best val acc: 83.813378\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 54.7586\n",
      "validation Loss: 0.0028 Acc: 73.9167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0027 Acc: 64.6866\n",
      "validation Loss: 0.0025 Acc: 80.0714\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0025 Acc: 72.4362\n",
      "validation Loss: 0.0022 Acc: 80.5714\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0023 Acc: 76.0490\n",
      "validation Loss: 0.0020 Acc: 81.0714\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 78.2781\n",
      "validation Loss: 0.0019 Acc: 81.6548\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 79.3465\n",
      "validation Loss: 0.0018 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 80.2958\n",
      "validation Loss: 0.0018 Acc: 82.3214\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 81.1856\n",
      "validation Loss: 0.0018 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 81.2779\n",
      "validation Loss: 0.0017 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 81.5606\n",
      "validation Loss: 0.0017 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 82.0398\n",
      "validation Loss: 0.0017 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 82.0100\n",
      "validation Loss: 0.0017 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0018 Acc: 82.1499\n",
      "validation Loss: 0.0017 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 82.3314\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 82.1945\n",
      "validation Loss: 0.0017 Acc: 83.0357\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 82.4534\n",
      "validation Loss: 0.0017 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 82.5993\n",
      "validation Loss: 0.0017 Acc: 83.1667\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 82.6112\n",
      "validation Loss: 0.0017 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 82.5308\n",
      "validation Loss: 0.0017 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 82.7272\n",
      "validation Loss: 0.0017 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 82.7600\n",
      "validation Loss: 0.0017 Acc: 83.3333\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 82.8671\n",
      "validation Loss: 0.0017 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 82.8046\n",
      "validation Loss: 0.0017 Acc: 83.4881\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 82.9266\n",
      "validation Loss: 0.0017 Acc: 83.5000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 82.6885\n",
      "validation Loss: 0.0017 Acc: 83.5238\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 82.9504\n",
      "validation Loss: 0.0017 Acc: 83.4286\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 82.7302\n",
      "validation Loss: 0.0017 Acc: 83.4524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 82.8730\n",
      "validation Loss: 0.0017 Acc: 83.3571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 82.8046\n",
      "validation Loss: 0.0017 Acc: 83.3929\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 83.1587\n",
      "validation Loss: 0.0017 Acc: 83.4048\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 83.2093\n",
      "validation Loss: 0.0017 Acc: 83.4762\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 82.9564\n",
      "validation Loss: 0.0017 Acc: 83.4405\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 82.9385\n",
      "validation Loss: 0.0017 Acc: 83.5238\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 83.0278\n",
      "validation Loss: 0.0017 Acc: 83.4286\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 82.8790\n",
      "validation Loss: 0.0017 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 83.1081\n",
      "validation Loss: 0.0017 Acc: 83.5238\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 83.0903\n",
      "validation Loss: 0.0017 Acc: 83.5000\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 83.0278\n",
      "validation Loss: 0.0017 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 83.1677\n",
      "validation Loss: 0.0017 Acc: 83.5119\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 82.9742\n",
      "validation Loss: 0.0017 Acc: 83.5595\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 83.3016\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 83.1647\n",
      "validation Loss: 0.0016 Acc: 83.5357\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 83.0457\n",
      "validation Loss: 0.0016 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 83.1171\n",
      "validation Loss: 0.0016 Acc: 83.5357\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 83.1825\n",
      "validation Loss: 0.0017 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 83.2361\n",
      "validation Loss: 0.0017 Acc: 83.5714\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 83.2927\n",
      "validation Loss: 0.0017 Acc: 83.5595\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 83.2361\n",
      "validation Loss: 0.0017 Acc: 83.5595\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 83.1885\n",
      "validation Loss: 0.0017 Acc: 83.6429\n",
      "Epoch 49/99\n",
      "training Loss: 0.0017 Acc: 83.3819\n",
      "validation Loss: 0.0017 Acc: 83.6310\n",
      "Epoch 50/99\n",
      "training Loss: 0.0017 Acc: 83.4861\n",
      "validation Loss: 0.0017 Acc: 83.6429\n",
      "Epoch 51/99\n",
      "training Loss: 0.0017 Acc: 83.4742\n",
      "validation Loss: 0.0016 Acc: 83.6310\n",
      "Epoch 52/99\n",
      "training Loss: 0.0017 Acc: 83.1885\n",
      "validation Loss: 0.0017 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0017 Acc: 83.2480\n",
      "validation Loss: 0.0016 Acc: 83.6905\n",
      "Epoch 54/99\n",
      "training Loss: 0.0017 Acc: 83.3194\n",
      "validation Loss: 0.0016 Acc: 83.7024\n",
      "Epoch 55/99\n",
      "training Loss: 0.0017 Acc: 83.3433\n",
      "validation Loss: 0.0017 Acc: 83.7143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0017 Acc: 83.3284\n",
      "validation Loss: 0.0016 Acc: 83.6905\n",
      "Epoch 57/99\n",
      "training Loss: 0.0017 Acc: 83.4534\n",
      "validation Loss: 0.0017 Acc: 83.6786\n",
      "Epoch 58/99\n",
      "training Loss: 0.0017 Acc: 83.2659\n",
      "validation Loss: 0.0016 Acc: 83.6190\n",
      "Epoch 59/99\n",
      "training Loss: 0.0017 Acc: 83.2659\n",
      "validation Loss: 0.0017 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0017 Acc: 83.4117\n",
      "validation Loss: 0.0016 Acc: 83.6786\n",
      "Epoch 61/99\n",
      "training Loss: 0.0017 Acc: 83.3284\n",
      "validation Loss: 0.0017 Acc: 83.6905\n",
      "Epoch 62/99\n",
      "training Loss: 0.0017 Acc: 83.3373\n",
      "validation Loss: 0.0017 Acc: 83.6786\n",
      "Epoch 63/99\n",
      "training Loss: 0.0017 Acc: 83.4296\n",
      "validation Loss: 0.0016 Acc: 83.7262\n",
      "Epoch 64/99\n",
      "training Loss: 0.0017 Acc: 83.3105\n",
      "validation Loss: 0.0017 Acc: 83.7381\n",
      "Epoch 65/99\n",
      "training Loss: 0.0017 Acc: 83.5843\n",
      "validation Loss: 0.0016 Acc: 83.7024\n",
      "Epoch 66/99\n",
      "training Loss: 0.0017 Acc: 83.3700\n",
      "validation Loss: 0.0017 Acc: 83.7381\n",
      "Epoch 67/99\n",
      "training Loss: 0.0017 Acc: 83.4236\n",
      "validation Loss: 0.0017 Acc: 83.7619\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0017 Acc: 83.4206\n",
      "validation Loss: 0.0016 Acc: 83.7381\n",
      "Epoch 69/99\n",
      "training Loss: 0.0017 Acc: 83.3403\n",
      "validation Loss: 0.0017 Acc: 83.7500\n",
      "Epoch 70/99\n",
      "training Loss: 0.0017 Acc: 83.3909\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 71/99\n",
      "training Loss: 0.0017 Acc: 83.5040\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 72/99\n",
      "training Loss: 0.0017 Acc: 83.4950\n",
      "validation Loss: 0.0016 Acc: 83.7143\n",
      "Epoch 73/99\n",
      "training Loss: 0.0017 Acc: 83.4742\n",
      "validation Loss: 0.0016 Acc: 83.6905\n",
      "Epoch 74/99\n",
      "training Loss: 0.0017 Acc: 83.3522\n",
      "validation Loss: 0.0016 Acc: 83.7024\n",
      "Epoch 75/99\n",
      "training Loss: 0.0017 Acc: 83.4861\n",
      "validation Loss: 0.0016 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0017 Acc: 83.5367\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 77/99\n",
      "training Loss: 0.0017 Acc: 83.4682\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 78/99\n",
      "training Loss: 0.0017 Acc: 83.5218\n",
      "validation Loss: 0.0017 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0017 Acc: 83.4296\n",
      "validation Loss: 0.0016 Acc: 83.7738\n",
      "Epoch 80/99\n",
      "training Loss: 0.0017 Acc: 83.3938\n",
      "validation Loss: 0.0016 Acc: 83.7857\n",
      "Epoch 81/99\n",
      "training Loss: 0.0017 Acc: 83.5754\n",
      "validation Loss: 0.0017 Acc: 83.7381\n",
      "Epoch 82/99\n",
      "training Loss: 0.0017 Acc: 83.5278\n",
      "validation Loss: 0.0016 Acc: 83.7500\n",
      "Epoch 83/99\n",
      "training Loss: 0.0017 Acc: 83.5278\n",
      "validation Loss: 0.0016 Acc: 83.7381\n",
      "Epoch 84/99\n",
      "training Loss: 0.0017 Acc: 83.4623\n",
      "validation Loss: 0.0016 Acc: 83.7857\n",
      "Epoch 85/99\n",
      "training Loss: 0.0017 Acc: 83.4623\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 86/99\n",
      "training Loss: 0.0017 Acc: 83.6051\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 87/99\n",
      "training Loss: 0.0017 Acc: 83.3938\n",
      "validation Loss: 0.0016 Acc: 83.7500\n",
      "Epoch 88/99\n",
      "training Loss: 0.0017 Acc: 83.5486\n",
      "validation Loss: 0.0016 Acc: 83.7738\n",
      "Epoch 89/99\n",
      "training Loss: 0.0017 Acc: 83.4742\n",
      "validation Loss: 0.0016 Acc: 83.7738\n",
      "Epoch 90/99\n",
      "training Loss: 0.0017 Acc: 83.4444\n",
      "validation Loss: 0.0016 Acc: 83.7857\n",
      "Epoch 91/99\n",
      "training Loss: 0.0017 Acc: 83.5069\n",
      "validation Loss: 0.0017 Acc: 83.7619\n",
      "Epoch 92/99\n",
      "training Loss: 0.0017 Acc: 83.5248\n",
      "validation Loss: 0.0016 Acc: 83.7857\n",
      "Epoch 93/99\n",
      "training Loss: 0.0017 Acc: 83.5546\n",
      "validation Loss: 0.0017 Acc: 83.7857\n",
      "Epoch 94/99\n",
      "training Loss: 0.0017 Acc: 83.4385\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 95/99\n",
      "training Loss: 0.0017 Acc: 83.3938\n",
      "validation Loss: 0.0017 Acc: 83.7738\n",
      "Epoch 96/99\n",
      "training Loss: 0.0017 Acc: 83.6319\n",
      "validation Loss: 0.0016 Acc: 83.7738\n",
      "Epoch 97/99\n",
      "training Loss: 0.0017 Acc: 83.5278\n",
      "validation Loss: 0.0016 Acc: 83.7619\n",
      "Epoch 98/99\n",
      "training Loss: 0.0017 Acc: 83.4534\n",
      "validation Loss: 0.0016 Acc: 83.7738\n",
      "Early stopped.\n",
      "Best val acc: 83.785714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 57.8983\n",
      "validation Loss: 0.0027 Acc: 79.0833\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 68.5078\n",
      "validation Loss: 0.0023 Acc: 80.0476\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 73.9480\n",
      "validation Loss: 0.0021 Acc: 80.6548\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0022 Acc: 76.9567\n",
      "validation Loss: 0.0019 Acc: 81.3571\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0021 Acc: 78.5013\n",
      "validation Loss: 0.0018 Acc: 81.8214\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 79.5965\n",
      "validation Loss: 0.0018 Acc: 82.0952\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 80.2720\n",
      "validation Loss: 0.0018 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 80.9059\n",
      "validation Loss: 0.0017 Acc: 82.6310\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 81.3642\n",
      "validation Loss: 0.0017 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 81.4654\n",
      "validation Loss: 0.0017 Acc: 83.0119\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 81.9148\n",
      "validation Loss: 0.0017 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 81.7392\n",
      "validation Loss: 0.0017 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 82.1737\n",
      "validation Loss: 0.0017 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 82.3344\n",
      "validation Loss: 0.0017 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 82.4296\n",
      "validation Loss: 0.0017 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 82.4237\n",
      "validation Loss: 0.0017 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 82.4683\n",
      "validation Loss: 0.0017 Acc: 83.2976\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 82.6022\n",
      "validation Loss: 0.0017 Acc: 83.3571\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 82.7867\n",
      "validation Loss: 0.0017 Acc: 83.3333\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 82.5754\n",
      "validation Loss: 0.0017 Acc: 83.3929\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 82.5487\n",
      "validation Loss: 0.0017 Acc: 83.4286\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 82.7957\n",
      "validation Loss: 0.0017 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 82.5695\n",
      "validation Loss: 0.0017 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 82.8314\n",
      "validation Loss: 0.0016 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 82.8225\n",
      "validation Loss: 0.0017 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 82.7957\n",
      "validation Loss: 0.0017 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 82.8611\n",
      "validation Loss: 0.0017 Acc: 83.5238\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 82.8939\n",
      "validation Loss: 0.0016 Acc: 83.3810\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 83.0337\n",
      "validation Loss: 0.0016 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 83.0397\n",
      "validation Loss: 0.0017 Acc: 83.5833\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 83.1677\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 83.0129\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 83.0486\n",
      "validation Loss: 0.0016 Acc: 83.5714\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 83.0278\n",
      "validation Loss: 0.0017 Acc: 83.5476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 82.9147\n",
      "validation Loss: 0.0016 Acc: 83.5476\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 83.0814\n",
      "validation Loss: 0.0016 Acc: 83.5833\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 83.1617\n",
      "validation Loss: 0.0016 Acc: 83.5714\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 83.1230\n",
      "validation Loss: 0.0016 Acc: 83.5833\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 83.0397\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 83.2599\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 83.0695\n",
      "validation Loss: 0.0016 Acc: 83.5357\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 83.2034\n",
      "validation Loss: 0.0016 Acc: 83.5119\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 82.9236\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 83.0903\n",
      "validation Loss: 0.0016 Acc: 83.5952\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 83.2837\n",
      "validation Loss: 0.0016 Acc: 83.5238\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 83.1528\n",
      "validation Loss: 0.0016 Acc: 83.5476\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 82.9415\n",
      "validation Loss: 0.0016 Acc: 83.5119\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 83.1349\n",
      "validation Loss: 0.0016 Acc: 83.5238\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 82.9326\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Early stopped.\n",
      "Best val acc: 83.619048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 56.0532\n",
      "validation Loss: 0.0028 Acc: 78.6548\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0027 Acc: 66.6478\n",
      "validation Loss: 0.0024 Acc: 79.0357\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 72.5016\n",
      "validation Loss: 0.0021 Acc: 79.7976\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0023 Acc: 75.9121\n",
      "validation Loss: 0.0020 Acc: 80.7500\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 77.8257\n",
      "validation Loss: 0.0019 Acc: 81.2738\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 79.2780\n",
      "validation Loss: 0.0019 Acc: 81.8214\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 79.8881\n",
      "validation Loss: 0.0018 Acc: 82.2024\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 80.7214\n",
      "validation Loss: 0.0018 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 81.3374\n",
      "validation Loss: 0.0018 Acc: 82.3452\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 81.5071\n",
      "validation Loss: 0.0017 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 81.7481\n",
      "validation Loss: 0.0017 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 82.0546\n",
      "validation Loss: 0.0017 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 82.1409\n",
      "validation Loss: 0.0017 Acc: 82.4881\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 82.0368\n",
      "validation Loss: 0.0017 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 82.3463\n",
      "validation Loss: 0.0017 Acc: 82.6429\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 82.4356\n",
      "validation Loss: 0.0017 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 82.5516\n",
      "validation Loss: 0.0017 Acc: 82.7381\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 82.6350\n",
      "validation Loss: 0.0017 Acc: 82.7500\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 82.5814\n",
      "validation Loss: 0.0017 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 82.6022\n",
      "validation Loss: 0.0017 Acc: 82.8095\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 82.8641\n",
      "validation Loss: 0.0017 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 82.8314\n",
      "validation Loss: 0.0017 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 82.7540\n",
      "validation Loss: 0.0017 Acc: 82.9762\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 83.0337\n",
      "validation Loss: 0.0017 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 82.8552\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 82.8016\n",
      "validation Loss: 0.0017 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 82.8909\n",
      "validation Loss: 0.0017 Acc: 83.1429\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 82.9117\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 83.0814\n",
      "validation Loss: 0.0017 Acc: 83.1190\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 83.0843\n",
      "validation Loss: 0.0017 Acc: 83.1429\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 83.1409\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 83.0635\n",
      "validation Loss: 0.0017 Acc: 83.1310\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 83.1825\n",
      "validation Loss: 0.0017 Acc: 83.0714\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 83.1706\n",
      "validation Loss: 0.0017 Acc: 83.1548\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 83.2123\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 83.1290\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.0992\n",
      "validation Loss: 0.0017 Acc: 83.1310\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 83.0546\n",
      "validation Loss: 0.0017 Acc: 83.1548\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 83.1498\n",
      "validation Loss: 0.0017 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 83.1885\n",
      "validation Loss: 0.0017 Acc: 83.2143\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 83.0814\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 83.2867\n",
      "validation Loss: 0.0017 Acc: 83.1429\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 83.2123\n",
      "validation Loss: 0.0017 Acc: 83.1786\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 83.1409\n",
      "validation Loss: 0.0017 Acc: 83.1786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 83.1260\n",
      "validation Loss: 0.0017 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 83.2689\n",
      "validation Loss: 0.0017 Acc: 83.1667\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 83.4355\n",
      "validation Loss: 0.0016 Acc: 83.2024\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 83.1915\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 83.2927\n",
      "validation Loss: 0.0017 Acc: 83.2143\n",
      "Epoch 49/99\n",
      "training Loss: 0.0017 Acc: 83.1141\n",
      "validation Loss: 0.0016 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0017 Acc: 83.2956\n",
      "validation Loss: 0.0017 Acc: 83.2619\n",
      "Epoch 51/99\n",
      "training Loss: 0.0017 Acc: 83.2093\n",
      "validation Loss: 0.0017 Acc: 83.2381\n",
      "Epoch 52/99\n",
      "training Loss: 0.0017 Acc: 83.2272\n",
      "validation Loss: 0.0017 Acc: 83.2262\n",
      "Epoch 53/99\n",
      "training Loss: 0.0017 Acc: 83.1855\n",
      "validation Loss: 0.0017 Acc: 83.2500\n",
      "Epoch 54/99\n",
      "training Loss: 0.0017 Acc: 83.3313\n",
      "validation Loss: 0.0017 Acc: 83.2143\n",
      "Epoch 55/99\n",
      "training Loss: 0.0017 Acc: 83.1915\n",
      "validation Loss: 0.0016 Acc: 83.2619\n",
      "Epoch 56/99\n",
      "training Loss: 0.0017 Acc: 83.2331\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Epoch 57/99\n",
      "training Loss: 0.0017 Acc: 83.2153\n",
      "validation Loss: 0.0016 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0017 Acc: 83.3611\n",
      "validation Loss: 0.0016 Acc: 83.2381\n",
      "Epoch 59/99\n",
      "training Loss: 0.0017 Acc: 83.1825\n",
      "validation Loss: 0.0016 Acc: 83.2262\n",
      "Epoch 60/99\n",
      "training Loss: 0.0017 Acc: 83.3730\n",
      "validation Loss: 0.0017 Acc: 83.1667\n",
      "Epoch 61/99\n",
      "training Loss: 0.0017 Acc: 83.3313\n",
      "validation Loss: 0.0017 Acc: 83.1786\n",
      "Epoch 62/99\n",
      "training Loss: 0.0017 Acc: 83.3552\n",
      "validation Loss: 0.0017 Acc: 83.1905\n",
      "Epoch 63/99\n",
      "training Loss: 0.0017 Acc: 83.4266\n",
      "validation Loss: 0.0017 Acc: 83.2976\n",
      "Epoch 64/99\n",
      "training Loss: 0.0017 Acc: 83.1379\n",
      "validation Loss: 0.0016 Acc: 83.2381\n",
      "Epoch 65/99\n",
      "training Loss: 0.0017 Acc: 83.3671\n",
      "validation Loss: 0.0017 Acc: 83.2262\n",
      "Epoch 66/99\n",
      "training Loss: 0.0017 Acc: 83.3611\n",
      "validation Loss: 0.0017 Acc: 83.2619\n",
      "Epoch 67/99\n",
      "training Loss: 0.0017 Acc: 83.3790\n",
      "validation Loss: 0.0017 Acc: 83.2500\n",
      "Epoch 68/99\n",
      "training Loss: 0.0017 Acc: 83.3819\n",
      "validation Loss: 0.0016 Acc: 83.2500\n",
      "Epoch 69/99\n",
      "training Loss: 0.0017 Acc: 83.2867\n",
      "validation Loss: 0.0016 Acc: 83.2619\n",
      "Epoch 70/99\n",
      "training Loss: 0.0017 Acc: 83.4712\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Epoch 71/99\n",
      "training Loss: 0.0017 Acc: 83.3968\n",
      "validation Loss: 0.0017 Acc: 83.1667\n",
      "Epoch 72/99\n",
      "training Loss: 0.0017 Acc: 83.2689\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Epoch 73/99\n",
      "training Loss: 0.0017 Acc: 83.3403\n",
      "validation Loss: 0.0017 Acc: 83.2143\n",
      "Epoch 74/99\n",
      "training Loss: 0.0017 Acc: 83.3611\n",
      "validation Loss: 0.0016 Acc: 83.2143\n",
      "Epoch 75/99\n",
      "training Loss: 0.0017 Acc: 83.3938\n",
      "validation Loss: 0.0016 Acc: 83.1548\n",
      "Epoch 76/99\n",
      "training Loss: 0.0017 Acc: 83.4355\n",
      "validation Loss: 0.0016 Acc: 83.1667\n",
      "Epoch 77/99\n",
      "training Loss: 0.0017 Acc: 83.3552\n",
      "validation Loss: 0.0016 Acc: 83.1667\n",
      "Early stopped.\n",
      "Best val acc: 83.297619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 56.8538\n",
      "validation Loss: 0.0027 Acc: 78.7976\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0027 Acc: 67.0883\n",
      "validation Loss: 0.0024 Acc: 79.3929\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 73.6801\n",
      "validation Loss: 0.0021 Acc: 80.2976\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0023 Acc: 76.4508\n",
      "validation Loss: 0.0020 Acc: 80.9167\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 78.2900\n",
      "validation Loss: 0.0019 Acc: 81.4167\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 79.6857\n",
      "validation Loss: 0.0018 Acc: 81.7500\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0020 Acc: 80.4446\n",
      "validation Loss: 0.0018 Acc: 82.2143\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 80.8970\n",
      "validation Loss: 0.0018 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 81.2511\n",
      "validation Loss: 0.0018 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 81.6469\n",
      "validation Loss: 0.0017 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 81.6886\n",
      "validation Loss: 0.0017 Acc: 82.7857\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 81.9892\n",
      "validation Loss: 0.0017 Acc: 82.7381\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 81.9981\n",
      "validation Loss: 0.0017 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0018 Acc: 82.3999\n",
      "validation Loss: 0.0017 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0018 Acc: 82.2421\n",
      "validation Loss: 0.0017 Acc: 82.9643\n",
      "Epoch 15/99\n",
      "training Loss: 0.0018 Acc: 82.5189\n",
      "validation Loss: 0.0017 Acc: 83.0357\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 82.5070\n",
      "validation Loss: 0.0017 Acc: 82.9643\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 82.4028\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 82.7986\n",
      "validation Loss: 0.0017 Acc: 83.0476\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 82.7332\n",
      "validation Loss: 0.0017 Acc: 83.0833\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 82.9326\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 82.9028\n",
      "validation Loss: 0.0017 Acc: 83.0476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 82.8492\n",
      "validation Loss: 0.0017 Acc: 83.1071\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 82.7064\n",
      "validation Loss: 0.0017 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 82.8195\n",
      "validation Loss: 0.0017 Acc: 83.0952\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 82.8730\n",
      "validation Loss: 0.0017 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 83.0129\n",
      "validation Loss: 0.0017 Acc: 83.1190\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 82.9742\n",
      "validation Loss: 0.0017 Acc: 83.1310\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 83.0605\n",
      "validation Loss: 0.0016 Acc: 83.1905\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 82.9653\n",
      "validation Loss: 0.0016 Acc: 83.1548\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.0070\n",
      "validation Loss: 0.0016 Acc: 83.0714\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 83.0635\n",
      "validation Loss: 0.0016 Acc: 83.1190\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 83.1290\n",
      "validation Loss: 0.0016 Acc: 83.1190\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 83.1677\n",
      "validation Loss: 0.0016 Acc: 83.0476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 83.1171\n",
      "validation Loss: 0.0016 Acc: 83.1310\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 83.1528\n",
      "validation Loss: 0.0016 Acc: 83.2024\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 83.2748\n",
      "validation Loss: 0.0017 Acc: 83.1667\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 83.2540\n",
      "validation Loss: 0.0016 Acc: 83.1548\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 83.2629\n",
      "validation Loss: 0.0016 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 83.2956\n",
      "validation Loss: 0.0016 Acc: 83.1786\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 83.2718\n",
      "validation Loss: 0.0016 Acc: 83.2024\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 83.2361\n",
      "validation Loss: 0.0016 Acc: 83.1548\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 83.2956\n",
      "validation Loss: 0.0016 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 83.2421\n",
      "validation Loss: 0.0016 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 83.2183\n",
      "validation Loss: 0.0016 Acc: 83.2500\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 83.2718\n",
      "validation Loss: 0.0016 Acc: 83.2262\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 83.2986\n",
      "validation Loss: 0.0016 Acc: 83.2500\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 83.4474\n",
      "validation Loss: 0.0016 Acc: 83.1786\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 83.2302\n",
      "validation Loss: 0.0016 Acc: 83.1905\n",
      "Epoch 49/99\n",
      "training Loss: 0.0017 Acc: 83.2837\n",
      "validation Loss: 0.0017 Acc: 83.2024\n",
      "Epoch 50/99\n",
      "training Loss: 0.0017 Acc: 83.3075\n",
      "validation Loss: 0.0016 Acc: 83.3690\n",
      "Epoch 51/99\n",
      "training Loss: 0.0017 Acc: 83.3165\n",
      "validation Loss: 0.0016 Acc: 83.2738\n",
      "Epoch 52/99\n",
      "training Loss: 0.0017 Acc: 83.4236\n",
      "validation Loss: 0.0016 Acc: 83.2500\n",
      "Epoch 53/99\n",
      "training Loss: 0.0017 Acc: 83.3671\n",
      "validation Loss: 0.0016 Acc: 83.2500\n",
      "Epoch 54/99\n",
      "training Loss: 0.0017 Acc: 83.4444\n",
      "validation Loss: 0.0016 Acc: 83.2738\n",
      "Epoch 55/99\n",
      "training Loss: 0.0017 Acc: 83.3373\n",
      "validation Loss: 0.0016 Acc: 83.2262\n",
      "Epoch 56/99\n",
      "training Loss: 0.0017 Acc: 83.3313\n",
      "validation Loss: 0.0016 Acc: 83.2262\n",
      "Epoch 57/99\n",
      "training Loss: 0.0017 Acc: 83.3165\n",
      "validation Loss: 0.0016 Acc: 83.3929\n",
      "Epoch 58/99\n",
      "training Loss: 0.0017 Acc: 83.3105\n",
      "validation Loss: 0.0016 Acc: 83.2857\n",
      "Epoch 59/99\n",
      "training Loss: 0.0017 Acc: 83.4028\n",
      "validation Loss: 0.0016 Acc: 83.3095\n",
      "Epoch 60/99\n",
      "training Loss: 0.0017 Acc: 83.4444\n",
      "validation Loss: 0.0016 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0017 Acc: 83.6290\n",
      "validation Loss: 0.0016 Acc: 83.3690\n",
      "Epoch 62/99\n",
      "training Loss: 0.0017 Acc: 83.5099\n",
      "validation Loss: 0.0016 Acc: 83.2500\n",
      "Epoch 63/99\n",
      "training Loss: 0.0017 Acc: 83.3581\n",
      "validation Loss: 0.0016 Acc: 83.2262\n",
      "Epoch 64/99\n",
      "training Loss: 0.0017 Acc: 83.4206\n",
      "validation Loss: 0.0016 Acc: 83.3452\n",
      "Epoch 65/99\n",
      "training Loss: 0.0017 Acc: 83.5516\n",
      "validation Loss: 0.0016 Acc: 83.2143\n",
      "Epoch 66/99\n",
      "training Loss: 0.0017 Acc: 83.5426\n",
      "validation Loss: 0.0016 Acc: 83.4167\n",
      "Epoch 67/99\n",
      "training Loss: 0.0017 Acc: 83.5813\n",
      "validation Loss: 0.0016 Acc: 83.4167\n",
      "Epoch 68/99\n",
      "training Loss: 0.0017 Acc: 83.5873\n",
      "validation Loss: 0.0016 Acc: 83.4286\n",
      "Epoch 69/99\n",
      "training Loss: 0.0017 Acc: 83.4801\n",
      "validation Loss: 0.0016 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0017 Acc: 83.4653\n",
      "validation Loss: 0.0016 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0017 Acc: 83.4087\n",
      "validation Loss: 0.0016 Acc: 83.3810\n",
      "Epoch 72/99\n",
      "training Loss: 0.0017 Acc: 83.5367\n",
      "validation Loss: 0.0016 Acc: 83.5000\n",
      "Epoch 73/99\n",
      "training Loss: 0.0017 Acc: 83.4861\n",
      "validation Loss: 0.0016 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0017 Acc: 83.7212\n",
      "validation Loss: 0.0016 Acc: 83.4762\n",
      "Epoch 75/99\n",
      "training Loss: 0.0017 Acc: 83.4534\n",
      "validation Loss: 0.0016 Acc: 83.4643\n",
      "Epoch 76/99\n",
      "training Loss: 0.0017 Acc: 83.6260\n",
      "validation Loss: 0.0016 Acc: 83.4405\n",
      "Epoch 77/99\n",
      "training Loss: 0.0017 Acc: 83.6022\n",
      "validation Loss: 0.0016 Acc: 83.5238\n",
      "Epoch 78/99\n",
      "training Loss: 0.0017 Acc: 83.4980\n",
      "validation Loss: 0.0016 Acc: 83.5238\n",
      "Epoch 79/99\n",
      "training Loss: 0.0017 Acc: 83.5932\n",
      "validation Loss: 0.0016 Acc: 83.5714\n",
      "Epoch 80/99\n",
      "training Loss: 0.0017 Acc: 83.6706\n",
      "validation Loss: 0.0016 Acc: 83.4643\n",
      "Epoch 81/99\n",
      "training Loss: 0.0017 Acc: 83.5903\n",
      "validation Loss: 0.0016 Acc: 83.5476\n",
      "Epoch 82/99\n",
      "training Loss: 0.0017 Acc: 83.7242\n",
      "validation Loss: 0.0016 Acc: 83.5476\n",
      "Epoch 83/99\n",
      "training Loss: 0.0017 Acc: 83.5665\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 84/99\n",
      "training Loss: 0.0017 Acc: 83.5367\n",
      "validation Loss: 0.0016 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 85/99\n",
      "training Loss: 0.0017 Acc: 83.6468\n",
      "validation Loss: 0.0016 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 86/99\n",
      "training Loss: 0.0017 Acc: 83.7391\n",
      "validation Loss: 0.0016 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 87/99\n",
      "training Loss: 0.0017 Acc: 83.6706\n",
      "validation Loss: 0.0016 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 88/99\n",
      "training Loss: 0.0017 Acc: 83.5129\n",
      "validation Loss: 0.0016 Acc: 83.5595\n",
      "Epoch 89/99\n",
      "training Loss: 0.0017 Acc: 83.7063\n",
      "validation Loss: 0.0016 Acc: 83.5119\n",
      "Epoch 90/99\n",
      "training Loss: 0.0017 Acc: 83.7242\n",
      "validation Loss: 0.0016 Acc: 83.6071\n",
      "Epoch 91/99\n",
      "training Loss: 0.0017 Acc: 83.8581\n",
      "validation Loss: 0.0016 Acc: 83.5952\n",
      "Epoch 92/99\n",
      "training Loss: 0.0017 Acc: 83.7748\n",
      "validation Loss: 0.0016 Acc: 83.6310\n",
      "Epoch 93/99\n",
      "training Loss: 0.0017 Acc: 83.8075\n",
      "validation Loss: 0.0016 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 94/99\n",
      "training Loss: 0.0017 Acc: 83.6200\n",
      "validation Loss: 0.0016 Acc: 83.5833\n",
      "Epoch 95/99\n",
      "training Loss: 0.0017 Acc: 83.8492\n",
      "validation Loss: 0.0016 Acc: 83.6310\n",
      "Epoch 96/99\n",
      "training Loss: 0.0017 Acc: 83.6795\n",
      "validation Loss: 0.0016 Acc: 83.6190\n",
      "Epoch 97/99\n",
      "training Loss: 0.0017 Acc: 83.5962\n",
      "validation Loss: 0.0016 Acc: 83.6071\n",
      "Epoch 98/99\n",
      "training Loss: 0.0017 Acc: 83.6260\n",
      "validation Loss: 0.0016 Acc: 83.6310\n",
      "Epoch 99/99\n",
      "training Loss: 0.0017 Acc: 83.7599\n",
      "validation Loss: 0.0016 Acc: 83.5952\n",
      "Best val acc: 83.642857\n",
      "----------\n",
      "Average best_acc across k-fold: 83.6317231725\n",
      "New configuration: {'learning_rate': 0.0360502342438504, 'initial_nodes': 343, 'dropout': 0.5226019108014156, 'batch_size': 56, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0111 Acc: 71.7113\n",
      "validation Loss: 0.0120 Acc: 76.5651\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0105 Acc: 70.6756\n",
      "validation Loss: 0.0088 Acc: 81.0045\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0126 Acc: 65.0417\n",
      "validation Loss: 0.0164 Acc: 52.4399\n",
      "Epoch 3/99\n",
      "training Loss: 0.0113 Acc: 63.5804\n",
      "validation Loss: 0.0102 Acc: 80.3023\n",
      "Epoch 4/99\n",
      "training Loss: 0.0135 Acc: 63.0357\n",
      "validation Loss: 0.0112 Acc: 71.1259\n",
      "Epoch 5/99\n",
      "training Loss: 0.0157 Acc: 65.7024\n",
      "validation Loss: 0.0120 Acc: 63.4611\n",
      "Epoch 6/99\n",
      "training Loss: 0.0121 Acc: 60.6369\n",
      "validation Loss: 0.0110 Acc: 77.2316\n",
      "Epoch 7/99\n",
      "training Loss: 0.0144 Acc: 65.4821\n",
      "validation Loss: 0.0111 Acc: 78.6241\n",
      "Epoch 8/99\n",
      "training Loss: 0.0116 Acc: 66.0119\n",
      "validation Loss: 0.0106 Acc: 79.5168\n",
      "Epoch 9/99\n",
      "training Loss: 0.0116 Acc: 65.8542\n",
      "validation Loss: 0.0106 Acc: 79.0526\n",
      "Epoch 10/99\n",
      "training Loss: 0.0116 Acc: 65.4643\n",
      "validation Loss: 0.0107 Acc: 78.9336\n",
      "Epoch 11/99\n",
      "training Loss: 0.0116 Acc: 66.1696\n",
      "validation Loss: 0.0107 Acc: 80.3856\n",
      "Epoch 12/99\n",
      "training Loss: 0.0116 Acc: 66.4286\n",
      "validation Loss: 0.0105 Acc: 80.4689\n",
      "Epoch 13/99\n",
      "training Loss: 0.0116 Acc: 66.2351\n",
      "validation Loss: 0.0108 Acc: 80.7189\n",
      "Epoch 14/99\n",
      "training Loss: 0.0115 Acc: 66.7708\n",
      "validation Loss: 0.0107 Acc: 80.8141\n",
      "Epoch 15/99\n",
      "training Loss: 0.0115 Acc: 66.9137\n",
      "validation Loss: 0.0107 Acc: 80.8141\n",
      "Epoch 16/99\n",
      "training Loss: 0.0115 Acc: 67.0476\n",
      "validation Loss: 0.0104 Acc: 80.6237\n",
      "Epoch 17/99\n",
      "training Loss: 0.0116 Acc: 66.4940\n",
      "validation Loss: 0.0106 Acc: 80.8141\n",
      "Epoch 18/99\n",
      "training Loss: 0.0115 Acc: 67.2798\n",
      "validation Loss: 0.0102 Acc: 80.8379\n",
      "Epoch 19/99\n",
      "training Loss: 0.0114 Acc: 67.8899\n",
      "validation Loss: 0.0104 Acc: 80.6832\n",
      "Epoch 20/99\n",
      "training Loss: 0.0114 Acc: 67.5298\n",
      "validation Loss: 0.0104 Acc: 80.7070\n",
      "Epoch 21/99\n",
      "training Loss: 0.0114 Acc: 67.5804\n",
      "validation Loss: 0.0103 Acc: 80.8022\n",
      "Early stopped.\n",
      "Best val acc: 81.004523\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0114 Acc: 70.1714\n",
      "validation Loss: 0.0098 Acc: 60.1667\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0137 Acc: 50.4434\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0124 Acc: 49.9316\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0124 Acc: 50.3422\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0124 Acc: 49.4822\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0124 Acc: 49.6756\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0124 Acc: 50.4107\n",
      "validation Loss: 0.0124 Acc: 50.1310\n",
      "Epoch 7/99\n",
      "training Loss: 0.0124 Acc: 49.9762\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0124 Acc: 49.9554\n",
      "validation Loss: 0.0124 Acc: 50.1310\n",
      "Epoch 9/99\n",
      "training Loss: 0.0124 Acc: 49.9732\n",
      "validation Loss: 0.0124 Acc: 50.1310\n",
      "Epoch 10/99\n",
      "training Loss: 0.0124 Acc: 50.2589\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0124 Acc: 50.0238\n",
      "validation Loss: 0.0122 Acc: 51.1190\n",
      "Epoch 12/99\n",
      "training Loss: 0.0154 Acc: 50.1428\n",
      "validation Loss: 0.0124 Acc: 50.5476\n",
      "Epoch 13/99\n",
      "training Loss: 0.0124 Acc: 49.9196\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 14/99\n",
      "training Loss: 0.0124 Acc: 50.3928\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 15/99\n",
      "training Loss: 0.0124 Acc: 50.0595\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0124 Acc: 49.8333\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0124 Acc: 49.5774\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 18/99\n",
      "training Loss: 0.0124 Acc: 50.0536\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0124 Acc: 50.3095\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0124 Acc: 50.2559\n",
      "validation Loss: 0.0124 Acc: 50.0000\n",
      "Early stopped.\n",
      "Best val acc: 60.166667\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0123 Acc: 67.9781\n",
      "validation Loss: 0.0096 Acc: 79.4405\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0117 Acc: 64.2938\n",
      "validation Loss: 0.0120 Acc: 74.0476\n",
      "Epoch 2/99\n",
      "training Loss: 0.0123 Acc: 59.2346\n",
      "validation Loss: 0.0116 Acc: 64.7500\n",
      "Epoch 3/99\n",
      "training Loss: 0.0120 Acc: 60.4577\n",
      "validation Loss: 0.0120 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0122 Acc: 51.5951\n",
      "validation Loss: 0.0118 Acc: 54.5000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0125 Acc: 51.6041\n",
      "validation Loss: 0.0120 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0122 Acc: 51.3838\n",
      "validation Loss: 0.0120 Acc: 53.4286\n",
      "Epoch 7/99\n",
      "training Loss: 0.0122 Acc: 51.3273\n",
      "validation Loss: 0.0119 Acc: 53.4286\n",
      "Epoch 8/99\n",
      "training Loss: 0.0122 Acc: 51.3184\n",
      "validation Loss: 0.0120 Acc: 50.0000\n",
      "Epoch 9/99\n",
      "training Loss: 0.0122 Acc: 51.3928\n",
      "validation Loss: 0.0118 Acc: 54.1905\n",
      "Epoch 10/99\n",
      "training Loss: 0.0121 Acc: 51.5237\n",
      "validation Loss: 0.0118 Acc: 54.2143\n",
      "Epoch 11/99\n",
      "training Loss: 0.0125 Acc: 51.2142\n",
      "validation Loss: 0.0123 Acc: 50.9286\n",
      "Epoch 12/99\n",
      "training Loss: 0.0123 Acc: 50.2708\n",
      "validation Loss: 0.0123 Acc: 50.0000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0123 Acc: 50.6160\n",
      "validation Loss: 0.0122 Acc: 51.3452\n",
      "Epoch 14/99\n",
      "training Loss: 0.0123 Acc: 50.0089\n",
      "validation Loss: 0.0122 Acc: 51.3452\n",
      "Epoch 15/99\n",
      "training Loss: 0.0123 Acc: 50.6012\n",
      "validation Loss: 0.0122 Acc: 51.3452\n",
      "Epoch 16/99\n",
      "training Loss: 0.0123 Acc: 50.2678\n",
      "validation Loss: 0.0122 Acc: 50.0000\n",
      "Epoch 17/99\n",
      "training Loss: 0.0123 Acc: 50.0238\n",
      "validation Loss: 0.0121 Acc: 51.9881\n",
      "Epoch 18/99\n",
      "training Loss: 0.0123 Acc: 50.8928\n",
      "validation Loss: 0.0121 Acc: 50.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0123 Acc: 50.5773\n",
      "validation Loss: 0.0121 Acc: 52.0119\n",
      "Epoch 20/99\n",
      "training Loss: 0.0123 Acc: 50.7886\n",
      "validation Loss: 0.0121 Acc: 52.0595\n",
      "Early stopped.\n",
      "Best val acc: 79.440476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0115 Acc: 72.3409\n",
      "validation Loss: 0.0095 Acc: 77.8810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0117 Acc: 67.8888\n",
      "validation Loss: 0.0102 Acc: 75.5833\n",
      "Epoch 2/99\n",
      "training Loss: 0.0110 Acc: 69.4363\n",
      "validation Loss: 0.0113 Acc: 68.5952\n",
      "Epoch 3/99\n",
      "training Loss: 0.0122 Acc: 59.2733\n",
      "validation Loss: 0.0120 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0121 Acc: 57.6097\n",
      "validation Loss: 0.0118 Acc: 67.6786\n",
      "Epoch 5/99\n",
      "training Loss: 0.0121 Acc: 58.8447\n",
      "validation Loss: 0.0114 Acc: 71.9286\n",
      "Epoch 6/99\n",
      "training Loss: 0.0119 Acc: 61.4428\n",
      "validation Loss: 0.0109 Acc: 76.4048\n",
      "Epoch 7/99\n",
      "training Loss: 0.0117 Acc: 64.8622\n",
      "validation Loss: 0.0106 Acc: 76.6190\n",
      "Epoch 8/99\n",
      "training Loss: 0.0117 Acc: 65.5348\n",
      "validation Loss: 0.0110 Acc: 75.1548\n",
      "Epoch 9/99\n",
      "training Loss: 0.0117 Acc: 65.9931\n",
      "validation Loss: 0.0106 Acc: 76.7262\n",
      "Epoch 10/99\n",
      "training Loss: 0.0116 Acc: 65.8651\n",
      "validation Loss: 0.0105 Acc: 78.0595\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0116 Acc: 66.0615\n",
      "validation Loss: 0.0108 Acc: 78.1548\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0116 Acc: 65.9752\n",
      "validation Loss: 0.0104 Acc: 78.0476\n",
      "Epoch 13/99\n",
      "training Loss: 0.0116 Acc: 67.1746\n",
      "validation Loss: 0.0107 Acc: 78.1429\n",
      "Epoch 14/99\n",
      "training Loss: 0.0115 Acc: 67.3829\n",
      "validation Loss: 0.0103 Acc: 78.8690\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0115 Acc: 67.7103\n",
      "validation Loss: 0.0103 Acc: 79.3810\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0115 Acc: 67.4216\n",
      "validation Loss: 0.0106 Acc: 79.4643\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0115 Acc: 66.5734\n",
      "validation Loss: 0.0104 Acc: 79.9048\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0115 Acc: 66.5109\n",
      "validation Loss: 0.0102 Acc: 80.7143\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0114 Acc: 68.1745\n",
      "validation Loss: 0.0101 Acc: 81.0833\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0113 Acc: 68.3055\n",
      "validation Loss: 0.0101 Acc: 81.3452\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0114 Acc: 68.6834\n",
      "validation Loss: 0.0101 Acc: 81.4643\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0114 Acc: 68.9632\n",
      "validation Loss: 0.0101 Acc: 81.2857\n",
      "Epoch 23/99\n",
      "training Loss: 0.0114 Acc: 68.8144\n",
      "validation Loss: 0.0102 Acc: 81.4167\n",
      "Epoch 24/99\n",
      "training Loss: 0.0114 Acc: 68.8173\n",
      "validation Loss: 0.0100 Acc: 81.4524\n",
      "Epoch 25/99\n",
      "training Loss: 0.0114 Acc: 68.8560\n",
      "validation Loss: 0.0101 Acc: 81.3333\n",
      "Epoch 26/99\n",
      "training Loss: 0.0113 Acc: 68.7548\n",
      "validation Loss: 0.0099 Acc: 81.7381\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0113 Acc: 69.3233\n",
      "validation Loss: 0.0101 Acc: 81.6071\n",
      "Epoch 28/99\n",
      "training Loss: 0.0113 Acc: 69.7786\n",
      "validation Loss: 0.0100 Acc: 81.6667\n",
      "Epoch 29/99\n",
      "training Loss: 0.0113 Acc: 69.5405\n",
      "validation Loss: 0.0099 Acc: 81.6310\n",
      "Epoch 30/99\n",
      "training Loss: 0.0114 Acc: 68.7548\n",
      "validation Loss: 0.0102 Acc: 81.7619\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0114 Acc: 69.2042\n",
      "validation Loss: 0.0101 Acc: 81.7262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0113 Acc: 69.7667\n",
      "validation Loss: 0.0100 Acc: 81.8333\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0114 Acc: 69.4542\n",
      "validation Loss: 0.0100 Acc: 81.8214\n",
      "Epoch 34/99\n",
      "training Loss: 0.0113 Acc: 69.8262\n",
      "validation Loss: 0.0100 Acc: 81.8690\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0113 Acc: 69.6536\n",
      "validation Loss: 0.0099 Acc: 81.8929\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0113 Acc: 69.8738\n",
      "validation Loss: 0.0100 Acc: 81.9762\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0113 Acc: 70.1446\n",
      "validation Loss: 0.0100 Acc: 81.9405\n",
      "Epoch 38/99\n",
      "training Loss: 0.0113 Acc: 69.8738\n",
      "validation Loss: 0.0099 Acc: 81.9643\n",
      "Epoch 39/99\n",
      "training Loss: 0.0113 Acc: 69.8411\n",
      "validation Loss: 0.0100 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0113 Acc: 69.8143\n",
      "validation Loss: 0.0099 Acc: 81.9524\n",
      "Epoch 41/99\n",
      "training Loss: 0.0113 Acc: 69.7220\n",
      "validation Loss: 0.0100 Acc: 82.0000\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0113 Acc: 69.8143\n",
      "validation Loss: 0.0100 Acc: 81.8690\n",
      "Epoch 43/99\n",
      "training Loss: 0.0113 Acc: 69.8441\n",
      "validation Loss: 0.0100 Acc: 81.9286\n",
      "Epoch 44/99\n",
      "training Loss: 0.0114 Acc: 69.5435\n",
      "validation Loss: 0.0100 Acc: 81.9286\n",
      "Epoch 45/99\n",
      "training Loss: 0.0113 Acc: 70.0077\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 46/99\n",
      "training Loss: 0.0113 Acc: 69.8619\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 47/99\n",
      "training Loss: 0.0113 Acc: 69.7905\n",
      "validation Loss: 0.0100 Acc: 81.9405\n",
      "Epoch 48/99\n",
      "training Loss: 0.0113 Acc: 69.7131\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 49/99\n",
      "training Loss: 0.0113 Acc: 69.8649\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 50/99\n",
      "training Loss: 0.0113 Acc: 69.8976\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 51/99\n",
      "training Loss: 0.0113 Acc: 69.8738\n",
      "validation Loss: 0.0100 Acc: 81.9643\n",
      "Epoch 52/99\n",
      "training Loss: 0.0113 Acc: 69.9810\n",
      "validation Loss: 0.0100 Acc: 81.9643\n",
      "Epoch 53/99\n",
      "training Loss: 0.0113 Acc: 70.0167\n",
      "validation Loss: 0.0099 Acc: 81.9643\n",
      "Epoch 54/99\n",
      "training Loss: 0.0113 Acc: 69.7935\n",
      "validation Loss: 0.0100 Acc: 81.9762\n",
      "Epoch 55/99\n",
      "training Loss: 0.0113 Acc: 70.0940\n",
      "validation Loss: 0.0100 Acc: 81.9762\n",
      "Epoch 56/99\n",
      "training Loss: 0.0113 Acc: 69.9810\n",
      "validation Loss: 0.0100 Acc: 81.9762\n",
      "Epoch 57/99\n",
      "training Loss: 0.0113 Acc: 69.8500\n",
      "validation Loss: 0.0100 Acc: 81.9762\n",
      "Epoch 58/99\n",
      "training Loss: 0.0113 Acc: 69.8827\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 59/99\n",
      "training Loss: 0.0113 Acc: 69.7339\n",
      "validation Loss: 0.0100 Acc: 81.9524\n",
      "Epoch 60/99\n",
      "training Loss: 0.0113 Acc: 70.3946\n",
      "validation Loss: 0.0099 Acc: 81.9524\n",
      "Epoch 61/99\n",
      "training Loss: 0.0113 Acc: 70.0494\n",
      "validation Loss: 0.0099 Acc: 81.9762\n",
      "Early stopped.\n",
      "Best val acc: 82.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0141 Acc: 68.8054\n",
      "validation Loss: 0.0118 Acc: 76.6667\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0147 Acc: 57.1781\n",
      "validation Loss: 0.0103 Acc: 79.1786\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0118 Acc: 56.4818\n",
      "validation Loss: 0.0112 Acc: 74.8214\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 59.5530\n",
      "validation Loss: 0.0096 Acc: 80.1071\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0118 Acc: 59.6036\n",
      "validation Loss: 0.0090 Acc: 80.5952\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0113 Acc: 62.8951\n",
      "validation Loss: 0.0093 Acc: 81.4881\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0111 Acc: 65.7639\n",
      "validation Loss: 0.0098 Acc: 79.4048\n",
      "Epoch 7/99\n",
      "training Loss: 0.0114 Acc: 61.9368\n",
      "validation Loss: 0.0098 Acc: 80.2976\n",
      "Epoch 8/99\n",
      "training Loss: 0.0114 Acc: 61.1273\n",
      "validation Loss: 0.0101 Acc: 80.6310\n",
      "Epoch 9/99\n",
      "training Loss: 0.0114 Acc: 60.8714\n",
      "validation Loss: 0.0096 Acc: 81.1905\n",
      "Epoch 10/99\n",
      "training Loss: 0.0117 Acc: 60.9398\n",
      "validation Loss: 0.0101 Acc: 80.4881\n",
      "Epoch 11/99\n",
      "training Loss: 0.0115 Acc: 60.8863\n",
      "validation Loss: 0.0107 Acc: 80.4167\n",
      "Epoch 12/99\n",
      "training Loss: 0.0115 Acc: 61.1422\n",
      "validation Loss: 0.0100 Acc: 79.9643\n",
      "Epoch 13/99\n",
      "training Loss: 0.0114 Acc: 61.1481\n",
      "validation Loss: 0.0097 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0115 Acc: 60.9964\n",
      "validation Loss: 0.0106 Acc: 81.7857\n",
      "Epoch 15/99\n",
      "training Loss: 0.0116 Acc: 60.9488\n",
      "validation Loss: 0.0094 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0118 Acc: 61.4725\n",
      "validation Loss: 0.0095 Acc: 82.2976\n",
      "Epoch 17/99\n",
      "training Loss: 0.0109 Acc: 60.9696\n",
      "validation Loss: 0.0103 Acc: 81.9762\n",
      "Epoch 18/99\n",
      "training Loss: 0.0109 Acc: 61.3862\n",
      "validation Loss: 0.0102 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0108 Acc: 62.2195\n",
      "validation Loss: 0.0094 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 64.9604\n",
      "validation Loss: 0.0153 Acc: 82.8333\n",
      "Epoch 21/99\n",
      "training Loss: 0.0122 Acc: 65.4277\n",
      "validation Loss: 0.0108 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0108 Acc: 65.4336\n",
      "validation Loss: 0.0113 Acc: 83.1310\n",
      "Epoch 23/99\n",
      "training Loss: 0.0104 Acc: 65.3443\n",
      "validation Loss: 0.0107 Acc: 83.2143\n",
      "Epoch 24/99\n",
      "training Loss: 0.0105 Acc: 65.6717\n",
      "validation Loss: 0.0109 Acc: 83.0595\n",
      "Epoch 25/99\n",
      "training Loss: 0.0103 Acc: 66.8323\n",
      "validation Loss: 0.0105 Acc: 83.0357\n",
      "Epoch 26/99\n",
      "training Loss: 0.0100 Acc: 67.3859\n",
      "validation Loss: 0.0106 Acc: 82.9762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0099 Acc: 69.2399\n",
      "validation Loss: 0.0104 Acc: 83.0952\n",
      "Epoch 28/99\n",
      "training Loss: 0.0096 Acc: 71.0761\n",
      "validation Loss: 0.0099 Acc: 80.0357\n",
      "Epoch 29/99\n",
      "training Loss: 0.0093 Acc: 71.5106\n",
      "validation Loss: 0.0098 Acc: 79.8690\n",
      "Epoch 30/99\n",
      "training Loss: 0.0092 Acc: 71.9332\n",
      "validation Loss: 0.0088 Acc: 82.4524\n",
      "Epoch 31/99\n",
      "training Loss: 0.0090 Acc: 72.2249\n",
      "validation Loss: 0.0092 Acc: 81.0952\n",
      "Epoch 32/99\n",
      "training Loss: 0.0089 Acc: 73.0284\n",
      "validation Loss: 0.0090 Acc: 81.7619\n",
      "Epoch 33/99\n",
      "training Loss: 0.0089 Acc: 72.9957\n",
      "validation Loss: 0.0093 Acc: 78.8095\n",
      "Epoch 34/99\n",
      "training Loss: 0.0086 Acc: 74.4331\n",
      "validation Loss: 0.0090 Acc: 81.4167\n",
      "Epoch 35/99\n",
      "training Loss: 0.0086 Acc: 75.3199\n",
      "validation Loss: 0.0090 Acc: 80.0595\n",
      "Epoch 36/99\n",
      "training Loss: 0.0085 Acc: 75.6949\n",
      "validation Loss: 0.0085 Acc: 82.5238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0084 Acc: 76.0967\n",
      "validation Loss: 0.0087 Acc: 81.8333\n",
      "Epoch 38/99\n",
      "training Loss: 0.0083 Acc: 77.0520\n",
      "validation Loss: 0.0088 Acc: 80.4643\n",
      "Epoch 39/99\n",
      "training Loss: 0.0081 Acc: 77.4061\n",
      "validation Loss: 0.0085 Acc: 82.2500\n",
      "Epoch 40/99\n",
      "training Loss: 0.0082 Acc: 77.5192\n",
      "validation Loss: 0.0089 Acc: 79.8810\n",
      "Epoch 41/99\n",
      "training Loss: 0.0081 Acc: 77.8852\n",
      "validation Loss: 0.0091 Acc: 78.0357\n",
      "Early stopped.\n",
      "Best val acc: 83.297619\n",
      "----------\n",
      "Average best_acc across k-fold: 77.1818569275\n",
      "New configuration: {'learning_rate': 0.0016629351485803244, 'initial_nodes': 200, 'dropout': 0.01, 'batch_size': 512, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0008 Acc: 81.5238\n",
      "validation Loss: 0.0008 Acc: 83.2302\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.3690\n",
      "validation Loss: 0.0007 Acc: 83.2540\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.6161\n",
      "validation Loss: 0.0007 Acc: 83.2540\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.7440\n",
      "validation Loss: 0.0007 Acc: 83.6110\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.7857\n",
      "validation Loss: 0.0007 Acc: 83.3968\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.8244\n",
      "validation Loss: 0.0007 Acc: 83.5753\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.9821\n",
      "validation Loss: 0.0007 Acc: 83.5515\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 84.0089\n",
      "validation Loss: 0.0007 Acc: 83.6706\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.9732\n",
      "validation Loss: 0.0007 Acc: 83.5158\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 84.0446\n",
      "validation Loss: 0.0007 Acc: 83.6825\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 84.2083\n",
      "validation Loss: 0.0007 Acc: 83.7063\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 84.3452\n",
      "validation Loss: 0.0007 Acc: 83.1231\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 84.2649\n",
      "validation Loss: 0.0007 Acc: 83.5753\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.2946\n",
      "validation Loss: 0.0007 Acc: 83.8134\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.3750\n",
      "validation Loss: 0.0007 Acc: 83.2302\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.3304\n",
      "validation Loss: 0.0007 Acc: 83.5277\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.3571\n",
      "validation Loss: 0.0007 Acc: 83.6706\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.3839\n",
      "validation Loss: 0.0007 Acc: 83.5634\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.5536\n",
      "validation Loss: 0.0007 Acc: 83.2897\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.6488\n",
      "validation Loss: 0.0007 Acc: 83.8134\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.6845\n",
      "validation Loss: 0.0007 Acc: 83.5753\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.9643\n",
      "validation Loss: 0.0007 Acc: 83.5991\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.7292\n",
      "validation Loss: 0.0007 Acc: 83.6468\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.9970\n",
      "validation Loss: 0.0007 Acc: 83.6348\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.9196\n",
      "validation Loss: 0.0007 Acc: 83.5515\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.9881\n",
      "validation Loss: 0.0007 Acc: 83.4920\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.9821\n",
      "validation Loss: 0.0007 Acc: 83.4087\n",
      "Epoch 27/99\n",
      "training Loss: 0.0006 Acc: 85.1667\n",
      "validation Loss: 0.0007 Acc: 83.3849\n",
      "Epoch 28/99\n",
      "training Loss: 0.0006 Acc: 85.0595\n",
      "validation Loss: 0.0007 Acc: 83.5753\n",
      "Epoch 29/99\n",
      "training Loss: 0.0006 Acc: 85.1190\n",
      "validation Loss: 0.0007 Acc: 83.5991\n",
      "Epoch 30/99\n",
      "training Loss: 0.0006 Acc: 85.1667\n",
      "validation Loss: 0.0007 Acc: 83.4682\n",
      "Epoch 31/99\n",
      "training Loss: 0.0006 Acc: 85.1696\n",
      "validation Loss: 0.0007 Acc: 83.5396\n",
      "Epoch 32/99\n",
      "training Loss: 0.0006 Acc: 85.2143\n",
      "validation Loss: 0.0007 Acc: 83.5753\n",
      "Epoch 33/99\n",
      "training Loss: 0.0006 Acc: 85.2589\n",
      "validation Loss: 0.0007 Acc: 83.4563\n",
      "Early stopped.\n",
      "Best val acc: 83.813378\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0008 Acc: 81.5993\n",
      "validation Loss: 0.0008 Acc: 82.1071\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.5337\n",
      "validation Loss: 0.0008 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.7926\n",
      "validation Loss: 0.0008 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.8938\n",
      "validation Loss: 0.0008 Acc: 82.6667\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.9950\n",
      "validation Loss: 0.0008 Acc: 82.6310\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.9087\n",
      "validation Loss: 0.0008 Acc: 82.6310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 84.1646\n",
      "validation Loss: 0.0007 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 84.0962\n",
      "validation Loss: 0.0008 Acc: 82.8095\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 84.2390\n",
      "validation Loss: 0.0008 Acc: 82.6786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 84.2956\n",
      "validation Loss: 0.0007 Acc: 82.8214\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 84.2420\n",
      "validation Loss: 0.0007 Acc: 82.5952\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 84.1765\n",
      "validation Loss: 0.0007 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 84.3521\n",
      "validation Loss: 0.0007 Acc: 82.8690\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.3789\n",
      "validation Loss: 0.0008 Acc: 82.7143\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.5426\n",
      "validation Loss: 0.0007 Acc: 82.9405\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.4890\n",
      "validation Loss: 0.0007 Acc: 82.9405\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.6170\n",
      "validation Loss: 0.0007 Acc: 82.8810\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.6765\n",
      "validation Loss: 0.0007 Acc: 82.6071\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.6110\n",
      "validation Loss: 0.0007 Acc: 83.0119\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.7211\n",
      "validation Loss: 0.0007 Acc: 82.6905\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.7122\n",
      "validation Loss: 0.0007 Acc: 83.0000\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.7152\n",
      "validation Loss: 0.0007 Acc: 82.7262\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.8283\n",
      "validation Loss: 0.0007 Acc: 82.8095\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.9443\n",
      "validation Loss: 0.0007 Acc: 82.9048\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 85.0812\n",
      "validation Loss: 0.0007 Acc: 82.7500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0006 Acc: 85.1854\n",
      "validation Loss: 0.0007 Acc: 82.8690\n",
      "Epoch 26/99\n",
      "training Loss: 0.0006 Acc: 85.2300\n",
      "validation Loss: 0.0007 Acc: 83.0000\n",
      "Epoch 27/99\n",
      "training Loss: 0.0006 Acc: 85.1973\n",
      "validation Loss: 0.0007 Acc: 82.9762\n",
      "Epoch 28/99\n",
      "training Loss: 0.0006 Acc: 85.3848\n",
      "validation Loss: 0.0007 Acc: 82.7857\n",
      "Epoch 29/99\n",
      "training Loss: 0.0006 Acc: 85.3074\n",
      "validation Loss: 0.0007 Acc: 82.7738\n",
      "Epoch 30/99\n",
      "training Loss: 0.0006 Acc: 85.2390\n",
      "validation Loss: 0.0007 Acc: 82.7143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0006 Acc: 85.3431\n",
      "validation Loss: 0.0007 Acc: 82.6667\n",
      "Early stopped.\n",
      "Best val acc: 83.142857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0008 Acc: 80.9892\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.0992\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.3581\n",
      "validation Loss: 0.0007 Acc: 83.9167\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.3522\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.6498\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.7153\n",
      "validation Loss: 0.0007 Acc: 84.2143\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.6825\n",
      "validation Loss: 0.0007 Acc: 83.9405\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.8254\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.7450\n",
      "validation Loss: 0.0007 Acc: 83.8571\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 84.0664\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.8760\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.8938\n",
      "validation Loss: 0.0007 Acc: 84.1548\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 84.0456\n",
      "validation Loss: 0.0007 Acc: 84.1071\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.0932\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.1974\n",
      "validation Loss: 0.0007 Acc: 84.2500\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.4087\n",
      "validation Loss: 0.0007 Acc: 84.2381\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.3670\n",
      "validation Loss: 0.0007 Acc: 84.3571\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.3968\n",
      "validation Loss: 0.0007 Acc: 84.1071\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.5069\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.4265\n",
      "validation Loss: 0.0007 Acc: 84.3333\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.3908\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.5366\n",
      "validation Loss: 0.0007 Acc: 84.3810\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.5783\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.6765\n",
      "validation Loss: 0.0007 Acc: 84.2738\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.7688\n",
      "validation Loss: 0.0007 Acc: 84.3214\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.6557\n",
      "validation Loss: 0.0007 Acc: 84.1786\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.7747\n",
      "validation Loss: 0.0007 Acc: 84.3095\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.6735\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.8283\n",
      "validation Loss: 0.0007 Acc: 84.2381\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.9086\n",
      "validation Loss: 0.0007 Acc: 84.1905\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.8610\n",
      "validation Loss: 0.0007 Acc: 84.3214\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.9086\n",
      "validation Loss: 0.0007 Acc: 84.3929\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0006 Acc: 84.8699\n",
      "validation Loss: 0.0007 Acc: 84.2857\n",
      "Epoch 33/99\n",
      "training Loss: 0.0006 Acc: 84.9711\n",
      "validation Loss: 0.0007 Acc: 84.1905\n",
      "Epoch 34/99\n",
      "training Loss: 0.0006 Acc: 84.8819\n",
      "validation Loss: 0.0007 Acc: 84.2738\n",
      "Epoch 35/99\n",
      "training Loss: 0.0006 Acc: 84.9473\n",
      "validation Loss: 0.0007 Acc: 84.4524\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0006 Acc: 84.9503\n",
      "validation Loss: 0.0007 Acc: 84.2143\n",
      "Epoch 37/99\n",
      "training Loss: 0.0006 Acc: 84.9027\n",
      "validation Loss: 0.0007 Acc: 84.2857\n",
      "Epoch 38/99\n",
      "training Loss: 0.0006 Acc: 85.0039\n",
      "validation Loss: 0.0007 Acc: 84.2619\n",
      "Epoch 39/99\n",
      "training Loss: 0.0006 Acc: 84.9979\n",
      "validation Loss: 0.0007 Acc: 84.2976\n",
      "Epoch 40/99\n",
      "training Loss: 0.0006 Acc: 85.0455\n",
      "validation Loss: 0.0007 Acc: 84.1905\n",
      "Epoch 41/99\n",
      "training Loss: 0.0006 Acc: 85.0455\n",
      "validation Loss: 0.0007 Acc: 84.1905\n",
      "Epoch 42/99\n",
      "training Loss: 0.0006 Acc: 84.9920\n",
      "validation Loss: 0.0007 Acc: 84.2500\n",
      "Epoch 43/99\n",
      "training Loss: 0.0006 Acc: 84.9741\n",
      "validation Loss: 0.0007 Acc: 84.1786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0006 Acc: 85.0366\n",
      "validation Loss: 0.0007 Acc: 84.2262\n",
      "Epoch 45/99\n",
      "training Loss: 0.0006 Acc: 85.0277\n",
      "validation Loss: 0.0007 Acc: 84.1905\n",
      "Epoch 46/99\n",
      "training Loss: 0.0006 Acc: 85.0783\n",
      "validation Loss: 0.0007 Acc: 84.3095\n",
      "Epoch 47/99\n",
      "training Loss: 0.0006 Acc: 85.0991\n",
      "validation Loss: 0.0007 Acc: 84.2738\n",
      "Epoch 48/99\n",
      "training Loss: 0.0006 Acc: 84.9979\n",
      "validation Loss: 0.0007 Acc: 84.2381\n",
      "Epoch 49/99\n",
      "training Loss: 0.0006 Acc: 85.0931\n",
      "validation Loss: 0.0007 Acc: 84.2619\n",
      "Epoch 50/99\n",
      "training Loss: 0.0006 Acc: 85.0842\n",
      "validation Loss: 0.0007 Acc: 84.1667\n",
      "Epoch 51/99\n",
      "training Loss: 0.0006 Acc: 85.0634\n",
      "validation Loss: 0.0007 Acc: 84.2143\n",
      "Epoch 52/99\n",
      "training Loss: 0.0006 Acc: 85.0128\n",
      "validation Loss: 0.0007 Acc: 84.1310\n",
      "Epoch 53/99\n",
      "training Loss: 0.0006 Acc: 85.1408\n",
      "validation Loss: 0.0007 Acc: 84.1786\n",
      "Epoch 54/99\n",
      "training Loss: 0.0006 Acc: 85.0902\n",
      "validation Loss: 0.0007 Acc: 84.2381\n",
      "Epoch 55/99\n",
      "training Loss: 0.0006 Acc: 85.0545\n",
      "validation Loss: 0.0007 Acc: 84.1667\n",
      "Early stopped.\n",
      "Best val acc: 84.452381\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0008 Acc: 81.2868\n",
      "validation Loss: 0.0008 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.1558\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.4861\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.6617\n",
      "validation Loss: 0.0007 Acc: 83.6071\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.8075\n",
      "validation Loss: 0.0007 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.8016\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.8730\n",
      "validation Loss: 0.0007 Acc: 83.5357\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.9593\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 84.1230\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 84.2122\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 84.2063\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 84.0992\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 84.1676\n",
      "validation Loss: 0.0007 Acc: 83.9643\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.1765\n",
      "validation Loss: 0.0007 Acc: 83.6071\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.3551\n",
      "validation Loss: 0.0007 Acc: 83.5595\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.3015\n",
      "validation Loss: 0.0007 Acc: 83.5714\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.2063\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.3551\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.4355\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.5604\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.6319\n",
      "validation Loss: 0.0007 Acc: 83.6190\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.4771\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.6051\n",
      "validation Loss: 0.0007 Acc: 83.4881\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.5991\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.7658\n",
      "validation Loss: 0.0007 Acc: 83.6905\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.9146\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 85.0545\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.9295\n",
      "validation Loss: 0.0007 Acc: 83.5952\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 85.0068\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 85.0485\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 30/99\n",
      "training Loss: 0.0006 Acc: 85.1080\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 31/99\n",
      "training Loss: 0.0006 Acc: 85.2152\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Early stopped.\n",
      "Best val acc: 84.059524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0008 Acc: 81.2392\n",
      "validation Loss: 0.0008 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.2510\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.4801\n",
      "validation Loss: 0.0007 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.8432\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.7272\n",
      "validation Loss: 0.0007 Acc: 83.0476\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.8283\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.9742\n",
      "validation Loss: 0.0007 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.8819\n",
      "validation Loss: 0.0007 Acc: 83.1667\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 84.0694\n",
      "validation Loss: 0.0007 Acc: 83.8333\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 84.0605\n",
      "validation Loss: 0.0007 Acc: 83.6190\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 84.1676\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 84.0634\n",
      "validation Loss: 0.0007 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.1617\n",
      "validation Loss: 0.0007 Acc: 83.2024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.2480\n",
      "validation Loss: 0.0007 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.4027\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.3491\n",
      "validation Loss: 0.0007 Acc: 83.7619\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.4414\n",
      "validation Loss: 0.0007 Acc: 83.7024\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.5009\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.4146\n",
      "validation Loss: 0.0007 Acc: 83.8333\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.4146\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.6200\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.7807\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.9235\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.8402\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.9324\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.9176\n",
      "validation Loss: 0.0007 Acc: 83.7738\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.9176\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.9265\n",
      "validation Loss: 0.0007 Acc: 83.9405\n",
      "Epoch 29/99\n",
      "training Loss: 0.0006 Acc: 85.1854\n",
      "validation Loss: 0.0007 Acc: 83.8571\n",
      "Epoch 30/99\n",
      "training Loss: 0.0006 Acc: 85.1914\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 31/99\n",
      "training Loss: 0.0006 Acc: 85.1705\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Epoch 32/99\n",
      "training Loss: 0.0006 Acc: 85.0842\n",
      "validation Loss: 0.0007 Acc: 84.0952\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0006 Acc: 85.1170\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 34/99\n",
      "training Loss: 0.0006 Acc: 85.1467\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Epoch 35/99\n",
      "training Loss: 0.0006 Acc: 85.3759\n",
      "validation Loss: 0.0007 Acc: 84.0476\n",
      "Epoch 36/99\n",
      "training Loss: 0.0006 Acc: 85.3699\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Epoch 37/99\n",
      "training Loss: 0.0006 Acc: 85.2896\n",
      "validation Loss: 0.0007 Acc: 83.7738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0006 Acc: 85.5128\n",
      "validation Loss: 0.0007 Acc: 84.1190\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0006 Acc: 85.3640\n",
      "validation Loss: 0.0007 Acc: 83.7619\n",
      "Epoch 40/99\n",
      "training Loss: 0.0006 Acc: 85.4384\n",
      "validation Loss: 0.0007 Acc: 84.1548\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0006 Acc: 85.4235\n",
      "validation Loss: 0.0007 Acc: 84.0357\n",
      "Epoch 42/99\n",
      "training Loss: 0.0006 Acc: 85.3669\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 43/99\n",
      "training Loss: 0.0006 Acc: 85.4681\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Epoch 44/99\n",
      "training Loss: 0.0006 Acc: 85.4294\n",
      "validation Loss: 0.0007 Acc: 84.0000\n",
      "Epoch 45/99\n",
      "training Loss: 0.0006 Acc: 85.3759\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Epoch 46/99\n",
      "training Loss: 0.0006 Acc: 85.4592\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Epoch 47/99\n",
      "training Loss: 0.0006 Acc: 85.5217\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Epoch 48/99\n",
      "training Loss: 0.0006 Acc: 85.4681\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Epoch 49/99\n",
      "training Loss: 0.0006 Acc: 85.5187\n",
      "validation Loss: 0.0007 Acc: 84.0000\n",
      "Epoch 50/99\n",
      "training Loss: 0.0006 Acc: 85.5425\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 51/99\n",
      "training Loss: 0.0006 Acc: 85.5425\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Epoch 52/99\n",
      "training Loss: 0.0006 Acc: 85.5306\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Epoch 53/99\n",
      "training Loss: 0.0006 Acc: 85.5187\n",
      "validation Loss: 0.0007 Acc: 84.0000\n",
      "Epoch 54/99\n",
      "training Loss: 0.0006 Acc: 85.5396\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Epoch 55/99\n",
      "training Loss: 0.0006 Acc: 85.5574\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Epoch 56/99\n",
      "training Loss: 0.0006 Acc: 85.5038\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Epoch 57/99\n",
      "training Loss: 0.0006 Acc: 85.5931\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 58/99\n",
      "training Loss: 0.0006 Acc: 85.5515\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Epoch 59/99\n",
      "training Loss: 0.0006 Acc: 85.4800\n",
      "validation Loss: 0.0007 Acc: 83.9881\n",
      "Epoch 60/99\n",
      "training Loss: 0.0006 Acc: 85.5604\n",
      "validation Loss: 0.0007 Acc: 83.9643\n",
      "Early stopped.\n",
      "Best val acc: 84.154762\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9245803153\n",
      "New configuration: {'learning_rate': 1.0074098311341868e-05, 'initial_nodes': 383, 'dropout': 0.4142490721541388, 'batch_size': 284, 'max_depth': 5}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0025 Acc: 49.9137\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 50.0923\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 50.0149\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 50.1250\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 50.4405\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 51.6399\n",
      "validation Loss: 0.0024 Acc: 50.0476\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0024 Acc: 53.7917\n",
      "validation Loss: 0.0024 Acc: 63.7943\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 56.6518\n",
      "validation Loss: 0.0023 Acc: 75.9700\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 59.2738\n",
      "validation Loss: 0.0023 Acc: 77.4458\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 61.4524\n",
      "validation Loss: 0.0022 Acc: 78.6955\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0022 Acc: 65.1101\n",
      "validation Loss: 0.0021 Acc: 79.5049\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 66.6250\n",
      "validation Loss: 0.0021 Acc: 80.3142\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 69.3810\n",
      "validation Loss: 0.0020 Acc: 81.0045\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 71.2798\n",
      "validation Loss: 0.0020 Acc: 81.3497\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 72.4375\n",
      "validation Loss: 0.0020 Acc: 81.7186\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 74.1696\n",
      "validation Loss: 0.0019 Acc: 81.9567\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 74.3601\n",
      "validation Loss: 0.0019 Acc: 82.0519\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 75.5238\n",
      "validation Loss: 0.0019 Acc: 82.3137\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 76.1458\n",
      "validation Loss: 0.0018 Acc: 82.3613\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 76.1756\n",
      "validation Loss: 0.0018 Acc: 82.3613\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 76.2946\n",
      "validation Loss: 0.0018 Acc: 82.1709\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 76.6339\n",
      "validation Loss: 0.0017 Acc: 81.8972\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 76.8095\n",
      "validation Loss: 0.0017 Acc: 81.9091\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 76.8750\n",
      "validation Loss: 0.0017 Acc: 81.8615\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 77.1905\n",
      "validation Loss: 0.0016 Acc: 81.8853\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 77.8810\n",
      "validation Loss: 0.0016 Acc: 81.8496\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 77.7589\n",
      "validation Loss: 0.0016 Acc: 81.8020\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 77.8720\n",
      "validation Loss: 0.0015 Acc: 81.8258\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 78.6667\n",
      "validation Loss: 0.0015 Acc: 81.8615\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 78.5804\n",
      "validation Loss: 0.0015 Acc: 81.9329\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 78.9911\n",
      "validation Loss: 0.0015 Acc: 81.9210\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 79.0744\n",
      "validation Loss: 0.0015 Acc: 81.9448\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 79.3304\n",
      "validation Loss: 0.0015 Acc: 82.0638\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 79.4851\n",
      "validation Loss: 0.0015 Acc: 81.9924\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 79.7292\n",
      "validation Loss: 0.0015 Acc: 82.1471\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 80.0952\n",
      "validation Loss: 0.0015 Acc: 82.2066\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 80.4673\n",
      "validation Loss: 0.0014 Acc: 82.2780\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 80.2202\n",
      "validation Loss: 0.0014 Acc: 82.2423\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 80.4673\n",
      "validation Loss: 0.0014 Acc: 82.2304\n",
      "Early stopped.\n",
      "Best val acc: 82.361343\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0025 Acc: 49.9970\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 49.9970\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0025 Acc: 49.9970\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0025 Acc: 50.0149\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 50.2411\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 51.5118\n",
      "validation Loss: 0.0024 Acc: 50.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 54.3331\n",
      "validation Loss: 0.0023 Acc: 50.1548\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 57.8448\n",
      "validation Loss: 0.0022 Acc: 71.9524\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0022 Acc: 60.6750\n",
      "validation Loss: 0.0021 Acc: 79.2262\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 64.2878\n",
      "validation Loss: 0.0020 Acc: 80.8929\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 66.7133\n",
      "validation Loss: 0.0019 Acc: 81.2024\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0020 Acc: 68.8590\n",
      "validation Loss: 0.0018 Acc: 81.5357\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0020 Acc: 70.8619\n",
      "validation Loss: 0.0018 Acc: 81.7143\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 72.6564\n",
      "validation Loss: 0.0017 Acc: 81.7024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 73.5373\n",
      "validation Loss: 0.0017 Acc: 81.5833\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 74.2694\n",
      "validation Loss: 0.0016 Acc: 81.7500\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 75.4747\n",
      "validation Loss: 0.0016 Acc: 81.8333\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0018 Acc: 75.5788\n",
      "validation Loss: 0.0015 Acc: 81.6905\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 76.2306\n",
      "validation Loss: 0.0015 Acc: 81.8095\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 76.6681\n",
      "validation Loss: 0.0015 Acc: 81.7738\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 76.8436\n",
      "validation Loss: 0.0015 Acc: 82.1429\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0017 Acc: 77.5311\n",
      "validation Loss: 0.0015 Acc: 82.0833\n",
      "Epoch 22/99\n",
      "training Loss: 0.0017 Acc: 77.4865\n",
      "validation Loss: 0.0015 Acc: 81.9881\n",
      "Epoch 23/99\n",
      "training Loss: 0.0017 Acc: 77.8971\n",
      "validation Loss: 0.0014 Acc: 82.2024\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0017 Acc: 78.2483\n",
      "validation Loss: 0.0014 Acc: 82.2857\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0017 Acc: 78.2632\n",
      "validation Loss: 0.0014 Acc: 82.2500\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 78.4685\n",
      "validation Loss: 0.0014 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 78.8405\n",
      "validation Loss: 0.0014 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 78.9298\n",
      "validation Loss: 0.0014 Acc: 82.4167\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 79.0191\n",
      "validation Loss: 0.0014 Acc: 82.5595\n",
      "Epoch 30/99\n",
      "training Loss: 0.0016 Acc: 79.3584\n",
      "validation Loss: 0.0014 Acc: 82.4881\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 79.3762\n",
      "validation Loss: 0.0014 Acc: 82.5595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 79.2631\n",
      "validation Loss: 0.0014 Acc: 82.5952\n",
      "Epoch 33/99\n",
      "training Loss: 0.0016 Acc: 79.3703\n",
      "validation Loss: 0.0014 Acc: 82.5595\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 79.4625\n",
      "validation Loss: 0.0014 Acc: 82.5952\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 79.5905\n",
      "validation Loss: 0.0014 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 80.2184\n",
      "validation Loss: 0.0014 Acc: 82.5238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 79.8167\n",
      "validation Loss: 0.0014 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 80.1678\n",
      "validation Loss: 0.0014 Acc: 82.7500\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 80.2036\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 80.2482\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 80.1649\n",
      "validation Loss: 0.0014 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 80.5785\n",
      "validation Loss: 0.0014 Acc: 82.8929\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 80.3524\n",
      "validation Loss: 0.0013 Acc: 82.9167\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 80.3553\n",
      "validation Loss: 0.0013 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 80.3970\n",
      "validation Loss: 0.0013 Acc: 83.0000\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 80.5934\n",
      "validation Loss: 0.0013 Acc: 82.9881\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 80.4595\n",
      "validation Loss: 0.0013 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 80.6619\n",
      "validation Loss: 0.0013 Acc: 83.0595\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 80.5369\n",
      "validation Loss: 0.0013 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 80.5875\n",
      "validation Loss: 0.0013 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 80.7809\n",
      "validation Loss: 0.0013 Acc: 83.2262\n",
      "Epoch 52/99\n",
      "training Loss: 0.0016 Acc: 80.4684\n",
      "validation Loss: 0.0013 Acc: 83.1905\n",
      "Epoch 53/99\n",
      "training Loss: 0.0016 Acc: 80.9119\n",
      "validation Loss: 0.0013 Acc: 83.1786\n",
      "Epoch 54/99\n",
      "training Loss: 0.0015 Acc: 80.8375\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0015 Acc: 81.0130\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Epoch 56/99\n",
      "training Loss: 0.0015 Acc: 81.3731\n",
      "validation Loss: 0.0013 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0015 Acc: 81.0071\n",
      "validation Loss: 0.0013 Acc: 83.2738\n",
      "Epoch 58/99\n",
      "training Loss: 0.0015 Acc: 80.9476\n",
      "validation Loss: 0.0013 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0015 Acc: 81.2273\n",
      "validation Loss: 0.0013 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0015 Acc: 81.1440\n",
      "validation Loss: 0.0013 Acc: 83.3690\n",
      "Epoch 61/99\n",
      "training Loss: 0.0015 Acc: 80.9743\n",
      "validation Loss: 0.0013 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0015 Acc: 81.2481\n",
      "validation Loss: 0.0013 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0015 Acc: 81.2095\n",
      "validation Loss: 0.0013 Acc: 83.4405\n",
      "Epoch 64/99\n",
      "training Loss: 0.0015 Acc: 80.9863\n",
      "validation Loss: 0.0013 Acc: 83.4286\n",
      "Epoch 65/99\n",
      "training Loss: 0.0015 Acc: 81.3225\n",
      "validation Loss: 0.0013 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0015 Acc: 81.2749\n",
      "validation Loss: 0.0013 Acc: 83.5714\n",
      "Epoch 67/99\n",
      "training Loss: 0.0015 Acc: 81.4297\n",
      "validation Loss: 0.0013 Acc: 83.4762\n",
      "Epoch 68/99\n",
      "training Loss: 0.0015 Acc: 81.2749\n",
      "validation Loss: 0.0013 Acc: 83.4881\n",
      "Epoch 69/99\n",
      "training Loss: 0.0015 Acc: 81.1291\n",
      "validation Loss: 0.0013 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0015 Acc: 81.4386\n",
      "validation Loss: 0.0013 Acc: 83.5476\n",
      "Epoch 71/99\n",
      "training Loss: 0.0015 Acc: 81.3255\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Epoch 72/99\n",
      "training Loss: 0.0015 Acc: 81.2690\n",
      "validation Loss: 0.0013 Acc: 83.5000\n",
      "Epoch 73/99\n",
      "training Loss: 0.0015 Acc: 81.3225\n",
      "validation Loss: 0.0013 Acc: 83.5000\n",
      "Epoch 74/99\n",
      "training Loss: 0.0015 Acc: 81.6469\n",
      "validation Loss: 0.0013 Acc: 83.4643\n",
      "Epoch 75/99\n",
      "training Loss: 0.0015 Acc: 81.4594\n",
      "validation Loss: 0.0013 Acc: 83.4881\n",
      "Epoch 76/99\n",
      "training Loss: 0.0015 Acc: 81.5606\n",
      "validation Loss: 0.0013 Acc: 83.4048\n",
      "Epoch 77/99\n",
      "training Loss: 0.0015 Acc: 81.4475\n",
      "validation Loss: 0.0013 Acc: 83.4762\n",
      "Epoch 78/99\n",
      "training Loss: 0.0015 Acc: 81.3850\n",
      "validation Loss: 0.0013 Acc: 83.5119\n",
      "Epoch 79/99\n",
      "training Loss: 0.0015 Acc: 81.3821\n",
      "validation Loss: 0.0013 Acc: 83.5119\n",
      "Epoch 80/99\n",
      "training Loss: 0.0015 Acc: 81.1916\n",
      "validation Loss: 0.0013 Acc: 83.5833\n",
      "Epoch 81/99\n",
      "training Loss: 0.0015 Acc: 81.4029\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Epoch 82/99\n",
      "training Loss: 0.0015 Acc: 81.4832\n",
      "validation Loss: 0.0013 Acc: 83.5595\n",
      "Epoch 83/99\n",
      "training Loss: 0.0015 Acc: 81.4178\n",
      "validation Loss: 0.0013 Acc: 83.5714\n",
      "Epoch 84/99\n",
      "training Loss: 0.0015 Acc: 81.3285\n",
      "validation Loss: 0.0013 Acc: 83.5476\n",
      "Epoch 85/99\n",
      "training Loss: 0.0015 Acc: 81.4773\n",
      "validation Loss: 0.0013 Acc: 83.6429\n",
      "Epoch 86/99\n",
      "training Loss: 0.0015 Acc: 81.5517\n",
      "validation Loss: 0.0013 Acc: 83.5833\n",
      "Epoch 87/99\n",
      "training Loss: 0.0015 Acc: 81.6737\n",
      "validation Loss: 0.0013 Acc: 83.6071\n",
      "Epoch 88/99\n",
      "training Loss: 0.0015 Acc: 81.6856\n",
      "validation Loss: 0.0013 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 89/99\n",
      "training Loss: 0.0015 Acc: 81.5398\n",
      "validation Loss: 0.0013 Acc: 83.6429\n",
      "Epoch 90/99\n",
      "training Loss: 0.0015 Acc: 81.6737\n",
      "validation Loss: 0.0013 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 91/99\n",
      "training Loss: 0.0015 Acc: 81.7064\n",
      "validation Loss: 0.0013 Acc: 83.7143\n",
      "Epoch 92/99\n",
      "training Loss: 0.0015 Acc: 81.5874\n",
      "validation Loss: 0.0013 Acc: 83.7262\n",
      "Epoch 93/99\n",
      "training Loss: 0.0015 Acc: 81.7570\n",
      "validation Loss: 0.0013 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 94/99\n",
      "training Loss: 0.0015 Acc: 81.5874\n",
      "validation Loss: 0.0013 Acc: 83.8214\n",
      "Saving..\n",
      "Epoch 95/99\n",
      "training Loss: 0.0015 Acc: 81.5755\n",
      "validation Loss: 0.0013 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 96/99\n",
      "training Loss: 0.0015 Acc: 81.6439\n",
      "validation Loss: 0.0013 Acc: 83.8214\n",
      "Epoch 97/99\n",
      "training Loss: 0.0015 Acc: 81.7184\n",
      "validation Loss: 0.0013 Acc: 83.6548\n",
      "Epoch 98/99\n",
      "training Loss: 0.0015 Acc: 81.6529\n",
      "validation Loss: 0.0013 Acc: 83.7857\n",
      "Epoch 99/99\n",
      "training Loss: 0.0015 Acc: 81.6261\n",
      "validation Loss: 0.0013 Acc: 83.7976\n",
      "Best val acc: 83.845238\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0025 Acc: 50.9315\n",
      "validation Loss: 0.0025 Acc: 50.0595\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0024 Acc: 52.3927\n",
      "validation Loss: 0.0025 Acc: 54.8690\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 54.0682\n",
      "validation Loss: 0.0024 Acc: 69.1667\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 56.5294\n",
      "validation Loss: 0.0024 Acc: 76.2500\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 58.5739\n",
      "validation Loss: 0.0024 Acc: 78.0833\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 60.9785\n",
      "validation Loss: 0.0023 Acc: 78.5476\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 61.9993\n",
      "validation Loss: 0.0023 Acc: 79.1071\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 63.7403\n",
      "validation Loss: 0.0022 Acc: 79.4762\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0022 Acc: 64.4366\n",
      "validation Loss: 0.0021 Acc: 79.9881\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0021 Acc: 65.6122\n",
      "validation Loss: 0.0020 Acc: 80.4881\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 66.3621\n",
      "validation Loss: 0.0019 Acc: 81.0833\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0020 Acc: 67.8442\n",
      "validation Loss: 0.0019 Acc: 81.3452\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0020 Acc: 69.3947\n",
      "validation Loss: 0.0018 Acc: 81.6667\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0020 Acc: 70.7577\n",
      "validation Loss: 0.0017 Acc: 81.8095\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 72.0344\n",
      "validation Loss: 0.0017 Acc: 82.0119\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 73.1534\n",
      "validation Loss: 0.0016 Acc: 82.2262\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 74.4331\n",
      "validation Loss: 0.0016 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 75.0461\n",
      "validation Loss: 0.0016 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 75.6562\n",
      "validation Loss: 0.0015 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 76.3586\n",
      "validation Loss: 0.0015 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 76.5579\n",
      "validation Loss: 0.0015 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 76.8704\n",
      "validation Loss: 0.0015 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 77.1859\n",
      "validation Loss: 0.0015 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 77.2484\n",
      "validation Loss: 0.0015 Acc: 83.1429\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 77.6293\n",
      "validation Loss: 0.0015 Acc: 83.1548\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 78.1650\n",
      "validation Loss: 0.0014 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 78.5489\n",
      "validation Loss: 0.0014 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 78.6858\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 78.7780\n",
      "validation Loss: 0.0014 Acc: 83.2500\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 79.3703\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 79.0578\n",
      "validation Loss: 0.0014 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 79.4060\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 79.4893\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 79.2512\n",
      "validation Loss: 0.0014 Acc: 83.3452\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 79.9744\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 79.8048\n",
      "validation Loss: 0.0014 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 79.9595\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 80.0934\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 80.3047\n",
      "validation Loss: 0.0014 Acc: 83.3214\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 80.2928\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 80.2958\n",
      "validation Loss: 0.0014 Acc: 83.2738\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 80.5666\n",
      "validation Loss: 0.0014 Acc: 83.3810\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 80.6648\n",
      "validation Loss: 0.0014 Acc: 83.2619\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 80.5904\n",
      "validation Loss: 0.0014 Acc: 83.4881\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 80.9952\n",
      "validation Loss: 0.0014 Acc: 83.5000\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 80.8613\n",
      "validation Loss: 0.0014 Acc: 83.3571\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 80.7035\n",
      "validation Loss: 0.0014 Acc: 83.4286\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 80.7154\n",
      "validation Loss: 0.0014 Acc: 83.3929\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 81.0517\n",
      "validation Loss: 0.0014 Acc: 83.2857\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 81.0160\n",
      "validation Loss: 0.0014 Acc: 83.3095\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 81.0726\n",
      "validation Loss: 0.0013 Acc: 83.3214\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 81.5666\n",
      "validation Loss: 0.0013 Acc: 83.3214\n",
      "Epoch 52/99\n",
      "training Loss: 0.0016 Acc: 81.1053\n",
      "validation Loss: 0.0013 Acc: 83.3333\n",
      "Epoch 53/99\n",
      "training Loss: 0.0016 Acc: 81.2600\n",
      "validation Loss: 0.0013 Acc: 83.3333\n",
      "Epoch 54/99\n",
      "training Loss: 0.0016 Acc: 81.5338\n",
      "validation Loss: 0.0013 Acc: 83.3810\n",
      "Epoch 55/99\n",
      "training Loss: 0.0016 Acc: 81.5100\n",
      "validation Loss: 0.0013 Acc: 83.3095\n",
      "Epoch 56/99\n",
      "training Loss: 0.0016 Acc: 81.3999\n",
      "validation Loss: 0.0013 Acc: 83.2976\n",
      "Epoch 57/99\n",
      "training Loss: 0.0016 Acc: 81.5130\n",
      "validation Loss: 0.0013 Acc: 83.4643\n",
      "Early stopped.\n",
      "Best val acc: 83.500000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0025 Acc: 50.3928\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 51.5684\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 53.0564\n",
      "validation Loss: 0.0024 Acc: 50.1548\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 56.5383\n",
      "validation Loss: 0.0024 Acc: 52.2857\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 60.1631\n",
      "validation Loss: 0.0024 Acc: 68.6786\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 63.8266\n",
      "validation Loss: 0.0024 Acc: 79.2143\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 67.4037\n",
      "validation Loss: 0.0023 Acc: 80.2976\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 70.7636\n",
      "validation Loss: 0.0022 Acc: 79.8690\n",
      "Epoch 8/99\n",
      "training Loss: 0.0022 Acc: 73.5790\n",
      "validation Loss: 0.0022 Acc: 79.8452\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 75.4479\n",
      "validation Loss: 0.0021 Acc: 80.0476\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 77.2811\n",
      "validation Loss: 0.0020 Acc: 80.3690\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 77.9924\n",
      "validation Loss: 0.0020 Acc: 80.8690\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0020 Acc: 78.9923\n",
      "validation Loss: 0.0019 Acc: 81.3095\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0020 Acc: 79.5637\n",
      "validation Loss: 0.0019 Acc: 81.7143\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 79.6232\n",
      "validation Loss: 0.0019 Acc: 81.9286\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 79.9685\n",
      "validation Loss: 0.0018 Acc: 82.1548\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 79.9833\n",
      "validation Loss: 0.0018 Acc: 82.1429\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 79.9893\n",
      "validation Loss: 0.0018 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 80.1619\n",
      "validation Loss: 0.0017 Acc: 82.2857\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 80.1411\n",
      "validation Loss: 0.0017 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 79.9268\n",
      "validation Loss: 0.0016 Acc: 82.3214\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 80.1798\n",
      "validation Loss: 0.0016 Acc: 82.3214\n",
      "Epoch 22/99\n",
      "training Loss: 0.0017 Acc: 80.1054\n",
      "validation Loss: 0.0016 Acc: 82.0952\n",
      "Epoch 23/99\n",
      "training Loss: 0.0017 Acc: 80.3077\n",
      "validation Loss: 0.0016 Acc: 82.1905\n",
      "Epoch 24/99\n",
      "training Loss: 0.0017 Acc: 80.1946\n",
      "validation Loss: 0.0015 Acc: 82.2262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0017 Acc: 80.3851\n",
      "validation Loss: 0.0015 Acc: 82.2024\n",
      "Epoch 26/99\n",
      "training Loss: 0.0017 Acc: 80.2750\n",
      "validation Loss: 0.0015 Acc: 82.2738\n",
      "Epoch 27/99\n",
      "training Loss: 0.0017 Acc: 80.5518\n",
      "validation Loss: 0.0015 Acc: 82.3095\n",
      "Epoch 28/99\n",
      "training Loss: 0.0017 Acc: 80.2988\n",
      "validation Loss: 0.0015 Acc: 82.2381\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 80.6678\n",
      "validation Loss: 0.0015 Acc: 82.3333\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 80.8672\n",
      "validation Loss: 0.0015 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0016 Acc: 80.8523\n",
      "validation Loss: 0.0015 Acc: 82.3214\n",
      "Epoch 32/99\n",
      "training Loss: 0.0016 Acc: 80.9922\n",
      "validation Loss: 0.0015 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0016 Acc: 81.1767\n",
      "validation Loss: 0.0015 Acc: 82.3452\n",
      "Epoch 34/99\n",
      "training Loss: 0.0016 Acc: 81.1231\n",
      "validation Loss: 0.0015 Acc: 82.5000\n",
      "Epoch 35/99\n",
      "training Loss: 0.0016 Acc: 81.0487\n",
      "validation Loss: 0.0014 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0016 Acc: 81.2422\n",
      "validation Loss: 0.0014 Acc: 82.5238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0016 Acc: 81.3702\n",
      "validation Loss: 0.0014 Acc: 82.4881\n",
      "Epoch 38/99\n",
      "training Loss: 0.0016 Acc: 81.3523\n",
      "validation Loss: 0.0014 Acc: 82.5357\n",
      "Epoch 39/99\n",
      "training Loss: 0.0016 Acc: 81.5517\n",
      "validation Loss: 0.0014 Acc: 82.5476\n",
      "Epoch 40/99\n",
      "training Loss: 0.0016 Acc: 81.4535\n",
      "validation Loss: 0.0014 Acc: 82.5357\n",
      "Epoch 41/99\n",
      "training Loss: 0.0016 Acc: 81.3017\n",
      "validation Loss: 0.0014 Acc: 82.5357\n",
      "Epoch 42/99\n",
      "training Loss: 0.0016 Acc: 81.7481\n",
      "validation Loss: 0.0014 Acc: 82.7262\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0016 Acc: 81.7451\n",
      "validation Loss: 0.0014 Acc: 82.6310\n",
      "Epoch 44/99\n",
      "training Loss: 0.0016 Acc: 81.6678\n",
      "validation Loss: 0.0014 Acc: 82.5833\n",
      "Epoch 45/99\n",
      "training Loss: 0.0016 Acc: 81.7213\n",
      "validation Loss: 0.0014 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0016 Acc: 81.8731\n",
      "validation Loss: 0.0014 Acc: 82.6786\n",
      "Epoch 47/99\n",
      "training Loss: 0.0016 Acc: 82.0279\n",
      "validation Loss: 0.0014 Acc: 82.7500\n",
      "Epoch 48/99\n",
      "training Loss: 0.0016 Acc: 81.9267\n",
      "validation Loss: 0.0014 Acc: 82.7262\n",
      "Epoch 49/99\n",
      "training Loss: 0.0016 Acc: 81.8493\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0016 Acc: 82.0100\n",
      "validation Loss: 0.0014 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0016 Acc: 82.1201\n",
      "validation Loss: 0.0014 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0015 Acc: 82.2957\n",
      "validation Loss: 0.0014 Acc: 82.8214\n",
      "Epoch 53/99\n",
      "training Loss: 0.0016 Acc: 82.2540\n",
      "validation Loss: 0.0014 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0016 Acc: 82.3433\n",
      "validation Loss: 0.0014 Acc: 82.8095\n",
      "Epoch 55/99\n",
      "training Loss: 0.0015 Acc: 82.3195\n",
      "validation Loss: 0.0014 Acc: 82.8095\n",
      "Epoch 56/99\n",
      "training Loss: 0.0015 Acc: 82.0368\n",
      "validation Loss: 0.0014 Acc: 82.8571\n",
      "Epoch 57/99\n",
      "training Loss: 0.0016 Acc: 82.2124\n",
      "validation Loss: 0.0014 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0015 Acc: 82.3314\n",
      "validation Loss: 0.0014 Acc: 82.9405\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0015 Acc: 82.3612\n",
      "validation Loss: 0.0014 Acc: 82.9167\n",
      "Epoch 60/99\n",
      "training Loss: 0.0015 Acc: 82.3046\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 61/99\n",
      "training Loss: 0.0015 Acc: 82.3374\n",
      "validation Loss: 0.0014 Acc: 82.8690\n",
      "Epoch 62/99\n",
      "training Loss: 0.0015 Acc: 82.1618\n",
      "validation Loss: 0.0014 Acc: 82.8929\n",
      "Epoch 63/99\n",
      "training Loss: 0.0015 Acc: 82.3969\n",
      "validation Loss: 0.0014 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0015 Acc: 82.5397\n",
      "validation Loss: 0.0014 Acc: 82.9048\n",
      "Epoch 65/99\n",
      "training Loss: 0.0015 Acc: 82.2540\n",
      "validation Loss: 0.0014 Acc: 82.9167\n",
      "Epoch 66/99\n",
      "training Loss: 0.0015 Acc: 82.3999\n",
      "validation Loss: 0.0014 Acc: 82.8690\n",
      "Epoch 67/99\n",
      "training Loss: 0.0015 Acc: 82.4832\n",
      "validation Loss: 0.0014 Acc: 82.9048\n",
      "Epoch 68/99\n",
      "training Loss: 0.0015 Acc: 82.3671\n",
      "validation Loss: 0.0014 Acc: 82.9048\n",
      "Epoch 69/99\n",
      "training Loss: 0.0015 Acc: 82.5278\n",
      "validation Loss: 0.0014 Acc: 82.8690\n",
      "Epoch 70/99\n",
      "training Loss: 0.0015 Acc: 82.4594\n",
      "validation Loss: 0.0014 Acc: 82.8333\n",
      "Epoch 71/99\n",
      "training Loss: 0.0015 Acc: 82.6737\n",
      "validation Loss: 0.0014 Acc: 82.8929\n",
      "Epoch 72/99\n",
      "training Loss: 0.0015 Acc: 82.3195\n",
      "validation Loss: 0.0014 Acc: 82.8571\n",
      "Epoch 73/99\n",
      "training Loss: 0.0015 Acc: 82.5248\n",
      "validation Loss: 0.0014 Acc: 82.8095\n",
      "Epoch 74/99\n",
      "training Loss: 0.0015 Acc: 82.5873\n",
      "validation Loss: 0.0014 Acc: 82.8810\n",
      "Epoch 75/99\n",
      "training Loss: 0.0015 Acc: 82.6290\n",
      "validation Loss: 0.0014 Acc: 82.9643\n",
      "Epoch 76/99\n",
      "training Loss: 0.0015 Acc: 82.6052\n",
      "validation Loss: 0.0014 Acc: 82.8095\n",
      "Epoch 77/99\n",
      "training Loss: 0.0015 Acc: 82.7034\n",
      "validation Loss: 0.0014 Acc: 82.8571\n",
      "Epoch 78/99\n",
      "training Loss: 0.0015 Acc: 82.6409\n",
      "validation Loss: 0.0014 Acc: 82.8214\n",
      "Epoch 79/99\n",
      "training Loss: 0.0015 Acc: 82.8790\n",
      "validation Loss: 0.0014 Acc: 82.8452\n",
      "Epoch 80/99\n",
      "training Loss: 0.0015 Acc: 82.5338\n",
      "validation Loss: 0.0014 Acc: 82.9286\n",
      "Epoch 81/99\n",
      "training Loss: 0.0015 Acc: 82.5070\n",
      "validation Loss: 0.0014 Acc: 82.8929\n",
      "Epoch 82/99\n",
      "training Loss: 0.0015 Acc: 82.7986\n",
      "validation Loss: 0.0013 Acc: 82.8929\n",
      "Epoch 83/99\n",
      "training Loss: 0.0015 Acc: 82.6975\n",
      "validation Loss: 0.0013 Acc: 82.8929\n",
      "Early stopped.\n",
      "Best val acc: 83.000000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0025 Acc: 49.9554\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 50.9732\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 2/99\n",
      "training Loss: 0.0025 Acc: 53.9105\n",
      "validation Loss: 0.0025 Acc: 50.0000\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 57.4311\n",
      "validation Loss: 0.0025 Acc: 52.2024\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 61.3833\n",
      "validation Loss: 0.0024 Acc: 75.7143\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 65.1449\n",
      "validation Loss: 0.0024 Acc: 76.0000\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 67.7430\n",
      "validation Loss: 0.0023 Acc: 74.6310\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 70.1179\n",
      "validation Loss: 0.0022 Acc: 75.4048\n",
      "Epoch 8/99\n",
      "training Loss: 0.0022 Acc: 72.5106\n",
      "validation Loss: 0.0022 Acc: 76.5952\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 74.0908\n",
      "validation Loss: 0.0021 Acc: 77.7143\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 75.7574\n",
      "validation Loss: 0.0021 Acc: 78.5714\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 76.6710\n",
      "validation Loss: 0.0020 Acc: 79.0833\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0021 Acc: 77.8257\n",
      "validation Loss: 0.0020 Acc: 79.5595\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0021 Acc: 78.2721\n",
      "validation Loss: 0.0020 Acc: 79.9524\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0021 Acc: 78.8733\n",
      "validation Loss: 0.0020 Acc: 80.2500\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0021 Acc: 79.4060\n",
      "validation Loss: 0.0020 Acc: 80.4048\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 80.0220\n",
      "validation Loss: 0.0020 Acc: 80.4524\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0020 Acc: 79.7899\n",
      "validation Loss: 0.0020 Acc: 80.6310\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0020 Acc: 80.4059\n",
      "validation Loss: 0.0020 Acc: 80.8333\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 80.3494\n",
      "validation Loss: 0.0019 Acc: 81.0833\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 80.5637\n",
      "validation Loss: 0.0019 Acc: 81.1429\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 80.6678\n",
      "validation Loss: 0.0019 Acc: 81.1786\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 80.7095\n",
      "validation Loss: 0.0019 Acc: 81.2619\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 80.8702\n",
      "validation Loss: 0.0019 Acc: 81.3095\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0020 Acc: 81.1053\n",
      "validation Loss: 0.0019 Acc: 81.4048\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0020 Acc: 81.1856\n",
      "validation Loss: 0.0019 Acc: 81.5238\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0020 Acc: 81.2839\n",
      "validation Loss: 0.0019 Acc: 81.5714\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0020 Acc: 81.1499\n",
      "validation Loss: 0.0019 Acc: 81.5357\n",
      "Epoch 28/99\n",
      "training Loss: 0.0020 Acc: 81.3106\n",
      "validation Loss: 0.0019 Acc: 81.6071\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0020 Acc: 81.5606\n",
      "validation Loss: 0.0019 Acc: 81.6429\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0020 Acc: 81.3910\n",
      "validation Loss: 0.0019 Acc: 81.7024\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0020 Acc: 81.3791\n",
      "validation Loss: 0.0019 Acc: 81.7143\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0020 Acc: 81.5190\n",
      "validation Loss: 0.0019 Acc: 81.7381\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0020 Acc: 81.5338\n",
      "validation Loss: 0.0019 Acc: 81.7857\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0020 Acc: 81.7184\n",
      "validation Loss: 0.0019 Acc: 81.8214\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0020 Acc: 81.5695\n",
      "validation Loss: 0.0019 Acc: 81.8095\n",
      "Epoch 36/99\n",
      "training Loss: 0.0019 Acc: 81.6588\n",
      "validation Loss: 0.0019 Acc: 81.8929\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0019 Acc: 81.8076\n",
      "validation Loss: 0.0019 Acc: 81.8452\n",
      "Epoch 38/99\n",
      "training Loss: 0.0020 Acc: 81.6261\n",
      "validation Loss: 0.0019 Acc: 81.8929\n",
      "Epoch 39/99\n",
      "training Loss: 0.0019 Acc: 81.7124\n",
      "validation Loss: 0.0019 Acc: 81.8452\n",
      "Epoch 40/99\n",
      "training Loss: 0.0019 Acc: 81.6291\n",
      "validation Loss: 0.0019 Acc: 81.9167\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0019 Acc: 81.8791\n",
      "validation Loss: 0.0019 Acc: 81.9048\n",
      "Epoch 42/99\n",
      "training Loss: 0.0019 Acc: 81.9505\n",
      "validation Loss: 0.0019 Acc: 81.9048\n",
      "Epoch 43/99\n",
      "training Loss: 0.0019 Acc: 81.9743\n",
      "validation Loss: 0.0019 Acc: 81.8929\n",
      "Epoch 44/99\n",
      "training Loss: 0.0019 Acc: 82.0100\n",
      "validation Loss: 0.0019 Acc: 81.9881\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0019 Acc: 81.9802\n",
      "validation Loss: 0.0019 Acc: 82.0357\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0019 Acc: 81.9088\n",
      "validation Loss: 0.0019 Acc: 82.1429\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0019 Acc: 82.0249\n",
      "validation Loss: 0.0019 Acc: 82.1071\n",
      "Epoch 48/99\n",
      "training Loss: 0.0019 Acc: 82.0249\n",
      "validation Loss: 0.0019 Acc: 82.0595\n",
      "Epoch 49/99\n",
      "training Loss: 0.0019 Acc: 81.9773\n",
      "validation Loss: 0.0019 Acc: 82.1429\n",
      "Epoch 50/99\n",
      "training Loss: 0.0019 Acc: 82.1290\n",
      "validation Loss: 0.0018 Acc: 82.1786\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0019 Acc: 81.9088\n",
      "validation Loss: 0.0018 Acc: 82.1190\n",
      "Epoch 52/99\n",
      "training Loss: 0.0019 Acc: 82.1915\n",
      "validation Loss: 0.0018 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0019 Acc: 82.1975\n",
      "validation Loss: 0.0018 Acc: 82.2976\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0019 Acc: 82.0517\n",
      "validation Loss: 0.0018 Acc: 82.3214\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0019 Acc: 82.3612\n",
      "validation Loss: 0.0018 Acc: 82.2143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0019 Acc: 82.1975\n",
      "validation Loss: 0.0018 Acc: 82.3571\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0019 Acc: 82.3909\n",
      "validation Loss: 0.0018 Acc: 82.3690\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0019 Acc: 82.4504\n",
      "validation Loss: 0.0018 Acc: 82.2976\n",
      "Epoch 59/99\n",
      "training Loss: 0.0019 Acc: 82.2838\n",
      "validation Loss: 0.0018 Acc: 82.4286\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0019 Acc: 82.1767\n",
      "validation Loss: 0.0018 Acc: 82.3333\n",
      "Epoch 61/99\n",
      "training Loss: 0.0019 Acc: 82.5338\n",
      "validation Loss: 0.0018 Acc: 82.3929\n",
      "Epoch 62/99\n",
      "training Loss: 0.0019 Acc: 82.3760\n",
      "validation Loss: 0.0018 Acc: 82.3810\n",
      "Epoch 63/99\n",
      "training Loss: 0.0019 Acc: 82.3522\n",
      "validation Loss: 0.0018 Acc: 82.4286\n",
      "Epoch 64/99\n",
      "training Loss: 0.0019 Acc: 82.3522\n",
      "validation Loss: 0.0018 Acc: 82.4286\n",
      "Epoch 65/99\n",
      "training Loss: 0.0019 Acc: 82.3433\n",
      "validation Loss: 0.0018 Acc: 82.4643\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0019 Acc: 82.3552\n",
      "validation Loss: 0.0018 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 67/99\n",
      "training Loss: 0.0019 Acc: 82.4445\n",
      "validation Loss: 0.0018 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0019 Acc: 82.5070\n",
      "validation Loss: 0.0018 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0019 Acc: 82.4534\n",
      "validation Loss: 0.0018 Acc: 82.5000\n",
      "Epoch 70/99\n",
      "training Loss: 0.0019 Acc: 82.5189\n",
      "validation Loss: 0.0018 Acc: 82.5238\n",
      "Epoch 71/99\n",
      "training Loss: 0.0019 Acc: 82.6052\n",
      "validation Loss: 0.0018 Acc: 82.5595\n",
      "Epoch 72/99\n",
      "training Loss: 0.0018 Acc: 82.5368\n",
      "validation Loss: 0.0018 Acc: 82.5119\n",
      "Epoch 73/99\n",
      "training Loss: 0.0019 Acc: 82.5368\n",
      "validation Loss: 0.0018 Acc: 82.5119\n",
      "Epoch 74/99\n",
      "training Loss: 0.0019 Acc: 82.4118\n",
      "validation Loss: 0.0018 Acc: 82.5357\n",
      "Epoch 75/99\n",
      "training Loss: 0.0019 Acc: 82.7867\n",
      "validation Loss: 0.0018 Acc: 82.5000\n",
      "Epoch 76/99\n",
      "training Loss: 0.0018 Acc: 82.4862\n",
      "validation Loss: 0.0018 Acc: 82.5119\n",
      "Epoch 77/99\n",
      "training Loss: 0.0019 Acc: 82.5754\n",
      "validation Loss: 0.0018 Acc: 82.4881\n",
      "Epoch 78/99\n",
      "training Loss: 0.0018 Acc: 82.6320\n",
      "validation Loss: 0.0018 Acc: 82.5357\n",
      "Epoch 79/99\n",
      "training Loss: 0.0018 Acc: 82.4653\n",
      "validation Loss: 0.0018 Acc: 82.5238\n",
      "Epoch 80/99\n",
      "training Loss: 0.0018 Acc: 82.4118\n",
      "validation Loss: 0.0018 Acc: 82.4762\n",
      "Epoch 81/99\n",
      "training Loss: 0.0018 Acc: 82.5993\n",
      "validation Loss: 0.0018 Acc: 82.5714\n",
      "Saving..\n",
      "Epoch 82/99\n",
      "training Loss: 0.0018 Acc: 82.4713\n",
      "validation Loss: 0.0018 Acc: 82.4762\n",
      "Epoch 83/99\n",
      "training Loss: 0.0018 Acc: 82.5368\n",
      "validation Loss: 0.0018 Acc: 82.4524\n",
      "Epoch 84/99\n",
      "training Loss: 0.0018 Acc: 82.6171\n",
      "validation Loss: 0.0018 Acc: 82.5119\n",
      "Epoch 85/99\n",
      "training Loss: 0.0018 Acc: 82.6528\n",
      "validation Loss: 0.0018 Acc: 82.5238\n",
      "Epoch 86/99\n",
      "training Loss: 0.0018 Acc: 82.5873\n",
      "validation Loss: 0.0018 Acc: 82.5000\n",
      "Epoch 87/99\n",
      "training Loss: 0.0018 Acc: 82.6528\n",
      "validation Loss: 0.0018 Acc: 82.5119\n",
      "Epoch 88/99\n",
      "training Loss: 0.0018 Acc: 82.5873\n",
      "validation Loss: 0.0018 Acc: 82.5357\n",
      "Epoch 89/99\n",
      "training Loss: 0.0018 Acc: 82.9355\n",
      "validation Loss: 0.0018 Acc: 82.5595\n",
      "Epoch 90/99\n",
      "training Loss: 0.0018 Acc: 82.7897\n",
      "validation Loss: 0.0018 Acc: 82.5000\n",
      "Epoch 91/99\n",
      "training Loss: 0.0018 Acc: 82.7153\n",
      "validation Loss: 0.0017 Acc: 82.4524\n",
      "Epoch 92/99\n",
      "training Loss: 0.0018 Acc: 82.6677\n",
      "validation Loss: 0.0017 Acc: 82.5833\n",
      "Saving..\n",
      "Epoch 93/99\n",
      "training Loss: 0.0018 Acc: 82.5427\n",
      "validation Loss: 0.0017 Acc: 82.5714\n",
      "Epoch 94/99\n",
      "training Loss: 0.0018 Acc: 82.7719\n",
      "validation Loss: 0.0017 Acc: 82.5119\n",
      "Epoch 95/99\n",
      "training Loss: 0.0018 Acc: 82.6469\n",
      "validation Loss: 0.0017 Acc: 82.5238\n",
      "Epoch 96/99\n",
      "training Loss: 0.0018 Acc: 82.6439\n",
      "validation Loss: 0.0017 Acc: 82.4643\n",
      "Epoch 97/99\n",
      "training Loss: 0.0018 Acc: 82.4713\n",
      "validation Loss: 0.0017 Acc: 82.4643\n",
      "Epoch 98/99\n",
      "training Loss: 0.0018 Acc: 82.6052\n",
      "validation Loss: 0.0017 Acc: 82.4524\n",
      "Epoch 99/99\n",
      "training Loss: 0.0018 Acc: 82.8522\n",
      "validation Loss: 0.0017 Acc: 82.4524\n",
      "Best val acc: 82.583333\n",
      "----------\n",
      "Average best_acc across k-fold: 83.0579827932\n",
      "New configuration: {'learning_rate': 0.054097323608298185, 'initial_nodes': 540, 'dropout': 0.2259095246170712, 'batch_size': 290, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0070 Acc: 77.0000\n",
      "validation Loss: 0.0015 Acc: 82.2423\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0016 Acc: 79.8750\n",
      "validation Loss: 0.0014 Acc: 82.4685\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0016 Acc: 80.0893\n",
      "validation Loss: 0.0014 Acc: 82.5161\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0016 Acc: 78.7738\n",
      "validation Loss: 0.0014 Acc: 80.7070\n",
      "Epoch 4/99\n",
      "training Loss: 0.0016 Acc: 77.0446\n",
      "validation Loss: 0.0014 Acc: 82.3137\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 78.7173\n",
      "validation Loss: 0.0014 Acc: 81.1950\n",
      "Epoch 6/99\n",
      "training Loss: 0.0016 Acc: 78.7292\n",
      "validation Loss: 0.0014 Acc: 82.1352\n",
      "Epoch 7/99\n",
      "training Loss: 0.0016 Acc: 76.8036\n",
      "validation Loss: 0.0014 Acc: 77.6244\n",
      "Epoch 8/99\n",
      "training Loss: 0.0016 Acc: 77.5536\n",
      "validation Loss: 0.0015 Acc: 81.7424\n",
      "Epoch 9/99\n",
      "training Loss: 0.0016 Acc: 76.5506\n",
      "validation Loss: 0.0014 Acc: 79.6239\n",
      "Epoch 10/99\n",
      "training Loss: 0.0016 Acc: 78.1012\n",
      "validation Loss: 0.0014 Acc: 82.4209\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 78.6161\n",
      "validation Loss: 0.0014 Acc: 80.5046\n",
      "Epoch 12/99\n",
      "training Loss: 0.0015 Acc: 77.8810\n",
      "validation Loss: 0.0013 Acc: 80.8617\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 78.0357\n",
      "validation Loss: 0.0014 Acc: 77.6125\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 78.1280\n",
      "validation Loss: 0.0014 Acc: 82.6946\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 80.5774\n",
      "validation Loss: 0.0014 Acc: 82.3732\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 80.0030\n",
      "validation Loss: 0.0013 Acc: 82.7184\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 79.9524\n",
      "validation Loss: 0.0013 Acc: 82.1709\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 79.2024\n",
      "validation Loss: 0.0013 Acc: 81.4211\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 77.7173\n",
      "validation Loss: 0.0013 Acc: 81.4568\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 77.9732\n",
      "validation Loss: 0.0013 Acc: 82.1709\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 78.3542\n",
      "validation Loss: 0.0013 Acc: 82.0162\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 79.1815\n",
      "validation Loss: 0.0013 Acc: 81.5639\n",
      "Epoch 23/99\n",
      "training Loss: 0.0015 Acc: 77.7827\n",
      "validation Loss: 0.0014 Acc: 82.8969\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0015 Acc: 80.2946\n",
      "validation Loss: 0.0014 Acc: 82.6946\n",
      "Epoch 25/99\n",
      "training Loss: 0.0015 Acc: 79.8958\n",
      "validation Loss: 0.0013 Acc: 82.5161\n",
      "Epoch 26/99\n",
      "training Loss: 0.0015 Acc: 79.4762\n",
      "validation Loss: 0.0013 Acc: 81.8020\n",
      "Epoch 27/99\n",
      "training Loss: 0.0015 Acc: 78.5625\n",
      "validation Loss: 0.0014 Acc: 81.8258\n",
      "Epoch 28/99\n",
      "training Loss: 0.0015 Acc: 78.5833\n",
      "validation Loss: 0.0013 Acc: 82.3256\n",
      "Epoch 29/99\n",
      "training Loss: 0.0015 Acc: 78.2887\n",
      "validation Loss: 0.0013 Acc: 82.0876\n",
      "Epoch 30/99\n",
      "training Loss: 0.0015 Acc: 78.5774\n",
      "validation Loss: 0.0013 Acc: 82.2304\n",
      "Epoch 31/99\n",
      "training Loss: 0.0015 Acc: 79.0536\n",
      "validation Loss: 0.0013 Acc: 81.5877\n",
      "Epoch 32/99\n",
      "training Loss: 0.0015 Acc: 79.0089\n",
      "validation Loss: 0.0013 Acc: 82.0281\n",
      "Epoch 33/99\n",
      "training Loss: 0.0015 Acc: 79.0952\n",
      "validation Loss: 0.0013 Acc: 82.4804\n",
      "Epoch 34/99\n",
      "training Loss: 0.0015 Acc: 80.1726\n",
      "validation Loss: 0.0013 Acc: 82.8017\n",
      "Epoch 35/99\n",
      "training Loss: 0.0015 Acc: 79.8244\n",
      "validation Loss: 0.0013 Acc: 82.4447\n",
      "Epoch 36/99\n",
      "training Loss: 0.0015 Acc: 79.7738\n",
      "validation Loss: 0.0013 Acc: 82.5161\n",
      "Epoch 37/99\n",
      "training Loss: 0.0015 Acc: 79.8780\n",
      "validation Loss: 0.0013 Acc: 83.2540\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0015 Acc: 80.3542\n",
      "validation Loss: 0.0013 Acc: 83.2064\n",
      "Epoch 39/99\n",
      "training Loss: 0.0014 Acc: 80.4524\n",
      "validation Loss: 0.0013 Acc: 83.1707\n",
      "Epoch 40/99\n",
      "training Loss: 0.0015 Acc: 80.3393\n",
      "validation Loss: 0.0013 Acc: 83.2540\n",
      "Epoch 41/99\n",
      "training Loss: 0.0014 Acc: 80.2321\n",
      "validation Loss: 0.0013 Acc: 83.2540\n",
      "Epoch 42/99\n",
      "training Loss: 0.0014 Acc: 80.0923\n",
      "validation Loss: 0.0013 Acc: 82.6827\n",
      "Epoch 43/99\n",
      "training Loss: 0.0014 Acc: 79.3720\n",
      "validation Loss: 0.0013 Acc: 82.2185\n",
      "Epoch 44/99\n",
      "training Loss: 0.0014 Acc: 79.3958\n",
      "validation Loss: 0.0013 Acc: 82.9207\n",
      "Epoch 45/99\n",
      "training Loss: 0.0014 Acc: 79.8869\n",
      "validation Loss: 0.0013 Acc: 83.1350\n",
      "Epoch 46/99\n",
      "training Loss: 0.0014 Acc: 80.2530\n",
      "validation Loss: 0.0013 Acc: 83.2659\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0014 Acc: 80.8482\n",
      "validation Loss: 0.0013 Acc: 83.3730\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0014 Acc: 80.0506\n",
      "validation Loss: 0.0013 Acc: 83.0398\n",
      "Epoch 49/99\n",
      "training Loss: 0.0014 Acc: 79.9940\n",
      "validation Loss: 0.0013 Acc: 83.2897\n",
      "Epoch 50/99\n",
      "training Loss: 0.0014 Acc: 80.1696\n",
      "validation Loss: 0.0013 Acc: 83.1707\n",
      "Epoch 51/99\n",
      "training Loss: 0.0014 Acc: 80.5298\n",
      "validation Loss: 0.0013 Acc: 83.2064\n",
      "Epoch 52/99\n",
      "training Loss: 0.0014 Acc: 79.7321\n",
      "validation Loss: 0.0013 Acc: 82.7898\n",
      "Epoch 53/99\n",
      "training Loss: 0.0014 Acc: 79.8512\n",
      "validation Loss: 0.0013 Acc: 83.3373\n",
      "Epoch 54/99\n",
      "training Loss: 0.0014 Acc: 80.3155\n",
      "validation Loss: 0.0013 Acc: 83.2064\n",
      "Epoch 55/99\n",
      "training Loss: 0.0014 Acc: 80.0565\n",
      "validation Loss: 0.0013 Acc: 83.3016\n",
      "Epoch 56/99\n",
      "training Loss: 0.0014 Acc: 80.1012\n",
      "validation Loss: 0.0013 Acc: 83.5515\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0014 Acc: 80.1637\n",
      "validation Loss: 0.0013 Acc: 83.3373\n",
      "Epoch 58/99\n",
      "training Loss: 0.0014 Acc: 80.5714\n",
      "validation Loss: 0.0013 Acc: 83.7658\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0014 Acc: 80.4435\n",
      "validation Loss: 0.0013 Acc: 83.0993\n",
      "Epoch 60/99\n",
      "training Loss: 0.0014 Acc: 80.0506\n",
      "validation Loss: 0.0013 Acc: 83.1112\n",
      "Epoch 61/99\n",
      "training Loss: 0.0014 Acc: 80.3929\n",
      "validation Loss: 0.0013 Acc: 83.2778\n",
      "Epoch 62/99\n",
      "training Loss: 0.0014 Acc: 80.4464\n",
      "validation Loss: 0.0013 Acc: 83.4801\n",
      "Epoch 63/99\n",
      "training Loss: 0.0014 Acc: 80.4583\n",
      "validation Loss: 0.0013 Acc: 83.3849\n",
      "Epoch 64/99\n",
      "training Loss: 0.0014 Acc: 80.2917\n",
      "validation Loss: 0.0013 Acc: 83.4801\n",
      "Epoch 65/99\n",
      "training Loss: 0.0014 Acc: 80.4256\n",
      "validation Loss: 0.0013 Acc: 83.4325\n",
      "Epoch 66/99\n",
      "training Loss: 0.0014 Acc: 80.3929\n",
      "validation Loss: 0.0013 Acc: 83.3611\n",
      "Epoch 67/99\n",
      "training Loss: 0.0014 Acc: 80.1310\n",
      "validation Loss: 0.0013 Acc: 83.3968\n",
      "Epoch 68/99\n",
      "training Loss: 0.0014 Acc: 80.4881\n",
      "validation Loss: 0.0013 Acc: 83.3968\n",
      "Epoch 69/99\n",
      "training Loss: 0.0014 Acc: 80.5476\n",
      "validation Loss: 0.0013 Acc: 83.3611\n",
      "Epoch 70/99\n",
      "training Loss: 0.0014 Acc: 80.7024\n",
      "validation Loss: 0.0013 Acc: 83.3611\n",
      "Epoch 71/99\n",
      "training Loss: 0.0014 Acc: 80.4018\n",
      "validation Loss: 0.0013 Acc: 83.3492\n",
      "Epoch 72/99\n",
      "training Loss: 0.0014 Acc: 80.5327\n",
      "validation Loss: 0.0013 Acc: 83.3373\n",
      "Epoch 73/99\n",
      "training Loss: 0.0014 Acc: 80.7619\n",
      "validation Loss: 0.0013 Acc: 83.3968\n",
      "Epoch 74/99\n",
      "training Loss: 0.0014 Acc: 80.4196\n",
      "validation Loss: 0.0013 Acc: 83.3849\n",
      "Epoch 75/99\n",
      "training Loss: 0.0014 Acc: 80.6637\n",
      "validation Loss: 0.0013 Acc: 83.4206\n",
      "Epoch 76/99\n",
      "training Loss: 0.0014 Acc: 80.6250\n",
      "validation Loss: 0.0013 Acc: 83.3730\n",
      "Epoch 77/99\n",
      "training Loss: 0.0014 Acc: 80.5565\n",
      "validation Loss: 0.0013 Acc: 83.3968\n",
      "Epoch 78/99\n",
      "training Loss: 0.0014 Acc: 80.8363\n",
      "validation Loss: 0.0013 Acc: 83.4206\n",
      "Early stopped.\n",
      "Best val acc: 83.765770\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0052 Acc: 77.3615\n",
      "validation Loss: 0.0014 Acc: 82.5952\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0015 Acc: 81.8552\n",
      "validation Loss: 0.0015 Acc: 82.5833\n",
      "Epoch 2/99\n",
      "training Loss: 0.0015 Acc: 81.6261\n",
      "validation Loss: 0.0014 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0015 Acc: 82.0249\n",
      "validation Loss: 0.0015 Acc: 81.7143\n",
      "Epoch 4/99\n",
      "training Loss: 0.0015 Acc: 81.4773\n",
      "validation Loss: 0.0015 Acc: 81.0476\n",
      "Epoch 5/99\n",
      "training Loss: 0.0016 Acc: 80.2065\n",
      "validation Loss: 0.0016 Acc: 79.0000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0015 Acc: 80.8821\n",
      "validation Loss: 0.0015 Acc: 81.9167\n",
      "Epoch 7/99\n",
      "training Loss: 0.0015 Acc: 81.1261\n",
      "validation Loss: 0.0015 Acc: 81.0595\n",
      "Epoch 8/99\n",
      "training Loss: 0.0015 Acc: 80.9357\n",
      "validation Loss: 0.0014 Acc: 81.3571\n",
      "Epoch 9/99\n",
      "training Loss: 0.0015 Acc: 80.3226\n",
      "validation Loss: 0.0014 Acc: 81.1071\n",
      "Epoch 10/99\n",
      "training Loss: 0.0014 Acc: 79.5726\n",
      "validation Loss: 0.0014 Acc: 81.3571\n",
      "Epoch 11/99\n",
      "training Loss: 0.0014 Acc: 81.5190\n",
      "validation Loss: 0.0014 Acc: 82.4881\n",
      "Epoch 12/99\n",
      "training Loss: 0.0014 Acc: 81.4654\n",
      "validation Loss: 0.0014 Acc: 81.2381\n",
      "Epoch 13/99\n",
      "training Loss: 0.0014 Acc: 81.1351\n",
      "validation Loss: 0.0014 Acc: 82.3214\n",
      "Epoch 14/99\n",
      "training Loss: 0.0014 Acc: 81.9743\n",
      "validation Loss: 0.0014 Acc: 82.6310\n",
      "Epoch 15/99\n",
      "training Loss: 0.0014 Acc: 81.9773\n",
      "validation Loss: 0.0014 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 82.0249\n",
      "validation Loss: 0.0014 Acc: 82.5714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0014 Acc: 80.9565\n",
      "validation Loss: 0.0013 Acc: 81.7976\n",
      "Epoch 18/99\n",
      "training Loss: 0.0014 Acc: 80.9178\n",
      "validation Loss: 0.0013 Acc: 82.5714\n",
      "Epoch 19/99\n",
      "training Loss: 0.0014 Acc: 80.9565\n",
      "validation Loss: 0.0013 Acc: 82.7262\n",
      "Epoch 20/99\n",
      "training Loss: 0.0014 Acc: 81.2868\n",
      "validation Loss: 0.0013 Acc: 82.7381\n",
      "Epoch 21/99\n",
      "training Loss: 0.0014 Acc: 81.5904\n",
      "validation Loss: 0.0013 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0014 Acc: 80.8970\n",
      "validation Loss: 0.0013 Acc: 82.4048\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 81.1470\n",
      "validation Loss: 0.0013 Acc: 83.0000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 81.6648\n",
      "validation Loss: 0.0013 Acc: 83.0476\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 81.0845\n",
      "validation Loss: 0.0013 Acc: 82.6667\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 81.2898\n",
      "validation Loss: 0.0013 Acc: 81.9524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 81.4207\n",
      "validation Loss: 0.0013 Acc: 82.9286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 81.6975\n",
      "validation Loss: 0.0013 Acc: 82.7143\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 81.7630\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 81.8701\n",
      "validation Loss: 0.0013 Acc: 83.0238\n",
      "Epoch 31/99\n",
      "training Loss: 0.0013 Acc: 81.5785\n",
      "validation Loss: 0.0013 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0013 Acc: 81.7124\n",
      "validation Loss: 0.0013 Acc: 82.4524\n",
      "Epoch 33/99\n",
      "training Loss: 0.0013 Acc: 82.0219\n",
      "validation Loss: 0.0013 Acc: 82.0595\n",
      "Epoch 34/99\n",
      "training Loss: 0.0013 Acc: 81.7898\n",
      "validation Loss: 0.0013 Acc: 81.9524\n",
      "Epoch 35/99\n",
      "training Loss: 0.0013 Acc: 82.0725\n",
      "validation Loss: 0.0013 Acc: 83.5000\n",
      "Epoch 36/99\n",
      "training Loss: 0.0013 Acc: 82.0160\n",
      "validation Loss: 0.0013 Acc: 83.3690\n",
      "Epoch 37/99\n",
      "training Loss: 0.0013 Acc: 82.1707\n",
      "validation Loss: 0.0013 Acc: 83.3452\n",
      "Epoch 38/99\n",
      "training Loss: 0.0013 Acc: 81.9535\n",
      "validation Loss: 0.0013 Acc: 83.1429\n",
      "Epoch 39/99\n",
      "training Loss: 0.0013 Acc: 82.1380\n",
      "validation Loss: 0.0013 Acc: 82.8095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0013 Acc: 81.8880\n",
      "validation Loss: 0.0013 Acc: 83.5119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0013 Acc: 82.1439\n",
      "validation Loss: 0.0013 Acc: 83.3571\n",
      "Epoch 42/99\n",
      "training Loss: 0.0013 Acc: 82.0933\n",
      "validation Loss: 0.0013 Acc: 83.3333\n",
      "Epoch 43/99\n",
      "training Loss: 0.0013 Acc: 82.2868\n",
      "validation Loss: 0.0013 Acc: 83.4524\n",
      "Epoch 44/99\n",
      "training Loss: 0.0013 Acc: 82.0933\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Epoch 45/99\n",
      "training Loss: 0.0013 Acc: 82.2808\n",
      "validation Loss: 0.0013 Acc: 83.4167\n",
      "Epoch 46/99\n",
      "training Loss: 0.0013 Acc: 82.2511\n",
      "validation Loss: 0.0013 Acc: 82.3333\n",
      "Epoch 47/99\n",
      "training Loss: 0.0013 Acc: 82.1886\n",
      "validation Loss: 0.0013 Acc: 83.4048\n",
      "Epoch 48/99\n",
      "training Loss: 0.0013 Acc: 82.2749\n",
      "validation Loss: 0.0013 Acc: 82.2738\n",
      "Epoch 49/99\n",
      "training Loss: 0.0013 Acc: 82.3522\n",
      "validation Loss: 0.0013 Acc: 82.5595\n",
      "Epoch 50/99\n",
      "training Loss: 0.0013 Acc: 82.0784\n",
      "validation Loss: 0.0013 Acc: 82.3452\n",
      "Epoch 51/99\n",
      "training Loss: 0.0013 Acc: 82.2362\n",
      "validation Loss: 0.0013 Acc: 82.2619\n",
      "Early stopped.\n",
      "Best val acc: 83.535714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0050 Acc: 78.3703\n",
      "validation Loss: 0.0014 Acc: 82.0357\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 82.3969\n",
      "validation Loss: 0.0013 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0013 Acc: 82.9623\n",
      "validation Loss: 0.0013 Acc: 82.3810\n",
      "Epoch 3/99\n",
      "training Loss: 0.0013 Acc: 82.7600\n",
      "validation Loss: 0.0013 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0013 Acc: 82.8165\n",
      "validation Loss: 0.0013 Acc: 82.6667\n",
      "Epoch 5/99\n",
      "training Loss: 0.0013 Acc: 83.1736\n",
      "validation Loss: 0.0014 Acc: 82.5000\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 83.4831\n",
      "validation Loss: 0.0013 Acc: 82.8333\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 83.3254\n",
      "validation Loss: 0.0013 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 83.4117\n",
      "validation Loss: 0.0013 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 83.4444\n",
      "validation Loss: 0.0013 Acc: 82.8810\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 83.5813\n",
      "validation Loss: 0.0013 Acc: 82.9643\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 83.3254\n",
      "validation Loss: 0.0013 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 83.4534\n",
      "validation Loss: 0.0013 Acc: 83.0952\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.3284\n",
      "validation Loss: 0.0013 Acc: 83.2500\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 83.4712\n",
      "validation Loss: 0.0013 Acc: 82.9524\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 83.3581\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Epoch 16/99\n",
      "training Loss: 0.0014 Acc: 82.9028\n",
      "validation Loss: 0.0013 Acc: 82.8452\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 82.9296\n",
      "validation Loss: 0.0013 Acc: 83.2738\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 82.6469\n",
      "validation Loss: 0.0013 Acc: 83.0119\n",
      "Epoch 19/99\n",
      "training Loss: 0.0013 Acc: 82.9177\n",
      "validation Loss: 0.0013 Acc: 82.9643\n",
      "Epoch 20/99\n",
      "training Loss: 0.0013 Acc: 83.0397\n",
      "validation Loss: 0.0013 Acc: 83.2500\n",
      "Epoch 21/99\n",
      "training Loss: 0.0013 Acc: 83.4474\n",
      "validation Loss: 0.0013 Acc: 83.3571\n",
      "Epoch 22/99\n",
      "training Loss: 0.0013 Acc: 83.4861\n",
      "validation Loss: 0.0013 Acc: 82.9524\n",
      "Epoch 23/99\n",
      "training Loss: 0.0013 Acc: 83.7182\n",
      "validation Loss: 0.0013 Acc: 83.0476\n",
      "Epoch 24/99\n",
      "training Loss: 0.0013 Acc: 83.6290\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Epoch 25/99\n",
      "training Loss: 0.0013 Acc: 83.3522\n",
      "validation Loss: 0.0013 Acc: 83.2024\n",
      "Epoch 26/99\n",
      "training Loss: 0.0013 Acc: 83.8254\n",
      "validation Loss: 0.0013 Acc: 83.0833\n",
      "Epoch 27/99\n",
      "training Loss: 0.0013 Acc: 83.7539\n",
      "validation Loss: 0.0013 Acc: 83.2857\n",
      "Epoch 28/99\n",
      "training Loss: 0.0013 Acc: 83.6468\n",
      "validation Loss: 0.0013 Acc: 83.2857\n",
      "Epoch 29/99\n",
      "training Loss: 0.0013 Acc: 83.8343\n",
      "validation Loss: 0.0013 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0013 Acc: 83.9236\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 84.0218\n",
      "validation Loss: 0.0013 Acc: 83.0833\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 83.9176\n",
      "validation Loss: 0.0013 Acc: 83.0119\n",
      "Epoch 33/99\n",
      "training Loss: 0.0012 Acc: 83.7718\n",
      "validation Loss: 0.0013 Acc: 83.1071\n",
      "Epoch 34/99\n",
      "training Loss: 0.0012 Acc: 83.8760\n",
      "validation Loss: 0.0013 Acc: 83.4286\n",
      "Epoch 35/99\n",
      "training Loss: 0.0012 Acc: 84.0664\n",
      "validation Loss: 0.0013 Acc: 83.1071\n",
      "Epoch 36/99\n",
      "training Loss: 0.0012 Acc: 84.0218\n",
      "validation Loss: 0.0013 Acc: 83.1548\n",
      "Epoch 37/99\n",
      "training Loss: 0.0012 Acc: 84.2242\n",
      "validation Loss: 0.0013 Acc: 83.2738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0012 Acc: 84.0634\n",
      "validation Loss: 0.0013 Acc: 83.2143\n",
      "Epoch 39/99\n",
      "training Loss: 0.0012 Acc: 84.1438\n",
      "validation Loss: 0.0013 Acc: 83.0714\n",
      "Epoch 40/99\n",
      "training Loss: 0.0012 Acc: 84.2212\n",
      "validation Loss: 0.0013 Acc: 83.2619\n",
      "Epoch 41/99\n",
      "training Loss: 0.0012 Acc: 84.1111\n",
      "validation Loss: 0.0013 Acc: 83.1905\n",
      "Epoch 42/99\n",
      "training Loss: 0.0012 Acc: 84.3164\n",
      "validation Loss: 0.0013 Acc: 83.0833\n",
      "Epoch 43/99\n",
      "training Loss: 0.0012 Acc: 84.3551\n",
      "validation Loss: 0.0013 Acc: 83.2143\n",
      "Epoch 44/99\n",
      "training Loss: 0.0012 Acc: 84.3432\n",
      "validation Loss: 0.0013 Acc: 83.3095\n",
      "Epoch 45/99\n",
      "training Loss: 0.0012 Acc: 84.3462\n",
      "validation Loss: 0.0013 Acc: 83.2262\n",
      "Epoch 46/99\n",
      "training Loss: 0.0012 Acc: 84.4831\n",
      "validation Loss: 0.0013 Acc: 83.2024\n",
      "Epoch 47/99\n",
      "training Loss: 0.0012 Acc: 84.5247\n",
      "validation Loss: 0.0013 Acc: 83.3452\n",
      "Epoch 48/99\n",
      "training Loss: 0.0012 Acc: 84.4295\n",
      "validation Loss: 0.0013 Acc: 83.3214\n",
      "Epoch 49/99\n",
      "training Loss: 0.0012 Acc: 84.4355\n",
      "validation Loss: 0.0013 Acc: 83.1548\n",
      "Early stopped.\n",
      "Best val acc: 83.464286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0047 Acc: 78.9864\n",
      "validation Loss: 0.0014 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 82.4415\n",
      "validation Loss: 0.0013 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0013 Acc: 82.9802\n",
      "validation Loss: 0.0013 Acc: 82.8571\n",
      "Epoch 3/99\n",
      "training Loss: 0.0013 Acc: 82.8135\n",
      "validation Loss: 0.0013 Acc: 83.2976\n",
      "Epoch 4/99\n",
      "training Loss: 0.0013 Acc: 83.0308\n",
      "validation Loss: 0.0013 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0013 Acc: 83.0337\n",
      "validation Loss: 0.0013 Acc: 83.0476\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 82.7361\n",
      "validation Loss: 0.0013 Acc: 83.5833\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 83.0635\n",
      "validation Loss: 0.0013 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 83.1379\n",
      "validation Loss: 0.0013 Acc: 83.8095\n",
      "Epoch 9/99\n",
      "training Loss: 0.0013 Acc: 83.2808\n",
      "validation Loss: 0.0012 Acc: 83.9643\n",
      "Epoch 10/99\n",
      "training Loss: 0.0013 Acc: 83.2183\n",
      "validation Loss: 0.0012 Acc: 83.9405\n",
      "Epoch 11/99\n",
      "training Loss: 0.0013 Acc: 83.3016\n",
      "validation Loss: 0.0013 Acc: 83.7381\n",
      "Epoch 12/99\n",
      "training Loss: 0.0013 Acc: 83.2956\n",
      "validation Loss: 0.0013 Acc: 84.0476\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0013 Acc: 83.4534\n",
      "validation Loss: 0.0013 Acc: 83.4881\n",
      "Epoch 14/99\n",
      "training Loss: 0.0013 Acc: 83.2153\n",
      "validation Loss: 0.0013 Acc: 83.7500\n",
      "Epoch 15/99\n",
      "training Loss: 0.0013 Acc: 83.1201\n",
      "validation Loss: 0.0013 Acc: 83.1548\n",
      "Epoch 16/99\n",
      "training Loss: 0.0013 Acc: 83.4296\n",
      "validation Loss: 0.0013 Acc: 84.0238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0013 Acc: 83.5397\n",
      "validation Loss: 0.0013 Acc: 83.9167\n",
      "Epoch 18/99\n",
      "training Loss: 0.0013 Acc: 83.6706\n",
      "validation Loss: 0.0012 Acc: 83.8095\n",
      "Epoch 19/99\n",
      "training Loss: 0.0012 Acc: 83.7301\n",
      "validation Loss: 0.0012 Acc: 83.8571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0012 Acc: 83.6438\n",
      "validation Loss: 0.0012 Acc: 83.9762\n",
      "Epoch 21/99\n",
      "training Loss: 0.0012 Acc: 83.7778\n",
      "validation Loss: 0.0012 Acc: 83.8571\n",
      "Epoch 22/99\n",
      "training Loss: 0.0012 Acc: 83.7272\n",
      "validation Loss: 0.0012 Acc: 84.0000\n",
      "Epoch 23/99\n",
      "training Loss: 0.0012 Acc: 83.7361\n",
      "validation Loss: 0.0012 Acc: 84.0000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0012 Acc: 83.8730\n",
      "validation Loss: 0.0012 Acc: 83.7143\n",
      "Epoch 25/99\n",
      "training Loss: 0.0012 Acc: 83.8908\n",
      "validation Loss: 0.0012 Acc: 83.7143\n",
      "Epoch 26/99\n",
      "training Loss: 0.0012 Acc: 84.0069\n",
      "validation Loss: 0.0012 Acc: 83.7738\n",
      "Epoch 27/99\n",
      "training Loss: 0.0012 Acc: 83.7986\n",
      "validation Loss: 0.0012 Acc: 83.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0012 Acc: 83.9712\n",
      "validation Loss: 0.0012 Acc: 83.6667\n",
      "Epoch 29/99\n",
      "training Loss: 0.0012 Acc: 84.0367\n",
      "validation Loss: 0.0012 Acc: 83.8571\n",
      "Epoch 30/99\n",
      "training Loss: 0.0012 Acc: 84.1706\n",
      "validation Loss: 0.0012 Acc: 83.9762\n",
      "Epoch 31/99\n",
      "training Loss: 0.0012 Acc: 84.2242\n",
      "validation Loss: 0.0012 Acc: 83.7976\n",
      "Epoch 32/99\n",
      "training Loss: 0.0012 Acc: 84.3581\n",
      "validation Loss: 0.0012 Acc: 83.7738\n",
      "Early stopped.\n",
      "Best val acc: 84.047619\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0067 Acc: 78.7126\n",
      "validation Loss: 0.0014 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0014 Acc: 82.3731\n",
      "validation Loss: 0.0013 Acc: 82.5357\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0014 Acc: 82.8641\n",
      "validation Loss: 0.0013 Acc: 82.6190\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0013 Acc: 83.1290\n",
      "validation Loss: 0.0013 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0013 Acc: 82.9207\n",
      "validation Loss: 0.0013 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0013 Acc: 83.2302\n",
      "validation Loss: 0.0013 Acc: 82.7262\n",
      "Epoch 6/99\n",
      "training Loss: 0.0013 Acc: 83.2064\n",
      "validation Loss: 0.0013 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0013 Acc: 83.1885\n",
      "validation Loss: 0.0013 Acc: 83.0952\n",
      "Epoch 8/99\n",
      "training Loss: 0.0013 Acc: 83.1855\n",
      "validation Loss: 0.0013 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0029 Acc: 78.5995\n",
      "validation Loss: 0.0022 Acc: 72.1667\n",
      "Epoch 10/99\n",
      "training Loss: 0.0017 Acc: 77.8942\n",
      "validation Loss: 0.0015 Acc: 79.8690\n",
      "Epoch 11/99\n",
      "training Loss: 0.0016 Acc: 79.3078\n",
      "validation Loss: 0.0015 Acc: 79.7976\n",
      "Epoch 12/99\n",
      "training Loss: 0.0016 Acc: 80.7214\n",
      "validation Loss: 0.0014 Acc: 78.8810\n",
      "Epoch 13/99\n",
      "training Loss: 0.0015 Acc: 79.0370\n",
      "validation Loss: 0.0014 Acc: 81.3095\n",
      "Epoch 14/99\n",
      "training Loss: 0.0015 Acc: 79.3881\n",
      "validation Loss: 0.0014 Acc: 81.4524\n",
      "Epoch 15/99\n",
      "training Loss: 0.0015 Acc: 80.4535\n",
      "validation Loss: 0.0014 Acc: 82.3571\n",
      "Epoch 16/99\n",
      "training Loss: 0.0015 Acc: 80.3643\n",
      "validation Loss: 0.0014 Acc: 81.6786\n",
      "Epoch 17/99\n",
      "training Loss: 0.0015 Acc: 80.3702\n",
      "validation Loss: 0.0014 Acc: 81.9643\n",
      "Epoch 18/99\n",
      "training Loss: 0.0015 Acc: 80.3643\n",
      "validation Loss: 0.0014 Acc: 82.7500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0015 Acc: 80.9624\n",
      "validation Loss: 0.0014 Acc: 82.0000\n",
      "Epoch 20/99\n",
      "training Loss: 0.0015 Acc: 81.0577\n",
      "validation Loss: 0.0014 Acc: 82.6071\n",
      "Epoch 21/99\n",
      "training Loss: 0.0015 Acc: 81.7184\n",
      "validation Loss: 0.0014 Acc: 81.4286\n",
      "Epoch 22/99\n",
      "training Loss: 0.0015 Acc: 82.3522\n",
      "validation Loss: 0.0014 Acc: 82.4643\n",
      "Epoch 23/99\n",
      "training Loss: 0.0014 Acc: 82.4832\n",
      "validation Loss: 0.0014 Acc: 82.7857\n",
      "Epoch 24/99\n",
      "training Loss: 0.0014 Acc: 82.3552\n",
      "validation Loss: 0.0014 Acc: 82.0714\n",
      "Epoch 25/99\n",
      "training Loss: 0.0014 Acc: 81.5100\n",
      "validation Loss: 0.0013 Acc: 82.0119\n",
      "Epoch 26/99\n",
      "training Loss: 0.0014 Acc: 81.0934\n",
      "validation Loss: 0.0014 Acc: 82.6786\n",
      "Epoch 27/99\n",
      "training Loss: 0.0014 Acc: 82.2421\n",
      "validation Loss: 0.0013 Acc: 82.4286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0014 Acc: 82.5189\n",
      "validation Loss: 0.0014 Acc: 82.3810\n",
      "Early stopped.\n",
      "Best val acc: 83.452381\n",
      "----------\n",
      "Average best_acc across k-fold: 83.6531540109\n",
      "New configuration: {'learning_rate': 1.9037024976555106e-05, 'initial_nodes': 1000, 'dropout': 0.3355368689342606, 'batch_size': 159, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 78.1310\n",
      "validation Loss: 0.0026 Acc: 81.8258\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 82.0536\n",
      "validation Loss: 0.0024 Acc: 82.6351\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 82.6964\n",
      "validation Loss: 0.0024 Acc: 82.8850\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 82.8095\n",
      "validation Loss: 0.0024 Acc: 82.8493\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 83.0000\n",
      "validation Loss: 0.0024 Acc: 83.0636\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0023 Acc: 83.1548\n",
      "validation Loss: 0.0024 Acc: 83.2183\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 83.2649\n",
      "validation Loss: 0.0023 Acc: 83.2659\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 83.2440\n",
      "validation Loss: 0.0023 Acc: 83.1469\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 83.3393\n",
      "validation Loss: 0.0023 Acc: 83.3730\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0023 Acc: 83.3095\n",
      "validation Loss: 0.0023 Acc: 83.2540\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 83.3601\n",
      "validation Loss: 0.0023 Acc: 83.3492\n",
      "Epoch 11/99\n",
      "training Loss: 0.0023 Acc: 83.3482\n",
      "validation Loss: 0.0023 Acc: 83.3611\n",
      "Epoch 12/99\n",
      "training Loss: 0.0023 Acc: 83.3988\n",
      "validation Loss: 0.0023 Acc: 83.2302\n",
      "Epoch 13/99\n",
      "training Loss: 0.0023 Acc: 83.5179\n",
      "validation Loss: 0.0023 Acc: 83.3135\n",
      "Epoch 14/99\n",
      "training Loss: 0.0023 Acc: 83.4792\n",
      "validation Loss: 0.0023 Acc: 83.3373\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 83.5625\n",
      "validation Loss: 0.0023 Acc: 83.2659\n",
      "Epoch 16/99\n",
      "training Loss: 0.0023 Acc: 83.5119\n",
      "validation Loss: 0.0023 Acc: 83.4682\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0023 Acc: 83.6161\n",
      "validation Loss: 0.0023 Acc: 83.3611\n",
      "Epoch 18/99\n",
      "training Loss: 0.0023 Acc: 83.5744\n",
      "validation Loss: 0.0023 Acc: 83.2302\n",
      "Epoch 19/99\n",
      "training Loss: 0.0023 Acc: 83.5804\n",
      "validation Loss: 0.0023 Acc: 83.2897\n",
      "Epoch 20/99\n",
      "training Loss: 0.0023 Acc: 83.6815\n",
      "validation Loss: 0.0023 Acc: 83.5396\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0023 Acc: 83.7173\n",
      "validation Loss: 0.0023 Acc: 83.5158\n",
      "Epoch 22/99\n",
      "training Loss: 0.0023 Acc: 83.7143\n",
      "validation Loss: 0.0023 Acc: 83.5039\n",
      "Epoch 23/99\n",
      "training Loss: 0.0022 Acc: 83.6518\n",
      "validation Loss: 0.0023 Acc: 83.6468\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0022 Acc: 83.7589\n",
      "validation Loss: 0.0023 Acc: 83.4920\n",
      "Epoch 25/99\n",
      "training Loss: 0.0023 Acc: 83.6964\n",
      "validation Loss: 0.0023 Acc: 83.6110\n",
      "Epoch 26/99\n",
      "training Loss: 0.0022 Acc: 83.7143\n",
      "validation Loss: 0.0023 Acc: 83.5039\n",
      "Epoch 27/99\n",
      "training Loss: 0.0022 Acc: 83.6786\n",
      "validation Loss: 0.0023 Acc: 83.4325\n",
      "Epoch 28/99\n",
      "training Loss: 0.0022 Acc: 83.7798\n",
      "validation Loss: 0.0023 Acc: 83.5991\n",
      "Epoch 29/99\n",
      "training Loss: 0.0022 Acc: 83.7708\n",
      "validation Loss: 0.0023 Acc: 83.6110\n",
      "Epoch 30/99\n",
      "training Loss: 0.0022 Acc: 83.9048\n",
      "validation Loss: 0.0023 Acc: 83.5991\n",
      "Epoch 31/99\n",
      "training Loss: 0.0022 Acc: 83.8631\n",
      "validation Loss: 0.0023 Acc: 83.6587\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0022 Acc: 83.8482\n",
      "validation Loss: 0.0023 Acc: 83.6348\n",
      "Epoch 33/99\n",
      "training Loss: 0.0022 Acc: 83.8393\n",
      "validation Loss: 0.0023 Acc: 83.6468\n",
      "Epoch 34/99\n",
      "training Loss: 0.0022 Acc: 83.8750\n",
      "validation Loss: 0.0023 Acc: 83.6110\n",
      "Epoch 35/99\n",
      "training Loss: 0.0022 Acc: 83.8958\n",
      "validation Loss: 0.0023 Acc: 83.6587\n",
      "Epoch 36/99\n",
      "training Loss: 0.0022 Acc: 83.9911\n",
      "validation Loss: 0.0023 Acc: 83.7777\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0022 Acc: 83.9077\n",
      "validation Loss: 0.0023 Acc: 83.6468\n",
      "Epoch 38/99\n",
      "training Loss: 0.0022 Acc: 83.9762\n",
      "validation Loss: 0.0023 Acc: 83.7658\n",
      "Epoch 39/99\n",
      "training Loss: 0.0022 Acc: 83.9762\n",
      "validation Loss: 0.0023 Acc: 83.7420\n",
      "Epoch 40/99\n",
      "training Loss: 0.0022 Acc: 84.0089\n",
      "validation Loss: 0.0023 Acc: 83.6825\n",
      "Epoch 41/99\n",
      "training Loss: 0.0022 Acc: 83.9435\n",
      "validation Loss: 0.0023 Acc: 83.8491\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0022 Acc: 84.0774\n",
      "validation Loss: 0.0023 Acc: 83.8015\n",
      "Epoch 43/99\n",
      "training Loss: 0.0022 Acc: 84.0238\n",
      "validation Loss: 0.0023 Acc: 83.7420\n",
      "Epoch 44/99\n",
      "training Loss: 0.0022 Acc: 84.1071\n",
      "validation Loss: 0.0023 Acc: 83.7896\n",
      "Epoch 45/99\n",
      "training Loss: 0.0022 Acc: 84.1131\n",
      "validation Loss: 0.0023 Acc: 83.7301\n",
      "Epoch 46/99\n",
      "training Loss: 0.0022 Acc: 84.0714\n",
      "validation Loss: 0.0023 Acc: 83.7896\n",
      "Epoch 47/99\n",
      "training Loss: 0.0022 Acc: 84.0089\n",
      "validation Loss: 0.0023 Acc: 83.8015\n",
      "Epoch 48/99\n",
      "training Loss: 0.0022 Acc: 84.0298\n",
      "validation Loss: 0.0023 Acc: 83.8848\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0022 Acc: 84.0506\n",
      "validation Loss: 0.0023 Acc: 83.8372\n",
      "Epoch 50/99\n",
      "training Loss: 0.0022 Acc: 84.1101\n",
      "validation Loss: 0.0023 Acc: 83.8253\n",
      "Epoch 51/99\n",
      "training Loss: 0.0022 Acc: 84.0982\n",
      "validation Loss: 0.0023 Acc: 83.9205\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0022 Acc: 84.1369\n",
      "validation Loss: 0.0023 Acc: 83.8134\n",
      "Epoch 53/99\n",
      "training Loss: 0.0022 Acc: 84.1012\n",
      "validation Loss: 0.0023 Acc: 83.8134\n",
      "Epoch 54/99\n",
      "training Loss: 0.0022 Acc: 84.1042\n",
      "validation Loss: 0.0023 Acc: 83.8372\n",
      "Epoch 55/99\n",
      "training Loss: 0.0022 Acc: 84.1310\n",
      "validation Loss: 0.0023 Acc: 83.8134\n",
      "Epoch 56/99\n",
      "training Loss: 0.0022 Acc: 84.1250\n",
      "validation Loss: 0.0023 Acc: 83.9443\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0022 Acc: 84.1994\n",
      "validation Loss: 0.0023 Acc: 83.8967\n",
      "Epoch 58/99\n",
      "training Loss: 0.0022 Acc: 84.2113\n",
      "validation Loss: 0.0023 Acc: 83.8610\n",
      "Epoch 59/99\n",
      "training Loss: 0.0022 Acc: 84.1518\n",
      "validation Loss: 0.0023 Acc: 83.8134\n",
      "Epoch 60/99\n",
      "training Loss: 0.0022 Acc: 84.2381\n",
      "validation Loss: 0.0023 Acc: 83.9086\n",
      "Epoch 61/99\n",
      "training Loss: 0.0022 Acc: 84.1815\n",
      "validation Loss: 0.0023 Acc: 83.9443\n",
      "Epoch 62/99\n",
      "training Loss: 0.0022 Acc: 84.2470\n",
      "validation Loss: 0.0023 Acc: 83.8729\n",
      "Epoch 63/99\n",
      "training Loss: 0.0022 Acc: 84.3661\n",
      "validation Loss: 0.0023 Acc: 83.9681\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0022 Acc: 84.1667\n",
      "validation Loss: 0.0023 Acc: 83.8491\n",
      "Epoch 65/99\n",
      "training Loss: 0.0022 Acc: 84.3482\n",
      "validation Loss: 0.0023 Acc: 83.9443\n",
      "Epoch 66/99\n",
      "training Loss: 0.0022 Acc: 84.3036\n",
      "validation Loss: 0.0023 Acc: 83.9681\n",
      "Epoch 67/99\n",
      "training Loss: 0.0022 Acc: 84.3482\n",
      "validation Loss: 0.0023 Acc: 84.0157\n",
      "Saving..\n",
      "Epoch 68/99\n",
      "training Loss: 0.0022 Acc: 84.2530\n",
      "validation Loss: 0.0023 Acc: 84.0038\n",
      "Epoch 69/99\n",
      "training Loss: 0.0022 Acc: 84.2976\n",
      "validation Loss: 0.0023 Acc: 83.9443\n",
      "Epoch 70/99\n",
      "training Loss: 0.0022 Acc: 84.2827\n",
      "validation Loss: 0.0023 Acc: 83.9919\n",
      "Epoch 71/99\n",
      "training Loss: 0.0022 Acc: 84.3869\n",
      "validation Loss: 0.0023 Acc: 83.9086\n",
      "Epoch 72/99\n",
      "training Loss: 0.0022 Acc: 84.2054\n",
      "validation Loss: 0.0023 Acc: 83.9800\n",
      "Epoch 73/99\n",
      "training Loss: 0.0022 Acc: 84.3482\n",
      "validation Loss: 0.0023 Acc: 83.9919\n",
      "Epoch 74/99\n",
      "training Loss: 0.0022 Acc: 84.2530\n",
      "validation Loss: 0.0023 Acc: 83.9205\n",
      "Epoch 75/99\n",
      "training Loss: 0.0022 Acc: 84.2232\n",
      "validation Loss: 0.0023 Acc: 83.8967\n",
      "Epoch 76/99\n",
      "training Loss: 0.0022 Acc: 84.3065\n",
      "validation Loss: 0.0023 Acc: 83.9562\n",
      "Epoch 77/99\n",
      "training Loss: 0.0022 Acc: 84.2560\n",
      "validation Loss: 0.0023 Acc: 83.9562\n",
      "Epoch 78/99\n",
      "training Loss: 0.0022 Acc: 84.2470\n",
      "validation Loss: 0.0023 Acc: 84.0157\n",
      "Epoch 79/99\n",
      "training Loss: 0.0022 Acc: 84.3601\n",
      "validation Loss: 0.0023 Acc: 83.9800\n",
      "Epoch 80/99\n",
      "training Loss: 0.0022 Acc: 84.2530\n",
      "validation Loss: 0.0023 Acc: 83.9562\n",
      "Epoch 81/99\n",
      "training Loss: 0.0022 Acc: 84.2143\n",
      "validation Loss: 0.0023 Acc: 83.9205\n",
      "Epoch 82/99\n",
      "training Loss: 0.0022 Acc: 84.3214\n",
      "validation Loss: 0.0023 Acc: 83.9443\n",
      "Epoch 83/99\n",
      "training Loss: 0.0022 Acc: 84.3512\n",
      "validation Loss: 0.0023 Acc: 83.9324\n",
      "Epoch 84/99\n",
      "training Loss: 0.0022 Acc: 84.4137\n",
      "validation Loss: 0.0023 Acc: 83.9681\n",
      "Epoch 85/99\n",
      "training Loss: 0.0022 Acc: 84.3482\n",
      "validation Loss: 0.0023 Acc: 83.9443\n",
      "Epoch 86/99\n",
      "training Loss: 0.0022 Acc: 84.4048\n",
      "validation Loss: 0.0023 Acc: 83.9800\n",
      "Epoch 87/99\n",
      "training Loss: 0.0022 Acc: 84.2946\n",
      "validation Loss: 0.0023 Acc: 83.9800\n",
      "Early stopped.\n",
      "Best val acc: 84.015711\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 77.9477\n",
      "validation Loss: 0.0026 Acc: 81.3810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0026 Acc: 81.8523\n",
      "validation Loss: 0.0025 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 82.5308\n",
      "validation Loss: 0.0024 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 82.8611\n",
      "validation Loss: 0.0024 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 83.0070\n",
      "validation Loss: 0.0023 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 82.9653\n",
      "validation Loss: 0.0023 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 83.0516\n",
      "validation Loss: 0.0023 Acc: 83.5238\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 83.2302\n",
      "validation Loss: 0.0023 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 83.2361\n",
      "validation Loss: 0.0023 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0023 Acc: 83.3938\n",
      "validation Loss: 0.0023 Acc: 83.5714\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 83.1260\n",
      "validation Loss: 0.0023 Acc: 83.6429\n",
      "Epoch 11/99\n",
      "training Loss: 0.0023 Acc: 83.2689\n",
      "validation Loss: 0.0023 Acc: 83.7024\n",
      "Epoch 12/99\n",
      "training Loss: 0.0023 Acc: 83.3492\n",
      "validation Loss: 0.0023 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0023 Acc: 83.3581\n",
      "validation Loss: 0.0023 Acc: 83.7262\n",
      "Epoch 14/99\n",
      "training Loss: 0.0023 Acc: 83.3700\n",
      "validation Loss: 0.0023 Acc: 83.7500\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 83.6141\n",
      "validation Loss: 0.0023 Acc: 83.7024\n",
      "Epoch 16/99\n",
      "training Loss: 0.0023 Acc: 83.4742\n",
      "validation Loss: 0.0023 Acc: 83.7500\n",
      "Epoch 17/99\n",
      "training Loss: 0.0023 Acc: 83.4296\n",
      "validation Loss: 0.0023 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0023 Acc: 83.4355\n",
      "validation Loss: 0.0023 Acc: 83.8690\n",
      "Epoch 19/99\n",
      "training Loss: 0.0023 Acc: 83.5367\n",
      "validation Loss: 0.0023 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0023 Acc: 83.4623\n",
      "validation Loss: 0.0023 Acc: 83.8690\n",
      "Epoch 21/99\n",
      "training Loss: 0.0023 Acc: 83.6438\n",
      "validation Loss: 0.0023 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0023 Acc: 83.6349\n",
      "validation Loss: 0.0023 Acc: 83.9048\n",
      "Epoch 23/99\n",
      "training Loss: 0.0023 Acc: 83.6319\n",
      "validation Loss: 0.0023 Acc: 83.8810\n",
      "Epoch 24/99\n",
      "training Loss: 0.0023 Acc: 83.6825\n",
      "validation Loss: 0.0023 Acc: 83.8810\n",
      "Epoch 25/99\n",
      "training Loss: 0.0023 Acc: 83.6200\n",
      "validation Loss: 0.0023 Acc: 84.0119\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0022 Acc: 83.5962\n",
      "validation Loss: 0.0023 Acc: 83.9048\n",
      "Epoch 27/99\n",
      "training Loss: 0.0022 Acc: 83.7004\n",
      "validation Loss: 0.0023 Acc: 83.9643\n",
      "Epoch 28/99\n",
      "training Loss: 0.0022 Acc: 83.7153\n",
      "validation Loss: 0.0023 Acc: 84.0119\n",
      "Epoch 29/99\n",
      "training Loss: 0.0022 Acc: 83.7301\n",
      "validation Loss: 0.0023 Acc: 84.0119\n",
      "Epoch 30/99\n",
      "training Loss: 0.0022 Acc: 83.6676\n",
      "validation Loss: 0.0023 Acc: 83.9286\n",
      "Epoch 31/99\n",
      "training Loss: 0.0022 Acc: 83.7718\n",
      "validation Loss: 0.0023 Acc: 84.0952\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0022 Acc: 83.8462\n",
      "validation Loss: 0.0023 Acc: 83.8571\n",
      "Epoch 33/99\n",
      "training Loss: 0.0022 Acc: 83.8432\n",
      "validation Loss: 0.0023 Acc: 84.0714\n",
      "Epoch 34/99\n",
      "training Loss: 0.0022 Acc: 83.8670\n",
      "validation Loss: 0.0023 Acc: 84.0833\n",
      "Epoch 35/99\n",
      "training Loss: 0.0022 Acc: 83.7688\n",
      "validation Loss: 0.0023 Acc: 84.0119\n",
      "Epoch 36/99\n",
      "training Loss: 0.0022 Acc: 83.9474\n",
      "validation Loss: 0.0023 Acc: 84.0119\n",
      "Epoch 37/99\n",
      "training Loss: 0.0022 Acc: 83.7986\n",
      "validation Loss: 0.0023 Acc: 84.1548\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0022 Acc: 83.9027\n",
      "validation Loss: 0.0023 Acc: 84.1190\n",
      "Epoch 39/99\n",
      "training Loss: 0.0022 Acc: 83.7926\n",
      "validation Loss: 0.0023 Acc: 84.0595\n",
      "Epoch 40/99\n",
      "training Loss: 0.0022 Acc: 83.9236\n",
      "validation Loss: 0.0023 Acc: 84.2262\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0022 Acc: 83.8670\n",
      "validation Loss: 0.0023 Acc: 84.1429\n",
      "Epoch 42/99\n",
      "training Loss: 0.0022 Acc: 83.9742\n",
      "validation Loss: 0.0023 Acc: 84.0833\n",
      "Epoch 43/99\n",
      "training Loss: 0.0022 Acc: 83.9295\n",
      "validation Loss: 0.0023 Acc: 84.1786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0022 Acc: 84.0545\n",
      "validation Loss: 0.0022 Acc: 84.1429\n",
      "Epoch 45/99\n",
      "training Loss: 0.0022 Acc: 83.9266\n",
      "validation Loss: 0.0023 Acc: 84.1310\n",
      "Epoch 46/99\n",
      "training Loss: 0.0022 Acc: 83.9712\n",
      "validation Loss: 0.0023 Acc: 84.1429\n",
      "Epoch 47/99\n",
      "training Loss: 0.0022 Acc: 83.9593\n",
      "validation Loss: 0.0023 Acc: 84.1429\n",
      "Epoch 48/99\n",
      "training Loss: 0.0022 Acc: 84.0307\n",
      "validation Loss: 0.0023 Acc: 84.1548\n",
      "Epoch 49/99\n",
      "training Loss: 0.0022 Acc: 83.9861\n",
      "validation Loss: 0.0023 Acc: 84.1310\n",
      "Epoch 50/99\n",
      "training Loss: 0.0022 Acc: 84.0664\n",
      "validation Loss: 0.0023 Acc: 84.1071\n",
      "Epoch 51/99\n",
      "training Loss: 0.0022 Acc: 84.1230\n",
      "validation Loss: 0.0022 Acc: 84.1667\n",
      "Epoch 52/99\n",
      "training Loss: 0.0022 Acc: 84.0962\n",
      "validation Loss: 0.0022 Acc: 84.1905\n",
      "Epoch 53/99\n",
      "training Loss: 0.0022 Acc: 84.1468\n",
      "validation Loss: 0.0022 Acc: 84.1071\n",
      "Epoch 54/99\n",
      "training Loss: 0.0022 Acc: 84.0515\n",
      "validation Loss: 0.0022 Acc: 84.1190\n",
      "Epoch 55/99\n",
      "training Loss: 0.0022 Acc: 84.0575\n",
      "validation Loss: 0.0022 Acc: 84.0595\n",
      "Epoch 56/99\n",
      "training Loss: 0.0022 Acc: 84.0158\n",
      "validation Loss: 0.0022 Acc: 84.0833\n",
      "Epoch 57/99\n",
      "training Loss: 0.0022 Acc: 84.1587\n",
      "validation Loss: 0.0022 Acc: 84.0952\n",
      "Epoch 58/99\n",
      "training Loss: 0.0022 Acc: 84.1498\n",
      "validation Loss: 0.0022 Acc: 84.1190\n",
      "Epoch 59/99\n",
      "training Loss: 0.0022 Acc: 84.0754\n",
      "validation Loss: 0.0022 Acc: 84.1071\n",
      "Epoch 60/99\n",
      "training Loss: 0.0022 Acc: 84.0605\n",
      "validation Loss: 0.0022 Acc: 84.1071\n",
      "Early stopped.\n",
      "Best val acc: 84.226190\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 78.9388\n",
      "validation Loss: 0.0026 Acc: 81.5119\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 82.1201\n",
      "validation Loss: 0.0024 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 82.7540\n",
      "validation Loss: 0.0024 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 82.8611\n",
      "validation Loss: 0.0024 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 83.1498\n",
      "validation Loss: 0.0023 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 83.1022\n",
      "validation Loss: 0.0023 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 83.3135\n",
      "validation Loss: 0.0023 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 83.3313\n",
      "validation Loss: 0.0023 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 83.3135\n",
      "validation Loss: 0.0023 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0023 Acc: 83.2689\n",
      "validation Loss: 0.0023 Acc: 83.3690\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 83.4057\n",
      "validation Loss: 0.0023 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0023 Acc: 83.4147\n",
      "validation Loss: 0.0023 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0023 Acc: 83.5248\n",
      "validation Loss: 0.0023 Acc: 83.5238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0023 Acc: 83.4593\n",
      "validation Loss: 0.0023 Acc: 83.5238\n",
      "Epoch 14/99\n",
      "training Loss: 0.0023 Acc: 83.5040\n",
      "validation Loss: 0.0023 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 83.4831\n",
      "validation Loss: 0.0023 Acc: 83.5000\n",
      "Epoch 16/99\n",
      "training Loss: 0.0023 Acc: 83.4444\n",
      "validation Loss: 0.0023 Acc: 83.6667\n",
      "Epoch 17/99\n",
      "training Loss: 0.0023 Acc: 83.5575\n",
      "validation Loss: 0.0023 Acc: 83.6190\n",
      "Epoch 18/99\n",
      "training Loss: 0.0023 Acc: 83.6081\n",
      "validation Loss: 0.0023 Acc: 83.5714\n",
      "Epoch 19/99\n",
      "training Loss: 0.0023 Acc: 83.6200\n",
      "validation Loss: 0.0023 Acc: 83.4762\n",
      "Epoch 20/99\n",
      "training Loss: 0.0023 Acc: 83.6825\n",
      "validation Loss: 0.0023 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0023 Acc: 83.5843\n",
      "validation Loss: 0.0023 Acc: 83.6548\n",
      "Epoch 22/99\n",
      "training Loss: 0.0023 Acc: 83.6587\n",
      "validation Loss: 0.0023 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0023 Acc: 83.7569\n",
      "validation Loss: 0.0023 Acc: 83.7143\n",
      "Epoch 24/99\n",
      "training Loss: 0.0023 Acc: 83.7569\n",
      "validation Loss: 0.0023 Acc: 83.7024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0023 Acc: 83.8164\n",
      "validation Loss: 0.0022 Acc: 83.7619\n",
      "Epoch 26/99\n",
      "training Loss: 0.0023 Acc: 83.6617\n",
      "validation Loss: 0.0023 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0022 Acc: 83.7510\n",
      "validation Loss: 0.0022 Acc: 83.7500\n",
      "Epoch 28/99\n",
      "training Loss: 0.0022 Acc: 83.7807\n",
      "validation Loss: 0.0022 Acc: 83.7738\n",
      "Epoch 29/99\n",
      "training Loss: 0.0022 Acc: 83.8908\n",
      "validation Loss: 0.0022 Acc: 83.6905\n",
      "Epoch 30/99\n",
      "training Loss: 0.0022 Acc: 83.7569\n",
      "validation Loss: 0.0022 Acc: 83.7381\n",
      "Epoch 31/99\n",
      "training Loss: 0.0022 Acc: 83.9652\n",
      "validation Loss: 0.0022 Acc: 83.7024\n",
      "Epoch 32/99\n",
      "training Loss: 0.0022 Acc: 83.8462\n",
      "validation Loss: 0.0022 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0022 Acc: 83.8373\n",
      "validation Loss: 0.0022 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0022 Acc: 83.8432\n",
      "validation Loss: 0.0022 Acc: 83.8333\n",
      "Epoch 35/99\n",
      "training Loss: 0.0022 Acc: 83.9385\n",
      "validation Loss: 0.0022 Acc: 83.7143\n",
      "Epoch 36/99\n",
      "training Loss: 0.0022 Acc: 83.9295\n",
      "validation Loss: 0.0022 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0022 Acc: 83.9563\n",
      "validation Loss: 0.0022 Acc: 83.7738\n",
      "Epoch 38/99\n",
      "training Loss: 0.0022 Acc: 84.0337\n",
      "validation Loss: 0.0022 Acc: 83.9286\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0022 Acc: 83.9414\n",
      "validation Loss: 0.0022 Acc: 83.8571\n",
      "Epoch 40/99\n",
      "training Loss: 0.0022 Acc: 83.9712\n",
      "validation Loss: 0.0022 Acc: 83.7976\n",
      "Epoch 41/99\n",
      "training Loss: 0.0022 Acc: 83.9801\n",
      "validation Loss: 0.0022 Acc: 83.8571\n",
      "Epoch 42/99\n",
      "training Loss: 0.0022 Acc: 83.9771\n",
      "validation Loss: 0.0022 Acc: 83.8214\n",
      "Epoch 43/99\n",
      "training Loss: 0.0022 Acc: 83.9593\n",
      "validation Loss: 0.0022 Acc: 83.7619\n",
      "Epoch 44/99\n",
      "training Loss: 0.0022 Acc: 84.0099\n",
      "validation Loss: 0.0022 Acc: 83.7738\n",
      "Epoch 45/99\n",
      "training Loss: 0.0022 Acc: 83.9652\n",
      "validation Loss: 0.0022 Acc: 83.8810\n",
      "Epoch 46/99\n",
      "training Loss: 0.0022 Acc: 84.0873\n",
      "validation Loss: 0.0022 Acc: 83.8452\n",
      "Epoch 47/99\n",
      "training Loss: 0.0022 Acc: 84.0010\n",
      "validation Loss: 0.0022 Acc: 83.8929\n",
      "Epoch 48/99\n",
      "training Loss: 0.0022 Acc: 84.0129\n",
      "validation Loss: 0.0022 Acc: 83.9643\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0022 Acc: 84.0992\n",
      "validation Loss: 0.0022 Acc: 83.8452\n",
      "Epoch 50/99\n",
      "training Loss: 0.0022 Acc: 84.1527\n",
      "validation Loss: 0.0022 Acc: 83.9643\n",
      "Epoch 51/99\n",
      "training Loss: 0.0022 Acc: 84.1378\n",
      "validation Loss: 0.0022 Acc: 83.9524\n",
      "Epoch 52/99\n",
      "training Loss: 0.0022 Acc: 84.0188\n",
      "validation Loss: 0.0022 Acc: 83.8333\n",
      "Epoch 53/99\n",
      "training Loss: 0.0022 Acc: 84.1587\n",
      "validation Loss: 0.0022 Acc: 84.0714\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0022 Acc: 84.1468\n",
      "validation Loss: 0.0022 Acc: 83.8214\n",
      "Epoch 55/99\n",
      "training Loss: 0.0022 Acc: 84.2539\n",
      "validation Loss: 0.0022 Acc: 83.8214\n",
      "Epoch 56/99\n",
      "training Loss: 0.0022 Acc: 84.0962\n",
      "validation Loss: 0.0022 Acc: 83.8929\n",
      "Epoch 57/99\n",
      "training Loss: 0.0022 Acc: 84.0902\n",
      "validation Loss: 0.0022 Acc: 83.7857\n",
      "Epoch 58/99\n",
      "training Loss: 0.0022 Acc: 84.0634\n",
      "validation Loss: 0.0022 Acc: 83.9405\n",
      "Epoch 59/99\n",
      "training Loss: 0.0022 Acc: 84.2599\n",
      "validation Loss: 0.0022 Acc: 84.0476\n",
      "Epoch 60/99\n",
      "training Loss: 0.0022 Acc: 84.1557\n",
      "validation Loss: 0.0022 Acc: 84.0119\n",
      "Epoch 61/99\n",
      "training Loss: 0.0022 Acc: 84.1676\n",
      "validation Loss: 0.0022 Acc: 83.9643\n",
      "Epoch 62/99\n",
      "training Loss: 0.0022 Acc: 84.1617\n",
      "validation Loss: 0.0022 Acc: 83.9167\n",
      "Epoch 63/99\n",
      "training Loss: 0.0022 Acc: 84.2152\n",
      "validation Loss: 0.0022 Acc: 83.8929\n",
      "Epoch 64/99\n",
      "training Loss: 0.0022 Acc: 84.1617\n",
      "validation Loss: 0.0022 Acc: 83.9881\n",
      "Epoch 65/99\n",
      "training Loss: 0.0022 Acc: 84.0575\n",
      "validation Loss: 0.0022 Acc: 84.0714\n",
      "Epoch 66/99\n",
      "training Loss: 0.0022 Acc: 84.2718\n",
      "validation Loss: 0.0022 Acc: 84.0119\n",
      "Epoch 67/99\n",
      "training Loss: 0.0022 Acc: 84.2747\n",
      "validation Loss: 0.0022 Acc: 84.0714\n",
      "Epoch 68/99\n",
      "training Loss: 0.0022 Acc: 84.3343\n",
      "validation Loss: 0.0022 Acc: 83.9762\n",
      "Epoch 69/99\n",
      "training Loss: 0.0022 Acc: 84.2361\n",
      "validation Loss: 0.0022 Acc: 83.8571\n",
      "Epoch 70/99\n",
      "training Loss: 0.0022 Acc: 84.2271\n",
      "validation Loss: 0.0022 Acc: 84.0714\n",
      "Epoch 71/99\n",
      "training Loss: 0.0022 Acc: 84.3343\n",
      "validation Loss: 0.0022 Acc: 84.0357\n",
      "Epoch 72/99\n",
      "training Loss: 0.0022 Acc: 84.1646\n",
      "validation Loss: 0.0022 Acc: 83.9048\n",
      "Epoch 73/99\n",
      "training Loss: 0.0022 Acc: 84.2271\n",
      "validation Loss: 0.0022 Acc: 83.9405\n",
      "Early stopped.\n",
      "Best val acc: 84.071429\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0030 Acc: 78.9090\n",
      "validation Loss: 0.0026 Acc: 81.4167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 82.1856\n",
      "validation Loss: 0.0025 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 82.7391\n",
      "validation Loss: 0.0024 Acc: 82.4524\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 82.9593\n",
      "validation Loss: 0.0024 Acc: 82.6310\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 82.9891\n",
      "validation Loss: 0.0024 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0024 Acc: 83.3135\n",
      "validation Loss: 0.0024 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 83.3046\n",
      "validation Loss: 0.0023 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 83.4206\n",
      "validation Loss: 0.0023 Acc: 82.9643\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 83.4980\n",
      "validation Loss: 0.0023 Acc: 82.9524\n",
      "Epoch 9/99\n",
      "training Loss: 0.0023 Acc: 83.3849\n",
      "validation Loss: 0.0023 Acc: 82.8810\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 83.4653\n",
      "validation Loss: 0.0023 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0023 Acc: 83.5665\n",
      "validation Loss: 0.0023 Acc: 83.0595\n",
      "Epoch 12/99\n",
      "training Loss: 0.0023 Acc: 83.5486\n",
      "validation Loss: 0.0023 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0023 Acc: 83.6051\n",
      "validation Loss: 0.0023 Acc: 83.1310\n",
      "Epoch 14/99\n",
      "training Loss: 0.0023 Acc: 83.5665\n",
      "validation Loss: 0.0023 Acc: 83.0833\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 83.6825\n",
      "validation Loss: 0.0023 Acc: 83.0357\n",
      "Epoch 16/99\n",
      "training Loss: 0.0023 Acc: 83.6260\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0023 Acc: 83.7034\n",
      "validation Loss: 0.0023 Acc: 83.1667\n",
      "Epoch 18/99\n",
      "training Loss: 0.0023 Acc: 83.7034\n",
      "validation Loss: 0.0023 Acc: 83.1190\n",
      "Epoch 19/99\n",
      "training Loss: 0.0023 Acc: 83.8045\n",
      "validation Loss: 0.0023 Acc: 83.1429\n",
      "Epoch 20/99\n",
      "training Loss: 0.0023 Acc: 83.7391\n",
      "validation Loss: 0.0023 Acc: 83.1310\n",
      "Epoch 21/99\n",
      "training Loss: 0.0023 Acc: 83.7569\n",
      "validation Loss: 0.0023 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0023 Acc: 83.8283\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Epoch 23/99\n",
      "training Loss: 0.0023 Acc: 83.8492\n",
      "validation Loss: 0.0023 Acc: 83.2381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0023 Acc: 83.8789\n",
      "validation Loss: 0.0023 Acc: 83.2857\n",
      "Epoch 25/99\n",
      "training Loss: 0.0023 Acc: 83.8135\n",
      "validation Loss: 0.0023 Acc: 83.2976\n",
      "Epoch 26/99\n",
      "training Loss: 0.0023 Acc: 83.8492\n",
      "validation Loss: 0.0023 Acc: 83.4643\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0023 Acc: 83.8700\n",
      "validation Loss: 0.0023 Acc: 83.2262\n",
      "Epoch 28/99\n",
      "training Loss: 0.0022 Acc: 83.8373\n",
      "validation Loss: 0.0023 Acc: 83.4167\n",
      "Epoch 29/99\n",
      "training Loss: 0.0022 Acc: 83.8730\n",
      "validation Loss: 0.0023 Acc: 83.4405\n",
      "Epoch 30/99\n",
      "training Loss: 0.0022 Acc: 83.9146\n",
      "validation Loss: 0.0023 Acc: 83.2381\n",
      "Epoch 31/99\n",
      "training Loss: 0.0022 Acc: 83.9474\n",
      "validation Loss: 0.0023 Acc: 83.3929\n",
      "Epoch 32/99\n",
      "training Loss: 0.0022 Acc: 83.9652\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Epoch 33/99\n",
      "training Loss: 0.0022 Acc: 83.9652\n",
      "validation Loss: 0.0023 Acc: 83.3214\n",
      "Epoch 34/99\n",
      "training Loss: 0.0022 Acc: 83.9742\n",
      "validation Loss: 0.0023 Acc: 83.4524\n",
      "Epoch 35/99\n",
      "training Loss: 0.0022 Acc: 84.0694\n",
      "validation Loss: 0.0023 Acc: 83.4167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0022 Acc: 84.0724\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Epoch 37/99\n",
      "training Loss: 0.0022 Acc: 83.9593\n",
      "validation Loss: 0.0023 Acc: 83.2857\n",
      "Epoch 38/99\n",
      "training Loss: 0.0022 Acc: 83.8908\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Epoch 39/99\n",
      "training Loss: 0.0022 Acc: 84.0545\n",
      "validation Loss: 0.0023 Acc: 83.3095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0022 Acc: 84.0724\n",
      "validation Loss: 0.0022 Acc: 83.3452\n",
      "Epoch 41/99\n",
      "training Loss: 0.0022 Acc: 84.0277\n",
      "validation Loss: 0.0022 Acc: 83.3690\n",
      "Epoch 42/99\n",
      "training Loss: 0.0022 Acc: 84.0099\n",
      "validation Loss: 0.0022 Acc: 83.2262\n",
      "Epoch 43/99\n",
      "training Loss: 0.0022 Acc: 84.1170\n",
      "validation Loss: 0.0022 Acc: 83.3571\n",
      "Epoch 44/99\n",
      "training Loss: 0.0022 Acc: 84.1289\n",
      "validation Loss: 0.0022 Acc: 83.3690\n",
      "Epoch 45/99\n",
      "training Loss: 0.0022 Acc: 84.0873\n",
      "validation Loss: 0.0022 Acc: 83.4048\n",
      "Epoch 46/99\n",
      "training Loss: 0.0022 Acc: 84.2361\n",
      "validation Loss: 0.0022 Acc: 83.3690\n",
      "Early stopped.\n",
      "Best val acc: 83.464286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0031 Acc: 78.5429\n",
      "validation Loss: 0.0026 Acc: 81.6310\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0025 Acc: 82.1350\n",
      "validation Loss: 0.0025 Acc: 82.5833\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0024 Acc: 82.8879\n",
      "validation Loss: 0.0024 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0024 Acc: 83.0367\n",
      "validation Loss: 0.0024 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0024 Acc: 83.1647\n",
      "validation Loss: 0.0024 Acc: 83.0952\n",
      "Epoch 5/99\n",
      "training Loss: 0.0023 Acc: 83.2867\n",
      "validation Loss: 0.0023 Acc: 83.1548\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 83.3760\n",
      "validation Loss: 0.0023 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0023 Acc: 83.4057\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0023 Acc: 83.4921\n",
      "validation Loss: 0.0023 Acc: 83.0119\n",
      "Epoch 9/99\n",
      "training Loss: 0.0023 Acc: 83.3909\n",
      "validation Loss: 0.0023 Acc: 83.0238\n",
      "Epoch 10/99\n",
      "training Loss: 0.0023 Acc: 83.4712\n",
      "validation Loss: 0.0023 Acc: 83.2024\n",
      "Epoch 11/99\n",
      "training Loss: 0.0023 Acc: 83.4742\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Epoch 12/99\n",
      "training Loss: 0.0023 Acc: 83.6885\n",
      "validation Loss: 0.0023 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0023 Acc: 83.6825\n",
      "validation Loss: 0.0023 Acc: 83.2976\n",
      "Epoch 14/99\n",
      "training Loss: 0.0023 Acc: 83.6200\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0023 Acc: 83.6825\n",
      "validation Loss: 0.0023 Acc: 83.2857\n",
      "Epoch 16/99\n",
      "training Loss: 0.0023 Acc: 83.6290\n",
      "validation Loss: 0.0023 Acc: 83.2857\n",
      "Epoch 17/99\n",
      "training Loss: 0.0023 Acc: 83.6587\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Epoch 18/99\n",
      "training Loss: 0.0023 Acc: 83.7212\n",
      "validation Loss: 0.0023 Acc: 83.3214\n",
      "Epoch 19/99\n",
      "training Loss: 0.0023 Acc: 83.7420\n",
      "validation Loss: 0.0023 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0023 Acc: 83.8343\n",
      "validation Loss: 0.0023 Acc: 83.2857\n",
      "Epoch 21/99\n",
      "training Loss: 0.0023 Acc: 83.8254\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Epoch 22/99\n",
      "training Loss: 0.0022 Acc: 83.8908\n",
      "validation Loss: 0.0023 Acc: 83.2619\n",
      "Epoch 23/99\n",
      "training Loss: 0.0022 Acc: 83.7956\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Epoch 24/99\n",
      "training Loss: 0.0022 Acc: 83.7748\n",
      "validation Loss: 0.0023 Acc: 83.2381\n",
      "Epoch 25/99\n",
      "training Loss: 0.0022 Acc: 83.9057\n",
      "validation Loss: 0.0023 Acc: 83.1786\n",
      "Epoch 26/99\n",
      "training Loss: 0.0022 Acc: 83.8760\n",
      "validation Loss: 0.0023 Acc: 83.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0022 Acc: 83.9623\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0022 Acc: 83.9742\n",
      "validation Loss: 0.0023 Acc: 83.2500\n",
      "Epoch 29/99\n",
      "training Loss: 0.0022 Acc: 83.7897\n",
      "validation Loss: 0.0023 Acc: 83.2619\n",
      "Epoch 30/99\n",
      "training Loss: 0.0022 Acc: 83.8313\n",
      "validation Loss: 0.0023 Acc: 83.2976\n",
      "Epoch 31/99\n",
      "training Loss: 0.0022 Acc: 83.9385\n",
      "validation Loss: 0.0023 Acc: 83.2143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0022 Acc: 83.8819\n",
      "validation Loss: 0.0023 Acc: 83.1667\n",
      "Epoch 33/99\n",
      "training Loss: 0.0022 Acc: 83.8462\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Epoch 34/99\n",
      "training Loss: 0.0022 Acc: 83.9533\n",
      "validation Loss: 0.0023 Acc: 83.1667\n",
      "Epoch 35/99\n",
      "training Loss: 0.0022 Acc: 84.0396\n",
      "validation Loss: 0.0023 Acc: 83.3214\n",
      "Epoch 36/99\n",
      "training Loss: 0.0022 Acc: 84.0962\n",
      "validation Loss: 0.0023 Acc: 83.2857\n",
      "Epoch 37/99\n",
      "training Loss: 0.0022 Acc: 84.0456\n",
      "validation Loss: 0.0023 Acc: 83.2619\n",
      "Epoch 38/99\n",
      "training Loss: 0.0022 Acc: 84.0902\n",
      "validation Loss: 0.0023 Acc: 83.2381\n",
      "Epoch 39/99\n",
      "training Loss: 0.0022 Acc: 84.0992\n",
      "validation Loss: 0.0023 Acc: 83.3571\n",
      "Early stopped.\n",
      "Best val acc: 83.380952\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8317135376\n",
      "New configuration: {'learning_rate': 0.00536280854935617, 'initial_nodes': 738, 'dropout': 0.4198768104484568, 'batch_size': 404, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.1637\n",
      "validation Loss: 0.0009 Acc: 82.9683\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.3036\n",
      "validation Loss: 0.0009 Acc: 83.2302\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.4375\n",
      "validation Loss: 0.0009 Acc: 83.5634\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.4554\n",
      "validation Loss: 0.0009 Acc: 83.3849\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.5357\n",
      "validation Loss: 0.0009 Acc: 83.6706\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.7232\n",
      "validation Loss: 0.0009 Acc: 83.4563\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.6310\n",
      "validation Loss: 0.0009 Acc: 83.4682\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.6458\n",
      "validation Loss: 0.0009 Acc: 83.5039\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.8482\n",
      "validation Loss: 0.0009 Acc: 83.5991\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.5952\n",
      "validation Loss: 0.0009 Acc: 83.5277\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.7887\n",
      "validation Loss: 0.0009 Acc: 83.4563\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.7381\n",
      "validation Loss: 0.0009 Acc: 83.4444\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.7411\n",
      "validation Loss: 0.0009 Acc: 83.6229\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.7500\n",
      "validation Loss: 0.0009 Acc: 83.3254\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.8780\n",
      "validation Loss: 0.0009 Acc: 83.6587\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 83.6339\n",
      "validation Loss: 0.0009 Acc: 83.4682\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 83.8899\n",
      "validation Loss: 0.0009 Acc: 83.6348\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 83.8661\n",
      "validation Loss: 0.0009 Acc: 83.6825\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 83.9643\n",
      "validation Loss: 0.0009 Acc: 83.7063\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 83.9851\n",
      "validation Loss: 0.0009 Acc: 83.6587\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.0417\n",
      "validation Loss: 0.0009 Acc: 83.8015\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.2292\n",
      "validation Loss: 0.0009 Acc: 83.7420\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.4107\n",
      "validation Loss: 0.0009 Acc: 83.9324\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.4970\n",
      "validation Loss: 0.0009 Acc: 83.6468\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.5149\n",
      "validation Loss: 0.0009 Acc: 83.8967\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.5982\n",
      "validation Loss: 0.0009 Acc: 83.7420\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.7173\n",
      "validation Loss: 0.0009 Acc: 83.7777\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.8810\n",
      "validation Loss: 0.0009 Acc: 83.8848\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.9762\n",
      "validation Loss: 0.0009 Acc: 83.8491\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 85.0982\n",
      "validation Loss: 0.0009 Acc: 83.7539\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 85.3661\n",
      "validation Loss: 0.0009 Acc: 83.8848\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.2440\n",
      "validation Loss: 0.0009 Acc: 83.8015\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.2083\n",
      "validation Loss: 0.0009 Acc: 83.8729\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.4107\n",
      "validation Loss: 0.0009 Acc: 83.8610\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.6875\n",
      "validation Loss: 0.0009 Acc: 83.8134\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.7679\n",
      "validation Loss: 0.0009 Acc: 83.7182\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.8125\n",
      "validation Loss: 0.0009 Acc: 83.6348\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.6696\n",
      "validation Loss: 0.0009 Acc: 83.6348\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 85.7917\n",
      "validation Loss: 0.0009 Acc: 83.5753\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 86.0000\n",
      "validation Loss: 0.0009 Acc: 83.4563\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 86.1012\n",
      "validation Loss: 0.0009 Acc: 83.5991\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 86.0714\n",
      "validation Loss: 0.0009 Acc: 83.5515\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 86.0923\n",
      "validation Loss: 0.0009 Acc: 83.6229\n",
      "Early stopped.\n",
      "Best val acc: 83.932397\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.4416\n",
      "validation Loss: 0.0010 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1945\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.3968\n",
      "validation Loss: 0.0009 Acc: 83.3929\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.4296\n",
      "validation Loss: 0.0009 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.6051\n",
      "validation Loss: 0.0009 Acc: 83.5833\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.5248\n",
      "validation Loss: 0.0009 Acc: 83.6905\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.7063\n",
      "validation Loss: 0.0009 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.6379\n",
      "validation Loss: 0.0009 Acc: 83.7738\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.4415\n",
      "validation Loss: 0.0009 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.7658\n",
      "validation Loss: 0.0009 Acc: 83.6667\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.6914\n",
      "validation Loss: 0.0009 Acc: 83.6786\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.8968\n",
      "validation Loss: 0.0009 Acc: 83.5238\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.8789\n",
      "validation Loss: 0.0009 Acc: 83.7381\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.7658\n",
      "validation Loss: 0.0009 Acc: 84.0476\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.8611\n",
      "validation Loss: 0.0009 Acc: 83.8452\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.0039\n",
      "validation Loss: 0.0009 Acc: 83.3095\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 83.7034\n",
      "validation Loss: 0.0009 Acc: 83.6548\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 83.9623\n",
      "validation Loss: 0.0009 Acc: 83.8571\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.1438\n",
      "validation Loss: 0.0009 Acc: 83.7857\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.0545\n",
      "validation Loss: 0.0009 Acc: 83.5595\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 83.9861\n",
      "validation Loss: 0.0009 Acc: 83.8333\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.2866\n",
      "validation Loss: 0.0009 Acc: 83.8810\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 83.9890\n",
      "validation Loss: 0.0009 Acc: 83.7738\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.3015\n",
      "validation Loss: 0.0009 Acc: 83.7976\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.5723\n",
      "validation Loss: 0.0009 Acc: 84.0357\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.7241\n",
      "validation Loss: 0.0009 Acc: 83.6310\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.6854\n",
      "validation Loss: 0.0009 Acc: 84.2857\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.9563\n",
      "validation Loss: 0.0009 Acc: 83.9524\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.8640\n",
      "validation Loss: 0.0009 Acc: 84.0000\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.8967\n",
      "validation Loss: 0.0009 Acc: 83.5952\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 85.1735\n",
      "validation Loss: 0.0009 Acc: 83.8214\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.0396\n",
      "validation Loss: 0.0009 Acc: 83.5952\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.1943\n",
      "validation Loss: 0.0009 Acc: 83.8571\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.6259\n",
      "validation Loss: 0.0009 Acc: 83.5595\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.7479\n",
      "validation Loss: 0.0009 Acc: 83.7500\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.7449\n",
      "validation Loss: 0.0009 Acc: 83.6429\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.9473\n",
      "validation Loss: 0.0009 Acc: 83.5119\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.9800\n",
      "validation Loss: 0.0009 Acc: 83.5833\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 85.8282\n",
      "validation Loss: 0.0009 Acc: 83.6667\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 86.2925\n",
      "validation Loss: 0.0009 Acc: 83.7500\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 86.3877\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 86.3341\n",
      "validation Loss: 0.0009 Acc: 83.3929\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 86.6437\n",
      "validation Loss: 0.0009 Acc: 83.5595\n",
      "Epoch 43/99\n",
      "training Loss: 0.0008 Acc: 86.4919\n",
      "validation Loss: 0.0009 Acc: 83.5952\n",
      "Epoch 44/99\n",
      "training Loss: 0.0008 Acc: 86.6794\n",
      "validation Loss: 0.0009 Acc: 83.3690\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 86.7716\n",
      "validation Loss: 0.0009 Acc: 83.4762\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 87.0127\n",
      "validation Loss: 0.0009 Acc: 83.3095\n",
      "Early stopped.\n",
      "Best val acc: 84.285714\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 81.9951\n",
      "validation Loss: 0.0010 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.4653\n",
      "validation Loss: 0.0009 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.3433\n",
      "validation Loss: 0.0010 Acc: 82.8571\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.5040\n",
      "validation Loss: 0.0009 Acc: 82.9405\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.7688\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.6885\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.6409\n",
      "validation Loss: 0.0009 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.8908\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.7480\n",
      "validation Loss: 0.0009 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.7212\n",
      "validation Loss: 0.0009 Acc: 83.0000\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.8135\n",
      "validation Loss: 0.0009 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.8968\n",
      "validation Loss: 0.0009 Acc: 83.3333\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.9117\n",
      "validation Loss: 0.0009 Acc: 83.2262\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.9533\n",
      "validation Loss: 0.0009 Acc: 83.1071\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.8164\n",
      "validation Loss: 0.0009 Acc: 82.7738\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 83.8432\n",
      "validation Loss: 0.0009 Acc: 83.4405\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 84.1051\n",
      "validation Loss: 0.0009 Acc: 83.2976\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.0337\n",
      "validation Loss: 0.0009 Acc: 83.0714\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.1825\n",
      "validation Loss: 0.0009 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.5039\n",
      "validation Loss: 0.0009 Acc: 83.1190\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.3402\n",
      "validation Loss: 0.0009 Acc: 83.3452\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.4920\n",
      "validation Loss: 0.0009 Acc: 83.2143\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.6319\n",
      "validation Loss: 0.0009 Acc: 83.4048\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.6795\n",
      "validation Loss: 0.0009 Acc: 83.2381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 85.0515\n",
      "validation Loss: 0.0009 Acc: 83.2500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 85.1735\n",
      "validation Loss: 0.0009 Acc: 83.2262\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 85.2122\n",
      "validation Loss: 0.0009 Acc: 83.3214\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 85.3550\n",
      "validation Loss: 0.0009 Acc: 83.3571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 85.2330\n",
      "validation Loss: 0.0009 Acc: 83.4167\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 85.3640\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 85.5515\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.5187\n",
      "validation Loss: 0.0009 Acc: 83.2381\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.7181\n",
      "validation Loss: 0.0009 Acc: 83.0952\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.8550\n",
      "validation Loss: 0.0009 Acc: 83.1905\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.7985\n",
      "validation Loss: 0.0009 Acc: 83.0000\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 86.0663\n",
      "validation Loss: 0.0010 Acc: 83.1429\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 86.2002\n",
      "validation Loss: 0.0009 Acc: 83.0238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 86.0723\n",
      "validation Loss: 0.0010 Acc: 82.9762\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 86.0931\n",
      "validation Loss: 0.0010 Acc: 83.0476\n",
      "Early stopped.\n",
      "Best val acc: 83.488095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 81.6261\n",
      "validation Loss: 0.0009 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.0129\n",
      "validation Loss: 0.0009 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.2361\n",
      "validation Loss: 0.0009 Acc: 83.8095\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.2361\n",
      "validation Loss: 0.0009 Acc: 84.0238\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.4742\n",
      "validation Loss: 0.0009 Acc: 84.2619\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.3313\n",
      "validation Loss: 0.0009 Acc: 84.1310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.4504\n",
      "validation Loss: 0.0009 Acc: 84.2857\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.4355\n",
      "validation Loss: 0.0009 Acc: 84.0714\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.4950\n",
      "validation Loss: 0.0009 Acc: 84.2976\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.5099\n",
      "validation Loss: 0.0009 Acc: 84.3452\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.6914\n",
      "validation Loss: 0.0009 Acc: 84.2857\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.5367\n",
      "validation Loss: 0.0009 Acc: 84.0357\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.5248\n",
      "validation Loss: 0.0009 Acc: 83.7976\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.6528\n",
      "validation Loss: 0.0009 Acc: 84.1429\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.5188\n",
      "validation Loss: 0.0009 Acc: 84.0952\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.0634\n",
      "validation Loss: 0.0009 Acc: 84.2500\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 84.1140\n",
      "validation Loss: 0.0009 Acc: 84.5119\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.1408\n",
      "validation Loss: 0.0009 Acc: 84.4524\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.0575\n",
      "validation Loss: 0.0009 Acc: 84.1310\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.0873\n",
      "validation Loss: 0.0009 Acc: 84.1905\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.0902\n",
      "validation Loss: 0.0009 Acc: 84.4405\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.4533\n",
      "validation Loss: 0.0009 Acc: 84.3452\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.6527\n",
      "validation Loss: 0.0009 Acc: 84.2976\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.6170\n",
      "validation Loss: 0.0009 Acc: 84.3095\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.5247\n",
      "validation Loss: 0.0009 Acc: 84.4643\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.6944\n",
      "validation Loss: 0.0009 Acc: 84.1429\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.8313\n",
      "validation Loss: 0.0009 Acc: 84.4762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.9801\n",
      "validation Loss: 0.0009 Acc: 84.3333\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 85.2271\n",
      "validation Loss: 0.0009 Acc: 84.4167\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 85.2598\n",
      "validation Loss: 0.0009 Acc: 84.4881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 85.2717\n",
      "validation Loss: 0.0009 Acc: 84.3929\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.2568\n",
      "validation Loss: 0.0009 Acc: 84.2857\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.4146\n",
      "validation Loss: 0.0009 Acc: 84.2143\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.6140\n",
      "validation Loss: 0.0009 Acc: 84.2976\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.7389\n",
      "validation Loss: 0.0009 Acc: 84.3214\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.7717\n",
      "validation Loss: 0.0009 Acc: 84.2619\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.6169\n",
      "validation Loss: 0.0009 Acc: 84.1429\n",
      "Early stopped.\n",
      "Best val acc: 84.511905\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 81.0577\n",
      "validation Loss: 0.0009 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1647\n",
      "validation Loss: 0.0010 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.4534\n",
      "validation Loss: 0.0010 Acc: 82.4286\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.4891\n",
      "validation Loss: 0.0010 Acc: 83.1548\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.6051\n",
      "validation Loss: 0.0009 Acc: 82.9762\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.4415\n",
      "validation Loss: 0.0009 Acc: 82.5714\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.3641\n",
      "validation Loss: 0.0009 Acc: 83.2024\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.7956\n",
      "validation Loss: 0.0009 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.7272\n",
      "validation Loss: 0.0009 Acc: 83.2500\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.7034\n",
      "validation Loss: 0.0009 Acc: 83.2381\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.7688\n",
      "validation Loss: 0.0009 Acc: 83.3452\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.9117\n",
      "validation Loss: 0.0009 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 84.0486\n",
      "validation Loss: 0.0009 Acc: 83.2500\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.9385\n",
      "validation Loss: 0.0009 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 84.2093\n",
      "validation Loss: 0.0009 Acc: 83.4048\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 84.3343\n",
      "validation Loss: 0.0009 Acc: 83.2738\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 84.3640\n",
      "validation Loss: 0.0009 Acc: 83.4762\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 84.4831\n",
      "validation Loss: 0.0009 Acc: 83.4762\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.5277\n",
      "validation Loss: 0.0009 Acc: 83.2976\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.4474\n",
      "validation Loss: 0.0009 Acc: 83.5119\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.5634\n",
      "validation Loss: 0.0009 Acc: 83.4524\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.5485\n",
      "validation Loss: 0.0009 Acc: 83.5238\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.3849\n",
      "validation Loss: 0.0009 Acc: 83.2024\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.5634\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.6497\n",
      "validation Loss: 0.0009 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.7569\n",
      "validation Loss: 0.0009 Acc: 83.4881\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.9979\n",
      "validation Loss: 0.0009 Acc: 83.2976\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 85.1616\n",
      "validation Loss: 0.0009 Acc: 83.2619\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 85.3134\n",
      "validation Loss: 0.0009 Acc: 83.2619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 85.3640\n",
      "validation Loss: 0.0009 Acc: 83.4286\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 85.3491\n",
      "validation Loss: 0.0009 Acc: 83.5238\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.2836\n",
      "validation Loss: 0.0009 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.6556\n",
      "validation Loss: 0.0009 Acc: 83.4643\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.7419\n",
      "validation Loss: 0.0009 Acc: 83.2976\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.8431\n",
      "validation Loss: 0.0009 Acc: 83.4286\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.8580\n",
      "validation Loss: 0.0009 Acc: 83.2976\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.9860\n",
      "validation Loss: 0.0009 Acc: 83.3571\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.8877\n",
      "validation Loss: 0.0009 Acc: 83.2024\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 86.2240\n",
      "validation Loss: 0.0009 Acc: 83.3929\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 86.3461\n",
      "validation Loss: 0.0009 Acc: 83.3810\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 86.3609\n",
      "validation Loss: 0.0009 Acc: 83.2024\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 86.1586\n",
      "validation Loss: 0.0009 Acc: 83.3214\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 86.2955\n",
      "validation Loss: 0.0009 Acc: 83.3690\n",
      "Epoch 43/99\n",
      "training Loss: 0.0008 Acc: 86.4532\n",
      "validation Loss: 0.0009 Acc: 83.3571\n",
      "Epoch 44/99\n",
      "training Loss: 0.0008 Acc: 86.4056\n",
      "validation Loss: 0.0009 Acc: 83.4524\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 86.5871\n",
      "validation Loss: 0.0009 Acc: 83.2024\n",
      "Epoch 46/99\n",
      "training Loss: 0.0008 Acc: 86.6020\n",
      "validation Loss: 0.0010 Acc: 83.4048\n",
      "Epoch 47/99\n",
      "training Loss: 0.0008 Acc: 86.5693\n",
      "validation Loss: 0.0009 Acc: 83.3690\n",
      "Epoch 48/99\n",
      "training Loss: 0.0008 Acc: 86.7478\n",
      "validation Loss: 0.0010 Acc: 83.3690\n",
      "Epoch 49/99\n",
      "training Loss: 0.0008 Acc: 86.7389\n",
      "validation Loss: 0.0010 Acc: 83.3452\n",
      "Epoch 50/99\n",
      "training Loss: 0.0008 Acc: 86.6734\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 51/99\n",
      "training Loss: 0.0008 Acc: 86.6913\n",
      "validation Loss: 0.0010 Acc: 83.4048\n",
      "Early stopped.\n",
      "Best val acc: 83.642857\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9721936954\n",
      "New configuration: {'learning_rate': 0.0009993292097751978, 'initial_nodes': 376, 'dropout': 0.6279291595015061, 'batch_size': 331, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 78.4881\n",
      "validation Loss: 0.0012 Acc: 83.1588\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 82.8452\n",
      "validation Loss: 0.0011 Acc: 83.7539\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0012 Acc: 83.0833\n",
      "validation Loss: 0.0011 Acc: 83.5753\n",
      "Epoch 3/99\n",
      "training Loss: 0.0012 Acc: 83.1935\n",
      "validation Loss: 0.0011 Acc: 83.2421\n",
      "Epoch 4/99\n",
      "training Loss: 0.0012 Acc: 83.1577\n",
      "validation Loss: 0.0011 Acc: 83.6348\n",
      "Epoch 5/99\n",
      "training Loss: 0.0012 Acc: 83.4196\n",
      "validation Loss: 0.0011 Acc: 83.5872\n",
      "Epoch 6/99\n",
      "training Loss: 0.0012 Acc: 83.3244\n",
      "validation Loss: 0.0011 Acc: 83.6468\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.3631\n",
      "validation Loss: 0.0011 Acc: 83.5872\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.4464\n",
      "validation Loss: 0.0011 Acc: 83.5991\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.6369\n",
      "validation Loss: 0.0011 Acc: 83.6944\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.6964\n",
      "validation Loss: 0.0011 Acc: 83.7301\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.6726\n",
      "validation Loss: 0.0011 Acc: 83.5039\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.6786\n",
      "validation Loss: 0.0011 Acc: 83.9443\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.5744\n",
      "validation Loss: 0.0011 Acc: 83.8610\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.6131\n",
      "validation Loss: 0.0011 Acc: 83.6468\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.7679\n",
      "validation Loss: 0.0011 Acc: 83.8134\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.6875\n",
      "validation Loss: 0.0011 Acc: 83.8253\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.8958\n",
      "validation Loss: 0.0011 Acc: 83.8610\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 83.8363\n",
      "validation Loss: 0.0011 Acc: 83.7896\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 84.0119\n",
      "validation Loss: 0.0011 Acc: 83.8610\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 83.8482\n",
      "validation Loss: 0.0011 Acc: 83.9205\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 83.9018\n",
      "validation Loss: 0.0011 Acc: 84.2537\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 84.0089\n",
      "validation Loss: 0.0011 Acc: 84.0038\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 83.8661\n",
      "validation Loss: 0.0011 Acc: 83.8491\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 83.9375\n",
      "validation Loss: 0.0011 Acc: 83.8015\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 83.9494\n",
      "validation Loss: 0.0011 Acc: 83.8491\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 84.1548\n",
      "validation Loss: 0.0011 Acc: 83.8967\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 84.2381\n",
      "validation Loss: 0.0011 Acc: 84.0752\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 84.1815\n",
      "validation Loss: 0.0011 Acc: 84.0633\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 84.0952\n",
      "validation Loss: 0.0011 Acc: 83.9919\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 84.1042\n",
      "validation Loss: 0.0011 Acc: 83.8372\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 84.1458\n",
      "validation Loss: 0.0011 Acc: 83.8134\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 84.2381\n",
      "validation Loss: 0.0011 Acc: 83.7777\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 84.4702\n",
      "validation Loss: 0.0011 Acc: 84.0157\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 84.2679\n",
      "validation Loss: 0.0011 Acc: 83.7896\n",
      "Epoch 35/99\n",
      "training Loss: 0.0011 Acc: 84.3631\n",
      "validation Loss: 0.0011 Acc: 83.8491\n",
      "Epoch 36/99\n",
      "training Loss: 0.0011 Acc: 84.3750\n",
      "validation Loss: 0.0011 Acc: 83.9919\n",
      "Epoch 37/99\n",
      "training Loss: 0.0011 Acc: 84.4226\n",
      "validation Loss: 0.0011 Acc: 84.0990\n",
      "Epoch 38/99\n",
      "training Loss: 0.0011 Acc: 84.3810\n",
      "validation Loss: 0.0011 Acc: 84.0276\n",
      "Epoch 39/99\n",
      "training Loss: 0.0011 Acc: 84.3631\n",
      "validation Loss: 0.0011 Acc: 83.8967\n",
      "Epoch 40/99\n",
      "training Loss: 0.0011 Acc: 84.5982\n",
      "validation Loss: 0.0011 Acc: 83.9681\n",
      "Epoch 41/99\n",
      "training Loss: 0.0011 Acc: 84.5565\n",
      "validation Loss: 0.0011 Acc: 83.8967\n",
      "Early stopped.\n",
      "Best val acc: 84.253749\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 79.4149\n",
      "validation Loss: 0.0012 Acc: 82.3095\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 83.0754\n",
      "validation Loss: 0.0012 Acc: 82.3333\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0012 Acc: 83.2748\n",
      "validation Loss: 0.0012 Acc: 82.4643\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0012 Acc: 83.4801\n",
      "validation Loss: 0.0012 Acc: 82.4881\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0012 Acc: 83.5010\n",
      "validation Loss: 0.0012 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0011 Acc: 83.7123\n",
      "validation Loss: 0.0012 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.7331\n",
      "validation Loss: 0.0012 Acc: 82.7024\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.5248\n",
      "validation Loss: 0.0012 Acc: 82.6667\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.7867\n",
      "validation Loss: 0.0012 Acc: 82.6667\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.8492\n",
      "validation Loss: 0.0012 Acc: 82.7500\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 84.0039\n",
      "validation Loss: 0.0011 Acc: 82.5238\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.8194\n",
      "validation Loss: 0.0012 Acc: 82.5595\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.9176\n",
      "validation Loss: 0.0011 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.7510\n",
      "validation Loss: 0.0012 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.9801\n",
      "validation Loss: 0.0012 Acc: 82.7619\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 84.0754\n",
      "validation Loss: 0.0012 Acc: 82.8690\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 84.0634\n",
      "validation Loss: 0.0011 Acc: 82.8214\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 84.2063\n",
      "validation Loss: 0.0012 Acc: 82.6786\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 84.2033\n",
      "validation Loss: 0.0012 Acc: 82.7500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 84.2063\n",
      "validation Loss: 0.0012 Acc: 82.8452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 84.0962\n",
      "validation Loss: 0.0011 Acc: 82.6905\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 84.2896\n",
      "validation Loss: 0.0012 Acc: 82.5238\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 84.4295\n",
      "validation Loss: 0.0012 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 84.3372\n",
      "validation Loss: 0.0011 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 84.3581\n",
      "validation Loss: 0.0012 Acc: 82.9286\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 84.4801\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 84.3432\n",
      "validation Loss: 0.0011 Acc: 82.8929\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 84.4533\n",
      "validation Loss: 0.0012 Acc: 83.1786\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 84.4265\n",
      "validation Loss: 0.0012 Acc: 82.9048\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 84.7479\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 84.7360\n",
      "validation Loss: 0.0011 Acc: 82.8810\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 84.6676\n",
      "validation Loss: 0.0012 Acc: 82.9286\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 84.9324\n",
      "validation Loss: 0.0012 Acc: 83.0000\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 84.8819\n",
      "validation Loss: 0.0012 Acc: 83.1667\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 84.8551\n",
      "validation Loss: 0.0011 Acc: 83.0357\n",
      "Epoch 35/99\n",
      "training Loss: 0.0010 Acc: 84.8908\n",
      "validation Loss: 0.0012 Acc: 83.1071\n",
      "Epoch 36/99\n",
      "training Loss: 0.0010 Acc: 84.8699\n",
      "validation Loss: 0.0011 Acc: 82.9762\n",
      "Epoch 37/99\n",
      "training Loss: 0.0010 Acc: 85.0485\n",
      "validation Loss: 0.0012 Acc: 82.7262\n",
      "Epoch 38/99\n",
      "training Loss: 0.0010 Acc: 84.8848\n",
      "validation Loss: 0.0011 Acc: 82.9405\n",
      "Epoch 39/99\n",
      "training Loss: 0.0010 Acc: 84.9682\n",
      "validation Loss: 0.0012 Acc: 82.7381\n",
      "Epoch 40/99\n",
      "training Loss: 0.0010 Acc: 85.0217\n",
      "validation Loss: 0.0012 Acc: 82.7024\n",
      "Epoch 41/99\n",
      "training Loss: 0.0010 Acc: 84.9771\n",
      "validation Loss: 0.0012 Acc: 82.8095\n",
      "Epoch 42/99\n",
      "training Loss: 0.0010 Acc: 85.0366\n",
      "validation Loss: 0.0012 Acc: 82.8214\n",
      "Epoch 43/99\n",
      "training Loss: 0.0010 Acc: 85.0812\n",
      "validation Loss: 0.0012 Acc: 82.8452\n",
      "Epoch 44/99\n",
      "training Loss: 0.0010 Acc: 85.0485\n",
      "validation Loss: 0.0012 Acc: 82.8214\n",
      "Epoch 45/99\n",
      "training Loss: 0.0010 Acc: 85.0604\n",
      "validation Loss: 0.0012 Acc: 82.7500\n",
      "Early stopped.\n",
      "Best val acc: 83.190476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 78.7721\n",
      "validation Loss: 0.0011 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 82.7927\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0012 Acc: 83.0576\n",
      "validation Loss: 0.0011 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0012 Acc: 83.1796\n",
      "validation Loss: 0.0011 Acc: 83.7262\n",
      "Epoch 4/99\n",
      "training Loss: 0.0012 Acc: 83.2153\n",
      "validation Loss: 0.0011 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0012 Acc: 83.2480\n",
      "validation Loss: 0.0011 Acc: 83.7619\n",
      "Epoch 6/99\n",
      "training Loss: 0.0012 Acc: 83.3968\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 7/99\n",
      "training Loss: 0.0012 Acc: 83.5516\n",
      "validation Loss: 0.0011 Acc: 84.0000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.5546\n",
      "validation Loss: 0.0011 Acc: 83.8929\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.5962\n",
      "validation Loss: 0.0011 Acc: 84.0952\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.5903\n",
      "validation Loss: 0.0011 Acc: 84.0357\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.7629\n",
      "validation Loss: 0.0011 Acc: 84.0119\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.6557\n",
      "validation Loss: 0.0011 Acc: 84.0357\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.6141\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.8105\n",
      "validation Loss: 0.0011 Acc: 83.9643\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.8760\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.9146\n",
      "validation Loss: 0.0011 Acc: 83.8333\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.9920\n",
      "validation Loss: 0.0011 Acc: 84.2024\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 84.1111\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 83.9801\n",
      "validation Loss: 0.0011 Acc: 84.0595\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 83.9414\n",
      "validation Loss: 0.0011 Acc: 84.0357\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 83.9414\n",
      "validation Loss: 0.0011 Acc: 84.1548\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 84.0754\n",
      "validation Loss: 0.0011 Acc: 84.2500\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 83.9533\n",
      "validation Loss: 0.0011 Acc: 84.0595\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 84.0813\n",
      "validation Loss: 0.0011 Acc: 84.2262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 84.0039\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 84.2658\n",
      "validation Loss: 0.0011 Acc: 84.2619\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 84.2093\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 84.2896\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 84.3462\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 84.4384\n",
      "validation Loss: 0.0011 Acc: 84.4286\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 84.4235\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 84.4831\n",
      "validation Loss: 0.0011 Acc: 84.2738\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 84.5039\n",
      "validation Loss: 0.0011 Acc: 84.2024\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 84.4533\n",
      "validation Loss: 0.0011 Acc: 84.2500\n",
      "Epoch 35/99\n",
      "training Loss: 0.0011 Acc: 84.5604\n",
      "validation Loss: 0.0011 Acc: 84.2976\n",
      "Epoch 36/99\n",
      "training Loss: 0.0011 Acc: 84.6378\n",
      "validation Loss: 0.0011 Acc: 84.2857\n",
      "Epoch 37/99\n",
      "training Loss: 0.0011 Acc: 84.5872\n",
      "validation Loss: 0.0011 Acc: 84.2619\n",
      "Epoch 38/99\n",
      "training Loss: 0.0011 Acc: 84.7569\n",
      "validation Loss: 0.0011 Acc: 84.2857\n",
      "Epoch 39/99\n",
      "training Loss: 0.0011 Acc: 84.6944\n",
      "validation Loss: 0.0011 Acc: 84.2857\n",
      "Epoch 40/99\n",
      "training Loss: 0.0011 Acc: 84.6348\n",
      "validation Loss: 0.0011 Acc: 84.2500\n",
      "Epoch 41/99\n",
      "training Loss: 0.0011 Acc: 84.7479\n",
      "validation Loss: 0.0011 Acc: 84.2976\n",
      "Epoch 42/99\n",
      "training Loss: 0.0011 Acc: 84.5843\n",
      "validation Loss: 0.0011 Acc: 84.2619\n",
      "Epoch 43/99\n",
      "training Loss: 0.0011 Acc: 84.6646\n",
      "validation Loss: 0.0011 Acc: 84.2500\n",
      "Epoch 44/99\n",
      "training Loss: 0.0011 Acc: 84.8878\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 45/99\n",
      "training Loss: 0.0011 Acc: 84.6319\n",
      "validation Loss: 0.0011 Acc: 84.2262\n",
      "Epoch 46/99\n",
      "training Loss: 0.0011 Acc: 84.7033\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 47/99\n",
      "training Loss: 0.0011 Acc: 84.8461\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0011 Acc: 84.7390\n",
      "validation Loss: 0.0011 Acc: 84.2381\n",
      "Epoch 49/99\n",
      "training Loss: 0.0011 Acc: 84.8134\n",
      "validation Loss: 0.0011 Acc: 84.1905\n",
      "Epoch 50/99\n",
      "training Loss: 0.0011 Acc: 84.8104\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Early stopped.\n",
      "Best val acc: 84.428571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 77.7751\n",
      "validation Loss: 0.0012 Acc: 82.7381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 82.8314\n",
      "validation Loss: 0.0012 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0012 Acc: 83.2659\n",
      "validation Loss: 0.0011 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0012 Acc: 83.1915\n",
      "validation Loss: 0.0012 Acc: 83.0238\n",
      "Epoch 4/99\n",
      "training Loss: 0.0012 Acc: 83.2153\n",
      "validation Loss: 0.0011 Acc: 83.1905\n",
      "Epoch 5/99\n",
      "training Loss: 0.0012 Acc: 83.4950\n",
      "validation Loss: 0.0011 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0011 Acc: 83.4325\n",
      "validation Loss: 0.0011 Acc: 83.6190\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.6409\n",
      "validation Loss: 0.0011 Acc: 83.6786\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.4563\n",
      "validation Loss: 0.0011 Acc: 83.6905\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.5129\n",
      "validation Loss: 0.0011 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.7778\n",
      "validation Loss: 0.0011 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.6022\n",
      "validation Loss: 0.0011 Acc: 83.5357\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.6587\n",
      "validation Loss: 0.0011 Acc: 83.7619\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.7331\n",
      "validation Loss: 0.0011 Acc: 83.8095\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.9563\n",
      "validation Loss: 0.0011 Acc: 83.6310\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.9295\n",
      "validation Loss: 0.0011 Acc: 83.7143\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.8879\n",
      "validation Loss: 0.0011 Acc: 83.7381\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.7063\n",
      "validation Loss: 0.0011 Acc: 83.6667\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 83.8879\n",
      "validation Loss: 0.0011 Acc: 83.6429\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 83.7658\n",
      "validation Loss: 0.0011 Acc: 83.5357\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 83.8373\n",
      "validation Loss: 0.0011 Acc: 83.5714\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 83.8641\n",
      "validation Loss: 0.0011 Acc: 83.5952\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 83.9385\n",
      "validation Loss: 0.0011 Acc: 83.5952\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 84.0575\n",
      "validation Loss: 0.0011 Acc: 83.3333\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 84.0486\n",
      "validation Loss: 0.0011 Acc: 83.8214\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 84.1111\n",
      "validation Loss: 0.0011 Acc: 83.7143\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 84.1527\n",
      "validation Loss: 0.0011 Acc: 83.7024\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 84.2599\n",
      "validation Loss: 0.0011 Acc: 83.6548\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 84.2599\n",
      "validation Loss: 0.0011 Acc: 83.7857\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 84.2926\n",
      "validation Loss: 0.0011 Acc: 83.4524\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 84.3164\n",
      "validation Loss: 0.0011 Acc: 83.7738\n",
      "Early stopped.\n",
      "Best val acc: 83.880952\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 78.4685\n",
      "validation Loss: 0.0012 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 82.6379\n",
      "validation Loss: 0.0011 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0012 Acc: 83.0486\n",
      "validation Loss: 0.0011 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0012 Acc: 82.9385\n",
      "validation Loss: 0.0011 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0012 Acc: 83.3194\n",
      "validation Loss: 0.0011 Acc: 83.7619\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0012 Acc: 83.4057\n",
      "validation Loss: 0.0011 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0012 Acc: 83.4236\n",
      "validation Loss: 0.0011 Acc: 83.8095\n",
      "Epoch 7/99\n",
      "training Loss: 0.0011 Acc: 83.3611\n",
      "validation Loss: 0.0011 Acc: 83.8333\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.5307\n",
      "validation Loss: 0.0011 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.6081\n",
      "validation Loss: 0.0011 Acc: 83.8214\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.6944\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.6468\n",
      "validation Loss: 0.0011 Acc: 84.0000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.7807\n",
      "validation Loss: 0.0011 Acc: 83.8214\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.6706\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.7212\n",
      "validation Loss: 0.0011 Acc: 84.2500\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.7182\n",
      "validation Loss: 0.0011 Acc: 84.0833\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.6855\n",
      "validation Loss: 0.0011 Acc: 83.9167\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.8254\n",
      "validation Loss: 0.0011 Acc: 83.9762\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 83.8016\n",
      "validation Loss: 0.0011 Acc: 83.9881\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 83.7004\n",
      "validation Loss: 0.0011 Acc: 84.2738\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 83.7956\n",
      "validation Loss: 0.0011 Acc: 84.3095\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 83.8730\n",
      "validation Loss: 0.0011 Acc: 84.0476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 84.0099\n",
      "validation Loss: 0.0011 Acc: 84.3333\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 83.9325\n",
      "validation Loss: 0.0011 Acc: 83.8810\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 83.9087\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 83.8492\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 84.0248\n",
      "validation Loss: 0.0011 Acc: 84.1667\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 83.9444\n",
      "validation Loss: 0.0011 Acc: 84.1310\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 84.1527\n",
      "validation Loss: 0.0011 Acc: 84.1667\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 84.2093\n",
      "validation Loss: 0.0011 Acc: 84.2976\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 84.1914\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0011 Acc: 84.2569\n",
      "validation Loss: 0.0011 Acc: 84.2619\n",
      "Epoch 32/99\n",
      "training Loss: 0.0011 Acc: 84.3938\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Epoch 33/99\n",
      "training Loss: 0.0011 Acc: 84.1974\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Epoch 34/99\n",
      "training Loss: 0.0011 Acc: 84.2896\n",
      "validation Loss: 0.0011 Acc: 84.1429\n",
      "Epoch 35/99\n",
      "training Loss: 0.0011 Acc: 84.3194\n",
      "validation Loss: 0.0011 Acc: 84.3452\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0011 Acc: 84.4116\n",
      "validation Loss: 0.0011 Acc: 84.1667\n",
      "Epoch 37/99\n",
      "training Loss: 0.0011 Acc: 84.3134\n",
      "validation Loss: 0.0011 Acc: 84.3690\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0011 Acc: 84.5456\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 39/99\n",
      "training Loss: 0.0011 Acc: 84.3521\n",
      "validation Loss: 0.0011 Acc: 83.9881\n",
      "Epoch 40/99\n",
      "training Loss: 0.0011 Acc: 84.4771\n",
      "validation Loss: 0.0011 Acc: 84.2381\n",
      "Epoch 41/99\n",
      "training Loss: 0.0011 Acc: 84.4295\n",
      "validation Loss: 0.0011 Acc: 83.9286\n",
      "Epoch 42/99\n",
      "training Loss: 0.0011 Acc: 84.6676\n",
      "validation Loss: 0.0011 Acc: 84.0476\n",
      "Epoch 43/99\n",
      "training Loss: 0.0011 Acc: 84.7241\n",
      "validation Loss: 0.0011 Acc: 84.1905\n",
      "Epoch 44/99\n",
      "training Loss: 0.0011 Acc: 84.8164\n",
      "validation Loss: 0.0011 Acc: 84.1786\n",
      "Epoch 45/99\n",
      "training Loss: 0.0011 Acc: 84.6706\n",
      "validation Loss: 0.0011 Acc: 84.0833\n",
      "Epoch 46/99\n",
      "training Loss: 0.0011 Acc: 84.7420\n",
      "validation Loss: 0.0011 Acc: 83.8690\n",
      "Epoch 47/99\n",
      "training Loss: 0.0011 Acc: 84.7985\n",
      "validation Loss: 0.0011 Acc: 84.1667\n",
      "Epoch 48/99\n",
      "training Loss: 0.0011 Acc: 84.9414\n",
      "validation Loss: 0.0011 Acc: 84.1548\n",
      "Epoch 49/99\n",
      "training Loss: 0.0011 Acc: 84.9027\n",
      "validation Loss: 0.0011 Acc: 84.1667\n",
      "Epoch 50/99\n",
      "training Loss: 0.0011 Acc: 84.7807\n",
      "validation Loss: 0.0011 Acc: 84.1071\n",
      "Epoch 51/99\n",
      "training Loss: 0.0010 Acc: 84.9235\n",
      "validation Loss: 0.0011 Acc: 84.1190\n",
      "Epoch 52/99\n",
      "training Loss: 0.0010 Acc: 85.0158\n",
      "validation Loss: 0.0011 Acc: 84.0000\n",
      "Epoch 53/99\n",
      "training Loss: 0.0010 Acc: 84.8402\n",
      "validation Loss: 0.0011 Acc: 84.0476\n",
      "Epoch 54/99\n",
      "training Loss: 0.0010 Acc: 84.9622\n",
      "validation Loss: 0.0011 Acc: 84.1905\n",
      "Epoch 55/99\n",
      "training Loss: 0.0010 Acc: 85.0098\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0010 Acc: 85.0753\n",
      "validation Loss: 0.0011 Acc: 84.2143\n",
      "Epoch 57/99\n",
      "training Loss: 0.0010 Acc: 85.0009\n",
      "validation Loss: 0.0011 Acc: 84.2024\n",
      "Early stopped.\n",
      "Best val acc: 84.369048\n",
      "----------\n",
      "Average best_acc across k-fold: 84.0245593453\n",
      "New configuration: {'learning_rate': 0.05036706306698309, 'initial_nodes': 297, 'dropout': 0.01, 'batch_size': 490, 'max_depth': 4}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 77.5982\n",
      "validation Loss: 0.0008 Acc: 82.5280\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 83.1696\n",
      "validation Loss: 0.0008 Acc: 83.4920\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 83.2857\n",
      "validation Loss: 0.0008 Acc: 82.1947\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.3810\n",
      "validation Loss: 0.0008 Acc: 82.8493\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.3869\n",
      "validation Loss: 0.0008 Acc: 83.2540\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.4048\n",
      "validation Loss: 0.0008 Acc: 83.0874\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.5833\n",
      "validation Loss: 0.0008 Acc: 83.6229\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6726\n",
      "validation Loss: 0.0008 Acc: 83.6348\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.5595\n",
      "validation Loss: 0.0008 Acc: 83.4325\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.5685\n",
      "validation Loss: 0.0008 Acc: 83.2778\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.4851\n",
      "validation Loss: 0.0008 Acc: 82.8850\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.3899\n",
      "validation Loss: 0.0008 Acc: 83.3373\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.6042\n",
      "validation Loss: 0.0008 Acc: 83.0993\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.6845\n",
      "validation Loss: 0.0008 Acc: 83.3730\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.5714\n",
      "validation Loss: 0.0008 Acc: 83.4087\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.6071\n",
      "validation Loss: 0.0008 Acc: 83.5515\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.8244\n",
      "validation Loss: 0.0008 Acc: 83.4920\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.8512\n",
      "validation Loss: 0.0008 Acc: 83.4563\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.6131\n",
      "validation Loss: 0.0008 Acc: 83.6587\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.6994\n",
      "validation Loss: 0.0008 Acc: 83.6468\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.8542\n",
      "validation Loss: 0.0008 Acc: 83.4444\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.8393\n",
      "validation Loss: 0.0008 Acc: 83.5396\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.7054\n",
      "validation Loss: 0.0008 Acc: 83.6229\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.8095\n",
      "validation Loss: 0.0008 Acc: 83.4325\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.9256\n",
      "validation Loss: 0.0008 Acc: 83.3730\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.8929\n",
      "validation Loss: 0.0008 Acc: 83.4563\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.2083\n",
      "validation Loss: 0.0008 Acc: 83.4325\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.2857\n",
      "validation Loss: 0.0008 Acc: 83.5158\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.2500\n",
      "validation Loss: 0.0008 Acc: 83.3611\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.4494\n",
      "validation Loss: 0.0008 Acc: 83.7301\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.2470\n",
      "validation Loss: 0.0008 Acc: 83.6468\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.4077\n",
      "validation Loss: 0.0008 Acc: 83.6110\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.6012\n",
      "validation Loss: 0.0008 Acc: 83.7896\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.8423\n",
      "validation Loss: 0.0008 Acc: 83.5515\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.7798\n",
      "validation Loss: 0.0008 Acc: 83.5872\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.8839\n",
      "validation Loss: 0.0008 Acc: 83.7182\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.8452\n",
      "validation Loss: 0.0008 Acc: 83.7539\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.8006\n",
      "validation Loss: 0.0008 Acc: 83.8848\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 85.1369\n",
      "validation Loss: 0.0008 Acc: 83.5872\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 85.1756\n",
      "validation Loss: 0.0008 Acc: 83.7182\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 85.2619\n",
      "validation Loss: 0.0008 Acc: 83.6825\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 85.4137\n",
      "validation Loss: 0.0008 Acc: 83.7301\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 85.3958\n",
      "validation Loss: 0.0008 Acc: 83.6706\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 85.4405\n",
      "validation Loss: 0.0008 Acc: 83.7063\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 85.6786\n",
      "validation Loss: 0.0008 Acc: 83.7182\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 85.7351\n",
      "validation Loss: 0.0008 Acc: 83.7658\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 85.6696\n",
      "validation Loss: 0.0008 Acc: 83.7777\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 85.7649\n",
      "validation Loss: 0.0008 Acc: 83.8253\n",
      "Epoch 48/99\n",
      "training Loss: 0.0006 Acc: 85.7917\n",
      "validation Loss: 0.0008 Acc: 83.9086\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0006 Acc: 85.7738\n",
      "validation Loss: 0.0008 Acc: 83.6229\n",
      "Epoch 50/99\n",
      "training Loss: 0.0006 Acc: 85.9286\n",
      "validation Loss: 0.0008 Acc: 83.7658\n",
      "Epoch 51/99\n",
      "training Loss: 0.0006 Acc: 85.8988\n",
      "validation Loss: 0.0009 Acc: 83.7539\n",
      "Epoch 52/99\n",
      "training Loss: 0.0006 Acc: 85.9940\n",
      "validation Loss: 0.0008 Acc: 83.7777\n",
      "Epoch 53/99\n",
      "training Loss: 0.0006 Acc: 85.9732\n",
      "validation Loss: 0.0008 Acc: 83.8015\n",
      "Epoch 54/99\n",
      "training Loss: 0.0006 Acc: 86.0536\n",
      "validation Loss: 0.0008 Acc: 83.7420\n",
      "Epoch 55/99\n",
      "training Loss: 0.0006 Acc: 86.0923\n",
      "validation Loss: 0.0008 Acc: 83.7420\n",
      "Epoch 56/99\n",
      "training Loss: 0.0006 Acc: 86.1399\n",
      "validation Loss: 0.0008 Acc: 83.7777\n",
      "Epoch 57/99\n",
      "training Loss: 0.0006 Acc: 86.2321\n",
      "validation Loss: 0.0008 Acc: 83.6706\n",
      "Epoch 58/99\n",
      "training Loss: 0.0006 Acc: 86.1458\n",
      "validation Loss: 0.0008 Acc: 83.7182\n",
      "Epoch 59/99\n",
      "training Loss: 0.0006 Acc: 86.1369\n",
      "validation Loss: 0.0008 Acc: 83.7539\n",
      "Epoch 60/99\n",
      "training Loss: 0.0006 Acc: 86.1250\n",
      "validation Loss: 0.0008 Acc: 83.6944\n",
      "Epoch 61/99\n",
      "training Loss: 0.0006 Acc: 86.1548\n",
      "validation Loss: 0.0008 Acc: 83.8015\n",
      "Epoch 62/99\n",
      "training Loss: 0.0006 Acc: 86.1786\n",
      "validation Loss: 0.0008 Acc: 83.7658\n",
      "Epoch 63/99\n",
      "training Loss: 0.0006 Acc: 86.2202\n",
      "validation Loss: 0.0008 Acc: 83.7539\n",
      "Epoch 64/99\n",
      "training Loss: 0.0006 Acc: 86.1964\n",
      "validation Loss: 0.0008 Acc: 83.7301\n",
      "Epoch 65/99\n",
      "training Loss: 0.0006 Acc: 86.2411\n",
      "validation Loss: 0.0009 Acc: 83.7420\n",
      "Epoch 66/99\n",
      "training Loss: 0.0006 Acc: 86.2500\n",
      "validation Loss: 0.0008 Acc: 83.7658\n",
      "Epoch 67/99\n",
      "training Loss: 0.0006 Acc: 86.1994\n",
      "validation Loss: 0.0008 Acc: 83.7420\n",
      "Epoch 68/99\n",
      "training Loss: 0.0006 Acc: 86.2768\n",
      "validation Loss: 0.0008 Acc: 83.7539\n",
      "Early stopped.\n",
      "Best val acc: 83.908593\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 76.2395\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.5873\n",
      "validation Loss: 0.0008 Acc: 83.1310\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.7332\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 82.5873\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.1260\n",
      "validation Loss: 0.0008 Acc: 82.9762\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.0992\n",
      "validation Loss: 0.0008 Acc: 83.1429\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.4057\n",
      "validation Loss: 0.0008 Acc: 83.1310\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 82.8165\n",
      "validation Loss: 0.0008 Acc: 82.9286\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.2837\n",
      "validation Loss: 0.0008 Acc: 83.1190\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.4534\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.3522\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.1796\n",
      "validation Loss: 0.0008 Acc: 83.2619\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.3998\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.3819\n",
      "validation Loss: 0.0008 Acc: 82.7976\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.4891\n",
      "validation Loss: 0.0008 Acc: 83.2024\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.4177\n",
      "validation Loss: 0.0008 Acc: 82.9286\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 82.8909\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.2093\n",
      "validation Loss: 0.0008 Acc: 82.5238\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 83.0635\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 83.1825\n",
      "validation Loss: 0.0008 Acc: 83.4167\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 83.2659\n",
      "validation Loss: 0.0008 Acc: 83.4286\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 83.0903\n",
      "validation Loss: 0.0008 Acc: 83.4286\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 83.2778\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 83.4712\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 83.0337\n",
      "validation Loss: 0.0008 Acc: 82.9762\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 83.4623\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 83.5337\n",
      "validation Loss: 0.0008 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 83.4415\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 83.5724\n",
      "validation Loss: 0.0008 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 83.5873\n",
      "validation Loss: 0.0008 Acc: 83.5000\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 83.4861\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 83.6855\n",
      "validation Loss: 0.0008 Acc: 83.2976\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 83.7658\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 83.8641\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 83.8194\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 83.7956\n",
      "validation Loss: 0.0008 Acc: 83.4167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 83.8164\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 83.8164\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 83.8789\n",
      "validation Loss: 0.0008 Acc: 83.1905\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 83.8670\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 83.7718\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 84.0188\n",
      "validation Loss: 0.0008 Acc: 83.5833\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 83.9146\n",
      "validation Loss: 0.0008 Acc: 83.2619\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 83.8045\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 84.0307\n",
      "validation Loss: 0.0008 Acc: 83.6071\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 83.9533\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 83.9027\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 83.8819\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 83.9057\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Early stopped.\n",
      "Best val acc: 83.678571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 78.2989\n",
      "validation Loss: 0.0008 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.7004\n",
      "validation Loss: 0.0008 Acc: 83.2262\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.9326\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.0010\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.2718\n",
      "validation Loss: 0.0008 Acc: 83.1905\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.2897\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.3194\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.4682\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.4742\n",
      "validation Loss: 0.0008 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.4563\n",
      "validation Loss: 0.0008 Acc: 83.6310\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.6260\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.7153\n",
      "validation Loss: 0.0008 Acc: 83.6310\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.7391\n",
      "validation Loss: 0.0008 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.9206\n",
      "validation Loss: 0.0008 Acc: 83.7976\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.7450\n",
      "validation Loss: 0.0008 Acc: 83.7857\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.7093\n",
      "validation Loss: 0.0008 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.0277\n",
      "validation Loss: 0.0008 Acc: 83.5952\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.8641\n",
      "validation Loss: 0.0008 Acc: 83.8095\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.9563\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.0724\n",
      "validation Loss: 0.0007 Acc: 83.8333\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.1259\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.1765\n",
      "validation Loss: 0.0008 Acc: 83.7976\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.2182\n",
      "validation Loss: 0.0008 Acc: 83.8810\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.2242\n",
      "validation Loss: 0.0008 Acc: 83.7976\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.9623\n",
      "validation Loss: 0.0008 Acc: 83.7381\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.0634\n",
      "validation Loss: 0.0008 Acc: 83.6429\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.3878\n",
      "validation Loss: 0.0008 Acc: 83.7619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.5545\n",
      "validation Loss: 0.0008 Acc: 83.8452\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.5396\n",
      "validation Loss: 0.0008 Acc: 83.7143\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.5991\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.7003\n",
      "validation Loss: 0.0008 Acc: 83.8571\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.7598\n",
      "validation Loss: 0.0008 Acc: 83.6310\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.8610\n",
      "validation Loss: 0.0008 Acc: 83.7738\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 85.0039\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 85.0872\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 85.1259\n",
      "validation Loss: 0.0008 Acc: 83.4286\n",
      "Early stopped.\n",
      "Best val acc: 83.940476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0011 Acc: 77.8108\n",
      "validation Loss: 0.0008 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.9564\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 83.3373\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.3492\n",
      "validation Loss: 0.0008 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 82.7540\n",
      "validation Loss: 0.0008 Acc: 82.8810\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 82.9415\n",
      "validation Loss: 0.0008 Acc: 82.7619\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 82.5427\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 82.9028\n",
      "validation Loss: 0.0008 Acc: 82.4762\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.5367\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.4712\n",
      "validation Loss: 0.0008 Acc: 83.6310\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.6468\n",
      "validation Loss: 0.0008 Acc: 83.6667\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.5218\n",
      "validation Loss: 0.0008 Acc: 83.6548\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.6230\n",
      "validation Loss: 0.0008 Acc: 83.8452\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.5992\n",
      "validation Loss: 0.0008 Acc: 83.5952\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.7658\n",
      "validation Loss: 0.0008 Acc: 83.6310\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.7837\n",
      "validation Loss: 0.0008 Acc: 83.7976\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.8879\n",
      "validation Loss: 0.0008 Acc: 83.7262\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.6795\n",
      "validation Loss: 0.0008 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.6855\n",
      "validation Loss: 0.0008 Acc: 83.7500\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.9474\n",
      "validation Loss: 0.0008 Acc: 83.8452\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.7926\n",
      "validation Loss: 0.0008 Acc: 83.9524\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.8135\n",
      "validation Loss: 0.0008 Acc: 83.7976\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.7539\n",
      "validation Loss: 0.0009 Acc: 83.9286\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 83.5605\n",
      "validation Loss: 0.0008 Acc: 83.7738\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.8998\n",
      "validation Loss: 0.0008 Acc: 83.8690\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 83.9712\n",
      "validation Loss: 0.0008 Acc: 83.7857\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.0962\n",
      "validation Loss: 0.0008 Acc: 83.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.0992\n",
      "validation Loss: 0.0008 Acc: 83.5595\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.1676\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.1319\n",
      "validation Loss: 0.0008 Acc: 83.7024\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.0843\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.2896\n",
      "validation Loss: 0.0008 Acc: 83.9286\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.4235\n",
      "validation Loss: 0.0008 Acc: 83.8452\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.3849\n",
      "validation Loss: 0.0008 Acc: 83.4762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.3253\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.5128\n",
      "validation Loss: 0.0008 Acc: 83.6786\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.7420\n",
      "validation Loss: 0.0008 Acc: 83.6429\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 84.6557\n",
      "validation Loss: 0.0008 Acc: 83.8214\n",
      "Early stopped.\n",
      "Best val acc: 83.988095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 75.1652\n",
      "validation Loss: 0.0008 Acc: 82.3214\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.8165\n",
      "validation Loss: 0.0008 Acc: 82.7738\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 83.0070\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.2242\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 82.9177\n",
      "validation Loss: 0.0008 Acc: 83.1310\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.4236\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.7242\n",
      "validation Loss: 0.0008 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.7242\n",
      "validation Loss: 0.0008 Acc: 83.1667\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.5397\n",
      "validation Loss: 0.0008 Acc: 83.2976\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.6111\n",
      "validation Loss: 0.0008 Acc: 83.1071\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.4504\n",
      "validation Loss: 0.0008 Acc: 83.2976\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.6230\n",
      "validation Loss: 0.0008 Acc: 82.7857\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.7153\n",
      "validation Loss: 0.0008 Acc: 82.8929\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.7301\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.0188\n",
      "validation Loss: 0.0008 Acc: 83.1786\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.0456\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.6855\n",
      "validation Loss: 0.0008 Acc: 83.1429\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.9623\n",
      "validation Loss: 0.0008 Acc: 83.0476\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.0992\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.3313\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.3670\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.4563\n",
      "validation Loss: 0.0008 Acc: 83.2024\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.4831\n",
      "validation Loss: 0.0008 Acc: 83.2976\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.5723\n",
      "validation Loss: 0.0008 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.5902\n",
      "validation Loss: 0.0008 Acc: 83.2262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.5664\n",
      "validation Loss: 0.0008 Acc: 83.0476\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.6021\n",
      "validation Loss: 0.0008 Acc: 83.5119\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.7450\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.9027\n",
      "validation Loss: 0.0008 Acc: 83.3690\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.9592\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.9563\n",
      "validation Loss: 0.0008 Acc: 83.2262\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 85.1437\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 85.0604\n",
      "validation Loss: 0.0008 Acc: 83.2976\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 85.2509\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 85.4830\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 85.4860\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 85.5038\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 85.6645\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 85.6140\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 85.8163\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 85.8729\n",
      "validation Loss: 0.0008 Acc: 83.3929\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 85.9354\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 85.9383\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 85.9175\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Early stopped.\n",
      "Best val acc: 83.583333\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8198138765\n",
      "New configuration: {'learning_rate': 1.58725333561267e-05, 'initial_nodes': 1000, 'dropout': 0.30409508170916566, 'batch_size': 32, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0133 Acc: 81.0685\n",
      "validation Loss: 0.0123 Acc: 81.8853\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0118 Acc: 82.9732\n",
      "validation Loss: 0.0120 Acc: 82.7065\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0116 Acc: 83.1190\n",
      "validation Loss: 0.0119 Acc: 82.6470\n",
      "Epoch 3/99\n",
      "training Loss: 0.0115 Acc: 83.3363\n",
      "validation Loss: 0.0118 Acc: 83.0159\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0114 Acc: 83.3185\n",
      "validation Loss: 0.0118 Acc: 82.7779\n",
      "Epoch 5/99\n",
      "training Loss: 0.0113 Acc: 83.4375\n",
      "validation Loss: 0.0118 Acc: 82.7779\n",
      "Epoch 6/99\n",
      "training Loss: 0.0113 Acc: 83.6250\n",
      "validation Loss: 0.0117 Acc: 82.8017\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.5298\n",
      "validation Loss: 0.0117 Acc: 82.8850\n",
      "Epoch 8/99\n",
      "training Loss: 0.0112 Acc: 83.7054\n",
      "validation Loss: 0.0117 Acc: 82.9445\n",
      "Epoch 9/99\n",
      "training Loss: 0.0112 Acc: 83.6964\n",
      "validation Loss: 0.0118 Acc: 82.6232\n",
      "Epoch 10/99\n",
      "training Loss: 0.0112 Acc: 83.7083\n",
      "validation Loss: 0.0117 Acc: 83.1112\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0111 Acc: 83.7857\n",
      "validation Loss: 0.0117 Acc: 82.9207\n",
      "Epoch 12/99\n",
      "training Loss: 0.0111 Acc: 83.9762\n",
      "validation Loss: 0.0117 Acc: 82.9088\n",
      "Epoch 13/99\n",
      "training Loss: 0.0111 Acc: 83.9970\n",
      "validation Loss: 0.0117 Acc: 82.8374\n",
      "Epoch 14/99\n",
      "training Loss: 0.0111 Acc: 83.9286\n",
      "validation Loss: 0.0117 Acc: 82.8374\n",
      "Epoch 15/99\n",
      "training Loss: 0.0110 Acc: 83.9137\n",
      "validation Loss: 0.0116 Acc: 82.8850\n",
      "Epoch 16/99\n",
      "training Loss: 0.0110 Acc: 84.0000\n",
      "validation Loss: 0.0117 Acc: 82.8493\n",
      "Epoch 17/99\n",
      "training Loss: 0.0110 Acc: 84.0714\n",
      "validation Loss: 0.0116 Acc: 83.0517\n",
      "Epoch 18/99\n",
      "training Loss: 0.0110 Acc: 83.9435\n",
      "validation Loss: 0.0117 Acc: 82.9683\n",
      "Epoch 19/99\n",
      "training Loss: 0.0110 Acc: 84.0327\n",
      "validation Loss: 0.0117 Acc: 82.9207\n",
      "Epoch 20/99\n",
      "training Loss: 0.0110 Acc: 84.1161\n",
      "validation Loss: 0.0116 Acc: 82.8612\n",
      "Epoch 21/99\n",
      "training Loss: 0.0109 Acc: 84.1518\n",
      "validation Loss: 0.0116 Acc: 82.9445\n",
      "Epoch 22/99\n",
      "training Loss: 0.0109 Acc: 84.1101\n",
      "validation Loss: 0.0116 Acc: 83.0993\n",
      "Epoch 23/99\n",
      "training Loss: 0.0109 Acc: 84.1399\n",
      "validation Loss: 0.0116 Acc: 83.0040\n",
      "Epoch 24/99\n",
      "training Loss: 0.0109 Acc: 84.2351\n",
      "validation Loss: 0.0116 Acc: 83.0159\n",
      "Epoch 25/99\n",
      "training Loss: 0.0109 Acc: 84.2321\n",
      "validation Loss: 0.0116 Acc: 82.9802\n",
      "Epoch 26/99\n",
      "training Loss: 0.0109 Acc: 84.2530\n",
      "validation Loss: 0.0116 Acc: 83.0993\n",
      "Epoch 27/99\n",
      "training Loss: 0.0109 Acc: 84.1726\n",
      "validation Loss: 0.0116 Acc: 82.9564\n",
      "Epoch 28/99\n",
      "training Loss: 0.0109 Acc: 84.2411\n",
      "validation Loss: 0.0116 Acc: 83.0159\n",
      "Epoch 29/99\n",
      "training Loss: 0.0108 Acc: 84.2560\n",
      "validation Loss: 0.0116 Acc: 83.0040\n",
      "Epoch 30/99\n",
      "training Loss: 0.0108 Acc: 84.3304\n",
      "validation Loss: 0.0116 Acc: 83.0040\n",
      "Early stopped.\n",
      "Best val acc: 83.111164\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.9178\n",
      "validation Loss: 0.0119 Acc: 82.8690\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.6617\n",
      "validation Loss: 0.0116 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 82.9593\n",
      "validation Loss: 0.0115 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.1141\n",
      "validation Loss: 0.0114 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.2480\n",
      "validation Loss: 0.0114 Acc: 83.5476\n",
      "Epoch 5/99\n",
      "training Loss: 0.0115 Acc: 83.2956\n",
      "validation Loss: 0.0113 Acc: 83.5238\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.3075\n",
      "validation Loss: 0.0113 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0114 Acc: 83.4444\n",
      "validation Loss: 0.0113 Acc: 83.5833\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.3611\n",
      "validation Loss: 0.0113 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.6290\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Epoch 10/99\n",
      "training Loss: 0.0113 Acc: 83.5129\n",
      "validation Loss: 0.0113 Acc: 83.7619\n",
      "Epoch 11/99\n",
      "training Loss: 0.0113 Acc: 83.5932\n",
      "validation Loss: 0.0113 Acc: 83.8333\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.7272\n",
      "validation Loss: 0.0112 Acc: 83.5000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.6349\n",
      "validation Loss: 0.0112 Acc: 83.7024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.8075\n",
      "validation Loss: 0.0112 Acc: 83.6190\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.8700\n",
      "validation Loss: 0.0112 Acc: 83.5238\n",
      "Epoch 16/99\n",
      "training Loss: 0.0112 Acc: 83.7212\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.7778\n",
      "validation Loss: 0.0112 Acc: 83.7619\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.8075\n",
      "validation Loss: 0.0112 Acc: 83.6310\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.7897\n",
      "validation Loss: 0.0112 Acc: 83.5476\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 83.8075\n",
      "validation Loss: 0.0112 Acc: 83.7976\n",
      "Epoch 21/99\n",
      "training Loss: 0.0110 Acc: 83.8343\n",
      "validation Loss: 0.0111 Acc: 83.5476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0110 Acc: 83.9325\n",
      "validation Loss: 0.0111 Acc: 83.7381\n",
      "Epoch 23/99\n",
      "training Loss: 0.0110 Acc: 83.9504\n",
      "validation Loss: 0.0112 Acc: 83.6429\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 83.8641\n",
      "validation Loss: 0.0111 Acc: 83.7857\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 84.0039\n",
      "validation Loss: 0.0112 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 83.9236\n",
      "validation Loss: 0.0111 Acc: 83.6786\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 84.0456\n",
      "validation Loss: 0.0111 Acc: 83.6786\n",
      "Epoch 28/99\n",
      "training Loss: 0.0110 Acc: 84.1111\n",
      "validation Loss: 0.0112 Acc: 83.9286\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0109 Acc: 84.0605\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 30/99\n",
      "training Loss: 0.0109 Acc: 84.0962\n",
      "validation Loss: 0.0111 Acc: 83.9167\n",
      "Epoch 31/99\n",
      "training Loss: 0.0109 Acc: 84.2063\n",
      "validation Loss: 0.0111 Acc: 83.4762\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.0575\n",
      "validation Loss: 0.0111 Acc: 83.8333\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.1825\n",
      "validation Loss: 0.0111 Acc: 83.7262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 84.1617\n",
      "validation Loss: 0.0111 Acc: 83.8929\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.2033\n",
      "validation Loss: 0.0111 Acc: 83.8095\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.3610\n",
      "validation Loss: 0.0111 Acc: 83.8214\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.2450\n",
      "validation Loss: 0.0111 Acc: 83.8214\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.1468\n",
      "validation Loss: 0.0111 Acc: 83.5000\n",
      "Epoch 39/99\n",
      "training Loss: 0.0108 Acc: 84.2301\n",
      "validation Loss: 0.0111 Acc: 83.6667\n",
      "Epoch 40/99\n",
      "training Loss: 0.0108 Acc: 84.1944\n",
      "validation Loss: 0.0111 Acc: 83.8452\n",
      "Epoch 41/99\n",
      "training Loss: 0.0109 Acc: 84.1944\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.3432\n",
      "validation Loss: 0.0111 Acc: 83.7857\n",
      "Epoch 43/99\n",
      "training Loss: 0.0108 Acc: 84.3640\n",
      "validation Loss: 0.0111 Acc: 83.7976\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.2986\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.3968\n",
      "validation Loss: 0.0111 Acc: 83.8095\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.4176\n",
      "validation Loss: 0.0111 Acc: 83.9524\n",
      "Epoch 47/99\n",
      "training Loss: 0.0108 Acc: 84.3878\n",
      "validation Loss: 0.0111 Acc: 83.8452\n",
      "Epoch 48/99\n",
      "training Loss: 0.0108 Acc: 84.4235\n",
      "validation Loss: 0.0111 Acc: 83.9881\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0108 Acc: 84.3819\n",
      "validation Loss: 0.0111 Acc: 83.9048\n",
      "Epoch 50/99\n",
      "training Loss: 0.0107 Acc: 84.3164\n",
      "validation Loss: 0.0111 Acc: 83.9286\n",
      "Epoch 51/99\n",
      "training Loss: 0.0108 Acc: 84.3462\n",
      "validation Loss: 0.0111 Acc: 83.9167\n",
      "Epoch 52/99\n",
      "training Loss: 0.0107 Acc: 84.3402\n",
      "validation Loss: 0.0111 Acc: 83.8810\n",
      "Epoch 53/99\n",
      "training Loss: 0.0107 Acc: 84.4533\n",
      "validation Loss: 0.0111 Acc: 83.8929\n",
      "Epoch 54/99\n",
      "training Loss: 0.0107 Acc: 84.4414\n",
      "validation Loss: 0.0111 Acc: 83.7024\n",
      "Epoch 55/99\n",
      "training Loss: 0.0107 Acc: 84.5158\n",
      "validation Loss: 0.0111 Acc: 83.8333\n",
      "Epoch 56/99\n",
      "training Loss: 0.0107 Acc: 84.5515\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 57/99\n",
      "training Loss: 0.0107 Acc: 84.4712\n",
      "validation Loss: 0.0111 Acc: 83.8452\n",
      "Epoch 58/99\n",
      "training Loss: 0.0107 Acc: 84.4265\n",
      "validation Loss: 0.0111 Acc: 83.9405\n",
      "Epoch 59/99\n",
      "training Loss: 0.0107 Acc: 84.4682\n",
      "validation Loss: 0.0111 Acc: 83.8929\n",
      "Epoch 60/99\n",
      "training Loss: 0.0107 Acc: 84.5753\n",
      "validation Loss: 0.0111 Acc: 83.8333\n",
      "Epoch 61/99\n",
      "training Loss: 0.0107 Acc: 84.5783\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 62/99\n",
      "training Loss: 0.0107 Acc: 84.4593\n",
      "validation Loss: 0.0111 Acc: 83.8333\n",
      "Epoch 63/99\n",
      "training Loss: 0.0107 Acc: 84.4474\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 64/99\n",
      "training Loss: 0.0107 Acc: 84.5694\n",
      "validation Loss: 0.0111 Acc: 83.8214\n",
      "Epoch 65/99\n",
      "training Loss: 0.0107 Acc: 84.5456\n",
      "validation Loss: 0.0111 Acc: 83.9167\n",
      "Epoch 66/99\n",
      "training Loss: 0.0107 Acc: 84.5575\n",
      "validation Loss: 0.0111 Acc: 83.8214\n",
      "Epoch 67/99\n",
      "training Loss: 0.0107 Acc: 84.5396\n",
      "validation Loss: 0.0111 Acc: 83.8452\n",
      "Epoch 68/99\n",
      "training Loss: 0.0107 Acc: 84.5575\n",
      "validation Loss: 0.0111 Acc: 83.8452\n",
      "Early stopped.\n",
      "Best val acc: 83.988095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.9357\n",
      "validation Loss: 0.0121 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.7927\n",
      "validation Loss: 0.0118 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 83.0516\n",
      "validation Loss: 0.0116 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.2510\n",
      "validation Loss: 0.0115 Acc: 82.9643\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.3016\n",
      "validation Loss: 0.0115 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.3968\n",
      "validation Loss: 0.0115 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.4772\n",
      "validation Loss: 0.0114 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.5397\n",
      "validation Loss: 0.0114 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.5962\n",
      "validation Loss: 0.0113 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.4891\n",
      "validation Loss: 0.0113 Acc: 83.2738\n",
      "Epoch 10/99\n",
      "training Loss: 0.0113 Acc: 83.7688\n",
      "validation Loss: 0.0113 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.6944\n",
      "validation Loss: 0.0113 Acc: 83.2976\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.7123\n",
      "validation Loss: 0.0113 Acc: 83.4286\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.7510\n",
      "validation Loss: 0.0112 Acc: 83.3571\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.8402\n",
      "validation Loss: 0.0112 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.8254\n",
      "validation Loss: 0.0112 Acc: 83.4405\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.7956\n",
      "validation Loss: 0.0112 Acc: 83.4048\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.9623\n",
      "validation Loss: 0.0112 Acc: 83.4881\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.9087\n",
      "validation Loss: 0.0112 Acc: 83.3452\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.8402\n",
      "validation Loss: 0.0112 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 84.0039\n",
      "validation Loss: 0.0112 Acc: 83.4286\n",
      "Epoch 21/99\n",
      "training Loss: 0.0111 Acc: 83.9027\n",
      "validation Loss: 0.0112 Acc: 83.4762\n",
      "Epoch 22/99\n",
      "training Loss: 0.0111 Acc: 83.9176\n",
      "validation Loss: 0.0112 Acc: 83.3929\n",
      "Epoch 23/99\n",
      "training Loss: 0.0111 Acc: 83.8730\n",
      "validation Loss: 0.0111 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 84.0188\n",
      "validation Loss: 0.0111 Acc: 83.4048\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.9682\n",
      "validation Loss: 0.0112 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 84.0515\n",
      "validation Loss: 0.0111 Acc: 83.4405\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 84.0367\n",
      "validation Loss: 0.0111 Acc: 83.5357\n",
      "Epoch 28/99\n",
      "training Loss: 0.0110 Acc: 84.0932\n",
      "validation Loss: 0.0111 Acc: 83.5833\n",
      "Epoch 29/99\n",
      "training Loss: 0.0110 Acc: 84.0902\n",
      "validation Loss: 0.0111 Acc: 83.5000\n",
      "Epoch 30/99\n",
      "training Loss: 0.0110 Acc: 84.0634\n",
      "validation Loss: 0.0111 Acc: 83.3810\n",
      "Epoch 31/99\n",
      "training Loss: 0.0110 Acc: 84.1111\n",
      "validation Loss: 0.0111 Acc: 83.5595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.0486\n",
      "validation Loss: 0.0111 Acc: 83.5357\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.1408\n",
      "validation Loss: 0.0111 Acc: 83.5476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0110 Acc: 84.1230\n",
      "validation Loss: 0.0111 Acc: 83.5714\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.0902\n",
      "validation Loss: 0.0111 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.2747\n",
      "validation Loss: 0.0111 Acc: 83.4524\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.1736\n",
      "validation Loss: 0.0111 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.2212\n",
      "validation Loss: 0.0111 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0109 Acc: 84.1795\n",
      "validation Loss: 0.0111 Acc: 83.5119\n",
      "Epoch 40/99\n",
      "training Loss: 0.0109 Acc: 84.1795\n",
      "validation Loss: 0.0111 Acc: 83.6905\n",
      "Epoch 41/99\n",
      "training Loss: 0.0109 Acc: 84.2450\n",
      "validation Loss: 0.0111 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.2152\n",
      "validation Loss: 0.0111 Acc: 83.6667\n",
      "Epoch 43/99\n",
      "training Loss: 0.0108 Acc: 84.2390\n",
      "validation Loss: 0.0111 Acc: 83.7619\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.3283\n",
      "validation Loss: 0.0111 Acc: 83.5357\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.1795\n",
      "validation Loss: 0.0110 Acc: 83.5714\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.3730\n",
      "validation Loss: 0.0111 Acc: 83.7262\n",
      "Epoch 47/99\n",
      "training Loss: 0.0108 Acc: 84.3491\n",
      "validation Loss: 0.0111 Acc: 83.5000\n",
      "Epoch 48/99\n",
      "training Loss: 0.0108 Acc: 84.3878\n",
      "validation Loss: 0.0111 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0108 Acc: 84.3432\n",
      "validation Loss: 0.0111 Acc: 83.5119\n",
      "Epoch 50/99\n",
      "training Loss: 0.0108 Acc: 84.4265\n",
      "validation Loss: 0.0111 Acc: 83.5714\n",
      "Epoch 51/99\n",
      "training Loss: 0.0108 Acc: 84.4325\n",
      "validation Loss: 0.0111 Acc: 83.5476\n",
      "Epoch 52/99\n",
      "training Loss: 0.0107 Acc: 84.3849\n",
      "validation Loss: 0.0111 Acc: 83.6667\n",
      "Epoch 53/99\n",
      "training Loss: 0.0107 Acc: 84.3462\n",
      "validation Loss: 0.0110 Acc: 83.5833\n",
      "Epoch 54/99\n",
      "training Loss: 0.0107 Acc: 84.4295\n",
      "validation Loss: 0.0110 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0107 Acc: 84.5277\n",
      "validation Loss: 0.0111 Acc: 83.6905\n",
      "Epoch 56/99\n",
      "training Loss: 0.0107 Acc: 84.4950\n",
      "validation Loss: 0.0110 Acc: 83.6429\n",
      "Epoch 57/99\n",
      "training Loss: 0.0107 Acc: 84.5456\n",
      "validation Loss: 0.0110 Acc: 83.7857\n",
      "Epoch 58/99\n",
      "training Loss: 0.0107 Acc: 84.3491\n",
      "validation Loss: 0.0111 Acc: 83.5833\n",
      "Epoch 59/99\n",
      "training Loss: 0.0107 Acc: 84.5158\n",
      "validation Loss: 0.0110 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0107 Acc: 84.4950\n",
      "validation Loss: 0.0110 Acc: 83.6786\n",
      "Epoch 61/99\n",
      "training Loss: 0.0107 Acc: 84.4593\n",
      "validation Loss: 0.0110 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0107 Acc: 84.3968\n",
      "validation Loss: 0.0110 Acc: 83.7143\n",
      "Epoch 63/99\n",
      "training Loss: 0.0107 Acc: 84.5932\n",
      "validation Loss: 0.0111 Acc: 83.7024\n",
      "Epoch 64/99\n",
      "training Loss: 0.0107 Acc: 84.5843\n",
      "validation Loss: 0.0110 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0107 Acc: 84.6289\n",
      "validation Loss: 0.0110 Acc: 83.8333\n",
      "Epoch 66/99\n",
      "training Loss: 0.0107 Acc: 84.5366\n",
      "validation Loss: 0.0110 Acc: 83.7143\n",
      "Epoch 67/99\n",
      "training Loss: 0.0106 Acc: 84.5872\n",
      "validation Loss: 0.0111 Acc: 83.7976\n",
      "Epoch 68/99\n",
      "training Loss: 0.0107 Acc: 84.5604\n",
      "validation Loss: 0.0110 Acc: 83.7738\n",
      "Epoch 69/99\n",
      "training Loss: 0.0107 Acc: 84.5337\n",
      "validation Loss: 0.0110 Acc: 83.7976\n",
      "Epoch 70/99\n",
      "training Loss: 0.0106 Acc: 84.6229\n",
      "validation Loss: 0.0110 Acc: 83.7976\n",
      "Epoch 71/99\n",
      "training Loss: 0.0106 Acc: 84.5694\n",
      "validation Loss: 0.0110 Acc: 83.7619\n",
      "Epoch 72/99\n",
      "training Loss: 0.0106 Acc: 84.5962\n",
      "validation Loss: 0.0110 Acc: 83.7738\n",
      "Epoch 73/99\n",
      "training Loss: 0.0106 Acc: 84.5128\n",
      "validation Loss: 0.0110 Acc: 83.7024\n",
      "Epoch 74/99\n",
      "training Loss: 0.0106 Acc: 84.5069\n",
      "validation Loss: 0.0110 Acc: 83.6190\n",
      "Epoch 75/99\n",
      "training Loss: 0.0106 Acc: 84.6408\n",
      "validation Loss: 0.0110 Acc: 83.7738\n",
      "Epoch 76/99\n",
      "training Loss: 0.0106 Acc: 84.5872\n",
      "validation Loss: 0.0110 Acc: 83.6071\n",
      "Epoch 77/99\n",
      "training Loss: 0.0106 Acc: 84.7003\n",
      "validation Loss: 0.0110 Acc: 83.6429\n",
      "Epoch 78/99\n",
      "training Loss: 0.0106 Acc: 84.5604\n",
      "validation Loss: 0.0110 Acc: 83.6071\n",
      "Epoch 79/99\n",
      "training Loss: 0.0106 Acc: 84.6735\n",
      "validation Loss: 0.0110 Acc: 83.6786\n",
      "Epoch 80/99\n",
      "training Loss: 0.0106 Acc: 84.7301\n",
      "validation Loss: 0.0110 Acc: 83.6190\n",
      "Epoch 81/99\n",
      "training Loss: 0.0106 Acc: 84.6854\n",
      "validation Loss: 0.0110 Acc: 83.6071\n",
      "Epoch 82/99\n",
      "training Loss: 0.0106 Acc: 84.6706\n",
      "validation Loss: 0.0110 Acc: 83.7262\n",
      "Epoch 83/99\n",
      "training Loss: 0.0106 Acc: 84.6438\n",
      "validation Loss: 0.0110 Acc: 83.6667\n",
      "Epoch 84/99\n",
      "training Loss: 0.0106 Acc: 84.7092\n",
      "validation Loss: 0.0110 Acc: 83.6310\n",
      "Early stopped.\n",
      "Best val acc: 83.904762\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.7958\n",
      "validation Loss: 0.0121 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.8820\n",
      "validation Loss: 0.0117 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 83.0962\n",
      "validation Loss: 0.0116 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.2808\n",
      "validation Loss: 0.0115 Acc: 83.1786\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.2689\n",
      "validation Loss: 0.0115 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.4355\n",
      "validation Loss: 0.0114 Acc: 83.1786\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.3968\n",
      "validation Loss: 0.0114 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.5367\n",
      "validation Loss: 0.0114 Acc: 83.3452\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.6409\n",
      "validation Loss: 0.0113 Acc: 83.5119\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.4831\n",
      "validation Loss: 0.0113 Acc: 83.4762\n",
      "Epoch 10/99\n",
      "training Loss: 0.0113 Acc: 83.6111\n",
      "validation Loss: 0.0114 Acc: 83.4762\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.6766\n",
      "validation Loss: 0.0113 Acc: 83.5357\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.6260\n",
      "validation Loss: 0.0113 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.5784\n",
      "validation Loss: 0.0113 Acc: 83.5476\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.7480\n",
      "validation Loss: 0.0113 Acc: 83.5833\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.7182\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.7420\n",
      "validation Loss: 0.0112 Acc: 83.6071\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.7480\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.8760\n",
      "validation Loss: 0.0112 Acc: 83.6786\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.8045\n",
      "validation Loss: 0.0112 Acc: 83.7857\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 83.8908\n",
      "validation Loss: 0.0112 Acc: 83.6548\n",
      "Epoch 21/99\n",
      "training Loss: 0.0111 Acc: 83.8700\n",
      "validation Loss: 0.0112 Acc: 83.7857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0111 Acc: 83.8789\n",
      "validation Loss: 0.0112 Acc: 83.7738\n",
      "Epoch 23/99\n",
      "training Loss: 0.0110 Acc: 83.9682\n",
      "validation Loss: 0.0112 Acc: 83.7024\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 83.9831\n",
      "validation Loss: 0.0112 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.9087\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 83.9652\n",
      "validation Loss: 0.0112 Acc: 83.7857\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 83.9325\n",
      "validation Loss: 0.0112 Acc: 83.8571\n",
      "Epoch 28/99\n",
      "training Loss: 0.0110 Acc: 83.9385\n",
      "validation Loss: 0.0112 Acc: 83.9286\n",
      "Epoch 29/99\n",
      "training Loss: 0.0110 Acc: 84.0515\n",
      "validation Loss: 0.0111 Acc: 83.6667\n",
      "Epoch 30/99\n",
      "training Loss: 0.0110 Acc: 83.9355\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Epoch 31/99\n",
      "training Loss: 0.0109 Acc: 84.1230\n",
      "validation Loss: 0.0111 Acc: 83.7738\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.0545\n",
      "validation Loss: 0.0112 Acc: 83.9048\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.0188\n",
      "validation Loss: 0.0111 Acc: 83.8214\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 84.0307\n",
      "validation Loss: 0.0112 Acc: 83.8571\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.0783\n",
      "validation Loss: 0.0111 Acc: 83.9643\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.1855\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.2033\n",
      "validation Loss: 0.0111 Acc: 83.7381\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.2361\n",
      "validation Loss: 0.0111 Acc: 83.9167\n",
      "Epoch 39/99\n",
      "training Loss: 0.0109 Acc: 84.0932\n",
      "validation Loss: 0.0111 Acc: 83.8810\n",
      "Epoch 40/99\n",
      "training Loss: 0.0109 Acc: 84.2271\n",
      "validation Loss: 0.0111 Acc: 83.9643\n",
      "Epoch 41/99\n",
      "training Loss: 0.0108 Acc: 84.2807\n",
      "validation Loss: 0.0111 Acc: 83.8810\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.2986\n",
      "validation Loss: 0.0112 Acc: 83.8929\n",
      "Epoch 43/99\n",
      "training Loss: 0.0108 Acc: 84.2271\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.2807\n",
      "validation Loss: 0.0111 Acc: 83.8929\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.3253\n",
      "validation Loss: 0.0111 Acc: 83.8929\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.2777\n",
      "validation Loss: 0.0111 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0108 Acc: 84.3581\n",
      "validation Loss: 0.0111 Acc: 83.9167\n",
      "Epoch 48/99\n",
      "training Loss: 0.0108 Acc: 84.2420\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 49/99\n",
      "training Loss: 0.0108 Acc: 84.2866\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 50/99\n",
      "training Loss: 0.0108 Acc: 84.3670\n",
      "validation Loss: 0.0111 Acc: 83.9286\n",
      "Epoch 51/99\n",
      "training Loss: 0.0108 Acc: 84.3640\n",
      "validation Loss: 0.0111 Acc: 84.0357\n",
      "Epoch 52/99\n",
      "training Loss: 0.0108 Acc: 84.3372\n",
      "validation Loss: 0.0111 Acc: 84.0357\n",
      "Epoch 53/99\n",
      "training Loss: 0.0108 Acc: 84.3581\n",
      "validation Loss: 0.0111 Acc: 84.0833\n",
      "Epoch 54/99\n",
      "training Loss: 0.0108 Acc: 84.3491\n",
      "validation Loss: 0.0111 Acc: 83.9643\n",
      "Epoch 55/99\n",
      "training Loss: 0.0108 Acc: 84.3908\n",
      "validation Loss: 0.0111 Acc: 83.8810\n",
      "Epoch 56/99\n",
      "training Loss: 0.0107 Acc: 84.5337\n",
      "validation Loss: 0.0111 Acc: 83.9643\n",
      "Epoch 57/99\n",
      "training Loss: 0.0107 Acc: 84.3134\n",
      "validation Loss: 0.0111 Acc: 84.0952\n",
      "Epoch 58/99\n",
      "training Loss: 0.0107 Acc: 84.5337\n",
      "validation Loss: 0.0111 Acc: 84.0476\n",
      "Epoch 59/99\n",
      "training Loss: 0.0107 Acc: 84.4712\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Epoch 60/99\n",
      "training Loss: 0.0107 Acc: 84.4622\n",
      "validation Loss: 0.0111 Acc: 84.0119\n",
      "Epoch 61/99\n",
      "training Loss: 0.0107 Acc: 84.4295\n",
      "validation Loss: 0.0111 Acc: 84.0000\n",
      "Epoch 62/99\n",
      "training Loss: 0.0107 Acc: 84.5664\n",
      "validation Loss: 0.0111 Acc: 83.8810\n",
      "Epoch 63/99\n",
      "training Loss: 0.0107 Acc: 84.4950\n",
      "validation Loss: 0.0111 Acc: 84.1905\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0107 Acc: 84.6646\n",
      "validation Loss: 0.0111 Acc: 84.0952\n",
      "Epoch 65/99\n",
      "training Loss: 0.0106 Acc: 84.5753\n",
      "validation Loss: 0.0111 Acc: 83.9881\n",
      "Epoch 66/99\n",
      "training Loss: 0.0107 Acc: 84.4444\n",
      "validation Loss: 0.0111 Acc: 84.1071\n",
      "Epoch 67/99\n",
      "training Loss: 0.0107 Acc: 84.5099\n",
      "validation Loss: 0.0111 Acc: 83.9524\n",
      "Epoch 68/99\n",
      "training Loss: 0.0107 Acc: 84.6378\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 69/99\n",
      "training Loss: 0.0106 Acc: 84.5962\n",
      "validation Loss: 0.0111 Acc: 84.1071\n",
      "Epoch 70/99\n",
      "training Loss: 0.0106 Acc: 84.5753\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Epoch 71/99\n",
      "training Loss: 0.0106 Acc: 84.5396\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 72/99\n",
      "training Loss: 0.0106 Acc: 84.5932\n",
      "validation Loss: 0.0111 Acc: 84.0833\n",
      "Epoch 73/99\n",
      "training Loss: 0.0106 Acc: 84.6884\n",
      "validation Loss: 0.0111 Acc: 84.1310\n",
      "Epoch 74/99\n",
      "training Loss: 0.0106 Acc: 84.7360\n",
      "validation Loss: 0.0111 Acc: 84.1548\n",
      "Epoch 75/99\n",
      "training Loss: 0.0106 Acc: 84.6110\n",
      "validation Loss: 0.0111 Acc: 83.9881\n",
      "Epoch 76/99\n",
      "training Loss: 0.0106 Acc: 84.5991\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 77/99\n",
      "training Loss: 0.0106 Acc: 84.7271\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 78/99\n",
      "training Loss: 0.0106 Acc: 84.6021\n",
      "validation Loss: 0.0111 Acc: 83.9524\n",
      "Epoch 79/99\n",
      "training Loss: 0.0106 Acc: 84.5843\n",
      "validation Loss: 0.0111 Acc: 84.0119\n",
      "Epoch 80/99\n",
      "training Loss: 0.0106 Acc: 84.7241\n",
      "validation Loss: 0.0111 Acc: 84.0833\n",
      "Epoch 81/99\n",
      "training Loss: 0.0106 Acc: 84.6884\n",
      "validation Loss: 0.0111 Acc: 84.1429\n",
      "Epoch 82/99\n",
      "training Loss: 0.0106 Acc: 84.6170\n",
      "validation Loss: 0.0111 Acc: 84.1786\n",
      "Epoch 83/99\n",
      "training Loss: 0.0106 Acc: 84.5664\n",
      "validation Loss: 0.0111 Acc: 84.1190\n",
      "Early stopped.\n",
      "Best val acc: 84.190476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.7898\n",
      "validation Loss: 0.0120 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0120 Acc: 82.6320\n",
      "validation Loss: 0.0116 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 82.9415\n",
      "validation Loss: 0.0115 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.0873\n",
      "validation Loss: 0.0114 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.1349\n",
      "validation Loss: 0.0113 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0115 Acc: 83.3075\n",
      "validation Loss: 0.0113 Acc: 83.6310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.4415\n",
      "validation Loss: 0.0113 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0114 Acc: 83.4682\n",
      "validation Loss: 0.0112 Acc: 83.8214\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.4177\n",
      "validation Loss: 0.0112 Acc: 83.8810\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.4385\n",
      "validation Loss: 0.0112 Acc: 83.9048\n",
      "Epoch 10/99\n",
      "training Loss: 0.0113 Acc: 83.5635\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.6051\n",
      "validation Loss: 0.0111 Acc: 84.1786\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.5486\n",
      "validation Loss: 0.0111 Acc: 84.1190\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.6230\n",
      "validation Loss: 0.0112 Acc: 84.0714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.7778\n",
      "validation Loss: 0.0111 Acc: 84.1905\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0112 Acc: 83.5486\n",
      "validation Loss: 0.0111 Acc: 84.1310\n",
      "Epoch 16/99\n",
      "training Loss: 0.0112 Acc: 83.7034\n",
      "validation Loss: 0.0111 Acc: 84.1310\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.7004\n",
      "validation Loss: 0.0111 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.6617\n",
      "validation Loss: 0.0111 Acc: 84.1786\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.7629\n",
      "validation Loss: 0.0110 Acc: 84.2619\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 83.8045\n",
      "validation Loss: 0.0110 Acc: 84.3929\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0111 Acc: 83.8254\n",
      "validation Loss: 0.0110 Acc: 84.3810\n",
      "Epoch 22/99\n",
      "training Loss: 0.0111 Acc: 83.8908\n",
      "validation Loss: 0.0110 Acc: 84.2857\n",
      "Epoch 23/99\n",
      "training Loss: 0.0111 Acc: 83.7956\n",
      "validation Loss: 0.0110 Acc: 84.3690\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 83.8760\n",
      "validation Loss: 0.0110 Acc: 84.2857\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.8164\n",
      "validation Loss: 0.0110 Acc: 84.3095\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 83.9027\n",
      "validation Loss: 0.0110 Acc: 84.4405\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 83.8789\n",
      "validation Loss: 0.0110 Acc: 84.3810\n",
      "Epoch 28/99\n",
      "training Loss: 0.0110 Acc: 83.8760\n",
      "validation Loss: 0.0110 Acc: 84.3214\n",
      "Epoch 29/99\n",
      "training Loss: 0.0110 Acc: 83.9117\n",
      "validation Loss: 0.0110 Acc: 84.2619\n",
      "Epoch 30/99\n",
      "training Loss: 0.0110 Acc: 83.8016\n",
      "validation Loss: 0.0110 Acc: 84.2143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0110 Acc: 83.8968\n",
      "validation Loss: 0.0110 Acc: 84.3571\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.0337\n",
      "validation Loss: 0.0110 Acc: 84.2381\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.0396\n",
      "validation Loss: 0.0110 Acc: 84.2262\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 83.9682\n",
      "validation Loss: 0.0110 Acc: 84.3810\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.0426\n",
      "validation Loss: 0.0110 Acc: 84.4167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.0158\n",
      "validation Loss: 0.0110 Acc: 84.3333\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.1378\n",
      "validation Loss: 0.0110 Acc: 84.1429\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.1081\n",
      "validation Loss: 0.0110 Acc: 84.1667\n",
      "Epoch 39/99\n",
      "training Loss: 0.0108 Acc: 84.1170\n",
      "validation Loss: 0.0110 Acc: 84.1667\n",
      "Epoch 40/99\n",
      "training Loss: 0.0109 Acc: 84.1319\n",
      "validation Loss: 0.0110 Acc: 84.3690\n",
      "Epoch 41/99\n",
      "training Loss: 0.0108 Acc: 84.1736\n",
      "validation Loss: 0.0109 Acc: 84.2262\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.1587\n",
      "validation Loss: 0.0109 Acc: 84.3810\n",
      "Epoch 43/99\n",
      "training Loss: 0.0109 Acc: 84.1438\n",
      "validation Loss: 0.0109 Acc: 84.2500\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.1498\n",
      "validation Loss: 0.0110 Acc: 84.4167\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.1408\n",
      "validation Loss: 0.0109 Acc: 84.3214\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.1914\n",
      "validation Loss: 0.0109 Acc: 84.2500\n",
      "Early stopped.\n",
      "Best val acc: 84.440476\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9269947065\n",
      "New configuration: {'learning_rate': 0.059266112108177685, 'initial_nodes': 664, 'dropout': 0.4292799813918358, 'batch_size': 202, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0062 Acc: 76.7589\n",
      "validation Loss: 0.0020 Acc: 82.9326\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0023 Acc: 78.8125\n",
      "validation Loss: 0.0020 Acc: 81.0283\n",
      "Epoch 2/99\n",
      "training Loss: 0.0028 Acc: 76.0298\n",
      "validation Loss: 0.0026 Acc: 82.2185\n",
      "Epoch 3/99\n",
      "training Loss: 0.0032 Acc: 73.3363\n",
      "validation Loss: 0.0040 Acc: 76.7079\n",
      "Epoch 4/99\n",
      "training Loss: 0.0030 Acc: 74.4196\n",
      "validation Loss: 0.0028 Acc: 83.3730\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0031 Acc: 73.9375\n",
      "validation Loss: 0.0026 Acc: 77.0888\n",
      "Epoch 6/99\n",
      "training Loss: 0.0027 Acc: 75.1488\n",
      "validation Loss: 0.0023 Acc: 81.2188\n",
      "Epoch 7/99\n",
      "training Loss: 0.0026 Acc: 75.2440\n",
      "validation Loss: 0.0022 Acc: 82.4447\n",
      "Epoch 8/99\n",
      "training Loss: 0.0026 Acc: 75.7530\n",
      "validation Loss: 0.0022 Acc: 83.0517\n",
      "Epoch 9/99\n",
      "training Loss: 0.0025 Acc: 76.2083\n",
      "validation Loss: 0.0021 Acc: 83.7539\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0025 Acc: 76.3304\n",
      "validation Loss: 0.0021 Acc: 83.7539\n",
      "Epoch 11/99\n",
      "training Loss: 0.0025 Acc: 76.0298\n",
      "validation Loss: 0.0024 Acc: 81.2307\n",
      "Epoch 12/99\n",
      "training Loss: 0.0025 Acc: 76.4256\n",
      "validation Loss: 0.0021 Acc: 83.1945\n",
      "Epoch 13/99\n",
      "training Loss: 0.0025 Acc: 76.7262\n",
      "validation Loss: 0.0021 Acc: 83.8967\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0025 Acc: 76.9732\n",
      "validation Loss: 0.0021 Acc: 82.9207\n",
      "Epoch 15/99\n",
      "training Loss: 0.0024 Acc: 77.3304\n",
      "validation Loss: 0.0021 Acc: 83.6944\n",
      "Epoch 16/99\n",
      "training Loss: 0.0025 Acc: 76.9940\n",
      "validation Loss: 0.0020 Acc: 83.9800\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0024 Acc: 77.0804\n",
      "validation Loss: 0.0021 Acc: 83.3611\n",
      "Epoch 18/99\n",
      "training Loss: 0.0025 Acc: 77.0595\n",
      "validation Loss: 0.0021 Acc: 82.9564\n",
      "Epoch 19/99\n",
      "training Loss: 0.0024 Acc: 77.2619\n",
      "validation Loss: 0.0020 Acc: 83.9562\n",
      "Epoch 20/99\n",
      "training Loss: 0.0024 Acc: 77.4405\n",
      "validation Loss: 0.0020 Acc: 83.6825\n",
      "Epoch 21/99\n",
      "training Loss: 0.0024 Acc: 77.3601\n",
      "validation Loss: 0.0021 Acc: 83.4563\n",
      "Epoch 22/99\n",
      "training Loss: 0.0024 Acc: 77.3631\n",
      "validation Loss: 0.0020 Acc: 84.1109\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0024 Acc: 77.5655\n",
      "validation Loss: 0.0020 Acc: 83.8729\n",
      "Epoch 24/99\n",
      "training Loss: 0.0024 Acc: 77.6637\n",
      "validation Loss: 0.0020 Acc: 83.8610\n",
      "Epoch 25/99\n",
      "training Loss: 0.0024 Acc: 77.6667\n",
      "validation Loss: 0.0020 Acc: 83.8372\n",
      "Epoch 26/99\n",
      "training Loss: 0.0024 Acc: 77.2202\n",
      "validation Loss: 0.0020 Acc: 84.1823\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0024 Acc: 77.9673\n",
      "validation Loss: 0.0020 Acc: 83.9681\n",
      "Epoch 28/99\n",
      "training Loss: 0.0024 Acc: 77.7946\n",
      "validation Loss: 0.0020 Acc: 83.7539\n",
      "Epoch 29/99\n",
      "training Loss: 0.0024 Acc: 77.7738\n",
      "validation Loss: 0.0020 Acc: 83.5634\n",
      "Epoch 30/99\n",
      "training Loss: 0.0024 Acc: 77.4970\n",
      "validation Loss: 0.0020 Acc: 83.7420\n",
      "Epoch 31/99\n",
      "training Loss: 0.0023 Acc: 78.0893\n",
      "validation Loss: 0.0020 Acc: 84.0633\n",
      "Epoch 32/99\n",
      "training Loss: 0.0024 Acc: 77.5506\n",
      "validation Loss: 0.0020 Acc: 83.9681\n",
      "Epoch 33/99\n",
      "training Loss: 0.0023 Acc: 78.2381\n",
      "validation Loss: 0.0020 Acc: 83.7777\n",
      "Epoch 34/99\n",
      "training Loss: 0.0023 Acc: 78.0179\n",
      "validation Loss: 0.0020 Acc: 83.7063\n",
      "Epoch 35/99\n",
      "training Loss: 0.0023 Acc: 77.8125\n",
      "validation Loss: 0.0020 Acc: 83.9205\n",
      "Epoch 36/99\n",
      "training Loss: 0.0023 Acc: 77.9494\n",
      "validation Loss: 0.0020 Acc: 84.0038\n",
      "Epoch 37/99\n",
      "training Loss: 0.0023 Acc: 78.0357\n",
      "validation Loss: 0.0020 Acc: 84.0038\n",
      "Epoch 38/99\n",
      "training Loss: 0.0023 Acc: 77.9494\n",
      "validation Loss: 0.0020 Acc: 83.8848\n",
      "Epoch 39/99\n",
      "training Loss: 0.0023 Acc: 77.5923\n",
      "validation Loss: 0.0020 Acc: 83.9562\n",
      "Epoch 40/99\n",
      "training Loss: 0.0023 Acc: 77.9048\n",
      "validation Loss: 0.0020 Acc: 83.8848\n",
      "Epoch 41/99\n",
      "training Loss: 0.0023 Acc: 78.0595\n",
      "validation Loss: 0.0020 Acc: 84.0871\n",
      "Epoch 42/99\n",
      "training Loss: 0.0023 Acc: 77.9702\n",
      "validation Loss: 0.0020 Acc: 84.1347\n",
      "Epoch 43/99\n",
      "training Loss: 0.0023 Acc: 77.8333\n",
      "validation Loss: 0.0020 Acc: 83.8729\n",
      "Epoch 44/99\n",
      "training Loss: 0.0023 Acc: 78.1756\n",
      "validation Loss: 0.0020 Acc: 84.1109\n",
      "Epoch 45/99\n",
      "training Loss: 0.0023 Acc: 78.0327\n",
      "validation Loss: 0.0020 Acc: 83.9562\n",
      "Epoch 46/99\n",
      "training Loss: 0.0023 Acc: 77.9464\n",
      "validation Loss: 0.0020 Acc: 83.9681\n",
      "Early stopped.\n",
      "Best val acc: 84.182338\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0067 Acc: 79.3465\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 82.4118\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 82.6856\n",
      "validation Loss: 0.0018 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.4743\n",
      "validation Loss: 0.0020 Acc: 82.8214\n",
      "Epoch 4/99\n",
      "training Loss: 0.0022 Acc: 81.1529\n",
      "validation Loss: 0.0019 Acc: 82.5119\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 81.7600\n",
      "validation Loss: 0.0019 Acc: 83.1905\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 81.9594\n",
      "validation Loss: 0.0018 Acc: 82.9286\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 81.9535\n",
      "validation Loss: 0.0019 Acc: 82.9762\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 81.6945\n",
      "validation Loss: 0.0019 Acc: 83.2024\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 82.4028\n",
      "validation Loss: 0.0018 Acc: 83.3929\n",
      "Epoch 10/99\n",
      "training Loss: 0.0020 Acc: 82.7153\n",
      "validation Loss: 0.0018 Acc: 83.0952\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.1409\n",
      "validation Loss: 0.0018 Acc: 83.4286\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.0814\n",
      "validation Loss: 0.0018 Acc: 83.2381\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.0218\n",
      "validation Loss: 0.0018 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.0070\n",
      "validation Loss: 0.0018 Acc: 82.8571\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.1468\n",
      "validation Loss: 0.0019 Acc: 81.9762\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 83.2659\n",
      "validation Loss: 0.0018 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.3730\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 83.6825\n",
      "validation Loss: 0.0018 Acc: 83.7381\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 83.6766\n",
      "validation Loss: 0.0018 Acc: 83.7262\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 83.6587\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 83.7093\n",
      "validation Loss: 0.0018 Acc: 83.7262\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 83.7748\n",
      "validation Loss: 0.0018 Acc: 83.5119\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 83.7331\n",
      "validation Loss: 0.0018 Acc: 83.8333\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 83.8611\n",
      "validation Loss: 0.0018 Acc: 83.6905\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 83.9533\n",
      "validation Loss: 0.0018 Acc: 83.6786\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 83.8492\n",
      "validation Loss: 0.0018 Acc: 83.1190\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 83.8760\n",
      "validation Loss: 0.0018 Acc: 83.4524\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 83.9325\n",
      "validation Loss: 0.0018 Acc: 83.5833\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 83.8789\n",
      "validation Loss: 0.0018 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.9890\n",
      "validation Loss: 0.0018 Acc: 83.4048\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 84.2122\n",
      "validation Loss: 0.0018 Acc: 83.6786\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 84.1617\n",
      "validation Loss: 0.0018 Acc: 83.7976\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 84.2003\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 84.4116\n",
      "validation Loss: 0.0018 Acc: 83.7500\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 84.3075\n",
      "validation Loss: 0.0018 Acc: 83.5714\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 84.2301\n",
      "validation Loss: 0.0018 Acc: 83.7143\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 84.4920\n",
      "validation Loss: 0.0018 Acc: 83.7857\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 84.5575\n",
      "validation Loss: 0.0018 Acc: 83.5357\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 84.5604\n",
      "validation Loss: 0.0018 Acc: 83.6667\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 84.7122\n",
      "validation Loss: 0.0018 Acc: 83.5952\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 84.5247\n",
      "validation Loss: 0.0018 Acc: 83.5595\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 84.6170\n",
      "validation Loss: 0.0018 Acc: 83.7143\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 84.7450\n",
      "validation Loss: 0.0018 Acc: 83.6310\n",
      "Epoch 44/99\n",
      "training Loss: 0.0017 Acc: 84.6914\n",
      "validation Loss: 0.0018 Acc: 83.6190\n",
      "Epoch 45/99\n",
      "training Loss: 0.0017 Acc: 84.8819\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 46/99\n",
      "training Loss: 0.0017 Acc: 84.7182\n",
      "validation Loss: 0.0018 Acc: 83.7619\n",
      "Epoch 47/99\n",
      "training Loss: 0.0017 Acc: 84.7271\n",
      "validation Loss: 0.0018 Acc: 83.8095\n",
      "Epoch 48/99\n",
      "training Loss: 0.0017 Acc: 84.7420\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Epoch 49/99\n",
      "training Loss: 0.0017 Acc: 84.6973\n",
      "validation Loss: 0.0018 Acc: 83.7738\n",
      "Early stopped.\n",
      "Best val acc: 83.869048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0058 Acc: 79.9149\n",
      "validation Loss: 0.0021 Acc: 82.5119\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 82.3433\n",
      "validation Loss: 0.0019 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 82.4624\n",
      "validation Loss: 0.0020 Acc: 81.5952\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.5338\n",
      "validation Loss: 0.0019 Acc: 83.0119\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.5933\n",
      "validation Loss: 0.0020 Acc: 82.6786\n",
      "Epoch 5/99\n",
      "training Loss: 0.0021 Acc: 82.0338\n",
      "validation Loss: 0.0022 Acc: 82.0357\n",
      "Epoch 6/99\n",
      "training Loss: 0.0022 Acc: 81.4237\n",
      "validation Loss: 0.0020 Acc: 81.7976\n",
      "Epoch 7/99\n",
      "training Loss: 0.0021 Acc: 81.4803\n",
      "validation Loss: 0.0020 Acc: 82.0595\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 81.9951\n",
      "validation Loss: 0.0020 Acc: 82.6310\n",
      "Epoch 9/99\n",
      "training Loss: 0.0022 Acc: 81.3940\n",
      "validation Loss: 0.0021 Acc: 82.7857\n",
      "Epoch 10/99\n",
      "training Loss: 0.0021 Acc: 82.1082\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0021 Acc: 82.2183\n",
      "validation Loss: 0.0022 Acc: 82.9881\n",
      "Epoch 12/99\n",
      "training Loss: 0.0022 Acc: 82.0606\n",
      "validation Loss: 0.0020 Acc: 82.8214\n",
      "Epoch 13/99\n",
      "training Loss: 0.0020 Acc: 82.3641\n",
      "validation Loss: 0.0020 Acc: 81.6190\n",
      "Epoch 14/99\n",
      "training Loss: 0.0020 Acc: 82.4891\n",
      "validation Loss: 0.0020 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0020 Acc: 82.7867\n",
      "validation Loss: 0.0019 Acc: 82.9048\n",
      "Epoch 16/99\n",
      "training Loss: 0.0019 Acc: 82.8135\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 82.9355\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 82.7957\n",
      "validation Loss: 0.0019 Acc: 83.2738\n",
      "Epoch 19/99\n",
      "training Loss: 0.0020 Acc: 82.6231\n",
      "validation Loss: 0.0020 Acc: 82.5357\n",
      "Epoch 20/99\n",
      "training Loss: 0.0020 Acc: 82.8076\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 21/99\n",
      "training Loss: 0.0020 Acc: 82.5576\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 22/99\n",
      "training Loss: 0.0020 Acc: 82.8225\n",
      "validation Loss: 0.0020 Acc: 83.3095\n",
      "Epoch 23/99\n",
      "training Loss: 0.0020 Acc: 82.9207\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 82.8284\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 25/99\n",
      "training Loss: 0.0019 Acc: 82.8611\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 26/99\n",
      "training Loss: 0.0019 Acc: 83.1141\n",
      "validation Loss: 0.0019 Acc: 82.8690\n",
      "Epoch 27/99\n",
      "training Loss: 0.0019 Acc: 83.0248\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0019 Acc: 83.4087\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Epoch 29/99\n",
      "training Loss: 0.0019 Acc: 83.3909\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 83.6885\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0019 Acc: 83.4504\n",
      "validation Loss: 0.0019 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 83.4177\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 33/99\n",
      "training Loss: 0.0019 Acc: 83.8938\n",
      "validation Loss: 0.0019 Acc: 83.5833\n",
      "Epoch 34/99\n",
      "training Loss: 0.0018 Acc: 83.9087\n",
      "validation Loss: 0.0019 Acc: 83.3452\n",
      "Epoch 35/99\n",
      "training Loss: 0.0018 Acc: 84.0099\n",
      "validation Loss: 0.0019 Acc: 83.4048\n",
      "Epoch 36/99\n",
      "training Loss: 0.0018 Acc: 83.9295\n",
      "validation Loss: 0.0019 Acc: 83.5714\n",
      "Epoch 37/99\n",
      "training Loss: 0.0018 Acc: 84.0099\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 38/99\n",
      "training Loss: 0.0018 Acc: 83.9652\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Epoch 39/99\n",
      "training Loss: 0.0018 Acc: 84.2063\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Epoch 40/99\n",
      "training Loss: 0.0018 Acc: 84.1170\n",
      "validation Loss: 0.0019 Acc: 83.6429\n",
      "Epoch 41/99\n",
      "training Loss: 0.0018 Acc: 84.2242\n",
      "validation Loss: 0.0019 Acc: 83.5595\n",
      "Epoch 42/99\n",
      "training Loss: 0.0018 Acc: 84.2390\n",
      "validation Loss: 0.0019 Acc: 83.4524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0018 Acc: 84.2926\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 44/99\n",
      "training Loss: 0.0018 Acc: 84.3849\n",
      "validation Loss: 0.0019 Acc: 83.3214\n",
      "Epoch 45/99\n",
      "training Loss: 0.0018 Acc: 84.3372\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 46/99\n",
      "training Loss: 0.0018 Acc: 84.4057\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 47/99\n",
      "training Loss: 0.0018 Acc: 84.3105\n",
      "validation Loss: 0.0019 Acc: 83.4405\n",
      "Epoch 48/99\n",
      "training Loss: 0.0018 Acc: 84.4593\n",
      "validation Loss: 0.0019 Acc: 83.5119\n",
      "Epoch 49/99\n",
      "training Loss: 0.0018 Acc: 84.3045\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 50/99\n",
      "training Loss: 0.0018 Acc: 84.4235\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 51/99\n",
      "training Loss: 0.0018 Acc: 84.3015\n",
      "validation Loss: 0.0019 Acc: 83.3810\n",
      "Early stopped.\n",
      "Best val acc: 83.738095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0060 Acc: 79.9179\n",
      "validation Loss: 0.0023 Acc: 81.7262\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 82.6796\n",
      "validation Loss: 0.0022 Acc: 82.8810\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0021 Acc: 82.3046\n",
      "validation Loss: 0.0023 Acc: 82.2619\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.3969\n",
      "validation Loss: 0.0023 Acc: 82.3214\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.2511\n",
      "validation Loss: 0.0025 Acc: 82.4405\n",
      "Epoch 5/99\n",
      "training Loss: 0.0020 Acc: 82.2778\n",
      "validation Loss: 0.0028 Acc: 78.1071\n",
      "Epoch 6/99\n",
      "training Loss: 0.0021 Acc: 82.2243\n",
      "validation Loss: 0.0025 Acc: 81.6310\n",
      "Epoch 7/99\n",
      "training Loss: 0.0020 Acc: 82.2719\n",
      "validation Loss: 0.0025 Acc: 82.6905\n",
      "Epoch 8/99\n",
      "training Loss: 0.0019 Acc: 83.0189\n",
      "validation Loss: 0.0024 Acc: 81.9881\n",
      "Epoch 9/99\n",
      "training Loss: 0.0019 Acc: 83.2480\n",
      "validation Loss: 0.0022 Acc: 82.8810\n",
      "Epoch 10/99\n",
      "training Loss: 0.0019 Acc: 83.5456\n",
      "validation Loss: 0.0022 Acc: 82.6786\n",
      "Epoch 11/99\n",
      "training Loss: 0.0019 Acc: 83.5159\n",
      "validation Loss: 0.0021 Acc: 82.5000\n",
      "Epoch 12/99\n",
      "training Loss: 0.0019 Acc: 83.4087\n",
      "validation Loss: 0.0021 Acc: 82.4167\n",
      "Epoch 13/99\n",
      "training Loss: 0.0019 Acc: 83.3492\n",
      "validation Loss: 0.0022 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.3611\n",
      "validation Loss: 0.0021 Acc: 82.8690\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.4682\n",
      "validation Loss: 0.0022 Acc: 82.0476\n",
      "Epoch 16/99\n",
      "training Loss: 0.0018 Acc: 83.6141\n",
      "validation Loss: 0.0022 Acc: 82.8810\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.5992\n",
      "validation Loss: 0.0021 Acc: 82.9524\n",
      "Epoch 18/99\n",
      "training Loss: 0.0018 Acc: 83.5099\n",
      "validation Loss: 0.0021 Acc: 82.6667\n",
      "Epoch 19/99\n",
      "training Loss: 0.0018 Acc: 83.8045\n",
      "validation Loss: 0.0021 Acc: 82.6548\n",
      "Epoch 20/99\n",
      "training Loss: 0.0018 Acc: 83.8551\n",
      "validation Loss: 0.0021 Acc: 82.9167\n",
      "Epoch 21/99\n",
      "training Loss: 0.0018 Acc: 83.7391\n",
      "validation Loss: 0.0022 Acc: 82.6548\n",
      "Epoch 22/99\n",
      "training Loss: 0.0018 Acc: 83.7778\n",
      "validation Loss: 0.0021 Acc: 82.8333\n",
      "Epoch 23/99\n",
      "training Loss: 0.0018 Acc: 83.7599\n",
      "validation Loss: 0.0021 Acc: 82.7262\n",
      "Epoch 24/99\n",
      "training Loss: 0.0018 Acc: 83.8343\n",
      "validation Loss: 0.0022 Acc: 82.7619\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 84.1408\n",
      "validation Loss: 0.0022 Acc: 82.8929\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 84.4533\n",
      "validation Loss: 0.0022 Acc: 82.7143\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 84.3372\n",
      "validation Loss: 0.0022 Acc: 82.9762\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 84.4801\n",
      "validation Loss: 0.0022 Acc: 83.0357\n",
      "Epoch 29/99\n",
      "training Loss: 0.0017 Acc: 84.3491\n",
      "validation Loss: 0.0022 Acc: 82.9048\n",
      "Epoch 30/99\n",
      "training Loss: 0.0017 Acc: 84.6259\n",
      "validation Loss: 0.0023 Acc: 82.9643\n",
      "Epoch 31/99\n",
      "training Loss: 0.0017 Acc: 84.6527\n",
      "validation Loss: 0.0022 Acc: 82.7738\n",
      "Epoch 32/99\n",
      "training Loss: 0.0017 Acc: 84.7331\n",
      "validation Loss: 0.0022 Acc: 82.7976\n",
      "Epoch 33/99\n",
      "training Loss: 0.0017 Acc: 84.6944\n",
      "validation Loss: 0.0023 Acc: 82.7857\n",
      "Early stopped.\n",
      "Best val acc: 83.071429\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0055 Acc: 79.6947\n",
      "validation Loss: 0.0019 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0020 Acc: 82.1409\n",
      "validation Loss: 0.0019 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0020 Acc: 82.2540\n",
      "validation Loss: 0.0019 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0020 Acc: 82.3463\n",
      "validation Loss: 0.0020 Acc: 82.0714\n",
      "Epoch 4/99\n",
      "training Loss: 0.0020 Acc: 82.1528\n",
      "validation Loss: 0.0021 Acc: 82.6429\n",
      "Epoch 5/99\n",
      "training Loss: 0.0022 Acc: 81.1975\n",
      "validation Loss: 0.0020 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0023 Acc: 80.8404\n",
      "validation Loss: 0.0021 Acc: 83.0714\n",
      "Epoch 7/99\n",
      "training Loss: 0.0022 Acc: 81.1946\n",
      "validation Loss: 0.0021 Acc: 82.9762\n",
      "Epoch 8/99\n",
      "training Loss: 0.0021 Acc: 82.2272\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0020 Acc: 82.4743\n",
      "validation Loss: 0.0020 Acc: 82.0357\n",
      "Epoch 10/99\n",
      "training Loss: 0.0020 Acc: 82.7213\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0020 Acc: 82.9861\n",
      "validation Loss: 0.0019 Acc: 83.1667\n",
      "Epoch 12/99\n",
      "training Loss: 0.0020 Acc: 82.9653\n",
      "validation Loss: 0.0020 Acc: 83.2024\n",
      "Epoch 13/99\n",
      "training Loss: 0.0020 Acc: 82.9742\n",
      "validation Loss: 0.0019 Acc: 82.8333\n",
      "Epoch 14/99\n",
      "training Loss: 0.0019 Acc: 83.3462\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0019 Acc: 83.3135\n",
      "validation Loss: 0.0021 Acc: 83.3571\n",
      "Epoch 16/99\n",
      "training Loss: 0.0020 Acc: 83.2778\n",
      "validation Loss: 0.0019 Acc: 83.2262\n",
      "Epoch 17/99\n",
      "training Loss: 0.0019 Acc: 83.2272\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0019 Acc: 83.5099\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Epoch 19/99\n",
      "training Loss: 0.0019 Acc: 83.5397\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 20/99\n",
      "training Loss: 0.0019 Acc: 83.7182\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 21/99\n",
      "training Loss: 0.0019 Acc: 83.4891\n",
      "validation Loss: 0.0019 Acc: 83.2381\n",
      "Epoch 22/99\n",
      "training Loss: 0.0019 Acc: 83.7569\n",
      "validation Loss: 0.0019 Acc: 83.3929\n",
      "Epoch 23/99\n",
      "training Loss: 0.0019 Acc: 83.7629\n",
      "validation Loss: 0.0019 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0019 Acc: 83.8522\n",
      "validation Loss: 0.0019 Acc: 83.5476\n",
      "Epoch 25/99\n",
      "training Loss: 0.0018 Acc: 84.0307\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 26/99\n",
      "training Loss: 0.0018 Acc: 84.2063\n",
      "validation Loss: 0.0019 Acc: 83.5000\n",
      "Epoch 27/99\n",
      "training Loss: 0.0018 Acc: 84.1468\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 28/99\n",
      "training Loss: 0.0018 Acc: 84.1944\n",
      "validation Loss: 0.0019 Acc: 83.5238\n",
      "Epoch 29/99\n",
      "training Loss: 0.0018 Acc: 84.2956\n",
      "validation Loss: 0.0019 Acc: 83.2024\n",
      "Epoch 30/99\n",
      "training Loss: 0.0018 Acc: 84.4176\n",
      "validation Loss: 0.0019 Acc: 83.4881\n",
      "Epoch 31/99\n",
      "training Loss: 0.0018 Acc: 84.6497\n",
      "validation Loss: 0.0019 Acc: 83.3571\n",
      "Epoch 32/99\n",
      "training Loss: 0.0018 Acc: 84.7063\n",
      "validation Loss: 0.0019 Acc: 83.5357\n",
      "Epoch 33/99\n",
      "training Loss: 0.0018 Acc: 84.6676\n",
      "validation Loss: 0.0019 Acc: 83.2143\n",
      "Epoch 34/99\n",
      "training Loss: 0.0017 Acc: 84.8402\n",
      "validation Loss: 0.0019 Acc: 83.4286\n",
      "Epoch 35/99\n",
      "training Loss: 0.0017 Acc: 84.8491\n",
      "validation Loss: 0.0019 Acc: 83.4167\n",
      "Epoch 36/99\n",
      "training Loss: 0.0017 Acc: 84.7955\n",
      "validation Loss: 0.0019 Acc: 83.5952\n",
      "Epoch 37/99\n",
      "training Loss: 0.0017 Acc: 85.0307\n",
      "validation Loss: 0.0019 Acc: 83.4762\n",
      "Epoch 38/99\n",
      "training Loss: 0.0017 Acc: 84.7717\n",
      "validation Loss: 0.0019 Acc: 83.4643\n",
      "Epoch 39/99\n",
      "training Loss: 0.0017 Acc: 85.0217\n",
      "validation Loss: 0.0019 Acc: 83.3810\n",
      "Epoch 40/99\n",
      "training Loss: 0.0017 Acc: 85.0396\n",
      "validation Loss: 0.0019 Acc: 83.2976\n",
      "Epoch 41/99\n",
      "training Loss: 0.0017 Acc: 85.1556\n",
      "validation Loss: 0.0019 Acc: 83.3333\n",
      "Epoch 42/99\n",
      "training Loss: 0.0017 Acc: 85.0366\n",
      "validation Loss: 0.0019 Acc: 83.3095\n",
      "Epoch 43/99\n",
      "training Loss: 0.0017 Acc: 85.1824\n",
      "validation Loss: 0.0019 Acc: 83.3690\n",
      "Early stopped.\n",
      "Best val acc: 83.654762\n",
      "----------\n",
      "Average best_acc across k-fold: 83.7031341744\n",
      "New configuration: {'learning_rate': 0.000133922486016919, 'initial_nodes': 1000, 'dropout': 0.06718684182620187, 'batch_size': 65, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0059 Acc: 82.7500\n",
      "validation Loss: 0.0058 Acc: 82.7303\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0057 Acc: 83.2887\n",
      "validation Loss: 0.0057 Acc: 83.3849\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0056 Acc: 83.4851\n",
      "validation Loss: 0.0056 Acc: 83.0993\n",
      "Epoch 3/99\n",
      "training Loss: 0.0055 Acc: 83.6637\n",
      "validation Loss: 0.0056 Acc: 83.2183\n",
      "Epoch 4/99\n",
      "training Loss: 0.0055 Acc: 83.8244\n",
      "validation Loss: 0.0056 Acc: 83.1350\n",
      "Epoch 5/99\n",
      "training Loss: 0.0055 Acc: 83.9792\n",
      "validation Loss: 0.0055 Acc: 83.4325\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0054 Acc: 84.0208\n",
      "validation Loss: 0.0055 Acc: 83.6229\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 84.0655\n",
      "validation Loss: 0.0056 Acc: 83.4087\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 84.0744\n",
      "validation Loss: 0.0056 Acc: 83.5515\n",
      "Epoch 9/99\n",
      "training Loss: 0.0054 Acc: 84.2173\n",
      "validation Loss: 0.0055 Acc: 83.2064\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 84.2292\n",
      "validation Loss: 0.0055 Acc: 83.5158\n",
      "Epoch 11/99\n",
      "training Loss: 0.0053 Acc: 84.2589\n",
      "validation Loss: 0.0055 Acc: 83.9086\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0053 Acc: 84.3601\n",
      "validation Loss: 0.0055 Acc: 83.6229\n",
      "Epoch 13/99\n",
      "training Loss: 0.0053 Acc: 84.3423\n",
      "validation Loss: 0.0055 Acc: 83.7539\n",
      "Epoch 14/99\n",
      "training Loss: 0.0053 Acc: 84.5000\n",
      "validation Loss: 0.0055 Acc: 83.6706\n",
      "Epoch 15/99\n",
      "training Loss: 0.0053 Acc: 84.5179\n",
      "validation Loss: 0.0055 Acc: 83.4444\n",
      "Epoch 16/99\n",
      "training Loss: 0.0052 Acc: 84.5476\n",
      "validation Loss: 0.0055 Acc: 83.7182\n",
      "Epoch 17/99\n",
      "training Loss: 0.0052 Acc: 84.6339\n",
      "validation Loss: 0.0055 Acc: 83.5039\n",
      "Epoch 18/99\n",
      "training Loss: 0.0052 Acc: 84.8661\n",
      "validation Loss: 0.0055 Acc: 83.5872\n",
      "Epoch 19/99\n",
      "training Loss: 0.0051 Acc: 84.8780\n",
      "validation Loss: 0.0055 Acc: 83.7420\n",
      "Epoch 20/99\n",
      "training Loss: 0.0051 Acc: 84.8423\n",
      "validation Loss: 0.0055 Acc: 83.7539\n",
      "Epoch 21/99\n",
      "training Loss: 0.0051 Acc: 84.9345\n",
      "validation Loss: 0.0055 Acc: 83.7539\n",
      "Epoch 22/99\n",
      "training Loss: 0.0051 Acc: 84.8482\n",
      "validation Loss: 0.0055 Acc: 83.7063\n",
      "Epoch 23/99\n",
      "training Loss: 0.0051 Acc: 84.9881\n",
      "validation Loss: 0.0055 Acc: 83.8015\n",
      "Epoch 24/99\n",
      "training Loss: 0.0051 Acc: 85.1339\n",
      "validation Loss: 0.0055 Acc: 83.7658\n",
      "Epoch 25/99\n",
      "training Loss: 0.0050 Acc: 85.0863\n",
      "validation Loss: 0.0055 Acc: 83.8491\n",
      "Epoch 26/99\n",
      "training Loss: 0.0050 Acc: 85.2054\n",
      "validation Loss: 0.0055 Acc: 83.6706\n",
      "Epoch 27/99\n",
      "training Loss: 0.0050 Acc: 85.2440\n",
      "validation Loss: 0.0055 Acc: 83.5515\n",
      "Epoch 28/99\n",
      "training Loss: 0.0050 Acc: 85.1458\n",
      "validation Loss: 0.0055 Acc: 83.5158\n",
      "Epoch 29/99\n",
      "training Loss: 0.0050 Acc: 85.2113\n",
      "validation Loss: 0.0055 Acc: 83.5515\n",
      "Epoch 30/99\n",
      "training Loss: 0.0050 Acc: 85.3304\n",
      "validation Loss: 0.0055 Acc: 83.7539\n",
      "Epoch 31/99\n",
      "training Loss: 0.0050 Acc: 85.2857\n",
      "validation Loss: 0.0055 Acc: 83.8729\n",
      "Early stopped.\n",
      "Best val acc: 83.908593\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0059 Acc: 82.4743\n",
      "validation Loss: 0.0058 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0056 Acc: 83.2867\n",
      "validation Loss: 0.0057 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0055 Acc: 83.5784\n",
      "validation Loss: 0.0056 Acc: 83.2500\n",
      "Epoch 3/99\n",
      "training Loss: 0.0055 Acc: 83.6825\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 4/99\n",
      "training Loss: 0.0055 Acc: 83.8164\n",
      "validation Loss: 0.0057 Acc: 83.3214\n",
      "Epoch 5/99\n",
      "training Loss: 0.0054 Acc: 83.9027\n",
      "validation Loss: 0.0056 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0054 Acc: 83.9593\n",
      "validation Loss: 0.0056 Acc: 83.3095\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 84.0664\n",
      "validation Loss: 0.0056 Acc: 83.2857\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 84.2480\n",
      "validation Loss: 0.0056 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0054 Acc: 84.1676\n",
      "validation Loss: 0.0056 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 84.2420\n",
      "validation Loss: 0.0055 Acc: 83.5714\n",
      "Epoch 11/99\n",
      "training Loss: 0.0053 Acc: 84.2390\n",
      "validation Loss: 0.0056 Acc: 83.3571\n",
      "Epoch 12/99\n",
      "training Loss: 0.0053 Acc: 84.3462\n",
      "validation Loss: 0.0055 Acc: 83.4881\n",
      "Epoch 13/99\n",
      "training Loss: 0.0053 Acc: 84.4295\n",
      "validation Loss: 0.0056 Acc: 83.3452\n",
      "Epoch 14/99\n",
      "training Loss: 0.0053 Acc: 84.3581\n",
      "validation Loss: 0.0056 Acc: 83.2619\n",
      "Epoch 15/99\n",
      "training Loss: 0.0053 Acc: 84.4712\n",
      "validation Loss: 0.0056 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0052 Acc: 84.4741\n",
      "validation Loss: 0.0055 Acc: 83.5238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0052 Acc: 84.5099\n",
      "validation Loss: 0.0055 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0052 Acc: 84.6616\n",
      "validation Loss: 0.0056 Acc: 83.7857\n",
      "Epoch 19/99\n",
      "training Loss: 0.0052 Acc: 84.6348\n",
      "validation Loss: 0.0056 Acc: 83.4524\n",
      "Epoch 20/99\n",
      "training Loss: 0.0052 Acc: 84.6646\n",
      "validation Loss: 0.0055 Acc: 83.7381\n",
      "Epoch 21/99\n",
      "training Loss: 0.0052 Acc: 84.7331\n",
      "validation Loss: 0.0056 Acc: 83.7857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0052 Acc: 84.8432\n",
      "validation Loss: 0.0056 Acc: 83.4643\n",
      "Epoch 23/99\n",
      "training Loss: 0.0051 Acc: 84.9473\n",
      "validation Loss: 0.0056 Acc: 83.6071\n",
      "Epoch 24/99\n",
      "training Loss: 0.0051 Acc: 84.9890\n",
      "validation Loss: 0.0055 Acc: 83.7738\n",
      "Epoch 25/99\n",
      "training Loss: 0.0051 Acc: 85.0693\n",
      "validation Loss: 0.0055 Acc: 83.7500\n",
      "Epoch 26/99\n",
      "training Loss: 0.0050 Acc: 85.1616\n",
      "validation Loss: 0.0056 Acc: 83.4762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0050 Acc: 85.2033\n",
      "validation Loss: 0.0055 Acc: 83.5357\n",
      "Epoch 28/99\n",
      "training Loss: 0.0050 Acc: 85.1497\n",
      "validation Loss: 0.0055 Acc: 83.7619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0050 Acc: 85.3312\n",
      "validation Loss: 0.0055 Acc: 83.6548\n",
      "Epoch 30/99\n",
      "training Loss: 0.0050 Acc: 85.3402\n",
      "validation Loss: 0.0055 Acc: 83.7262\n",
      "Epoch 31/99\n",
      "training Loss: 0.0050 Acc: 85.4294\n",
      "validation Loss: 0.0055 Acc: 83.7619\n",
      "Epoch 32/99\n",
      "training Loss: 0.0050 Acc: 85.3253\n",
      "validation Loss: 0.0055 Acc: 83.6190\n",
      "Epoch 33/99\n",
      "training Loss: 0.0050 Acc: 85.5128\n",
      "validation Loss: 0.0056 Acc: 83.5476\n",
      "Epoch 34/99\n",
      "training Loss: 0.0049 Acc: 85.5663\n",
      "validation Loss: 0.0055 Acc: 83.6071\n",
      "Epoch 35/99\n",
      "training Loss: 0.0049 Acc: 85.4652\n",
      "validation Loss: 0.0055 Acc: 83.5595\n",
      "Epoch 36/99\n",
      "training Loss: 0.0049 Acc: 85.4771\n",
      "validation Loss: 0.0055 Acc: 83.5595\n",
      "Epoch 37/99\n",
      "training Loss: 0.0049 Acc: 85.5723\n",
      "validation Loss: 0.0055 Acc: 83.4881\n",
      "Early stopped.\n",
      "Best val acc: 83.880952\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0060 Acc: 82.2005\n",
      "validation Loss: 0.0057 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0057 Acc: 83.1379\n",
      "validation Loss: 0.0056 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0056 Acc: 83.3760\n",
      "validation Loss: 0.0055 Acc: 83.8214\n",
      "Epoch 3/99\n",
      "training Loss: 0.0055 Acc: 83.5337\n",
      "validation Loss: 0.0055 Acc: 84.1429\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0055 Acc: 83.7807\n",
      "validation Loss: 0.0056 Acc: 83.9167\n",
      "Epoch 5/99\n",
      "training Loss: 0.0055 Acc: 83.6855\n",
      "validation Loss: 0.0055 Acc: 84.3214\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0055 Acc: 83.8075\n",
      "validation Loss: 0.0055 Acc: 84.0952\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 84.0307\n",
      "validation Loss: 0.0054 Acc: 84.0714\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 84.0248\n",
      "validation Loss: 0.0054 Acc: 84.3810\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0054 Acc: 83.9831\n",
      "validation Loss: 0.0054 Acc: 84.1667\n",
      "Epoch 10/99\n",
      "training Loss: 0.0054 Acc: 84.1081\n",
      "validation Loss: 0.0054 Acc: 84.2381\n",
      "Epoch 11/99\n",
      "training Loss: 0.0054 Acc: 83.9861\n",
      "validation Loss: 0.0054 Acc: 84.4405\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0053 Acc: 84.1021\n",
      "validation Loss: 0.0054 Acc: 84.2024\n",
      "Epoch 13/99\n",
      "training Loss: 0.0053 Acc: 84.0843\n",
      "validation Loss: 0.0054 Acc: 84.2619\n",
      "Epoch 14/99\n",
      "training Loss: 0.0053 Acc: 84.3581\n",
      "validation Loss: 0.0054 Acc: 84.2024\n",
      "Epoch 15/99\n",
      "training Loss: 0.0053 Acc: 84.2509\n",
      "validation Loss: 0.0054 Acc: 84.1548\n",
      "Epoch 16/99\n",
      "training Loss: 0.0053 Acc: 84.2777\n",
      "validation Loss: 0.0055 Acc: 84.0238\n",
      "Epoch 17/99\n",
      "training Loss: 0.0053 Acc: 84.4087\n",
      "validation Loss: 0.0055 Acc: 84.2024\n",
      "Epoch 18/99\n",
      "training Loss: 0.0052 Acc: 84.4682\n",
      "validation Loss: 0.0054 Acc: 84.2262\n",
      "Epoch 19/99\n",
      "training Loss: 0.0052 Acc: 84.3521\n",
      "validation Loss: 0.0055 Acc: 84.2024\n",
      "Epoch 20/99\n",
      "training Loss: 0.0052 Acc: 84.4920\n",
      "validation Loss: 0.0054 Acc: 84.2857\n",
      "Epoch 21/99\n",
      "training Loss: 0.0052 Acc: 84.4622\n",
      "validation Loss: 0.0054 Acc: 84.3095\n",
      "Epoch 22/99\n",
      "training Loss: 0.0051 Acc: 84.6884\n",
      "validation Loss: 0.0054 Acc: 84.2500\n",
      "Epoch 23/99\n",
      "training Loss: 0.0051 Acc: 84.8521\n",
      "validation Loss: 0.0054 Acc: 84.2976\n",
      "Epoch 24/99\n",
      "training Loss: 0.0051 Acc: 84.8819\n",
      "validation Loss: 0.0054 Acc: 84.2024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0051 Acc: 84.9057\n",
      "validation Loss: 0.0054 Acc: 84.2857\n",
      "Epoch 26/99\n",
      "training Loss: 0.0051 Acc: 84.9086\n",
      "validation Loss: 0.0054 Acc: 84.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0051 Acc: 85.0336\n",
      "validation Loss: 0.0054 Acc: 84.3214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0050 Acc: 85.0217\n",
      "validation Loss: 0.0054 Acc: 84.3095\n",
      "Epoch 29/99\n",
      "training Loss: 0.0050 Acc: 85.1080\n",
      "validation Loss: 0.0054 Acc: 84.2024\n",
      "Epoch 30/99\n",
      "training Loss: 0.0050 Acc: 85.1021\n",
      "validation Loss: 0.0054 Acc: 84.2500\n",
      "Epoch 31/99\n",
      "training Loss: 0.0050 Acc: 85.2687\n",
      "validation Loss: 0.0054 Acc: 84.3214\n",
      "Early stopped.\n",
      "Best val acc: 84.440476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0059 Acc: 82.7510\n",
      "validation Loss: 0.0058 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0056 Acc: 83.4296\n",
      "validation Loss: 0.0057 Acc: 82.8929\n",
      "Epoch 2/99\n",
      "training Loss: 0.0055 Acc: 83.7480\n",
      "validation Loss: 0.0057 Acc: 82.8333\n",
      "Epoch 3/99\n",
      "training Loss: 0.0055 Acc: 83.7599\n",
      "validation Loss: 0.0057 Acc: 82.7738\n",
      "Epoch 4/99\n",
      "training Loss: 0.0055 Acc: 83.9325\n",
      "validation Loss: 0.0057 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0054 Acc: 83.9176\n",
      "validation Loss: 0.0056 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0054 Acc: 84.0426\n",
      "validation Loss: 0.0056 Acc: 83.0952\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 84.0396\n",
      "validation Loss: 0.0056 Acc: 83.0833\n",
      "Epoch 8/99\n",
      "training Loss: 0.0053 Acc: 84.1974\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0053 Acc: 84.2420\n",
      "validation Loss: 0.0057 Acc: 82.9643\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 84.3938\n",
      "validation Loss: 0.0057 Acc: 82.8810\n",
      "Epoch 11/99\n",
      "training Loss: 0.0053 Acc: 84.2628\n",
      "validation Loss: 0.0057 Acc: 83.0714\n",
      "Epoch 12/99\n",
      "training Loss: 0.0052 Acc: 84.5843\n",
      "validation Loss: 0.0056 Acc: 83.0238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0052 Acc: 84.6319\n",
      "validation Loss: 0.0056 Acc: 83.0833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0052 Acc: 84.7271\n",
      "validation Loss: 0.0057 Acc: 83.1429\n",
      "Epoch 15/99\n",
      "training Loss: 0.0052 Acc: 84.7509\n",
      "validation Loss: 0.0057 Acc: 82.9405\n",
      "Epoch 16/99\n",
      "training Loss: 0.0052 Acc: 84.8194\n",
      "validation Loss: 0.0056 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0051 Acc: 84.8194\n",
      "validation Loss: 0.0056 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0051 Acc: 84.9741\n",
      "validation Loss: 0.0056 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0051 Acc: 85.0247\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 20/99\n",
      "training Loss: 0.0051 Acc: 84.9295\n",
      "validation Loss: 0.0056 Acc: 83.2143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0051 Acc: 84.8699\n",
      "validation Loss: 0.0057 Acc: 83.1548\n",
      "Epoch 22/99\n",
      "training Loss: 0.0051 Acc: 85.0187\n",
      "validation Loss: 0.0056 Acc: 83.0714\n",
      "Epoch 23/99\n",
      "training Loss: 0.0051 Acc: 84.9473\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0051 Acc: 85.1021\n",
      "validation Loss: 0.0056 Acc: 83.2619\n",
      "Epoch 25/99\n",
      "training Loss: 0.0051 Acc: 85.0991\n",
      "validation Loss: 0.0056 Acc: 83.2262\n",
      "Epoch 26/99\n",
      "training Loss: 0.0051 Acc: 85.1586\n",
      "validation Loss: 0.0056 Acc: 83.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0050 Acc: 85.1497\n",
      "validation Loss: 0.0056 Acc: 83.0833\n",
      "Epoch 28/99\n",
      "training Loss: 0.0050 Acc: 85.2271\n",
      "validation Loss: 0.0056 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0050 Acc: 85.2628\n",
      "validation Loss: 0.0056 Acc: 83.1667\n",
      "Epoch 30/99\n",
      "training Loss: 0.0050 Acc: 85.2241\n",
      "validation Loss: 0.0056 Acc: 83.3214\n",
      "Epoch 31/99\n",
      "training Loss: 0.0050 Acc: 85.2181\n",
      "validation Loss: 0.0056 Acc: 83.1667\n",
      "Epoch 32/99\n",
      "training Loss: 0.0050 Acc: 85.3253\n",
      "validation Loss: 0.0056 Acc: 83.3810\n",
      "Epoch 33/99\n",
      "training Loss: 0.0050 Acc: 85.2955\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 34/99\n",
      "training Loss: 0.0050 Acc: 85.1824\n",
      "validation Loss: 0.0056 Acc: 83.1190\n",
      "Epoch 35/99\n",
      "training Loss: 0.0050 Acc: 85.2360\n",
      "validation Loss: 0.0056 Acc: 83.1905\n",
      "Epoch 36/99\n",
      "training Loss: 0.0050 Acc: 85.3312\n",
      "validation Loss: 0.0057 Acc: 83.1905\n",
      "Epoch 37/99\n",
      "training Loss: 0.0050 Acc: 85.2062\n",
      "validation Loss: 0.0056 Acc: 83.2262\n",
      "Epoch 38/99\n",
      "training Loss: 0.0050 Acc: 85.2836\n",
      "validation Loss: 0.0056 Acc: 83.2857\n",
      "Epoch 39/99\n",
      "training Loss: 0.0050 Acc: 85.3134\n",
      "validation Loss: 0.0056 Acc: 83.1905\n",
      "Epoch 40/99\n",
      "training Loss: 0.0050 Acc: 85.2806\n",
      "validation Loss: 0.0056 Acc: 83.2262\n",
      "Epoch 41/99\n",
      "training Loss: 0.0050 Acc: 85.3223\n",
      "validation Loss: 0.0056 Acc: 83.2738\n",
      "Epoch 42/99\n",
      "training Loss: 0.0050 Acc: 85.2330\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 43/99\n",
      "training Loss: 0.0050 Acc: 85.3550\n",
      "validation Loss: 0.0056 Acc: 83.2500\n",
      "Epoch 44/99\n",
      "training Loss: 0.0050 Acc: 85.3134\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 45/99\n",
      "training Loss: 0.0050 Acc: 85.2509\n",
      "validation Loss: 0.0056 Acc: 83.1786\n",
      "Epoch 46/99\n",
      "training Loss: 0.0050 Acc: 85.2896\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 47/99\n",
      "training Loss: 0.0050 Acc: 85.3461\n",
      "validation Loss: 0.0056 Acc: 83.2381\n",
      "Epoch 48/99\n",
      "training Loss: 0.0050 Acc: 85.3848\n",
      "validation Loss: 0.0056 Acc: 83.2500\n",
      "Early stopped.\n",
      "Best val acc: 83.380952\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0059 Acc: 82.4237\n",
      "validation Loss: 0.0057 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0056 Acc: 83.2153\n",
      "validation Loss: 0.0056 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0056 Acc: 83.3790\n",
      "validation Loss: 0.0056 Acc: 83.6548\n",
      "Epoch 3/99\n",
      "training Loss: 0.0055 Acc: 83.5813\n",
      "validation Loss: 0.0056 Acc: 83.7500\n",
      "Epoch 4/99\n",
      "training Loss: 0.0055 Acc: 83.6111\n",
      "validation Loss: 0.0057 Acc: 83.3810\n",
      "Epoch 5/99\n",
      "training Loss: 0.0055 Acc: 83.7688\n",
      "validation Loss: 0.0056 Acc: 83.6310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0054 Acc: 83.8522\n",
      "validation Loss: 0.0056 Acc: 83.5476\n",
      "Epoch 7/99\n",
      "training Loss: 0.0054 Acc: 83.9533\n",
      "validation Loss: 0.0055 Acc: 83.7024\n",
      "Epoch 8/99\n",
      "training Loss: 0.0054 Acc: 83.9504\n",
      "validation Loss: 0.0056 Acc: 83.9048\n",
      "Epoch 9/99\n",
      "training Loss: 0.0054 Acc: 84.0188\n",
      "validation Loss: 0.0055 Acc: 83.8571\n",
      "Epoch 10/99\n",
      "training Loss: 0.0053 Acc: 84.1527\n",
      "validation Loss: 0.0055 Acc: 83.6905\n",
      "Epoch 11/99\n",
      "training Loss: 0.0053 Acc: 84.1378\n",
      "validation Loss: 0.0055 Acc: 83.8571\n",
      "Epoch 12/99\n",
      "training Loss: 0.0053 Acc: 84.3551\n",
      "validation Loss: 0.0055 Acc: 83.9524\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0053 Acc: 84.2688\n",
      "validation Loss: 0.0055 Acc: 83.8929\n",
      "Epoch 14/99\n",
      "training Loss: 0.0053 Acc: 84.3610\n",
      "validation Loss: 0.0055 Acc: 83.7262\n",
      "Epoch 15/99\n",
      "training Loss: 0.0053 Acc: 84.4771\n",
      "validation Loss: 0.0055 Acc: 83.6786\n",
      "Epoch 16/99\n",
      "training Loss: 0.0052 Acc: 84.4355\n",
      "validation Loss: 0.0055 Acc: 83.8929\n",
      "Epoch 17/99\n",
      "training Loss: 0.0052 Acc: 84.4325\n",
      "validation Loss: 0.0055 Acc: 83.7024\n",
      "Epoch 18/99\n",
      "training Loss: 0.0052 Acc: 84.5337\n",
      "validation Loss: 0.0055 Acc: 83.8214\n",
      "Epoch 19/99\n",
      "training Loss: 0.0052 Acc: 84.5694\n",
      "validation Loss: 0.0056 Acc: 83.5714\n",
      "Epoch 20/99\n",
      "training Loss: 0.0052 Acc: 84.6825\n",
      "validation Loss: 0.0055 Acc: 83.9405\n",
      "Epoch 21/99\n",
      "training Loss: 0.0052 Acc: 84.8104\n",
      "validation Loss: 0.0057 Acc: 82.9524\n",
      "Epoch 22/99\n",
      "training Loss: 0.0051 Acc: 84.8134\n",
      "validation Loss: 0.0056 Acc: 83.4167\n",
      "Epoch 23/99\n",
      "training Loss: 0.0052 Acc: 84.9324\n",
      "validation Loss: 0.0055 Acc: 83.7738\n",
      "Epoch 24/99\n",
      "training Loss: 0.0051 Acc: 84.8461\n",
      "validation Loss: 0.0055 Acc: 83.7024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0051 Acc: 85.1467\n",
      "validation Loss: 0.0055 Acc: 83.7262\n",
      "Epoch 26/99\n",
      "training Loss: 0.0050 Acc: 85.3521\n",
      "validation Loss: 0.0055 Acc: 83.6310\n",
      "Epoch 27/99\n",
      "training Loss: 0.0050 Acc: 85.1914\n",
      "validation Loss: 0.0055 Acc: 83.5952\n",
      "Epoch 28/99\n",
      "training Loss: 0.0050 Acc: 85.3223\n",
      "validation Loss: 0.0055 Acc: 83.7143\n",
      "Epoch 29/99\n",
      "training Loss: 0.0050 Acc: 85.3015\n",
      "validation Loss: 0.0055 Acc: 83.6667\n",
      "Epoch 30/99\n",
      "training Loss: 0.0050 Acc: 85.2390\n",
      "validation Loss: 0.0055 Acc: 83.7857\n",
      "Epoch 31/99\n",
      "training Loss: 0.0050 Acc: 85.5068\n",
      "validation Loss: 0.0055 Acc: 83.7143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0050 Acc: 85.4741\n",
      "validation Loss: 0.0055 Acc: 83.5595\n",
      "Early stopped.\n",
      "Best val acc: 83.952381\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9126710194\n",
      "New configuration: {'learning_rate': 0.005583754966547714, 'initial_nodes': 738, 'dropout': 0.3596618806187056, 'batch_size': 448, 'max_depth': 3}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 81.2946\n",
      "validation Loss: 0.0008 Acc: 83.5277\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 83.0714\n",
      "validation Loss: 0.0008 Acc: 83.6706\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 83.3244\n",
      "validation Loss: 0.0008 Acc: 84.3133\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.4196\n",
      "validation Loss: 0.0008 Acc: 83.8372\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.3512\n",
      "validation Loss: 0.0008 Acc: 84.0157\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.5893\n",
      "validation Loss: 0.0008 Acc: 83.6706\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.6310\n",
      "validation Loss: 0.0008 Acc: 84.1466\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.6994\n",
      "validation Loss: 0.0008 Acc: 83.6706\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.5952\n",
      "validation Loss: 0.0008 Acc: 83.9086\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.7887\n",
      "validation Loss: 0.0008 Acc: 83.8253\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.8750\n",
      "validation Loss: 0.0008 Acc: 83.7539\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.8244\n",
      "validation Loss: 0.0008 Acc: 83.8610\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.8304\n",
      "validation Loss: 0.0008 Acc: 83.5991\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.8720\n",
      "validation Loss: 0.0008 Acc: 84.2657\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.7708\n",
      "validation Loss: 0.0008 Acc: 84.0276\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.9821\n",
      "validation Loss: 0.0008 Acc: 83.9681\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 84.1012\n",
      "validation Loss: 0.0008 Acc: 83.7658\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.9464\n",
      "validation Loss: 0.0008 Acc: 83.7896\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 83.8482\n",
      "validation Loss: 0.0008 Acc: 84.0633\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 84.0804\n",
      "validation Loss: 0.0008 Acc: 84.0752\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 84.3571\n",
      "validation Loss: 0.0008 Acc: 84.0157\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.7292\n",
      "validation Loss: 0.0008 Acc: 84.2537\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.5774\n",
      "validation Loss: 0.0008 Acc: 83.8372\n",
      "Early stopped.\n",
      "Best val acc: 84.313259\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 81.6291\n",
      "validation Loss: 0.0009 Acc: 82.9286\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 83.4147\n",
      "validation Loss: 0.0008 Acc: 82.8810\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 83.5873\n",
      "validation Loss: 0.0009 Acc: 82.7262\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.4950\n",
      "validation Loss: 0.0008 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.2867\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.6170\n",
      "validation Loss: 0.0008 Acc: 83.1310\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.6260\n",
      "validation Loss: 0.0008 Acc: 83.0714\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.8105\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.7331\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.7123\n",
      "validation Loss: 0.0008 Acc: 82.9643\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.8016\n",
      "validation Loss: 0.0008 Acc: 82.8810\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.9087\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.6290\n",
      "validation Loss: 0.0008 Acc: 83.1310\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.8016\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.5962\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.3194\n",
      "validation Loss: 0.0008 Acc: 82.9167\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.8551\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.8075\n",
      "validation Loss: 0.0009 Acc: 83.0595\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 83.4950\n",
      "validation Loss: 0.0008 Acc: 83.3214\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 83.9146\n",
      "validation Loss: 0.0008 Acc: 83.2024\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 84.0099\n",
      "validation Loss: 0.0008 Acc: 83.0476\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.2390\n",
      "validation Loss: 0.0008 Acc: 83.1429\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.3551\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.3640\n",
      "validation Loss: 0.0008 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.2450\n",
      "validation Loss: 0.0008 Acc: 83.2024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.6467\n",
      "validation Loss: 0.0008 Acc: 82.8452\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.4741\n",
      "validation Loss: 0.0008 Acc: 83.2619\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.6438\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.9473\n",
      "validation Loss: 0.0008 Acc: 83.2619\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.9384\n",
      "validation Loss: 0.0008 Acc: 82.9881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 84.6557\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 84.9860\n",
      "validation Loss: 0.0008 Acc: 83.1429\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.9533\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.1646\n",
      "validation Loss: 0.0008 Acc: 83.1071\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 85.1080\n",
      "validation Loss: 0.0008 Acc: 83.1429\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 85.2896\n",
      "validation Loss: 0.0008 Acc: 83.1786\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.4205\n",
      "validation Loss: 0.0008 Acc: 83.1190\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 85.3759\n",
      "validation Loss: 0.0008 Acc: 83.1667\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 85.4741\n",
      "validation Loss: 0.0008 Acc: 83.0595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 85.6140\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 85.7598\n",
      "validation Loss: 0.0008 Acc: 83.1071\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 85.7032\n",
      "validation Loss: 0.0008 Acc: 83.2976\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 85.7003\n",
      "validation Loss: 0.0008 Acc: 83.1786\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 85.8133\n",
      "validation Loss: 0.0008 Acc: 83.1667\n",
      "Early stopped.\n",
      "Best val acc: 83.511905\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 81.8999\n",
      "validation Loss: 0.0009 Acc: 82.5476\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 82.9742\n",
      "validation Loss: 0.0009 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.5635\n",
      "validation Loss: 0.0008 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.3492\n",
      "validation Loss: 0.0009 Acc: 82.5238\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.6081\n",
      "validation Loss: 0.0008 Acc: 82.7024\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.7837\n",
      "validation Loss: 0.0009 Acc: 82.8452\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.3224\n",
      "validation Loss: 0.0008 Acc: 83.0595\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.7093\n",
      "validation Loss: 0.0009 Acc: 82.4762\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.7182\n",
      "validation Loss: 0.0008 Acc: 82.4286\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.7361\n",
      "validation Loss: 0.0008 Acc: 82.5952\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.7807\n",
      "validation Loss: 0.0008 Acc: 82.1071\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.7004\n",
      "validation Loss: 0.0008 Acc: 83.0952\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.6349\n",
      "validation Loss: 0.0008 Acc: 82.9643\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 84.1468\n",
      "validation Loss: 0.0008 Acc: 82.9405\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.8819\n",
      "validation Loss: 0.0009 Acc: 82.3452\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.6438\n",
      "validation Loss: 0.0008 Acc: 82.5476\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.8760\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Epoch 17/99\n",
      "training Loss: 0.0009 Acc: 83.9176\n",
      "validation Loss: 0.0009 Acc: 82.8571\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 83.1171\n",
      "validation Loss: 0.0009 Acc: 83.1310\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 83.0218\n",
      "validation Loss: 0.0008 Acc: 82.9524\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 83.9712\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.0426\n",
      "validation Loss: 0.0008 Acc: 83.3095\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 83.9593\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.2331\n",
      "validation Loss: 0.0008 Acc: 83.2381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.2152\n",
      "validation Loss: 0.0008 Acc: 83.0119\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.2301\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.2122\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.4920\n",
      "validation Loss: 0.0008 Acc: 83.0119\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.5218\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.6229\n",
      "validation Loss: 0.0008 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 84.6497\n",
      "validation Loss: 0.0008 Acc: 83.4167\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 84.5158\n",
      "validation Loss: 0.0008 Acc: 83.4167\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 84.6259\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 84.8283\n",
      "validation Loss: 0.0008 Acc: 83.2500\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 84.9176\n",
      "validation Loss: 0.0008 Acc: 83.4762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.0277\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.3223\n",
      "validation Loss: 0.0008 Acc: 83.3929\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.2390\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 85.2181\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 85.2300\n",
      "validation Loss: 0.0008 Acc: 83.4286\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 85.3669\n",
      "validation Loss: 0.0008 Acc: 83.3333\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 85.3521\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 85.5663\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 85.5009\n",
      "validation Loss: 0.0008 Acc: 83.4167\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 85.5187\n",
      "validation Loss: 0.0008 Acc: 83.4762\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 85.6199\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 85.5544\n",
      "validation Loss: 0.0009 Acc: 83.3929\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 85.7241\n",
      "validation Loss: 0.0009 Acc: 83.2619\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 85.7925\n",
      "validation Loss: 0.0009 Acc: 83.3333\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 85.7628\n",
      "validation Loss: 0.0009 Acc: 83.3810\n",
      "Early stopped.\n",
      "Best val acc: 83.511905\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 81.9416\n",
      "validation Loss: 0.0009 Acc: 82.6786\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 83.2004\n",
      "validation Loss: 0.0008 Acc: 82.6667\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 83.3403\n",
      "validation Loss: 0.0009 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.5932\n",
      "validation Loss: 0.0008 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.3998\n",
      "validation Loss: 0.0008 Acc: 83.2738\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.5069\n",
      "validation Loss: 0.0008 Acc: 83.3690\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.3016\n",
      "validation Loss: 0.0008 Acc: 83.4524\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.6587\n",
      "validation Loss: 0.0008 Acc: 83.3690\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.2540\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.5486\n",
      "validation Loss: 0.0008 Acc: 83.5833\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.7420\n",
      "validation Loss: 0.0008 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.7391\n",
      "validation Loss: 0.0008 Acc: 83.3690\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.7956\n",
      "validation Loss: 0.0009 Acc: 82.6786\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.5813\n",
      "validation Loss: 0.0008 Acc: 83.5952\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.8908\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.8194\n",
      "validation Loss: 0.0008 Acc: 83.8095\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.8968\n",
      "validation Loss: 0.0008 Acc: 83.6667\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 84.0694\n",
      "validation Loss: 0.0008 Acc: 83.4405\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 84.4950\n",
      "validation Loss: 0.0008 Acc: 83.4048\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 84.0218\n",
      "validation Loss: 0.0008 Acc: 83.5714\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 84.2866\n",
      "validation Loss: 0.0008 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.2926\n",
      "validation Loss: 0.0008 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.4027\n",
      "validation Loss: 0.0008 Acc: 83.8452\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.4295\n",
      "validation Loss: 0.0008 Acc: 83.3810\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.6348\n",
      "validation Loss: 0.0008 Acc: 83.8929\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.6438\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.6438\n",
      "validation Loss: 0.0008 Acc: 83.6310\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.6081\n",
      "validation Loss: 0.0008 Acc: 83.7143\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.8789\n",
      "validation Loss: 0.0008 Acc: 83.5833\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 85.0634\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 85.0693\n",
      "validation Loss: 0.0008 Acc: 83.6190\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 85.2449\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 85.2330\n",
      "validation Loss: 0.0008 Acc: 83.4881\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 85.3759\n",
      "validation Loss: 0.0008 Acc: 83.4762\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 85.3223\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 85.5187\n",
      "validation Loss: 0.0008 Acc: 83.5476\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 85.5396\n",
      "validation Loss: 0.0008 Acc: 83.5238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 85.5574\n",
      "validation Loss: 0.0008 Acc: 83.5357\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 85.6467\n",
      "validation Loss: 0.0008 Acc: 83.6190\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 86.0336\n",
      "validation Loss: 0.0009 Acc: 83.5357\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 86.0574\n",
      "validation Loss: 0.0009 Acc: 83.3690\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 85.9651\n",
      "validation Loss: 0.0009 Acc: 83.3929\n",
      "Early stopped.\n",
      "Best val acc: 83.916667\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 81.5547\n",
      "validation Loss: 0.0008 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 82.8611\n",
      "validation Loss: 0.0008 Acc: 84.0000\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.1468\n",
      "validation Loss: 0.0008 Acc: 84.1786\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.1796\n",
      "validation Loss: 0.0008 Acc: 83.6429\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.1974\n",
      "validation Loss: 0.0008 Acc: 83.9643\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.2956\n",
      "validation Loss: 0.0008 Acc: 83.7857\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.3522\n",
      "validation Loss: 0.0008 Acc: 83.6905\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.4801\n",
      "validation Loss: 0.0008 Acc: 84.1071\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.4593\n",
      "validation Loss: 0.0008 Acc: 83.8929\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.5248\n",
      "validation Loss: 0.0008 Acc: 84.0595\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.6885\n",
      "validation Loss: 0.0008 Acc: 83.9048\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.6141\n",
      "validation Loss: 0.0008 Acc: 84.1190\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.2927\n",
      "validation Loss: 0.0008 Acc: 83.9524\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.6944\n",
      "validation Loss: 0.0008 Acc: 84.0833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.6944\n",
      "validation Loss: 0.0008 Acc: 84.0119\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.8938\n",
      "validation Loss: 0.0008 Acc: 83.7738\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.9295\n",
      "validation Loss: 0.0008 Acc: 84.0714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.8730\n",
      "validation Loss: 0.0008 Acc: 84.0357\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 84.0218\n",
      "validation Loss: 0.0008 Acc: 84.0952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.0218\n",
      "validation Loss: 0.0008 Acc: 83.9405\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 83.5010\n",
      "validation Loss: 0.0008 Acc: 83.8929\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 83.9712\n",
      "validation Loss: 0.0008 Acc: 83.7976\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.0515\n",
      "validation Loss: 0.0008 Acc: 83.8929\n",
      "Early stopped.\n",
      "Best val acc: 84.178571\n",
      "----------\n",
      "Average best_acc across k-fold: 83.8864612734\n",
      "New configuration: {'learning_rate': 1e-05, 'initial_nodes': 1000, 'dropout': 0.194392861917166, 'batch_size': 36, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0127 Acc: 79.9940\n",
      "validation Loss: 0.0109 Acc: 82.5994\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0108 Acc: 82.6399\n",
      "validation Loss: 0.0105 Acc: 82.8731\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0105 Acc: 82.9375\n",
      "validation Loss: 0.0104 Acc: 83.0279\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0104 Acc: 83.0774\n",
      "validation Loss: 0.0103 Acc: 83.0874\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0103 Acc: 83.1667\n",
      "validation Loss: 0.0102 Acc: 83.2183\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0103 Acc: 83.2887\n",
      "validation Loss: 0.0102 Acc: 83.0874\n",
      "Epoch 6/99\n",
      "training Loss: 0.0102 Acc: 83.2976\n",
      "validation Loss: 0.0102 Acc: 83.0755\n",
      "Epoch 7/99\n",
      "training Loss: 0.0102 Acc: 83.3631\n",
      "validation Loss: 0.0101 Acc: 83.2897\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0102 Acc: 83.4435\n",
      "validation Loss: 0.0102 Acc: 83.0636\n",
      "Epoch 9/99\n",
      "training Loss: 0.0101 Acc: 83.5893\n",
      "validation Loss: 0.0102 Acc: 83.2302\n",
      "Epoch 10/99\n",
      "training Loss: 0.0101 Acc: 83.5446\n",
      "validation Loss: 0.0101 Acc: 83.2421\n",
      "Epoch 11/99\n",
      "training Loss: 0.0101 Acc: 83.5893\n",
      "validation Loss: 0.0101 Acc: 83.3611\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0101 Acc: 83.5804\n",
      "validation Loss: 0.0101 Acc: 83.3254\n",
      "Epoch 13/99\n",
      "training Loss: 0.0100 Acc: 83.6071\n",
      "validation Loss: 0.0101 Acc: 83.3968\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0100 Acc: 83.5804\n",
      "validation Loss: 0.0101 Acc: 83.4087\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0100 Acc: 83.7262\n",
      "validation Loss: 0.0100 Acc: 83.3373\n",
      "Epoch 16/99\n",
      "training Loss: 0.0100 Acc: 83.6399\n",
      "validation Loss: 0.0101 Acc: 83.4087\n",
      "Epoch 17/99\n",
      "training Loss: 0.0100 Acc: 83.6905\n",
      "validation Loss: 0.0101 Acc: 83.3135\n",
      "Epoch 18/99\n",
      "training Loss: 0.0100 Acc: 83.6786\n",
      "validation Loss: 0.0100 Acc: 83.5396\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0099 Acc: 83.7500\n",
      "validation Loss: 0.0100 Acc: 83.4563\n",
      "Epoch 20/99\n",
      "training Loss: 0.0099 Acc: 83.7024\n",
      "validation Loss: 0.0100 Acc: 83.5039\n",
      "Epoch 21/99\n",
      "training Loss: 0.0099 Acc: 83.8185\n",
      "validation Loss: 0.0100 Acc: 83.5515\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0099 Acc: 83.8065\n",
      "validation Loss: 0.0100 Acc: 83.4682\n",
      "Epoch 23/99\n",
      "training Loss: 0.0099 Acc: 83.7976\n",
      "validation Loss: 0.0100 Acc: 83.5277\n",
      "Epoch 24/99\n",
      "training Loss: 0.0099 Acc: 83.8482\n",
      "validation Loss: 0.0100 Acc: 83.5753\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0099 Acc: 83.8512\n",
      "validation Loss: 0.0100 Acc: 83.5277\n",
      "Epoch 26/99\n",
      "training Loss: 0.0099 Acc: 83.8571\n",
      "validation Loss: 0.0100 Acc: 83.6587\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0099 Acc: 83.8631\n",
      "validation Loss: 0.0100 Acc: 83.6229\n",
      "Epoch 28/99\n",
      "training Loss: 0.0099 Acc: 83.9226\n",
      "validation Loss: 0.0099 Acc: 83.6110\n",
      "Epoch 29/99\n",
      "training Loss: 0.0098 Acc: 83.8750\n",
      "validation Loss: 0.0100 Acc: 83.5634\n",
      "Epoch 30/99\n",
      "training Loss: 0.0098 Acc: 83.9167\n",
      "validation Loss: 0.0099 Acc: 83.7301\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0098 Acc: 83.9732\n",
      "validation Loss: 0.0100 Acc: 83.6706\n",
      "Epoch 32/99\n",
      "training Loss: 0.0098 Acc: 83.9970\n",
      "validation Loss: 0.0099 Acc: 83.6229\n",
      "Epoch 33/99\n",
      "training Loss: 0.0098 Acc: 84.0000\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 34/99\n",
      "training Loss: 0.0098 Acc: 83.8988\n",
      "validation Loss: 0.0099 Acc: 83.5872\n",
      "Epoch 35/99\n",
      "training Loss: 0.0098 Acc: 84.0298\n",
      "validation Loss: 0.0099 Acc: 83.6229\n",
      "Epoch 36/99\n",
      "training Loss: 0.0098 Acc: 83.9256\n",
      "validation Loss: 0.0099 Acc: 83.6468\n",
      "Epoch 37/99\n",
      "training Loss: 0.0098 Acc: 84.0030\n",
      "validation Loss: 0.0099 Acc: 83.6587\n",
      "Epoch 38/99\n",
      "training Loss: 0.0098 Acc: 84.0119\n",
      "validation Loss: 0.0100 Acc: 83.6468\n",
      "Epoch 39/99\n",
      "training Loss: 0.0098 Acc: 84.0149\n",
      "validation Loss: 0.0099 Acc: 83.7420\n",
      "Saving..\n",
      "Epoch 40/99\n",
      "training Loss: 0.0098 Acc: 84.0179\n",
      "validation Loss: 0.0099 Acc: 83.7301\n",
      "Epoch 41/99\n",
      "training Loss: 0.0098 Acc: 84.0387\n",
      "validation Loss: 0.0099 Acc: 83.5991\n",
      "Epoch 42/99\n",
      "training Loss: 0.0097 Acc: 84.0268\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 43/99\n",
      "training Loss: 0.0097 Acc: 84.0952\n",
      "validation Loss: 0.0099 Acc: 83.6468\n",
      "Epoch 44/99\n",
      "training Loss: 0.0097 Acc: 84.2292\n",
      "validation Loss: 0.0099 Acc: 83.6348\n",
      "Epoch 45/99\n",
      "training Loss: 0.0097 Acc: 84.1548\n",
      "validation Loss: 0.0099 Acc: 83.7539\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0097 Acc: 84.1310\n",
      "validation Loss: 0.0099 Acc: 83.6587\n",
      "Epoch 47/99\n",
      "training Loss: 0.0097 Acc: 84.2768\n",
      "validation Loss: 0.0099 Acc: 83.6229\n",
      "Epoch 48/99\n",
      "training Loss: 0.0097 Acc: 84.1815\n",
      "validation Loss: 0.0099 Acc: 83.6468\n",
      "Epoch 49/99\n",
      "training Loss: 0.0097 Acc: 84.1994\n",
      "validation Loss: 0.0099 Acc: 83.7182\n",
      "Epoch 50/99\n",
      "training Loss: 0.0097 Acc: 84.1637\n",
      "validation Loss: 0.0099 Acc: 83.5515\n",
      "Epoch 51/99\n",
      "training Loss: 0.0097 Acc: 84.2292\n",
      "validation Loss: 0.0099 Acc: 83.5396\n",
      "Epoch 52/99\n",
      "training Loss: 0.0097 Acc: 84.1667\n",
      "validation Loss: 0.0099 Acc: 83.5753\n",
      "Epoch 53/99\n",
      "training Loss: 0.0097 Acc: 84.2054\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 54/99\n",
      "training Loss: 0.0097 Acc: 84.1577\n",
      "validation Loss: 0.0099 Acc: 83.6587\n",
      "Epoch 55/99\n",
      "training Loss: 0.0097 Acc: 84.1786\n",
      "validation Loss: 0.0099 Acc: 83.7301\n",
      "Epoch 56/99\n",
      "training Loss: 0.0097 Acc: 84.3006\n",
      "validation Loss: 0.0099 Acc: 83.7182\n",
      "Epoch 57/99\n",
      "training Loss: 0.0097 Acc: 84.1696\n",
      "validation Loss: 0.0099 Acc: 83.6110\n",
      "Epoch 58/99\n",
      "training Loss: 0.0097 Acc: 84.2530\n",
      "validation Loss: 0.0099 Acc: 83.7063\n",
      "Epoch 59/99\n",
      "training Loss: 0.0097 Acc: 84.2649\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 60/99\n",
      "training Loss: 0.0097 Acc: 84.2530\n",
      "validation Loss: 0.0099 Acc: 83.6825\n",
      "Epoch 61/99\n",
      "training Loss: 0.0097 Acc: 84.2530\n",
      "validation Loss: 0.0099 Acc: 83.6944\n",
      "Epoch 62/99\n",
      "training Loss: 0.0096 Acc: 84.2173\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 63/99\n",
      "training Loss: 0.0096 Acc: 84.1845\n",
      "validation Loss: 0.0099 Acc: 83.6944\n",
      "Epoch 64/99\n",
      "training Loss: 0.0097 Acc: 84.2530\n",
      "validation Loss: 0.0099 Acc: 83.7658\n",
      "Saving..\n",
      "Epoch 65/99\n",
      "training Loss: 0.0096 Acc: 84.2143\n",
      "validation Loss: 0.0098 Acc: 83.6706\n",
      "Epoch 66/99\n",
      "training Loss: 0.0096 Acc: 84.1905\n",
      "validation Loss: 0.0099 Acc: 83.6944\n",
      "Epoch 67/99\n",
      "training Loss: 0.0096 Acc: 84.2083\n",
      "validation Loss: 0.0099 Acc: 83.6468\n",
      "Epoch 68/99\n",
      "training Loss: 0.0096 Acc: 84.2798\n",
      "validation Loss: 0.0099 Acc: 83.6825\n",
      "Epoch 69/99\n",
      "training Loss: 0.0096 Acc: 84.2292\n",
      "validation Loss: 0.0099 Acc: 83.6468\n",
      "Epoch 70/99\n",
      "training Loss: 0.0096 Acc: 84.3482\n",
      "validation Loss: 0.0099 Acc: 83.7777\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0096 Acc: 84.3006\n",
      "validation Loss: 0.0099 Acc: 83.6587\n",
      "Epoch 72/99\n",
      "training Loss: 0.0096 Acc: 84.2292\n",
      "validation Loss: 0.0099 Acc: 83.7182\n",
      "Epoch 73/99\n",
      "training Loss: 0.0096 Acc: 84.3155\n",
      "validation Loss: 0.0099 Acc: 83.7539\n",
      "Epoch 74/99\n",
      "training Loss: 0.0096 Acc: 84.3274\n",
      "validation Loss: 0.0098 Acc: 83.6944\n",
      "Epoch 75/99\n",
      "training Loss: 0.0096 Acc: 84.2321\n",
      "validation Loss: 0.0098 Acc: 83.6348\n",
      "Epoch 76/99\n",
      "training Loss: 0.0096 Acc: 84.2946\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 77/99\n",
      "training Loss: 0.0096 Acc: 84.2857\n",
      "validation Loss: 0.0099 Acc: 83.6587\n",
      "Epoch 78/99\n",
      "training Loss: 0.0096 Acc: 84.2262\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 79/99\n",
      "training Loss: 0.0096 Acc: 84.3423\n",
      "validation Loss: 0.0098 Acc: 83.6825\n",
      "Epoch 80/99\n",
      "training Loss: 0.0096 Acc: 84.3750\n",
      "validation Loss: 0.0098 Acc: 83.6587\n",
      "Epoch 81/99\n",
      "training Loss: 0.0096 Acc: 84.3244\n",
      "validation Loss: 0.0099 Acc: 83.6706\n",
      "Epoch 82/99\n",
      "training Loss: 0.0096 Acc: 84.3839\n",
      "validation Loss: 0.0098 Acc: 83.6944\n",
      "Epoch 83/99\n",
      "training Loss: 0.0096 Acc: 84.3214\n",
      "validation Loss: 0.0099 Acc: 83.6944\n",
      "Epoch 84/99\n",
      "training Loss: 0.0096 Acc: 84.3542\n",
      "validation Loss: 0.0098 Acc: 83.6468\n",
      "Epoch 85/99\n",
      "training Loss: 0.0096 Acc: 84.3304\n",
      "validation Loss: 0.0098 Acc: 83.6706\n",
      "Epoch 86/99\n",
      "training Loss: 0.0096 Acc: 84.3631\n",
      "validation Loss: 0.0099 Acc: 83.6825\n",
      "Epoch 87/99\n",
      "training Loss: 0.0096 Acc: 84.4137\n",
      "validation Loss: 0.0099 Acc: 83.6825\n",
      "Epoch 88/99\n",
      "training Loss: 0.0096 Acc: 84.2768\n",
      "validation Loss: 0.0098 Acc: 83.6825\n",
      "Epoch 89/99\n",
      "training Loss: 0.0096 Acc: 84.3571\n",
      "validation Loss: 0.0098 Acc: 83.6587\n",
      "Epoch 90/99\n",
      "training Loss: 0.0096 Acc: 84.3988\n",
      "validation Loss: 0.0098 Acc: 83.6944\n",
      "Early stopped.\n",
      "Best val acc: 83.777672\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0126 Acc: 80.1351\n",
      "validation Loss: 0.0112 Acc: 81.6786\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0108 Acc: 82.6707\n",
      "validation Loss: 0.0108 Acc: 82.5595\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0105 Acc: 82.9326\n",
      "validation Loss: 0.0106 Acc: 82.6905\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0104 Acc: 83.1201\n",
      "validation Loss: 0.0106 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0103 Acc: 83.2361\n",
      "validation Loss: 0.0105 Acc: 82.9762\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0102 Acc: 83.2956\n",
      "validation Loss: 0.0104 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0102 Acc: 83.2867\n",
      "validation Loss: 0.0103 Acc: 82.9881\n",
      "Epoch 7/99\n",
      "training Loss: 0.0102 Acc: 83.4415\n",
      "validation Loss: 0.0103 Acc: 82.8690\n",
      "Epoch 8/99\n",
      "training Loss: 0.0101 Acc: 83.4623\n",
      "validation Loss: 0.0103 Acc: 83.0476\n",
      "Epoch 9/99\n",
      "training Loss: 0.0101 Acc: 83.5337\n",
      "validation Loss: 0.0103 Acc: 83.0595\n",
      "Epoch 10/99\n",
      "training Loss: 0.0101 Acc: 83.5635\n",
      "validation Loss: 0.0103 Acc: 83.0476\n",
      "Epoch 11/99\n",
      "training Loss: 0.0101 Acc: 83.6081\n",
      "validation Loss: 0.0102 Acc: 83.1429\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0100 Acc: 83.6349\n",
      "validation Loss: 0.0102 Acc: 83.2143\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0100 Acc: 83.7004\n",
      "validation Loss: 0.0102 Acc: 83.1786\n",
      "Epoch 14/99\n",
      "training Loss: 0.0100 Acc: 83.7599\n",
      "validation Loss: 0.0102 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0100 Acc: 83.7063\n",
      "validation Loss: 0.0102 Acc: 83.0714\n",
      "Epoch 16/99\n",
      "training Loss: 0.0100 Acc: 83.6736\n",
      "validation Loss: 0.0102 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0099 Acc: 83.7034\n",
      "validation Loss: 0.0101 Acc: 83.0952\n",
      "Epoch 18/99\n",
      "training Loss: 0.0099 Acc: 83.7986\n",
      "validation Loss: 0.0101 Acc: 83.2857\n",
      "Epoch 19/99\n",
      "training Loss: 0.0099 Acc: 83.7867\n",
      "validation Loss: 0.0101 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0099 Acc: 83.8105\n",
      "validation Loss: 0.0101 Acc: 83.2143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0099 Acc: 83.9236\n",
      "validation Loss: 0.0101 Acc: 83.1905\n",
      "Epoch 22/99\n",
      "training Loss: 0.0099 Acc: 83.8164\n",
      "validation Loss: 0.0101 Acc: 83.2024\n",
      "Epoch 23/99\n",
      "training Loss: 0.0099 Acc: 83.8670\n",
      "validation Loss: 0.0101 Acc: 83.3929\n",
      "Epoch 24/99\n",
      "training Loss: 0.0099 Acc: 83.8194\n",
      "validation Loss: 0.0101 Acc: 83.2262\n",
      "Epoch 25/99\n",
      "training Loss: 0.0098 Acc: 83.8492\n",
      "validation Loss: 0.0101 Acc: 83.4048\n",
      "Epoch 26/99\n",
      "training Loss: 0.0098 Acc: 83.8849\n",
      "validation Loss: 0.0101 Acc: 83.3095\n",
      "Epoch 27/99\n",
      "training Loss: 0.0098 Acc: 83.9295\n",
      "validation Loss: 0.0101 Acc: 83.2857\n",
      "Epoch 28/99\n",
      "training Loss: 0.0098 Acc: 83.9801\n",
      "validation Loss: 0.0100 Acc: 83.3333\n",
      "Epoch 29/99\n",
      "training Loss: 0.0098 Acc: 83.9712\n",
      "validation Loss: 0.0101 Acc: 83.2619\n",
      "Epoch 30/99\n",
      "training Loss: 0.0098 Acc: 84.0039\n",
      "validation Loss: 0.0101 Acc: 83.3690\n",
      "Epoch 31/99\n",
      "training Loss: 0.0098 Acc: 83.9950\n",
      "validation Loss: 0.0100 Acc: 83.4643\n",
      "Epoch 32/99\n",
      "training Loss: 0.0098 Acc: 84.0396\n",
      "validation Loss: 0.0101 Acc: 83.2262\n",
      "Epoch 33/99\n",
      "training Loss: 0.0098 Acc: 83.9385\n",
      "validation Loss: 0.0100 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0098 Acc: 83.9087\n",
      "validation Loss: 0.0100 Acc: 83.4167\n",
      "Epoch 35/99\n",
      "training Loss: 0.0098 Acc: 84.0754\n",
      "validation Loss: 0.0100 Acc: 83.5000\n",
      "Epoch 36/99\n",
      "training Loss: 0.0097 Acc: 84.0575\n",
      "validation Loss: 0.0100 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0097 Acc: 84.1498\n",
      "validation Loss: 0.0100 Acc: 83.3333\n",
      "Epoch 38/99\n",
      "training Loss: 0.0097 Acc: 84.0456\n",
      "validation Loss: 0.0100 Acc: 83.3810\n",
      "Epoch 39/99\n",
      "training Loss: 0.0097 Acc: 84.1557\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 40/99\n",
      "training Loss: 0.0097 Acc: 84.1319\n",
      "validation Loss: 0.0100 Acc: 83.4762\n",
      "Epoch 41/99\n",
      "training Loss: 0.0097 Acc: 84.1974\n",
      "validation Loss: 0.0100 Acc: 83.4286\n",
      "Epoch 42/99\n",
      "training Loss: 0.0097 Acc: 84.1468\n",
      "validation Loss: 0.0100 Acc: 83.3810\n",
      "Epoch 43/99\n",
      "training Loss: 0.0097 Acc: 84.1319\n",
      "validation Loss: 0.0100 Acc: 83.3452\n",
      "Epoch 44/99\n",
      "training Loss: 0.0097 Acc: 84.2152\n",
      "validation Loss: 0.0100 Acc: 83.4286\n",
      "Epoch 45/99\n",
      "training Loss: 0.0097 Acc: 84.1765\n",
      "validation Loss: 0.0100 Acc: 83.3690\n",
      "Epoch 46/99\n",
      "training Loss: 0.0097 Acc: 84.2003\n",
      "validation Loss: 0.0100 Acc: 83.4405\n",
      "Epoch 47/99\n",
      "training Loss: 0.0097 Acc: 84.2420\n",
      "validation Loss: 0.0100 Acc: 83.3571\n",
      "Epoch 48/99\n",
      "training Loss: 0.0097 Acc: 84.1587\n",
      "validation Loss: 0.0100 Acc: 83.3810\n",
      "Epoch 49/99\n",
      "training Loss: 0.0097 Acc: 84.3015\n",
      "validation Loss: 0.0100 Acc: 83.4405\n",
      "Epoch 50/99\n",
      "training Loss: 0.0097 Acc: 84.0932\n",
      "validation Loss: 0.0100 Acc: 83.3929\n",
      "Epoch 51/99\n",
      "training Loss: 0.0097 Acc: 84.1706\n",
      "validation Loss: 0.0100 Acc: 83.5357\n",
      "Epoch 52/99\n",
      "training Loss: 0.0096 Acc: 84.2628\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 53/99\n",
      "training Loss: 0.0096 Acc: 84.2450\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 54/99\n",
      "training Loss: 0.0096 Acc: 84.2509\n",
      "validation Loss: 0.0100 Acc: 83.5000\n",
      "Epoch 55/99\n",
      "training Loss: 0.0096 Acc: 84.3402\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 56/99\n",
      "training Loss: 0.0096 Acc: 84.2747\n",
      "validation Loss: 0.0100 Acc: 83.4167\n",
      "Early stopped.\n",
      "Best val acc: 83.630952\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0124 Acc: 80.3851\n",
      "validation Loss: 0.0110 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0107 Acc: 82.6737\n",
      "validation Loss: 0.0106 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0105 Acc: 82.8373\n",
      "validation Loss: 0.0104 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0104 Acc: 83.0546\n",
      "validation Loss: 0.0104 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0103 Acc: 83.2242\n",
      "validation Loss: 0.0103 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0102 Acc: 83.3760\n",
      "validation Loss: 0.0102 Acc: 83.3214\n",
      "Epoch 6/99\n",
      "training Loss: 0.0102 Acc: 83.3909\n",
      "validation Loss: 0.0102 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0101 Acc: 83.3849\n",
      "validation Loss: 0.0102 Acc: 83.4881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0101 Acc: 83.4653\n",
      "validation Loss: 0.0102 Acc: 83.3214\n",
      "Epoch 9/99\n",
      "training Loss: 0.0101 Acc: 83.5069\n",
      "validation Loss: 0.0102 Acc: 83.3690\n",
      "Epoch 10/99\n",
      "training Loss: 0.0101 Acc: 83.5516\n",
      "validation Loss: 0.0101 Acc: 83.5000\n",
      "Epoch 11/99\n",
      "training Loss: 0.0101 Acc: 83.4653\n",
      "validation Loss: 0.0102 Acc: 83.4762\n",
      "Epoch 12/99\n",
      "training Loss: 0.0100 Acc: 83.5516\n",
      "validation Loss: 0.0101 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0100 Acc: 83.5426\n",
      "validation Loss: 0.0101 Acc: 83.4524\n",
      "Epoch 14/99\n",
      "training Loss: 0.0100 Acc: 83.7123\n",
      "validation Loss: 0.0101 Acc: 83.3810\n",
      "Epoch 15/99\n",
      "training Loss: 0.0100 Acc: 83.7063\n",
      "validation Loss: 0.0101 Acc: 83.5119\n",
      "Epoch 16/99\n",
      "training Loss: 0.0099 Acc: 83.7331\n",
      "validation Loss: 0.0101 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0099 Acc: 83.6438\n",
      "validation Loss: 0.0101 Acc: 83.5119\n",
      "Epoch 18/99\n",
      "training Loss: 0.0099 Acc: 83.7331\n",
      "validation Loss: 0.0101 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0099 Acc: 83.8194\n",
      "validation Loss: 0.0101 Acc: 83.4524\n",
      "Epoch 20/99\n",
      "training Loss: 0.0099 Acc: 83.6944\n",
      "validation Loss: 0.0100 Acc: 83.5357\n",
      "Epoch 21/99\n",
      "training Loss: 0.0099 Acc: 83.6557\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0099 Acc: 83.8283\n",
      "validation Loss: 0.0101 Acc: 83.5595\n",
      "Epoch 23/99\n",
      "training Loss: 0.0099 Acc: 83.7837\n",
      "validation Loss: 0.0101 Acc: 83.3690\n",
      "Epoch 24/99\n",
      "training Loss: 0.0099 Acc: 83.8492\n",
      "validation Loss: 0.0100 Acc: 83.4762\n",
      "Epoch 25/99\n",
      "training Loss: 0.0098 Acc: 83.8254\n",
      "validation Loss: 0.0101 Acc: 83.5119\n",
      "Epoch 26/99\n",
      "training Loss: 0.0098 Acc: 83.8849\n",
      "validation Loss: 0.0100 Acc: 83.5000\n",
      "Epoch 27/99\n",
      "training Loss: 0.0098 Acc: 83.9355\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 28/99\n",
      "training Loss: 0.0098 Acc: 83.9355\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 29/99\n",
      "training Loss: 0.0098 Acc: 83.9474\n",
      "validation Loss: 0.0100 Acc: 83.4643\n",
      "Epoch 30/99\n",
      "training Loss: 0.0098 Acc: 83.9533\n",
      "validation Loss: 0.0100 Acc: 83.5238\n",
      "Epoch 31/99\n",
      "training Loss: 0.0098 Acc: 83.9176\n",
      "validation Loss: 0.0100 Acc: 83.5238\n",
      "Epoch 32/99\n",
      "training Loss: 0.0098 Acc: 83.8938\n",
      "validation Loss: 0.0100 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0098 Acc: 83.9801\n",
      "validation Loss: 0.0101 Acc: 83.4762\n",
      "Epoch 34/99\n",
      "training Loss: 0.0098 Acc: 84.0873\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 35/99\n",
      "training Loss: 0.0098 Acc: 83.9920\n",
      "validation Loss: 0.0100 Acc: 83.4286\n",
      "Epoch 36/99\n",
      "training Loss: 0.0097 Acc: 83.9504\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 37/99\n",
      "training Loss: 0.0097 Acc: 84.1438\n",
      "validation Loss: 0.0100 Acc: 83.6190\n",
      "Epoch 38/99\n",
      "training Loss: 0.0097 Acc: 84.0039\n",
      "validation Loss: 0.0100 Acc: 83.5238\n",
      "Epoch 39/99\n",
      "training Loss: 0.0097 Acc: 84.0158\n",
      "validation Loss: 0.0100 Acc: 83.5833\n",
      "Epoch 40/99\n",
      "training Loss: 0.0097 Acc: 84.1408\n",
      "validation Loss: 0.0100 Acc: 83.6310\n",
      "Epoch 41/99\n",
      "training Loss: 0.0097 Acc: 84.0188\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 42/99\n",
      "training Loss: 0.0097 Acc: 84.1765\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 43/99\n",
      "training Loss: 0.0097 Acc: 84.1349\n",
      "validation Loss: 0.0100 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0097 Acc: 84.2212\n",
      "validation Loss: 0.0100 Acc: 83.5000\n",
      "Epoch 45/99\n",
      "training Loss: 0.0097 Acc: 84.1230\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 46/99\n",
      "training Loss: 0.0097 Acc: 84.1855\n",
      "validation Loss: 0.0100 Acc: 83.5833\n",
      "Epoch 47/99\n",
      "training Loss: 0.0097 Acc: 84.0962\n",
      "validation Loss: 0.0100 Acc: 83.6548\n",
      "Epoch 48/99\n",
      "training Loss: 0.0097 Acc: 84.2003\n",
      "validation Loss: 0.0100 Acc: 83.6548\n",
      "Epoch 49/99\n",
      "training Loss: 0.0097 Acc: 84.1914\n",
      "validation Loss: 0.0100 Acc: 83.4524\n",
      "Epoch 50/99\n",
      "training Loss: 0.0097 Acc: 84.2182\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 51/99\n",
      "training Loss: 0.0097 Acc: 84.1736\n",
      "validation Loss: 0.0100 Acc: 83.4762\n",
      "Epoch 52/99\n",
      "training Loss: 0.0096 Acc: 84.2718\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 53/99\n",
      "training Loss: 0.0096 Acc: 84.2182\n",
      "validation Loss: 0.0100 Acc: 83.4881\n",
      "Epoch 54/99\n",
      "training Loss: 0.0096 Acc: 84.1051\n",
      "validation Loss: 0.0100 Acc: 83.4762\n",
      "Epoch 55/99\n",
      "training Loss: 0.0096 Acc: 84.3105\n",
      "validation Loss: 0.0100 Acc: 83.4881\n",
      "Epoch 56/99\n",
      "training Loss: 0.0096 Acc: 84.1974\n",
      "validation Loss: 0.0100 Acc: 83.4762\n",
      "Epoch 57/99\n",
      "training Loss: 0.0096 Acc: 84.2688\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 58/99\n",
      "training Loss: 0.0096 Acc: 84.2063\n",
      "validation Loss: 0.0100 Acc: 83.6071\n",
      "Epoch 59/99\n",
      "training Loss: 0.0096 Acc: 84.0873\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Epoch 60/99\n",
      "training Loss: 0.0096 Acc: 84.2599\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 61/99\n",
      "training Loss: 0.0096 Acc: 84.2480\n",
      "validation Loss: 0.0100 Acc: 83.5952\n",
      "Epoch 62/99\n",
      "training Loss: 0.0096 Acc: 84.2182\n",
      "validation Loss: 0.0100 Acc: 83.5476\n",
      "Epoch 63/99\n",
      "training Loss: 0.0096 Acc: 84.2569\n",
      "validation Loss: 0.0100 Acc: 83.5595\n",
      "Early stopped.\n",
      "Best val acc: 83.702381\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0126 Acc: 80.5071\n",
      "validation Loss: 0.0111 Acc: 82.2738\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0108 Acc: 82.6379\n",
      "validation Loss: 0.0107 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0105 Acc: 82.9713\n",
      "validation Loss: 0.0105 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0104 Acc: 83.0903\n",
      "validation Loss: 0.0104 Acc: 83.0119\n",
      "Epoch 4/99\n",
      "training Loss: 0.0103 Acc: 83.2748\n",
      "validation Loss: 0.0103 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0102 Acc: 83.2361\n",
      "validation Loss: 0.0103 Acc: 83.2381\n",
      "Epoch 6/99\n",
      "training Loss: 0.0102 Acc: 83.2569\n",
      "validation Loss: 0.0103 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0102 Acc: 83.3700\n",
      "validation Loss: 0.0102 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0101 Acc: 83.3462\n",
      "validation Loss: 0.0102 Acc: 83.3333\n",
      "Epoch 9/99\n",
      "training Loss: 0.0101 Acc: 83.4266\n",
      "validation Loss: 0.0102 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0101 Acc: 83.4891\n",
      "validation Loss: 0.0102 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0101 Acc: 83.6022\n",
      "validation Loss: 0.0102 Acc: 83.5238\n",
      "Epoch 12/99\n",
      "training Loss: 0.0100 Acc: 83.4980\n",
      "validation Loss: 0.0102 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0100 Acc: 83.6914\n",
      "validation Loss: 0.0101 Acc: 83.5714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0100 Acc: 83.5099\n",
      "validation Loss: 0.0101 Acc: 83.5595\n",
      "Epoch 15/99\n",
      "training Loss: 0.0100 Acc: 83.5932\n",
      "validation Loss: 0.0101 Acc: 83.5833\n",
      "Epoch 16/99\n",
      "training Loss: 0.0100 Acc: 83.5903\n",
      "validation Loss: 0.0101 Acc: 83.6548\n",
      "Epoch 17/99\n",
      "training Loss: 0.0099 Acc: 83.5694\n",
      "validation Loss: 0.0100 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0099 Acc: 83.6200\n",
      "validation Loss: 0.0100 Acc: 83.5833\n",
      "Epoch 19/99\n",
      "training Loss: 0.0099 Acc: 83.7450\n",
      "validation Loss: 0.0100 Acc: 83.8690\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0099 Acc: 83.7123\n",
      "validation Loss: 0.0100 Acc: 83.7857\n",
      "Epoch 21/99\n",
      "training Loss: 0.0099 Acc: 83.8283\n",
      "validation Loss: 0.0100 Acc: 83.6548\n",
      "Epoch 22/99\n",
      "training Loss: 0.0099 Acc: 83.7123\n",
      "validation Loss: 0.0100 Acc: 83.6190\n",
      "Epoch 23/99\n",
      "training Loss: 0.0099 Acc: 83.7778\n",
      "validation Loss: 0.0100 Acc: 83.6667\n",
      "Epoch 24/99\n",
      "training Loss: 0.0099 Acc: 83.8611\n",
      "validation Loss: 0.0100 Acc: 83.7143\n",
      "Epoch 25/99\n",
      "training Loss: 0.0099 Acc: 83.8432\n",
      "validation Loss: 0.0100 Acc: 83.7619\n",
      "Epoch 26/99\n",
      "training Loss: 0.0098 Acc: 83.8819\n",
      "validation Loss: 0.0100 Acc: 83.7381\n",
      "Epoch 27/99\n",
      "training Loss: 0.0098 Acc: 83.8492\n",
      "validation Loss: 0.0100 Acc: 83.7024\n",
      "Epoch 28/99\n",
      "training Loss: 0.0098 Acc: 83.8611\n",
      "validation Loss: 0.0100 Acc: 83.7500\n",
      "Epoch 29/99\n",
      "training Loss: 0.0098 Acc: 83.9563\n",
      "validation Loss: 0.0100 Acc: 83.7143\n",
      "Epoch 30/99\n",
      "training Loss: 0.0098 Acc: 83.8522\n",
      "validation Loss: 0.0100 Acc: 83.7143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0098 Acc: 83.8789\n",
      "validation Loss: 0.0100 Acc: 83.6905\n",
      "Epoch 32/99\n",
      "training Loss: 0.0098 Acc: 83.9295\n",
      "validation Loss: 0.0100 Acc: 83.6667\n",
      "Epoch 33/99\n",
      "training Loss: 0.0098 Acc: 83.9146\n",
      "validation Loss: 0.0100 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0098 Acc: 83.9236\n",
      "validation Loss: 0.0099 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0098 Acc: 83.9980\n",
      "validation Loss: 0.0099 Acc: 83.8214\n",
      "Epoch 36/99\n",
      "training Loss: 0.0098 Acc: 84.0307\n",
      "validation Loss: 0.0100 Acc: 83.6548\n",
      "Epoch 37/99\n",
      "training Loss: 0.0098 Acc: 84.0396\n",
      "validation Loss: 0.0099 Acc: 83.7976\n",
      "Epoch 38/99\n",
      "training Loss: 0.0097 Acc: 83.9087\n",
      "validation Loss: 0.0100 Acc: 83.8095\n",
      "Epoch 39/99\n",
      "training Loss: 0.0097 Acc: 84.0843\n",
      "validation Loss: 0.0100 Acc: 83.8214\n",
      "Epoch 40/99\n",
      "training Loss: 0.0097 Acc: 84.0277\n",
      "validation Loss: 0.0100 Acc: 83.7857\n",
      "Epoch 41/99\n",
      "training Loss: 0.0097 Acc: 84.1408\n",
      "validation Loss: 0.0099 Acc: 83.8333\n",
      "Epoch 42/99\n",
      "training Loss: 0.0097 Acc: 84.0129\n",
      "validation Loss: 0.0100 Acc: 83.7500\n",
      "Epoch 43/99\n",
      "training Loss: 0.0097 Acc: 84.0962\n",
      "validation Loss: 0.0099 Acc: 83.6786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0097 Acc: 84.0962\n",
      "validation Loss: 0.0100 Acc: 84.0000\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0097 Acc: 84.1230\n",
      "validation Loss: 0.0099 Acc: 83.9048\n",
      "Epoch 46/99\n",
      "training Loss: 0.0097 Acc: 84.0486\n",
      "validation Loss: 0.0100 Acc: 83.8095\n",
      "Epoch 47/99\n",
      "training Loss: 0.0097 Acc: 84.1289\n",
      "validation Loss: 0.0100 Acc: 83.7857\n",
      "Epoch 48/99\n",
      "training Loss: 0.0097 Acc: 84.1378\n",
      "validation Loss: 0.0099 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0097 Acc: 84.1557\n",
      "validation Loss: 0.0099 Acc: 83.8214\n",
      "Epoch 50/99\n",
      "training Loss: 0.0097 Acc: 84.1051\n",
      "validation Loss: 0.0099 Acc: 83.8810\n",
      "Epoch 51/99\n",
      "training Loss: 0.0097 Acc: 84.2182\n",
      "validation Loss: 0.0099 Acc: 83.7500\n",
      "Epoch 52/99\n",
      "training Loss: 0.0097 Acc: 84.1378\n",
      "validation Loss: 0.0099 Acc: 83.9524\n",
      "Epoch 53/99\n",
      "training Loss: 0.0097 Acc: 84.1825\n",
      "validation Loss: 0.0099 Acc: 83.8095\n",
      "Epoch 54/99\n",
      "training Loss: 0.0097 Acc: 84.1081\n",
      "validation Loss: 0.0099 Acc: 83.9881\n",
      "Epoch 55/99\n",
      "training Loss: 0.0097 Acc: 84.1438\n",
      "validation Loss: 0.0099 Acc: 83.9881\n",
      "Epoch 56/99\n",
      "training Loss: 0.0096 Acc: 84.3164\n",
      "validation Loss: 0.0099 Acc: 83.9167\n",
      "Epoch 57/99\n",
      "training Loss: 0.0096 Acc: 84.1170\n",
      "validation Loss: 0.0100 Acc: 83.8333\n",
      "Epoch 58/99\n",
      "training Loss: 0.0096 Acc: 84.2480\n",
      "validation Loss: 0.0099 Acc: 83.8929\n",
      "Epoch 59/99\n",
      "training Loss: 0.0096 Acc: 84.2420\n",
      "validation Loss: 0.0099 Acc: 83.9048\n",
      "Epoch 60/99\n",
      "training Loss: 0.0096 Acc: 84.1706\n",
      "validation Loss: 0.0099 Acc: 83.8929\n",
      "Epoch 61/99\n",
      "training Loss: 0.0096 Acc: 84.2390\n",
      "validation Loss: 0.0099 Acc: 83.8690\n",
      "Epoch 62/99\n",
      "training Loss: 0.0096 Acc: 84.2242\n",
      "validation Loss: 0.0099 Acc: 83.9048\n",
      "Epoch 63/99\n",
      "training Loss: 0.0096 Acc: 84.0873\n",
      "validation Loss: 0.0099 Acc: 83.9167\n",
      "Epoch 64/99\n",
      "training Loss: 0.0096 Acc: 84.2242\n",
      "validation Loss: 0.0099 Acc: 83.9167\n",
      "Epoch 65/99\n",
      "training Loss: 0.0096 Acc: 84.2182\n",
      "validation Loss: 0.0099 Acc: 83.9405\n",
      "Epoch 66/99\n",
      "training Loss: 0.0096 Acc: 84.2628\n",
      "validation Loss: 0.0099 Acc: 83.8929\n",
      "Epoch 67/99\n",
      "training Loss: 0.0096 Acc: 84.1646\n",
      "validation Loss: 0.0099 Acc: 83.8690\n",
      "Epoch 68/99\n",
      "training Loss: 0.0096 Acc: 84.1706\n",
      "validation Loss: 0.0099 Acc: 83.9048\n",
      "Early stopped.\n",
      "Best val acc: 84.059524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0127 Acc: 80.2988\n",
      "validation Loss: 0.0110 Acc: 82.0714\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0108 Acc: 82.6945\n",
      "validation Loss: 0.0106 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0105 Acc: 82.9713\n",
      "validation Loss: 0.0104 Acc: 82.6548\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0104 Acc: 83.1587\n",
      "validation Loss: 0.0104 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0103 Acc: 83.0903\n",
      "validation Loss: 0.0103 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0102 Acc: 83.3165\n",
      "validation Loss: 0.0102 Acc: 83.0714\n",
      "Epoch 6/99\n",
      "training Loss: 0.0102 Acc: 83.3046\n",
      "validation Loss: 0.0102 Acc: 83.1667\n",
      "Epoch 7/99\n",
      "training Loss: 0.0102 Acc: 83.4742\n",
      "validation Loss: 0.0101 Acc: 83.1548\n",
      "Epoch 8/99\n",
      "training Loss: 0.0101 Acc: 83.5069\n",
      "validation Loss: 0.0101 Acc: 83.0238\n",
      "Epoch 9/99\n",
      "training Loss: 0.0101 Acc: 83.5426\n",
      "validation Loss: 0.0101 Acc: 83.2024\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0101 Acc: 83.5040\n",
      "validation Loss: 0.0101 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0101 Acc: 83.6498\n",
      "validation Loss: 0.0101 Acc: 83.2381\n",
      "Epoch 12/99\n",
      "training Loss: 0.0100 Acc: 83.6676\n",
      "validation Loss: 0.0101 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0100 Acc: 83.7153\n",
      "validation Loss: 0.0100 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0100 Acc: 83.6051\n",
      "validation Loss: 0.0101 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0100 Acc: 83.7093\n",
      "validation Loss: 0.0100 Acc: 83.3929\n",
      "Epoch 16/99\n",
      "training Loss: 0.0100 Acc: 83.7242\n",
      "validation Loss: 0.0100 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0099 Acc: 83.7629\n",
      "validation Loss: 0.0100 Acc: 83.1786\n",
      "Epoch 18/99\n",
      "training Loss: 0.0099 Acc: 83.7956\n",
      "validation Loss: 0.0100 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0099 Acc: 83.7658\n",
      "validation Loss: 0.0100 Acc: 83.4524\n",
      "Epoch 20/99\n",
      "training Loss: 0.0099 Acc: 83.8283\n",
      "validation Loss: 0.0100 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0099 Acc: 83.8105\n",
      "validation Loss: 0.0100 Acc: 83.3929\n",
      "Epoch 22/99\n",
      "training Loss: 0.0099 Acc: 83.7480\n",
      "validation Loss: 0.0100 Acc: 83.4286\n",
      "Epoch 23/99\n",
      "training Loss: 0.0099 Acc: 83.8254\n",
      "validation Loss: 0.0100 Acc: 83.3810\n",
      "Epoch 24/99\n",
      "training Loss: 0.0099 Acc: 83.8700\n",
      "validation Loss: 0.0100 Acc: 83.4167\n",
      "Epoch 25/99\n",
      "training Loss: 0.0099 Acc: 83.9414\n",
      "validation Loss: 0.0100 Acc: 83.4881\n",
      "Epoch 26/99\n",
      "training Loss: 0.0098 Acc: 83.9414\n",
      "validation Loss: 0.0100 Acc: 83.3810\n",
      "Epoch 27/99\n",
      "training Loss: 0.0098 Acc: 83.8551\n",
      "validation Loss: 0.0099 Acc: 83.5119\n",
      "Epoch 28/99\n",
      "training Loss: 0.0098 Acc: 83.9355\n",
      "validation Loss: 0.0099 Acc: 83.3929\n",
      "Epoch 29/99\n",
      "training Loss: 0.0098 Acc: 83.9861\n",
      "validation Loss: 0.0099 Acc: 83.4881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0098 Acc: 84.0158\n",
      "validation Loss: 0.0099 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0098 Acc: 84.0783\n",
      "validation Loss: 0.0099 Acc: 83.4881\n",
      "Epoch 32/99\n",
      "training Loss: 0.0098 Acc: 84.0634\n",
      "validation Loss: 0.0099 Acc: 83.6071\n",
      "Epoch 33/99\n",
      "training Loss: 0.0098 Acc: 84.1111\n",
      "validation Loss: 0.0099 Acc: 83.5833\n",
      "Epoch 34/99\n",
      "training Loss: 0.0098 Acc: 83.9861\n",
      "validation Loss: 0.0099 Acc: 83.5357\n",
      "Epoch 35/99\n",
      "training Loss: 0.0098 Acc: 84.1200\n",
      "validation Loss: 0.0099 Acc: 83.5357\n",
      "Epoch 36/99\n",
      "training Loss: 0.0098 Acc: 84.1736\n",
      "validation Loss: 0.0099 Acc: 83.5833\n",
      "Epoch 37/99\n",
      "training Loss: 0.0098 Acc: 84.1944\n",
      "validation Loss: 0.0099 Acc: 83.4643\n",
      "Epoch 38/99\n",
      "training Loss: 0.0098 Acc: 84.0456\n",
      "validation Loss: 0.0099 Acc: 83.4643\n",
      "Epoch 39/99\n",
      "training Loss: 0.0097 Acc: 84.1349\n",
      "validation Loss: 0.0099 Acc: 83.4167\n",
      "Epoch 40/99\n",
      "training Loss: 0.0097 Acc: 84.0545\n",
      "validation Loss: 0.0099 Acc: 83.4524\n",
      "Epoch 41/99\n",
      "training Loss: 0.0097 Acc: 84.0843\n",
      "validation Loss: 0.0099 Acc: 83.5476\n",
      "Epoch 42/99\n",
      "training Loss: 0.0097 Acc: 84.1111\n",
      "validation Loss: 0.0099 Acc: 83.5833\n",
      "Epoch 43/99\n",
      "training Loss: 0.0097 Acc: 84.2212\n",
      "validation Loss: 0.0099 Acc: 83.5238\n",
      "Epoch 44/99\n",
      "training Loss: 0.0097 Acc: 84.2747\n",
      "validation Loss: 0.0099 Acc: 83.5357\n",
      "Epoch 45/99\n",
      "training Loss: 0.0097 Acc: 84.1021\n",
      "validation Loss: 0.0099 Acc: 83.5476\n",
      "Epoch 46/99\n",
      "training Loss: 0.0097 Acc: 84.2122\n",
      "validation Loss: 0.0099 Acc: 83.5357\n",
      "Epoch 47/99\n",
      "training Loss: 0.0097 Acc: 84.2777\n",
      "validation Loss: 0.0099 Acc: 83.6190\n",
      "Epoch 48/99\n",
      "training Loss: 0.0097 Acc: 84.2152\n",
      "validation Loss: 0.0099 Acc: 83.6310\n",
      "Epoch 49/99\n",
      "training Loss: 0.0097 Acc: 84.1795\n",
      "validation Loss: 0.0099 Acc: 83.6548\n",
      "Epoch 50/99\n",
      "training Loss: 0.0097 Acc: 84.2688\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0097 Acc: 84.2628\n",
      "validation Loss: 0.0099 Acc: 83.7143\n",
      "Epoch 52/99\n",
      "training Loss: 0.0097 Acc: 84.2569\n",
      "validation Loss: 0.0099 Acc: 83.7024\n",
      "Epoch 53/99\n",
      "training Loss: 0.0097 Acc: 84.2986\n",
      "validation Loss: 0.0099 Acc: 83.5952\n",
      "Epoch 54/99\n",
      "training Loss: 0.0097 Acc: 84.3045\n",
      "validation Loss: 0.0099 Acc: 83.6548\n",
      "Epoch 55/99\n",
      "training Loss: 0.0097 Acc: 84.2718\n",
      "validation Loss: 0.0099 Acc: 83.7143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0097 Acc: 84.2182\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Epoch 57/99\n",
      "training Loss: 0.0097 Acc: 84.3789\n",
      "validation Loss: 0.0099 Acc: 83.5952\n",
      "Epoch 58/99\n",
      "training Loss: 0.0097 Acc: 84.2807\n",
      "validation Loss: 0.0099 Acc: 83.7024\n",
      "Epoch 59/99\n",
      "training Loss: 0.0096 Acc: 84.2926\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Epoch 60/99\n",
      "training Loss: 0.0097 Acc: 84.3164\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0096 Acc: 84.2956\n",
      "validation Loss: 0.0099 Acc: 83.6667\n",
      "Epoch 62/99\n",
      "training Loss: 0.0097 Acc: 84.2063\n",
      "validation Loss: 0.0099 Acc: 83.6905\n",
      "Epoch 63/99\n",
      "training Loss: 0.0096 Acc: 84.3253\n",
      "validation Loss: 0.0099 Acc: 83.6071\n",
      "Epoch 64/99\n",
      "training Loss: 0.0096 Acc: 84.3283\n",
      "validation Loss: 0.0099 Acc: 83.5952\n",
      "Epoch 65/99\n",
      "training Loss: 0.0096 Acc: 84.3432\n",
      "validation Loss: 0.0099 Acc: 83.5833\n",
      "Epoch 66/99\n",
      "training Loss: 0.0096 Acc: 84.2747\n",
      "validation Loss: 0.0099 Acc: 83.6905\n",
      "Epoch 67/99\n",
      "training Loss: 0.0097 Acc: 84.2837\n",
      "validation Loss: 0.0099 Acc: 83.6667\n",
      "Epoch 68/99\n",
      "training Loss: 0.0096 Acc: 84.2658\n",
      "validation Loss: 0.0099 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 69/99\n",
      "training Loss: 0.0096 Acc: 84.2896\n",
      "validation Loss: 0.0099 Acc: 83.7143\n",
      "Epoch 70/99\n",
      "training Loss: 0.0096 Acc: 84.3105\n",
      "validation Loss: 0.0099 Acc: 83.7024\n",
      "Epoch 71/99\n",
      "training Loss: 0.0096 Acc: 84.3938\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Epoch 72/99\n",
      "training Loss: 0.0096 Acc: 84.3283\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Epoch 73/99\n",
      "training Loss: 0.0096 Acc: 84.2301\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Epoch 74/99\n",
      "training Loss: 0.0096 Acc: 84.3670\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Epoch 75/99\n",
      "training Loss: 0.0096 Acc: 84.3759\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Epoch 76/99\n",
      "training Loss: 0.0096 Acc: 84.3997\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Epoch 77/99\n",
      "training Loss: 0.0096 Acc: 84.3997\n",
      "validation Loss: 0.0099 Acc: 83.7143\n",
      "Epoch 78/99\n",
      "training Loss: 0.0096 Acc: 84.2509\n",
      "validation Loss: 0.0099 Acc: 83.7024\n",
      "Epoch 79/99\n",
      "training Loss: 0.0096 Acc: 84.3224\n",
      "validation Loss: 0.0099 Acc: 83.7143\n",
      "Epoch 80/99\n",
      "training Loss: 0.0096 Acc: 84.3372\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Epoch 81/99\n",
      "training Loss: 0.0096 Acc: 84.3402\n",
      "validation Loss: 0.0099 Acc: 83.7262\n",
      "Epoch 82/99\n",
      "training Loss: 0.0096 Acc: 84.4176\n",
      "validation Loss: 0.0099 Acc: 83.6905\n",
      "Epoch 83/99\n",
      "training Loss: 0.0096 Acc: 84.2986\n",
      "validation Loss: 0.0099 Acc: 83.6905\n",
      "Epoch 84/99\n",
      "training Loss: 0.0096 Acc: 84.3402\n",
      "validation Loss: 0.0099 Acc: 83.7143\n",
      "Epoch 85/99\n",
      "training Loss: 0.0096 Acc: 84.3521\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Epoch 86/99\n",
      "training Loss: 0.0096 Acc: 84.3610\n",
      "validation Loss: 0.0099 Acc: 83.7381\n",
      "Epoch 87/99\n",
      "training Loss: 0.0096 Acc: 84.3402\n",
      "validation Loss: 0.0099 Acc: 83.7500\n",
      "Epoch 88/99\n",
      "training Loss: 0.0096 Acc: 84.3283\n",
      "validation Loss: 0.0099 Acc: 83.7619\n",
      "Early stopped.\n",
      "Best val acc: 83.773810\n",
      "----------\n",
      "Average best_acc across k-fold: 83.7888677299\n",
      "New configuration: {'learning_rate': 0.0011097572929211351, 'initial_nodes': 200, 'dropout': 0.5592130559750618, 'batch_size': 512, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 80.1815\n",
      "validation Loss: 0.0008 Acc: 82.2542\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 82.9792\n",
      "validation Loss: 0.0008 Acc: 82.6708\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.3601\n",
      "validation Loss: 0.0008 Acc: 82.8850\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.4524\n",
      "validation Loss: 0.0007 Acc: 83.0040\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.5268\n",
      "validation Loss: 0.0008 Acc: 83.0279\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.6310\n",
      "validation Loss: 0.0007 Acc: 82.8969\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.4911\n",
      "validation Loss: 0.0007 Acc: 83.3016\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6399\n",
      "validation Loss: 0.0007 Acc: 83.1588\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.6190\n",
      "validation Loss: 0.0007 Acc: 83.2540\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.8095\n",
      "validation Loss: 0.0007 Acc: 83.2183\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.7113\n",
      "validation Loss: 0.0007 Acc: 83.1826\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.8869\n",
      "validation Loss: 0.0007 Acc: 83.4087\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.8690\n",
      "validation Loss: 0.0007 Acc: 83.4206\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.0833\n",
      "validation Loss: 0.0007 Acc: 83.4801\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.9643\n",
      "validation Loss: 0.0007 Acc: 83.5039\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.0982\n",
      "validation Loss: 0.0007 Acc: 83.4563\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.0387\n",
      "validation Loss: 0.0007 Acc: 83.4682\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.1042\n",
      "validation Loss: 0.0007 Acc: 83.5277\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.9613\n",
      "validation Loss: 0.0007 Acc: 83.4682\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.1577\n",
      "validation Loss: 0.0007 Acc: 83.3492\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.1458\n",
      "validation Loss: 0.0007 Acc: 83.4682\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.0774\n",
      "validation Loss: 0.0007 Acc: 83.7063\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.0774\n",
      "validation Loss: 0.0007 Acc: 83.3730\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.0982\n",
      "validation Loss: 0.0007 Acc: 83.4444\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.2113\n",
      "validation Loss: 0.0007 Acc: 83.4444\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.2173\n",
      "validation Loss: 0.0007 Acc: 83.3849\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.2411\n",
      "validation Loss: 0.0007 Acc: 83.6229\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.1786\n",
      "validation Loss: 0.0007 Acc: 83.4563\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.3214\n",
      "validation Loss: 0.0007 Acc: 83.3492\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.3185\n",
      "validation Loss: 0.0007 Acc: 83.3849\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.3185\n",
      "validation Loss: 0.0007 Acc: 83.4206\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.2857\n",
      "validation Loss: 0.0007 Acc: 83.3968\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.4048\n",
      "validation Loss: 0.0007 Acc: 83.5158\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.4226\n",
      "validation Loss: 0.0007 Acc: 83.4444\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.4464\n",
      "validation Loss: 0.0007 Acc: 83.6110\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.3333\n",
      "validation Loss: 0.0007 Acc: 83.5515\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.4226\n",
      "validation Loss: 0.0007 Acc: 83.5991\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.4673\n",
      "validation Loss: 0.0007 Acc: 83.6348\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 84.5298\n",
      "validation Loss: 0.0007 Acc: 83.4206\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 84.4702\n",
      "validation Loss: 0.0007 Acc: 83.5277\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 84.6042\n",
      "validation Loss: 0.0007 Acc: 83.4206\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 84.7321\n",
      "validation Loss: 0.0007 Acc: 83.4801\n",
      "Early stopped.\n",
      "Best val acc: 83.706260\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 80.0131\n",
      "validation Loss: 0.0008 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 82.8552\n",
      "validation Loss: 0.0007 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.1081\n",
      "validation Loss: 0.0007 Acc: 84.4524\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.2272\n",
      "validation Loss: 0.0007 Acc: 84.4881\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.2450\n",
      "validation Loss: 0.0007 Acc: 84.5595\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.2718\n",
      "validation Loss: 0.0007 Acc: 84.4048\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.4325\n",
      "validation Loss: 0.0007 Acc: 84.5595\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0007 Acc: 84.3929\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.4712\n",
      "validation Loss: 0.0007 Acc: 84.5714\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.4772\n",
      "validation Loss: 0.0007 Acc: 84.6667\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0007 Acc: 84.4167\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.5813\n",
      "validation Loss: 0.0007 Acc: 84.7500\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.6141\n",
      "validation Loss: 0.0007 Acc: 84.5238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.6647\n",
      "validation Loss: 0.0007 Acc: 84.7738\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.6855\n",
      "validation Loss: 0.0007 Acc: 84.6667\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.8016\n",
      "validation Loss: 0.0007 Acc: 84.6190\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.6885\n",
      "validation Loss: 0.0007 Acc: 84.4881\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.8135\n",
      "validation Loss: 0.0007 Acc: 84.5119\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.7182\n",
      "validation Loss: 0.0007 Acc: 84.6310\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.7867\n",
      "validation Loss: 0.0007 Acc: 84.5833\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.7510\n",
      "validation Loss: 0.0007 Acc: 84.8571\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.8998\n",
      "validation Loss: 0.0007 Acc: 84.6310\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.0248\n",
      "validation Loss: 0.0007 Acc: 84.6667\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.9444\n",
      "validation Loss: 0.0007 Acc: 84.5476\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.8700\n",
      "validation Loss: 0.0007 Acc: 84.6786\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.9355\n",
      "validation Loss: 0.0007 Acc: 84.6548\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.0664\n",
      "validation Loss: 0.0007 Acc: 84.6429\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.2033\n",
      "validation Loss: 0.0007 Acc: 84.6786\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.1557\n",
      "validation Loss: 0.0007 Acc: 84.5714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.1259\n",
      "validation Loss: 0.0007 Acc: 84.4762\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.1289\n",
      "validation Loss: 0.0007 Acc: 84.5833\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.1200\n",
      "validation Loss: 0.0007 Acc: 84.7143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.1825\n",
      "validation Loss: 0.0007 Acc: 84.7024\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.0902\n",
      "validation Loss: 0.0007 Acc: 84.5238\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.1468\n",
      "validation Loss: 0.0007 Acc: 84.6190\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.2599\n",
      "validation Loss: 0.0007 Acc: 84.6905\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.3224\n",
      "validation Loss: 0.0007 Acc: 84.5238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.3075\n",
      "validation Loss: 0.0007 Acc: 84.4048\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 84.1884\n",
      "validation Loss: 0.0007 Acc: 84.5476\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 84.3283\n",
      "validation Loss: 0.0007 Acc: 84.5833\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 84.2866\n",
      "validation Loss: 0.0007 Acc: 84.7024\n",
      "Early stopped.\n",
      "Best val acc: 84.857143\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 79.9238\n",
      "validation Loss: 0.0008 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.1736\n",
      "validation Loss: 0.0007 Acc: 82.7024\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.4147\n",
      "validation Loss: 0.0007 Acc: 82.8214\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.6260\n",
      "validation Loss: 0.0007 Acc: 82.8452\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.7063\n",
      "validation Loss: 0.0007 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.8105\n",
      "validation Loss: 0.0007 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.8373\n",
      "validation Loss: 0.0007 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.7599\n",
      "validation Loss: 0.0007 Acc: 82.9048\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.9593\n",
      "validation Loss: 0.0007 Acc: 83.0476\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.9176\n",
      "validation Loss: 0.0007 Acc: 83.1548\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.9087\n",
      "validation Loss: 0.0007 Acc: 82.7024\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 84.0010\n",
      "validation Loss: 0.0007 Acc: 82.8929\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 84.0545\n",
      "validation Loss: 0.0007 Acc: 83.1786\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 84.0694\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 84.0426\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 84.0099\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.1438\n",
      "validation Loss: 0.0007 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.0337\n",
      "validation Loss: 0.0007 Acc: 83.1548\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.1914\n",
      "validation Loss: 0.0007 Acc: 83.1190\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.1259\n",
      "validation Loss: 0.0007 Acc: 83.0833\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.1438\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.1587\n",
      "validation Loss: 0.0007 Acc: 83.2024\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.3164\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.1736\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.2896\n",
      "validation Loss: 0.0007 Acc: 83.2024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.3759\n",
      "validation Loss: 0.0007 Acc: 83.0833\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.2599\n",
      "validation Loss: 0.0007 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.3164\n",
      "validation Loss: 0.0007 Acc: 83.0357\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.4087\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.3759\n",
      "validation Loss: 0.0007 Acc: 83.2619\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.4355\n",
      "validation Loss: 0.0007 Acc: 83.1667\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.5039\n",
      "validation Loss: 0.0007 Acc: 83.1429\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.5366\n",
      "validation Loss: 0.0007 Acc: 83.2143\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.6735\n",
      "validation Loss: 0.0007 Acc: 83.1548\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.5843\n",
      "validation Loss: 0.0007 Acc: 83.2024\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.4116\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.6319\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.5515\n",
      "validation Loss: 0.0007 Acc: 83.2500\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 84.6289\n",
      "validation Loss: 0.0007 Acc: 83.0595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 84.5337\n",
      "validation Loss: 0.0007 Acc: 83.1071\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 84.5337\n",
      "validation Loss: 0.0007 Acc: 83.2143\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 84.6497\n",
      "validation Loss: 0.0007 Acc: 83.2857\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 84.7063\n",
      "validation Loss: 0.0007 Acc: 83.1310\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 84.7033\n",
      "validation Loss: 0.0007 Acc: 83.3452\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 84.6646\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 84.7628\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 84.6200\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 84.7479\n",
      "validation Loss: 0.0007 Acc: 83.3214\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 84.8313\n",
      "validation Loss: 0.0007 Acc: 83.2738\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 84.6795\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Epoch 50/99\n",
      "training Loss: 0.0007 Acc: 84.7331\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 51/99\n",
      "training Loss: 0.0007 Acc: 84.6735\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Epoch 52/99\n",
      "training Loss: 0.0007 Acc: 84.7390\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 53/99\n",
      "training Loss: 0.0007 Acc: 84.7628\n",
      "validation Loss: 0.0007 Acc: 83.3690\n",
      "Epoch 54/99\n",
      "training Loss: 0.0007 Acc: 84.7122\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0007 Acc: 84.7807\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 56/99\n",
      "training Loss: 0.0007 Acc: 84.8432\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 57/99\n",
      "training Loss: 0.0007 Acc: 84.7569\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 58/99\n",
      "training Loss: 0.0007 Acc: 84.7628\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 59/99\n",
      "training Loss: 0.0007 Acc: 84.8491\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Saving..\n",
      "Epoch 60/99\n",
      "training Loss: 0.0007 Acc: 84.7777\n",
      "validation Loss: 0.0007 Acc: 83.3690\n",
      "Epoch 61/99\n",
      "training Loss: 0.0007 Acc: 84.6438\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 62/99\n",
      "training Loss: 0.0007 Acc: 84.8283\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 63/99\n",
      "training Loss: 0.0007 Acc: 84.7360\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 64/99\n",
      "training Loss: 0.0007 Acc: 84.8015\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 65/99\n",
      "training Loss: 0.0007 Acc: 84.7598\n",
      "validation Loss: 0.0007 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0007 Acc: 84.8878\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 67/99\n",
      "training Loss: 0.0007 Acc: 84.7152\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 68/99\n",
      "training Loss: 0.0007 Acc: 84.7896\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 69/99\n",
      "training Loss: 0.0007 Acc: 84.7717\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 70/99\n",
      "training Loss: 0.0007 Acc: 84.7450\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 71/99\n",
      "training Loss: 0.0007 Acc: 84.8194\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 72/99\n",
      "training Loss: 0.0007 Acc: 84.8164\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 73/99\n",
      "training Loss: 0.0007 Acc: 84.6765\n",
      "validation Loss: 0.0007 Acc: 83.3690\n",
      "Epoch 74/99\n",
      "training Loss: 0.0007 Acc: 84.8342\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 75/99\n",
      "training Loss: 0.0007 Acc: 84.8313\n",
      "validation Loss: 0.0007 Acc: 83.3690\n",
      "Epoch 76/99\n",
      "training Loss: 0.0007 Acc: 84.8521\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 77/99\n",
      "training Loss: 0.0007 Acc: 84.7658\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 78/99\n",
      "training Loss: 0.0007 Acc: 84.7033\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 79/99\n",
      "training Loss: 0.0007 Acc: 84.8461\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 80/99\n",
      "training Loss: 0.0007 Acc: 84.8194\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 81/99\n",
      "training Loss: 0.0007 Acc: 84.7539\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 82/99\n",
      "training Loss: 0.0007 Acc: 84.7033\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 83/99\n",
      "training Loss: 0.0007 Acc: 84.8610\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 84/99\n",
      "training Loss: 0.0007 Acc: 84.8729\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 85/99\n",
      "training Loss: 0.0007 Acc: 84.7598\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Early stopped.\n",
      "Best val acc: 83.440476\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 79.4149\n",
      "validation Loss: 0.0008 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.7600\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.1736\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.2897\n",
      "validation Loss: 0.0007 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.5099\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.5605\n",
      "validation Loss: 0.0007 Acc: 83.7738\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.4921\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.6200\n",
      "validation Loss: 0.0007 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.6676\n",
      "validation Loss: 0.0007 Acc: 83.9643\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.7420\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.7986\n",
      "validation Loss: 0.0007 Acc: 84.0238\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.7956\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.9117\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.7867\n",
      "validation Loss: 0.0007 Acc: 83.7024\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.8789\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.9980\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.8730\n",
      "validation Loss: 0.0007 Acc: 83.7976\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.0724\n",
      "validation Loss: 0.0007 Acc: 83.8095\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.0069\n",
      "validation Loss: 0.0007 Acc: 83.8690\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.9355\n",
      "validation Loss: 0.0007 Acc: 83.8214\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.0545\n",
      "validation Loss: 0.0007 Acc: 83.9405\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.0069\n",
      "validation Loss: 0.0007 Acc: 83.9643\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.1200\n",
      "validation Loss: 0.0007 Acc: 84.0000\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.0932\n",
      "validation Loss: 0.0007 Acc: 83.8929\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.0486\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.1349\n",
      "validation Loss: 0.0007 Acc: 83.8929\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.1706\n",
      "validation Loss: 0.0007 Acc: 83.8333\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.3610\n",
      "validation Loss: 0.0007 Acc: 83.8214\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.1557\n",
      "validation Loss: 0.0007 Acc: 83.8214\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.2361\n",
      "validation Loss: 0.0007 Acc: 83.9167\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.1974\n",
      "validation Loss: 0.0007 Acc: 83.8690\n",
      "Early stopped.\n",
      "Best val acc: 84.023810\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 79.9119\n",
      "validation Loss: 0.0008 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0007 Acc: 83.0605\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0007 Acc: 83.2034\n",
      "validation Loss: 0.0007 Acc: 83.2381\n",
      "Epoch 3/99\n",
      "training Loss: 0.0007 Acc: 83.4236\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 4/99\n",
      "training Loss: 0.0007 Acc: 83.5873\n",
      "validation Loss: 0.0007 Acc: 83.4405\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0007 Acc: 83.6885\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 6/99\n",
      "training Loss: 0.0007 Acc: 83.5754\n",
      "validation Loss: 0.0007 Acc: 83.2500\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.7599\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.8164\n",
      "validation Loss: 0.0007 Acc: 83.2381\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.7301\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.8611\n",
      "validation Loss: 0.0007 Acc: 83.4881\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.9504\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.9325\n",
      "validation Loss: 0.0007 Acc: 83.5238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.9593\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.9444\n",
      "validation Loss: 0.0007 Acc: 83.4643\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.9146\n",
      "validation Loss: 0.0007 Acc: 83.3452\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 84.0664\n",
      "validation Loss: 0.0007 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 84.0783\n",
      "validation Loss: 0.0007 Acc: 83.6905\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 84.0367\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 84.0337\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 84.0456\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 84.1527\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 84.0069\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 84.1587\n",
      "validation Loss: 0.0007 Acc: 83.6905\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 84.1200\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 84.1914\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 84.2003\n",
      "validation Loss: 0.0007 Acc: 83.3214\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 84.2271\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 84.4831\n",
      "validation Loss: 0.0007 Acc: 83.5714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 84.3968\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 84.4146\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 84.3610\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 84.4563\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 84.4176\n",
      "validation Loss: 0.0007 Acc: 83.5238\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 84.5962\n",
      "validation Loss: 0.0007 Acc: 83.5833\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 84.6021\n",
      "validation Loss: 0.0007 Acc: 83.7024\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 84.5247\n",
      "validation Loss: 0.0007 Acc: 83.5357\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 84.3819\n",
      "validation Loss: 0.0007 Acc: 83.6190\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 84.5426\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 84.6706\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 84.5723\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 84.5902\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 84.4950\n",
      "validation Loss: 0.0007 Acc: 83.6190\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 84.6140\n",
      "validation Loss: 0.0007 Acc: 83.7619\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 84.5991\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 84.6527\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 84.5247\n",
      "validation Loss: 0.0007 Acc: 83.6190\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 84.5843\n",
      "validation Loss: 0.0007 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 84.6616\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 84.6021\n",
      "validation Loss: 0.0007 Acc: 83.6786\n",
      "Epoch 50/99\n",
      "training Loss: 0.0007 Acc: 84.5575\n",
      "validation Loss: 0.0007 Acc: 83.6190\n",
      "Epoch 51/99\n",
      "training Loss: 0.0007 Acc: 84.6378\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 52/99\n",
      "training Loss: 0.0007 Acc: 84.6973\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0007 Acc: 84.6051\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 54/99\n",
      "training Loss: 0.0007 Acc: 84.5753\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Epoch 55/99\n",
      "training Loss: 0.0007 Acc: 84.7241\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 56/99\n",
      "training Loss: 0.0007 Acc: 84.6527\n",
      "validation Loss: 0.0007 Acc: 83.5952\n",
      "Epoch 57/99\n",
      "training Loss: 0.0007 Acc: 84.7211\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 58/99\n",
      "training Loss: 0.0007 Acc: 84.7182\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 59/99\n",
      "training Loss: 0.0007 Acc: 84.7122\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 60/99\n",
      "training Loss: 0.0007 Acc: 84.8670\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 61/99\n",
      "training Loss: 0.0007 Acc: 84.6973\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 62/99\n",
      "training Loss: 0.0007 Acc: 84.8223\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 63/99\n",
      "training Loss: 0.0007 Acc: 84.8848\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 64/99\n",
      "training Loss: 0.0007 Acc: 84.7866\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Epoch 65/99\n",
      "training Loss: 0.0007 Acc: 84.7926\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 66/99\n",
      "training Loss: 0.0007 Acc: 84.7063\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 67/99\n",
      "training Loss: 0.0007 Acc: 84.7509\n",
      "validation Loss: 0.0007 Acc: 83.6429\n",
      "Epoch 68/99\n",
      "training Loss: 0.0007 Acc: 84.8372\n",
      "validation Loss: 0.0007 Acc: 83.6905\n",
      "Epoch 69/99\n",
      "training Loss: 0.0007 Acc: 84.7390\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 70/99\n",
      "training Loss: 0.0007 Acc: 84.7628\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Epoch 71/99\n",
      "training Loss: 0.0007 Acc: 84.8342\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 72/99\n",
      "training Loss: 0.0007 Acc: 84.7569\n",
      "validation Loss: 0.0007 Acc: 83.6905\n",
      "Early stopped.\n",
      "Best val acc: 83.785714\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9626806543\n",
      "New configuration: {'learning_rate': 1.6223510741508284e-05, 'initial_nodes': 1000, 'dropout': 0.1890425559490671, 'batch_size': 130, 'max_depth': 2}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 78.1161\n",
      "validation Loss: 0.0031 Acc: 82.1471\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 82.1875\n",
      "validation Loss: 0.0029 Acc: 82.8255\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 82.8452\n",
      "validation Loss: 0.0029 Acc: 82.8731\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 82.8363\n",
      "validation Loss: 0.0028 Acc: 83.0993\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0029 Acc: 83.1131\n",
      "validation Loss: 0.0028 Acc: 83.4801\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0029 Acc: 83.1815\n",
      "validation Loss: 0.0028 Acc: 83.3968\n",
      "Epoch 6/99\n",
      "training Loss: 0.0028 Acc: 83.1250\n",
      "validation Loss: 0.0028 Acc: 83.5158\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0028 Acc: 83.3512\n",
      "validation Loss: 0.0028 Acc: 83.6944\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0028 Acc: 83.3720\n",
      "validation Loss: 0.0028 Acc: 83.6587\n",
      "Epoch 9/99\n",
      "training Loss: 0.0028 Acc: 83.4048\n",
      "validation Loss: 0.0028 Acc: 83.6944\n",
      "Epoch 10/99\n",
      "training Loss: 0.0028 Acc: 83.5357\n",
      "validation Loss: 0.0028 Acc: 83.8491\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0028 Acc: 83.5060\n",
      "validation Loss: 0.0028 Acc: 83.8848\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0028 Acc: 83.4435\n",
      "validation Loss: 0.0028 Acc: 83.8967\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0028 Acc: 83.5417\n",
      "validation Loss: 0.0028 Acc: 83.6348\n",
      "Epoch 14/99\n",
      "training Loss: 0.0028 Acc: 83.7708\n",
      "validation Loss: 0.0027 Acc: 83.9205\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0028 Acc: 83.7589\n",
      "validation Loss: 0.0027 Acc: 83.8967\n",
      "Epoch 16/99\n",
      "training Loss: 0.0028 Acc: 83.7113\n",
      "validation Loss: 0.0027 Acc: 84.0633\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0028 Acc: 83.6101\n",
      "validation Loss: 0.0027 Acc: 83.9562\n",
      "Epoch 18/99\n",
      "training Loss: 0.0028 Acc: 83.8690\n",
      "validation Loss: 0.0027 Acc: 83.8134\n",
      "Epoch 19/99\n",
      "training Loss: 0.0028 Acc: 83.8006\n",
      "validation Loss: 0.0027 Acc: 83.9681\n",
      "Epoch 20/99\n",
      "training Loss: 0.0027 Acc: 83.8512\n",
      "validation Loss: 0.0027 Acc: 84.0395\n",
      "Epoch 21/99\n",
      "training Loss: 0.0027 Acc: 83.8393\n",
      "validation Loss: 0.0027 Acc: 83.9324\n",
      "Epoch 22/99\n",
      "training Loss: 0.0027 Acc: 83.9256\n",
      "validation Loss: 0.0027 Acc: 83.9562\n",
      "Epoch 23/99\n",
      "training Loss: 0.0027 Acc: 83.8839\n",
      "validation Loss: 0.0027 Acc: 83.9443\n",
      "Epoch 24/99\n",
      "training Loss: 0.0027 Acc: 83.8333\n",
      "validation Loss: 0.0027 Acc: 84.0276\n",
      "Epoch 25/99\n",
      "training Loss: 0.0027 Acc: 83.8988\n",
      "validation Loss: 0.0027 Acc: 84.0157\n",
      "Epoch 26/99\n",
      "training Loss: 0.0027 Acc: 83.9494\n",
      "validation Loss: 0.0027 Acc: 83.8729\n",
      "Epoch 27/99\n",
      "training Loss: 0.0027 Acc: 83.9762\n",
      "validation Loss: 0.0027 Acc: 84.1466\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0027 Acc: 84.0625\n",
      "validation Loss: 0.0027 Acc: 83.9800\n",
      "Epoch 29/99\n",
      "training Loss: 0.0027 Acc: 84.0208\n",
      "validation Loss: 0.0027 Acc: 83.8729\n",
      "Epoch 30/99\n",
      "training Loss: 0.0027 Acc: 84.0268\n",
      "validation Loss: 0.0027 Acc: 83.9800\n",
      "Epoch 31/99\n",
      "training Loss: 0.0027 Acc: 84.0268\n",
      "validation Loss: 0.0027 Acc: 84.1228\n",
      "Epoch 32/99\n",
      "training Loss: 0.0027 Acc: 84.0744\n",
      "validation Loss: 0.0027 Acc: 84.0871\n",
      "Epoch 33/99\n",
      "training Loss: 0.0027 Acc: 84.1429\n",
      "validation Loss: 0.0027 Acc: 83.8491\n",
      "Epoch 34/99\n",
      "training Loss: 0.0027 Acc: 84.1369\n",
      "validation Loss: 0.0027 Acc: 83.9919\n",
      "Epoch 35/99\n",
      "training Loss: 0.0027 Acc: 84.1280\n",
      "validation Loss: 0.0027 Acc: 83.9800\n",
      "Epoch 36/99\n",
      "training Loss: 0.0027 Acc: 84.2589\n",
      "validation Loss: 0.0027 Acc: 84.0633\n",
      "Epoch 37/99\n",
      "training Loss: 0.0027 Acc: 84.1756\n",
      "validation Loss: 0.0027 Acc: 83.9919\n",
      "Epoch 38/99\n",
      "training Loss: 0.0027 Acc: 84.1726\n",
      "validation Loss: 0.0027 Acc: 84.0276\n",
      "Epoch 39/99\n",
      "training Loss: 0.0027 Acc: 84.2917\n",
      "validation Loss: 0.0027 Acc: 84.1228\n",
      "Epoch 40/99\n",
      "training Loss: 0.0027 Acc: 84.2321\n",
      "validation Loss: 0.0027 Acc: 84.1228\n",
      "Epoch 41/99\n",
      "training Loss: 0.0027 Acc: 84.2381\n",
      "validation Loss: 0.0027 Acc: 84.0038\n",
      "Epoch 42/99\n",
      "training Loss: 0.0027 Acc: 84.1994\n",
      "validation Loss: 0.0027 Acc: 83.9562\n",
      "Epoch 43/99\n",
      "training Loss: 0.0027 Acc: 84.1994\n",
      "validation Loss: 0.0027 Acc: 84.1466\n",
      "Epoch 44/99\n",
      "training Loss: 0.0027 Acc: 84.1667\n",
      "validation Loss: 0.0027 Acc: 84.0752\n",
      "Epoch 45/99\n",
      "training Loss: 0.0027 Acc: 84.1577\n",
      "validation Loss: 0.0027 Acc: 84.1228\n",
      "Epoch 46/99\n",
      "training Loss: 0.0027 Acc: 84.2827\n",
      "validation Loss: 0.0027 Acc: 84.0990\n",
      "Epoch 47/99\n",
      "training Loss: 0.0027 Acc: 84.4077\n",
      "validation Loss: 0.0027 Acc: 84.0990\n",
      "Early stopped.\n",
      "Best val acc: 84.146632\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0038 Acc: 79.4090\n",
      "validation Loss: 0.0032 Acc: 81.9524\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 82.4445\n",
      "validation Loss: 0.0030 Acc: 83.0952\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 82.8909\n",
      "validation Loss: 0.0029 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 83.1587\n",
      "validation Loss: 0.0029 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0028 Acc: 83.1796\n",
      "validation Loss: 0.0029 Acc: 83.5238\n",
      "Epoch 5/99\n",
      "training Loss: 0.0028 Acc: 83.3224\n",
      "validation Loss: 0.0029 Acc: 83.4881\n",
      "Epoch 6/99\n",
      "training Loss: 0.0028 Acc: 83.2421\n",
      "validation Loss: 0.0029 Acc: 83.5119\n",
      "Epoch 7/99\n",
      "training Loss: 0.0028 Acc: 83.3611\n",
      "validation Loss: 0.0029 Acc: 83.5833\n",
      "Epoch 8/99\n",
      "training Loss: 0.0028 Acc: 83.3909\n",
      "validation Loss: 0.0029 Acc: 83.5357\n",
      "Epoch 9/99\n",
      "training Loss: 0.0028 Acc: 83.4444\n",
      "validation Loss: 0.0028 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0028 Acc: 83.5784\n",
      "validation Loss: 0.0028 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0028 Acc: 83.6676\n",
      "validation Loss: 0.0028 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0028 Acc: 83.6468\n",
      "validation Loss: 0.0028 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0027 Acc: 83.6647\n",
      "validation Loss: 0.0028 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0027 Acc: 83.7420\n",
      "validation Loss: 0.0028 Acc: 83.8095\n",
      "Epoch 15/99\n",
      "training Loss: 0.0027 Acc: 83.7956\n",
      "validation Loss: 0.0028 Acc: 83.7619\n",
      "Epoch 16/99\n",
      "training Loss: 0.0027 Acc: 83.8908\n",
      "validation Loss: 0.0028 Acc: 83.7262\n",
      "Epoch 17/99\n",
      "training Loss: 0.0027 Acc: 83.7569\n",
      "validation Loss: 0.0028 Acc: 83.8452\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0027 Acc: 83.8164\n",
      "validation Loss: 0.0028 Acc: 83.8333\n",
      "Epoch 19/99\n",
      "training Loss: 0.0027 Acc: 83.9176\n",
      "validation Loss: 0.0028 Acc: 83.8452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0027 Acc: 83.8313\n",
      "validation Loss: 0.0028 Acc: 83.9643\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0027 Acc: 83.8075\n",
      "validation Loss: 0.0028 Acc: 83.9524\n",
      "Epoch 22/99\n",
      "training Loss: 0.0027 Acc: 83.9623\n",
      "validation Loss: 0.0028 Acc: 83.9286\n",
      "Epoch 23/99\n",
      "training Loss: 0.0027 Acc: 83.9861\n",
      "validation Loss: 0.0028 Acc: 83.9167\n",
      "Epoch 24/99\n",
      "training Loss: 0.0027 Acc: 83.9385\n",
      "validation Loss: 0.0028 Acc: 83.9643\n",
      "Epoch 25/99\n",
      "training Loss: 0.0027 Acc: 83.9771\n",
      "validation Loss: 0.0028 Acc: 84.0000\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0027 Acc: 83.9266\n",
      "validation Loss: 0.0028 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0027 Acc: 84.0664\n",
      "validation Loss: 0.0028 Acc: 83.9762\n",
      "Epoch 28/99\n",
      "training Loss: 0.0027 Acc: 84.1468\n",
      "validation Loss: 0.0028 Acc: 83.9643\n",
      "Epoch 29/99\n",
      "training Loss: 0.0027 Acc: 84.0010\n",
      "validation Loss: 0.0028 Acc: 84.0000\n",
      "Epoch 30/99\n",
      "training Loss: 0.0027 Acc: 84.0992\n",
      "validation Loss: 0.0028 Acc: 84.0952\n",
      "Epoch 31/99\n",
      "training Loss: 0.0027 Acc: 84.1081\n",
      "validation Loss: 0.0028 Acc: 84.1786\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0027 Acc: 84.0992\n",
      "validation Loss: 0.0028 Acc: 84.0119\n",
      "Epoch 33/99\n",
      "training Loss: 0.0027 Acc: 84.1349\n",
      "validation Loss: 0.0028 Acc: 84.1310\n",
      "Epoch 34/99\n",
      "training Loss: 0.0027 Acc: 84.0515\n",
      "validation Loss: 0.0028 Acc: 84.0357\n",
      "Epoch 35/99\n",
      "training Loss: 0.0027 Acc: 83.9950\n",
      "validation Loss: 0.0028 Acc: 84.0595\n",
      "Epoch 36/99\n",
      "training Loss: 0.0027 Acc: 84.2093\n",
      "validation Loss: 0.0028 Acc: 83.9643\n",
      "Epoch 37/99\n",
      "training Loss: 0.0027 Acc: 84.2539\n",
      "validation Loss: 0.0028 Acc: 84.1786\n",
      "Epoch 38/99\n",
      "training Loss: 0.0027 Acc: 84.2599\n",
      "validation Loss: 0.0028 Acc: 84.0952\n",
      "Epoch 39/99\n",
      "training Loss: 0.0027 Acc: 84.2063\n",
      "validation Loss: 0.0028 Acc: 83.8810\n",
      "Epoch 40/99\n",
      "training Loss: 0.0027 Acc: 84.2063\n",
      "validation Loss: 0.0028 Acc: 84.0357\n",
      "Epoch 41/99\n",
      "training Loss: 0.0027 Acc: 84.2390\n",
      "validation Loss: 0.0028 Acc: 83.9048\n",
      "Epoch 42/99\n",
      "training Loss: 0.0027 Acc: 84.2866\n",
      "validation Loss: 0.0028 Acc: 83.9881\n",
      "Epoch 43/99\n",
      "training Loss: 0.0027 Acc: 84.3610\n",
      "validation Loss: 0.0028 Acc: 84.0357\n",
      "Epoch 44/99\n",
      "training Loss: 0.0027 Acc: 84.2539\n",
      "validation Loss: 0.0028 Acc: 84.1667\n",
      "Epoch 45/99\n",
      "training Loss: 0.0026 Acc: 84.3134\n",
      "validation Loss: 0.0028 Acc: 84.1071\n",
      "Epoch 46/99\n",
      "training Loss: 0.0026 Acc: 84.3491\n",
      "validation Loss: 0.0028 Acc: 84.1190\n",
      "Epoch 47/99\n",
      "training Loss: 0.0026 Acc: 84.4116\n",
      "validation Loss: 0.0028 Acc: 84.1786\n",
      "Epoch 48/99\n",
      "training Loss: 0.0027 Acc: 84.1914\n",
      "validation Loss: 0.0028 Acc: 84.0119\n",
      "Epoch 49/99\n",
      "training Loss: 0.0026 Acc: 84.3849\n",
      "validation Loss: 0.0028 Acc: 84.0357\n",
      "Epoch 50/99\n",
      "training Loss: 0.0026 Acc: 84.4325\n",
      "validation Loss: 0.0028 Acc: 83.9405\n",
      "Epoch 51/99\n",
      "training Loss: 0.0026 Acc: 84.4474\n",
      "validation Loss: 0.0028 Acc: 84.0476\n",
      "Early stopped.\n",
      "Best val acc: 84.178571\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 78.7780\n",
      "validation Loss: 0.0032 Acc: 80.7976\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 82.4921\n",
      "validation Loss: 0.0031 Acc: 81.6905\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 82.9177\n",
      "validation Loss: 0.0030 Acc: 81.8810\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 83.2927\n",
      "validation Loss: 0.0029 Acc: 82.1905\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0029 Acc: 83.3492\n",
      "validation Loss: 0.0029 Acc: 82.1190\n",
      "Epoch 5/99\n",
      "training Loss: 0.0028 Acc: 83.3998\n",
      "validation Loss: 0.0029 Acc: 82.4167\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0028 Acc: 83.6230\n",
      "validation Loss: 0.0029 Acc: 82.3810\n",
      "Epoch 7/99\n",
      "training Loss: 0.0028 Acc: 83.5754\n",
      "validation Loss: 0.0029 Acc: 82.4762\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0028 Acc: 83.6498\n",
      "validation Loss: 0.0029 Acc: 82.5000\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0028 Acc: 83.7093\n",
      "validation Loss: 0.0029 Acc: 82.6429\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0028 Acc: 83.7688\n",
      "validation Loss: 0.0029 Acc: 82.6429\n",
      "Epoch 11/99\n",
      "training Loss: 0.0028 Acc: 83.8611\n",
      "validation Loss: 0.0029 Acc: 82.5476\n",
      "Epoch 12/99\n",
      "training Loss: 0.0028 Acc: 83.8849\n",
      "validation Loss: 0.0029 Acc: 82.7857\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0028 Acc: 83.8968\n",
      "validation Loss: 0.0029 Acc: 82.5714\n",
      "Epoch 14/99\n",
      "training Loss: 0.0028 Acc: 83.8641\n",
      "validation Loss: 0.0029 Acc: 82.7381\n",
      "Epoch 15/99\n",
      "training Loss: 0.0028 Acc: 83.8462\n",
      "validation Loss: 0.0028 Acc: 82.7143\n",
      "Epoch 16/99\n",
      "training Loss: 0.0027 Acc: 83.9742\n",
      "validation Loss: 0.0028 Acc: 82.7738\n",
      "Epoch 17/99\n",
      "training Loss: 0.0027 Acc: 83.8908\n",
      "validation Loss: 0.0029 Acc: 82.5238\n",
      "Epoch 18/99\n",
      "training Loss: 0.0027 Acc: 84.0277\n",
      "validation Loss: 0.0028 Acc: 82.6905\n",
      "Epoch 19/99\n",
      "training Loss: 0.0027 Acc: 84.0486\n",
      "validation Loss: 0.0028 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0027 Acc: 83.9801\n",
      "validation Loss: 0.0028 Acc: 82.7143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0027 Acc: 84.1825\n",
      "validation Loss: 0.0028 Acc: 82.7143\n",
      "Epoch 22/99\n",
      "training Loss: 0.0027 Acc: 84.0664\n",
      "validation Loss: 0.0028 Acc: 82.9167\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0027 Acc: 84.1170\n",
      "validation Loss: 0.0028 Acc: 82.7738\n",
      "Epoch 24/99\n",
      "training Loss: 0.0027 Acc: 84.0813\n",
      "validation Loss: 0.0028 Acc: 82.8690\n",
      "Epoch 25/99\n",
      "training Loss: 0.0027 Acc: 84.1259\n",
      "validation Loss: 0.0028 Acc: 82.7143\n",
      "Epoch 26/99\n",
      "training Loss: 0.0027 Acc: 84.2628\n",
      "validation Loss: 0.0028 Acc: 82.8571\n",
      "Epoch 27/99\n",
      "training Loss: 0.0027 Acc: 84.1587\n",
      "validation Loss: 0.0028 Acc: 82.7500\n",
      "Epoch 28/99\n",
      "training Loss: 0.0027 Acc: 84.3521\n",
      "validation Loss: 0.0028 Acc: 82.8452\n",
      "Epoch 29/99\n",
      "training Loss: 0.0027 Acc: 84.2509\n",
      "validation Loss: 0.0028 Acc: 82.7262\n",
      "Epoch 30/99\n",
      "training Loss: 0.0027 Acc: 84.2986\n",
      "validation Loss: 0.0028 Acc: 82.6786\n",
      "Epoch 31/99\n",
      "training Loss: 0.0027 Acc: 84.2777\n",
      "validation Loss: 0.0028 Acc: 82.9167\n",
      "Epoch 32/99\n",
      "training Loss: 0.0027 Acc: 84.2866\n",
      "validation Loss: 0.0028 Acc: 82.9524\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0027 Acc: 84.4087\n",
      "validation Loss: 0.0028 Acc: 82.8929\n",
      "Epoch 34/99\n",
      "training Loss: 0.0027 Acc: 84.3670\n",
      "validation Loss: 0.0028 Acc: 82.8095\n",
      "Epoch 35/99\n",
      "training Loss: 0.0027 Acc: 84.3730\n",
      "validation Loss: 0.0028 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0027 Acc: 84.4235\n",
      "validation Loss: 0.0028 Acc: 82.7857\n",
      "Epoch 37/99\n",
      "training Loss: 0.0027 Acc: 84.4146\n",
      "validation Loss: 0.0028 Acc: 82.7024\n",
      "Epoch 38/99\n",
      "training Loss: 0.0027 Acc: 84.3551\n",
      "validation Loss: 0.0028 Acc: 82.6310\n",
      "Epoch 39/99\n",
      "training Loss: 0.0027 Acc: 84.4295\n",
      "validation Loss: 0.0028 Acc: 82.8333\n",
      "Epoch 40/99\n",
      "training Loss: 0.0027 Acc: 84.4384\n",
      "validation Loss: 0.0028 Acc: 82.6190\n",
      "Epoch 41/99\n",
      "training Loss: 0.0027 Acc: 84.5337\n",
      "validation Loss: 0.0028 Acc: 82.7738\n",
      "Epoch 42/99\n",
      "training Loss: 0.0027 Acc: 84.4444\n",
      "validation Loss: 0.0028 Acc: 82.9286\n",
      "Epoch 43/99\n",
      "training Loss: 0.0026 Acc: 84.5515\n",
      "validation Loss: 0.0028 Acc: 82.6905\n",
      "Epoch 44/99\n",
      "training Loss: 0.0027 Acc: 84.5723\n",
      "validation Loss: 0.0028 Acc: 82.7738\n",
      "Epoch 45/99\n",
      "training Loss: 0.0026 Acc: 84.5902\n",
      "validation Loss: 0.0028 Acc: 82.7500\n",
      "Epoch 46/99\n",
      "training Loss: 0.0026 Acc: 84.7122\n",
      "validation Loss: 0.0028 Acc: 82.7738\n",
      "Epoch 47/99\n",
      "training Loss: 0.0026 Acc: 84.5545\n",
      "validation Loss: 0.0028 Acc: 82.9524\n",
      "Epoch 48/99\n",
      "training Loss: 0.0026 Acc: 84.5694\n",
      "validation Loss: 0.0028 Acc: 83.0595\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0026 Acc: 84.6378\n",
      "validation Loss: 0.0028 Acc: 82.9524\n",
      "Epoch 50/99\n",
      "training Loss: 0.0026 Acc: 84.5991\n",
      "validation Loss: 0.0028 Acc: 82.6667\n",
      "Epoch 51/99\n",
      "training Loss: 0.0026 Acc: 84.6884\n",
      "validation Loss: 0.0028 Acc: 82.6429\n",
      "Epoch 52/99\n",
      "training Loss: 0.0026 Acc: 84.6408\n",
      "validation Loss: 0.0028 Acc: 82.6667\n",
      "Epoch 53/99\n",
      "training Loss: 0.0026 Acc: 84.6587\n",
      "validation Loss: 0.0028 Acc: 82.8571\n",
      "Epoch 54/99\n",
      "training Loss: 0.0026 Acc: 84.5485\n",
      "validation Loss: 0.0028 Acc: 82.7976\n",
      "Epoch 55/99\n",
      "training Loss: 0.0026 Acc: 84.4533\n",
      "validation Loss: 0.0028 Acc: 82.9643\n",
      "Epoch 56/99\n",
      "training Loss: 0.0026 Acc: 84.7092\n",
      "validation Loss: 0.0028 Acc: 82.7500\n",
      "Epoch 57/99\n",
      "training Loss: 0.0026 Acc: 84.6467\n",
      "validation Loss: 0.0028 Acc: 82.7857\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 84.6735\n",
      "validation Loss: 0.0028 Acc: 82.8690\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 84.7033\n",
      "validation Loss: 0.0028 Acc: 82.7262\n",
      "Epoch 60/99\n",
      "training Loss: 0.0026 Acc: 84.6646\n",
      "validation Loss: 0.0028 Acc: 82.9167\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 84.7122\n",
      "validation Loss: 0.0028 Acc: 82.9048\n",
      "Epoch 62/99\n",
      "training Loss: 0.0026 Acc: 84.5783\n",
      "validation Loss: 0.0028 Acc: 82.8452\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 84.5307\n",
      "validation Loss: 0.0028 Acc: 82.7976\n",
      "Epoch 64/99\n",
      "training Loss: 0.0026 Acc: 84.6825\n",
      "validation Loss: 0.0028 Acc: 82.8333\n",
      "Epoch 65/99\n",
      "training Loss: 0.0026 Acc: 84.7331\n",
      "validation Loss: 0.0028 Acc: 82.7619\n",
      "Epoch 66/99\n",
      "training Loss: 0.0026 Acc: 84.8670\n",
      "validation Loss: 0.0028 Acc: 82.8571\n",
      "Epoch 67/99\n",
      "training Loss: 0.0026 Acc: 84.7003\n",
      "validation Loss: 0.0028 Acc: 82.7024\n",
      "Epoch 68/99\n",
      "training Loss: 0.0026 Acc: 84.8610\n",
      "validation Loss: 0.0028 Acc: 82.8452\n",
      "Early stopped.\n",
      "Best val acc: 83.059524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 78.3555\n",
      "validation Loss: 0.0031 Acc: 81.8571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0031 Acc: 82.1588\n",
      "validation Loss: 0.0029 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 82.5993\n",
      "validation Loss: 0.0028 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 83.0040\n",
      "validation Loss: 0.0028 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0029 Acc: 83.0605\n",
      "validation Loss: 0.0028 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0029 Acc: 83.1260\n",
      "validation Loss: 0.0028 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0028 Acc: 83.2302\n",
      "validation Loss: 0.0028 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0028 Acc: 83.3313\n",
      "validation Loss: 0.0028 Acc: 83.8571\n",
      "Epoch 8/99\n",
      "training Loss: 0.0028 Acc: 83.3462\n",
      "validation Loss: 0.0028 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0028 Acc: 83.3760\n",
      "validation Loss: 0.0028 Acc: 83.9762\n",
      "Epoch 10/99\n",
      "training Loss: 0.0028 Acc: 83.4593\n",
      "validation Loss: 0.0027 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0028 Acc: 83.5010\n",
      "validation Loss: 0.0027 Acc: 84.0357\n",
      "Epoch 12/99\n",
      "training Loss: 0.0028 Acc: 83.5248\n",
      "validation Loss: 0.0027 Acc: 84.0714\n",
      "Epoch 13/99\n",
      "training Loss: 0.0028 Acc: 83.5784\n",
      "validation Loss: 0.0027 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0028 Acc: 83.5784\n",
      "validation Loss: 0.0027 Acc: 84.1190\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0028 Acc: 83.5724\n",
      "validation Loss: 0.0027 Acc: 84.2619\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0028 Acc: 83.6676\n",
      "validation Loss: 0.0027 Acc: 84.0714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0028 Acc: 83.6647\n",
      "validation Loss: 0.0027 Acc: 84.2619\n",
      "Epoch 18/99\n",
      "training Loss: 0.0028 Acc: 83.7123\n",
      "validation Loss: 0.0027 Acc: 84.1310\n",
      "Epoch 19/99\n",
      "training Loss: 0.0027 Acc: 83.7748\n",
      "validation Loss: 0.0027 Acc: 84.2262\n",
      "Epoch 20/99\n",
      "training Loss: 0.0027 Acc: 83.9057\n",
      "validation Loss: 0.0027 Acc: 84.2143\n",
      "Epoch 21/99\n",
      "training Loss: 0.0027 Acc: 83.9385\n",
      "validation Loss: 0.0027 Acc: 84.2857\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0027 Acc: 83.8522\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0027 Acc: 83.7986\n",
      "validation Loss: 0.0027 Acc: 84.2143\n",
      "Epoch 24/99\n",
      "training Loss: 0.0027 Acc: 83.7837\n",
      "validation Loss: 0.0027 Acc: 84.0357\n",
      "Epoch 25/99\n",
      "training Loss: 0.0027 Acc: 83.8819\n",
      "validation Loss: 0.0027 Acc: 84.3810\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0027 Acc: 83.8462\n",
      "validation Loss: 0.0027 Acc: 84.2857\n",
      "Epoch 27/99\n",
      "training Loss: 0.0027 Acc: 83.9117\n",
      "validation Loss: 0.0027 Acc: 84.1548\n",
      "Epoch 28/99\n",
      "training Loss: 0.0027 Acc: 83.8551\n",
      "validation Loss: 0.0027 Acc: 84.2262\n",
      "Epoch 29/99\n",
      "training Loss: 0.0027 Acc: 83.8760\n",
      "validation Loss: 0.0027 Acc: 84.2143\n",
      "Epoch 30/99\n",
      "training Loss: 0.0027 Acc: 83.9712\n",
      "validation Loss: 0.0027 Acc: 84.1667\n",
      "Epoch 31/99\n",
      "training Loss: 0.0027 Acc: 84.0129\n",
      "validation Loss: 0.0027 Acc: 84.1310\n",
      "Epoch 32/99\n",
      "training Loss: 0.0027 Acc: 84.0307\n",
      "validation Loss: 0.0027 Acc: 84.2976\n",
      "Epoch 33/99\n",
      "training Loss: 0.0027 Acc: 84.0129\n",
      "validation Loss: 0.0027 Acc: 84.2143\n",
      "Epoch 34/99\n",
      "training Loss: 0.0027 Acc: 84.0783\n",
      "validation Loss: 0.0027 Acc: 84.3929\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0027 Acc: 84.0843\n",
      "validation Loss: 0.0027 Acc: 84.3095\n",
      "Epoch 36/99\n",
      "training Loss: 0.0027 Acc: 84.1349\n",
      "validation Loss: 0.0027 Acc: 84.3095\n",
      "Epoch 37/99\n",
      "training Loss: 0.0027 Acc: 83.9861\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Epoch 38/99\n",
      "training Loss: 0.0027 Acc: 84.1587\n",
      "validation Loss: 0.0027 Acc: 84.2262\n",
      "Epoch 39/99\n",
      "training Loss: 0.0027 Acc: 84.1617\n",
      "validation Loss: 0.0027 Acc: 84.2738\n",
      "Epoch 40/99\n",
      "training Loss: 0.0027 Acc: 84.0873\n",
      "validation Loss: 0.0027 Acc: 84.2857\n",
      "Epoch 41/99\n",
      "training Loss: 0.0027 Acc: 84.2420\n",
      "validation Loss: 0.0027 Acc: 84.2381\n",
      "Epoch 42/99\n",
      "training Loss: 0.0027 Acc: 84.1736\n",
      "validation Loss: 0.0027 Acc: 84.3929\n",
      "Epoch 43/99\n",
      "training Loss: 0.0027 Acc: 84.1914\n",
      "validation Loss: 0.0027 Acc: 84.2857\n",
      "Epoch 44/99\n",
      "training Loss: 0.0027 Acc: 84.2271\n",
      "validation Loss: 0.0027 Acc: 84.2500\n",
      "Epoch 45/99\n",
      "training Loss: 0.0027 Acc: 84.2182\n",
      "validation Loss: 0.0027 Acc: 84.3095\n",
      "Epoch 46/99\n",
      "training Loss: 0.0027 Acc: 84.3908\n",
      "validation Loss: 0.0027 Acc: 84.2500\n",
      "Epoch 47/99\n",
      "training Loss: 0.0027 Acc: 84.3610\n",
      "validation Loss: 0.0027 Acc: 84.1667\n",
      "Epoch 48/99\n",
      "training Loss: 0.0027 Acc: 84.2242\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Epoch 49/99\n",
      "training Loss: 0.0027 Acc: 84.3849\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0027 Acc: 84.3789\n",
      "validation Loss: 0.0027 Acc: 84.1905\n",
      "Epoch 51/99\n",
      "training Loss: 0.0027 Acc: 84.3730\n",
      "validation Loss: 0.0027 Acc: 84.3333\n",
      "Epoch 52/99\n",
      "training Loss: 0.0026 Acc: 84.4235\n",
      "validation Loss: 0.0027 Acc: 84.3810\n",
      "Epoch 53/99\n",
      "training Loss: 0.0027 Acc: 84.4057\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Epoch 54/99\n",
      "training Loss: 0.0026 Acc: 84.4801\n",
      "validation Loss: 0.0027 Acc: 84.3214\n",
      "Epoch 55/99\n",
      "training Loss: 0.0026 Acc: 84.4265\n",
      "validation Loss: 0.0027 Acc: 84.3214\n",
      "Epoch 56/99\n",
      "training Loss: 0.0026 Acc: 84.5277\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Epoch 57/99\n",
      "training Loss: 0.0026 Acc: 84.4563\n",
      "validation Loss: 0.0027 Acc: 84.4405\n",
      "Saving..\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 84.3253\n",
      "validation Loss: 0.0027 Acc: 84.4286\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 84.3402\n",
      "validation Loss: 0.0027 Acc: 84.4286\n",
      "Epoch 60/99\n",
      "training Loss: 0.0026 Acc: 84.4593\n",
      "validation Loss: 0.0027 Acc: 84.4643\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 84.4533\n",
      "validation Loss: 0.0027 Acc: 84.4286\n",
      "Epoch 62/99\n",
      "training Loss: 0.0026 Acc: 84.5009\n",
      "validation Loss: 0.0027 Acc: 84.3571\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 84.6081\n",
      "validation Loss: 0.0027 Acc: 84.3452\n",
      "Epoch 64/99\n",
      "training Loss: 0.0026 Acc: 84.4712\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Epoch 65/99\n",
      "training Loss: 0.0026 Acc: 84.5872\n",
      "validation Loss: 0.0027 Acc: 84.3452\n",
      "Epoch 66/99\n",
      "training Loss: 0.0026 Acc: 84.5485\n",
      "validation Loss: 0.0027 Acc: 84.3571\n",
      "Epoch 67/99\n",
      "training Loss: 0.0026 Acc: 84.4771\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Epoch 68/99\n",
      "training Loss: 0.0026 Acc: 84.3640\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Epoch 69/99\n",
      "training Loss: 0.0026 Acc: 84.4503\n",
      "validation Loss: 0.0027 Acc: 84.4286\n",
      "Epoch 70/99\n",
      "training Loss: 0.0026 Acc: 84.5247\n",
      "validation Loss: 0.0027 Acc: 84.3690\n",
      "Epoch 71/99\n",
      "training Loss: 0.0026 Acc: 84.3908\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Epoch 72/99\n",
      "training Loss: 0.0026 Acc: 84.5634\n",
      "validation Loss: 0.0027 Acc: 84.3929\n",
      "Epoch 73/99\n",
      "training Loss: 0.0026 Acc: 84.6259\n",
      "validation Loss: 0.0027 Acc: 84.3929\n",
      "Epoch 74/99\n",
      "training Loss: 0.0026 Acc: 84.5723\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Epoch 75/99\n",
      "training Loss: 0.0026 Acc: 84.5099\n",
      "validation Loss: 0.0027 Acc: 84.4048\n",
      "Epoch 76/99\n",
      "training Loss: 0.0026 Acc: 84.5366\n",
      "validation Loss: 0.0027 Acc: 84.4048\n",
      "Epoch 77/99\n",
      "training Loss: 0.0026 Acc: 84.6021\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Epoch 78/99\n",
      "training Loss: 0.0026 Acc: 84.5813\n",
      "validation Loss: 0.0027 Acc: 84.4405\n",
      "Epoch 79/99\n",
      "training Loss: 0.0026 Acc: 84.5218\n",
      "validation Loss: 0.0027 Acc: 84.4048\n",
      "Epoch 80/99\n",
      "training Loss: 0.0026 Acc: 84.4890\n",
      "validation Loss: 0.0027 Acc: 84.4167\n",
      "Early stopped.\n",
      "Best val acc: 84.464286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0039 Acc: 79.1709\n",
      "validation Loss: 0.0031 Acc: 81.9762\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0030 Acc: 82.4177\n",
      "validation Loss: 0.0029 Acc: 82.7619\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0029 Acc: 82.8344\n",
      "validation Loss: 0.0029 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0029 Acc: 83.1587\n",
      "validation Loss: 0.0029 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0029 Acc: 83.2808\n",
      "validation Loss: 0.0028 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0028 Acc: 83.3462\n",
      "validation Loss: 0.0028 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0028 Acc: 83.4623\n",
      "validation Loss: 0.0028 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0028 Acc: 83.4801\n",
      "validation Loss: 0.0028 Acc: 83.4762\n",
      "Epoch 8/99\n",
      "training Loss: 0.0028 Acc: 83.4087\n",
      "validation Loss: 0.0028 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0028 Acc: 83.5218\n",
      "validation Loss: 0.0028 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0028 Acc: 83.5159\n",
      "validation Loss: 0.0028 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0028 Acc: 83.5724\n",
      "validation Loss: 0.0028 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0028 Acc: 83.5694\n",
      "validation Loss: 0.0028 Acc: 83.4881\n",
      "Epoch 13/99\n",
      "training Loss: 0.0028 Acc: 83.7063\n",
      "validation Loss: 0.0028 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0028 Acc: 83.6468\n",
      "validation Loss: 0.0028 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0028 Acc: 83.6647\n",
      "validation Loss: 0.0028 Acc: 83.6071\n",
      "Epoch 16/99\n",
      "training Loss: 0.0028 Acc: 83.7748\n",
      "validation Loss: 0.0028 Acc: 83.5357\n",
      "Epoch 17/99\n",
      "training Loss: 0.0028 Acc: 83.7956\n",
      "validation Loss: 0.0028 Acc: 83.5595\n",
      "Epoch 18/99\n",
      "training Loss: 0.0028 Acc: 83.7391\n",
      "validation Loss: 0.0028 Acc: 83.4762\n",
      "Epoch 19/99\n",
      "training Loss: 0.0027 Acc: 83.7926\n",
      "validation Loss: 0.0028 Acc: 83.6786\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0027 Acc: 83.7658\n",
      "validation Loss: 0.0028 Acc: 83.6190\n",
      "Epoch 21/99\n",
      "training Loss: 0.0027 Acc: 83.8611\n",
      "validation Loss: 0.0028 Acc: 83.5595\n",
      "Epoch 22/99\n",
      "training Loss: 0.0027 Acc: 83.9206\n",
      "validation Loss: 0.0028 Acc: 83.6548\n",
      "Epoch 23/99\n",
      "training Loss: 0.0027 Acc: 83.9920\n",
      "validation Loss: 0.0028 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0027 Acc: 83.9206\n",
      "validation Loss: 0.0028 Acc: 83.6071\n",
      "Epoch 25/99\n",
      "training Loss: 0.0027 Acc: 83.9504\n",
      "validation Loss: 0.0028 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0027 Acc: 83.9801\n",
      "validation Loss: 0.0028 Acc: 83.5476\n",
      "Epoch 27/99\n",
      "training Loss: 0.0027 Acc: 84.0099\n",
      "validation Loss: 0.0028 Acc: 83.5595\n",
      "Epoch 28/99\n",
      "training Loss: 0.0027 Acc: 83.9831\n",
      "validation Loss: 0.0028 Acc: 83.5714\n",
      "Epoch 29/99\n",
      "training Loss: 0.0027 Acc: 84.0992\n",
      "validation Loss: 0.0028 Acc: 83.6190\n",
      "Epoch 30/99\n",
      "training Loss: 0.0027 Acc: 83.9504\n",
      "validation Loss: 0.0028 Acc: 83.6071\n",
      "Epoch 31/99\n",
      "training Loss: 0.0027 Acc: 84.1051\n",
      "validation Loss: 0.0028 Acc: 83.6310\n",
      "Epoch 32/99\n",
      "training Loss: 0.0027 Acc: 84.1527\n",
      "validation Loss: 0.0028 Acc: 83.5000\n",
      "Epoch 33/99\n",
      "training Loss: 0.0027 Acc: 84.1111\n",
      "validation Loss: 0.0028 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0027 Acc: 84.1646\n",
      "validation Loss: 0.0028 Acc: 83.7619\n",
      "Epoch 35/99\n",
      "training Loss: 0.0027 Acc: 84.1914\n",
      "validation Loss: 0.0028 Acc: 83.5952\n",
      "Epoch 36/99\n",
      "training Loss: 0.0027 Acc: 84.0694\n",
      "validation Loss: 0.0028 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0027 Acc: 84.3045\n",
      "validation Loss: 0.0028 Acc: 83.4405\n",
      "Epoch 38/99\n",
      "training Loss: 0.0027 Acc: 84.1111\n",
      "validation Loss: 0.0028 Acc: 83.5952\n",
      "Epoch 39/99\n",
      "training Loss: 0.0027 Acc: 84.1914\n",
      "validation Loss: 0.0028 Acc: 83.6667\n",
      "Epoch 40/99\n",
      "training Loss: 0.0027 Acc: 84.1557\n",
      "validation Loss: 0.0028 Acc: 83.7262\n",
      "Epoch 41/99\n",
      "training Loss: 0.0027 Acc: 84.2539\n",
      "validation Loss: 0.0028 Acc: 83.6786\n",
      "Epoch 42/99\n",
      "training Loss: 0.0027 Acc: 84.2986\n",
      "validation Loss: 0.0027 Acc: 83.6071\n",
      "Epoch 43/99\n",
      "training Loss: 0.0027 Acc: 84.2450\n",
      "validation Loss: 0.0028 Acc: 83.7024\n",
      "Epoch 44/99\n",
      "training Loss: 0.0027 Acc: 84.2896\n",
      "validation Loss: 0.0027 Acc: 83.7619\n",
      "Epoch 45/99\n",
      "training Loss: 0.0027 Acc: 84.3313\n",
      "validation Loss: 0.0027 Acc: 83.7262\n",
      "Epoch 46/99\n",
      "training Loss: 0.0027 Acc: 84.3938\n",
      "validation Loss: 0.0028 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0027 Acc: 84.4087\n",
      "validation Loss: 0.0027 Acc: 83.7500\n",
      "Epoch 48/99\n",
      "training Loss: 0.0027 Acc: 84.3700\n",
      "validation Loss: 0.0028 Acc: 83.6190\n",
      "Epoch 49/99\n",
      "training Loss: 0.0027 Acc: 84.3789\n",
      "validation Loss: 0.0027 Acc: 83.6190\n",
      "Epoch 50/99\n",
      "training Loss: 0.0027 Acc: 84.4801\n",
      "validation Loss: 0.0027 Acc: 83.7857\n",
      "Epoch 51/99\n",
      "training Loss: 0.0027 Acc: 84.3670\n",
      "validation Loss: 0.0027 Acc: 83.7024\n",
      "Epoch 52/99\n",
      "training Loss: 0.0027 Acc: 84.3253\n",
      "validation Loss: 0.0027 Acc: 83.6905\n",
      "Epoch 53/99\n",
      "training Loss: 0.0026 Acc: 84.5723\n",
      "validation Loss: 0.0027 Acc: 83.7024\n",
      "Epoch 54/99\n",
      "training Loss: 0.0026 Acc: 84.5456\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 55/99\n",
      "training Loss: 0.0026 Acc: 84.5694\n",
      "validation Loss: 0.0027 Acc: 83.7857\n",
      "Epoch 56/99\n",
      "training Loss: 0.0026 Acc: 84.4414\n",
      "validation Loss: 0.0028 Acc: 83.5952\n",
      "Epoch 57/99\n",
      "training Loss: 0.0026 Acc: 84.5247\n",
      "validation Loss: 0.0027 Acc: 83.7381\n",
      "Epoch 58/99\n",
      "training Loss: 0.0026 Acc: 84.5158\n",
      "validation Loss: 0.0027 Acc: 83.8214\n",
      "Epoch 59/99\n",
      "training Loss: 0.0026 Acc: 84.4831\n",
      "validation Loss: 0.0027 Acc: 83.6905\n",
      "Epoch 60/99\n",
      "training Loss: 0.0026 Acc: 84.5396\n",
      "validation Loss: 0.0027 Acc: 83.7500\n",
      "Epoch 61/99\n",
      "training Loss: 0.0026 Acc: 84.5634\n",
      "validation Loss: 0.0027 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0026 Acc: 84.5158\n",
      "validation Loss: 0.0027 Acc: 83.8095\n",
      "Epoch 63/99\n",
      "training Loss: 0.0026 Acc: 84.6319\n",
      "validation Loss: 0.0027 Acc: 83.7024\n",
      "Epoch 64/99\n",
      "training Loss: 0.0026 Acc: 84.5932\n",
      "validation Loss: 0.0027 Acc: 83.7619\n",
      "Epoch 65/99\n",
      "training Loss: 0.0026 Acc: 84.6021\n",
      "validation Loss: 0.0027 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0026 Acc: 84.4474\n",
      "validation Loss: 0.0027 Acc: 83.8810\n",
      "Epoch 67/99\n",
      "training Loss: 0.0026 Acc: 84.5694\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 68/99\n",
      "training Loss: 0.0026 Acc: 84.5247\n",
      "validation Loss: 0.0027 Acc: 83.8571\n",
      "Epoch 69/99\n",
      "training Loss: 0.0026 Acc: 84.7866\n",
      "validation Loss: 0.0027 Acc: 83.8095\n",
      "Epoch 70/99\n",
      "training Loss: 0.0026 Acc: 84.6200\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 71/99\n",
      "training Loss: 0.0026 Acc: 84.6676\n",
      "validation Loss: 0.0027 Acc: 83.8571\n",
      "Epoch 72/99\n",
      "training Loss: 0.0026 Acc: 84.6081\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 73/99\n",
      "training Loss: 0.0026 Acc: 84.5099\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 74/99\n",
      "training Loss: 0.0026 Acc: 84.6408\n",
      "validation Loss: 0.0027 Acc: 83.8214\n",
      "Epoch 75/99\n",
      "training Loss: 0.0026 Acc: 84.6497\n",
      "validation Loss: 0.0027 Acc: 83.8571\n",
      "Epoch 76/99\n",
      "training Loss: 0.0026 Acc: 84.6378\n",
      "validation Loss: 0.0027 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0026 Acc: 84.7569\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 78/99\n",
      "training Loss: 0.0026 Acc: 84.6795\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 79/99\n",
      "training Loss: 0.0026 Acc: 84.6944\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 80/99\n",
      "training Loss: 0.0026 Acc: 84.6735\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 81/99\n",
      "training Loss: 0.0026 Acc: 84.6646\n",
      "validation Loss: 0.0027 Acc: 83.8333\n",
      "Epoch 82/99\n",
      "training Loss: 0.0026 Acc: 84.6408\n",
      "validation Loss: 0.0027 Acc: 83.9167\n",
      "Epoch 83/99\n",
      "training Loss: 0.0026 Acc: 84.6795\n",
      "validation Loss: 0.0027 Acc: 83.8929\n",
      "Epoch 84/99\n",
      "training Loss: 0.0026 Acc: 84.7182\n",
      "validation Loss: 0.0027 Acc: 83.8810\n",
      "Epoch 85/99\n",
      "training Loss: 0.0026 Acc: 84.7003\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 86/99\n",
      "training Loss: 0.0026 Acc: 84.5575\n",
      "validation Loss: 0.0027 Acc: 83.8571\n",
      "Epoch 87/99\n",
      "training Loss: 0.0026 Acc: 84.6646\n",
      "validation Loss: 0.0027 Acc: 83.8571\n",
      "Epoch 88/99\n",
      "training Loss: 0.0026 Acc: 84.7747\n",
      "validation Loss: 0.0027 Acc: 83.8690\n",
      "Epoch 89/99\n",
      "training Loss: 0.0026 Acc: 84.6289\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 90/99\n",
      "training Loss: 0.0026 Acc: 84.7777\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 91/99\n",
      "training Loss: 0.0026 Acc: 84.6557\n",
      "validation Loss: 0.0027 Acc: 83.8333\n",
      "Epoch 92/99\n",
      "training Loss: 0.0026 Acc: 84.6021\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 93/99\n",
      "training Loss: 0.0026 Acc: 84.6587\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Epoch 94/99\n",
      "training Loss: 0.0026 Acc: 84.7241\n",
      "validation Loss: 0.0027 Acc: 83.8333\n",
      "Epoch 95/99\n",
      "training Loss: 0.0026 Acc: 84.6646\n",
      "validation Loss: 0.0027 Acc: 83.8333\n",
      "Epoch 96/99\n",
      "training Loss: 0.0026 Acc: 84.6676\n",
      "validation Loss: 0.0027 Acc: 83.8452\n",
      "Early stopped.\n",
      "Best val acc: 83.976190\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9650406366\n",
      "New configuration: {'learning_rate': 1.513316918048321e-05, 'initial_nodes': 1000, 'dropout': 0.27025493243024906, 'batch_size': 32, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.6488\n",
      "validation Loss: 0.0122 Acc: 82.3137\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.7649\n",
      "validation Loss: 0.0118 Acc: 83.0755\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 83.0893\n",
      "validation Loss: 0.0117 Acc: 82.9802\n",
      "Epoch 3/99\n",
      "training Loss: 0.0115 Acc: 83.3423\n",
      "validation Loss: 0.0116 Acc: 83.1469\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.4405\n",
      "validation Loss: 0.0115 Acc: 83.1945\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.5030\n",
      "validation Loss: 0.0115 Acc: 82.9921\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.4762\n",
      "validation Loss: 0.0114 Acc: 83.4087\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.5952\n",
      "validation Loss: 0.0114 Acc: 83.4920\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.6964\n",
      "validation Loss: 0.0114 Acc: 83.3968\n",
      "Epoch 9/99\n",
      "training Loss: 0.0112 Acc: 83.6429\n",
      "validation Loss: 0.0114 Acc: 83.2540\n",
      "Epoch 10/99\n",
      "training Loss: 0.0112 Acc: 83.7500\n",
      "validation Loss: 0.0114 Acc: 83.4206\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.6905\n",
      "validation Loss: 0.0114 Acc: 83.2659\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.7381\n",
      "validation Loss: 0.0114 Acc: 82.9564\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.7381\n",
      "validation Loss: 0.0113 Acc: 83.1231\n",
      "Epoch 14/99\n",
      "training Loss: 0.0111 Acc: 83.8423\n",
      "validation Loss: 0.0113 Acc: 83.3254\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.8125\n",
      "validation Loss: 0.0113 Acc: 83.3135\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.8839\n",
      "validation Loss: 0.0113 Acc: 83.2183\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.9196\n",
      "validation Loss: 0.0113 Acc: 83.3849\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.9345\n",
      "validation Loss: 0.0113 Acc: 83.2421\n",
      "Epoch 19/99\n",
      "training Loss: 0.0110 Acc: 84.0089\n",
      "validation Loss: 0.0113 Acc: 83.3730\n",
      "Epoch 20/99\n",
      "training Loss: 0.0110 Acc: 83.9732\n",
      "validation Loss: 0.0114 Acc: 82.9683\n",
      "Epoch 21/99\n",
      "training Loss: 0.0110 Acc: 83.9792\n",
      "validation Loss: 0.0112 Acc: 83.3611\n",
      "Epoch 22/99\n",
      "training Loss: 0.0110 Acc: 83.9732\n",
      "validation Loss: 0.0113 Acc: 83.3730\n",
      "Epoch 23/99\n",
      "training Loss: 0.0110 Acc: 84.0506\n",
      "validation Loss: 0.0113 Acc: 83.4444\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 84.0149\n",
      "validation Loss: 0.0112 Acc: 83.3849\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.9315\n",
      "validation Loss: 0.0112 Acc: 83.3492\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 84.1161\n",
      "validation Loss: 0.0113 Acc: 83.2421\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 84.0179\n",
      "validation Loss: 0.0113 Acc: 83.2302\n",
      "Early stopped.\n",
      "Best val acc: 83.492026\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.5250\n",
      "validation Loss: 0.0120 Acc: 82.7143\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0120 Acc: 82.7927\n",
      "validation Loss: 0.0117 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 82.9980\n",
      "validation Loss: 0.0116 Acc: 82.9881\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.1855\n",
      "validation Loss: 0.0115 Acc: 83.0357\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.3075\n",
      "validation Loss: 0.0114 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.3849\n",
      "validation Loss: 0.0114 Acc: 83.5833\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.4504\n",
      "validation Loss: 0.0114 Acc: 83.3452\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.5218\n",
      "validation Loss: 0.0113 Acc: 83.4881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.5367\n",
      "validation Loss: 0.0113 Acc: 83.7024\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.6081\n",
      "validation Loss: 0.0113 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0112 Acc: 83.6528\n",
      "validation Loss: 0.0113 Acc: 83.5833\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.5129\n",
      "validation Loss: 0.0113 Acc: 83.7619\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.5932\n",
      "validation Loss: 0.0113 Acc: 83.7381\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.6825\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.6260\n",
      "validation Loss: 0.0112 Acc: 83.7381\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.7420\n",
      "validation Loss: 0.0112 Acc: 83.7619\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.7807\n",
      "validation Loss: 0.0112 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.7926\n",
      "validation Loss: 0.0112 Acc: 83.8452\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.7807\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.7986\n",
      "validation Loss: 0.0112 Acc: 83.8571\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 83.6944\n",
      "validation Loss: 0.0112 Acc: 83.9405\n",
      "Epoch 21/99\n",
      "training Loss: 0.0110 Acc: 83.9027\n",
      "validation Loss: 0.0112 Acc: 83.8214\n",
      "Epoch 22/99\n",
      "training Loss: 0.0110 Acc: 83.9206\n",
      "validation Loss: 0.0112 Acc: 83.6429\n",
      "Epoch 23/99\n",
      "training Loss: 0.0110 Acc: 83.9623\n",
      "validation Loss: 0.0112 Acc: 83.5952\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 84.0277\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.9533\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 83.8879\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 83.9504\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Epoch 28/99\n",
      "training Loss: 0.0110 Acc: 84.0188\n",
      "validation Loss: 0.0112 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0110 Acc: 83.9206\n",
      "validation Loss: 0.0111 Acc: 84.0238\n",
      "Epoch 30/99\n",
      "training Loss: 0.0109 Acc: 84.0426\n",
      "validation Loss: 0.0111 Acc: 83.9881\n",
      "Epoch 31/99\n",
      "training Loss: 0.0109 Acc: 84.0277\n",
      "validation Loss: 0.0111 Acc: 83.9405\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.0873\n",
      "validation Loss: 0.0111 Acc: 83.7738\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.0337\n",
      "validation Loss: 0.0111 Acc: 84.0357\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 84.0010\n",
      "validation Loss: 0.0111 Acc: 84.1429\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.1706\n",
      "validation Loss: 0.0111 Acc: 83.7143\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.1170\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.0754\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.1557\n",
      "validation Loss: 0.0111 Acc: 83.9048\n",
      "Epoch 39/99\n",
      "training Loss: 0.0109 Acc: 84.1736\n",
      "validation Loss: 0.0111 Acc: 83.9524\n",
      "Epoch 40/99\n",
      "training Loss: 0.0108 Acc: 84.1646\n",
      "validation Loss: 0.0111 Acc: 83.8690\n",
      "Epoch 41/99\n",
      "training Loss: 0.0108 Acc: 84.1438\n",
      "validation Loss: 0.0111 Acc: 84.0238\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.1944\n",
      "validation Loss: 0.0111 Acc: 83.8214\n",
      "Epoch 43/99\n",
      "training Loss: 0.0108 Acc: 84.1974\n",
      "validation Loss: 0.0111 Acc: 84.0952\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.1289\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.2242\n",
      "validation Loss: 0.0111 Acc: 84.0119\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.2956\n",
      "validation Loss: 0.0111 Acc: 84.1429\n",
      "Epoch 47/99\n",
      "training Loss: 0.0108 Acc: 84.3551\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 48/99\n",
      "training Loss: 0.0108 Acc: 84.3015\n",
      "validation Loss: 0.0111 Acc: 84.1429\n",
      "Epoch 49/99\n",
      "training Loss: 0.0108 Acc: 84.2033\n",
      "validation Loss: 0.0111 Acc: 84.0357\n",
      "Epoch 50/99\n",
      "training Loss: 0.0108 Acc: 84.3968\n",
      "validation Loss: 0.0110 Acc: 84.1548\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0107 Acc: 84.3343\n",
      "validation Loss: 0.0110 Acc: 84.1548\n",
      "Epoch 52/99\n",
      "training Loss: 0.0107 Acc: 84.3224\n",
      "validation Loss: 0.0111 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0107 Acc: 84.3105\n",
      "validation Loss: 0.0111 Acc: 84.0833\n",
      "Epoch 54/99\n",
      "training Loss: 0.0107 Acc: 84.3521\n",
      "validation Loss: 0.0111 Acc: 84.1310\n",
      "Epoch 55/99\n",
      "training Loss: 0.0107 Acc: 84.4414\n",
      "validation Loss: 0.0111 Acc: 84.1667\n",
      "Epoch 56/99\n",
      "training Loss: 0.0107 Acc: 84.2986\n",
      "validation Loss: 0.0110 Acc: 84.2024\n",
      "Epoch 57/99\n",
      "training Loss: 0.0107 Acc: 84.4950\n",
      "validation Loss: 0.0110 Acc: 84.1310\n",
      "Epoch 58/99\n",
      "training Loss: 0.0107 Acc: 84.4265\n",
      "validation Loss: 0.0111 Acc: 84.1905\n",
      "Epoch 59/99\n",
      "training Loss: 0.0107 Acc: 84.3581\n",
      "validation Loss: 0.0111 Acc: 84.2381\n",
      "Epoch 60/99\n",
      "training Loss: 0.0107 Acc: 84.4295\n",
      "validation Loss: 0.0110 Acc: 84.1786\n",
      "Epoch 61/99\n",
      "training Loss: 0.0107 Acc: 84.4325\n",
      "validation Loss: 0.0110 Acc: 84.2143\n",
      "Epoch 62/99\n",
      "training Loss: 0.0107 Acc: 84.5277\n",
      "validation Loss: 0.0110 Acc: 84.2262\n",
      "Epoch 63/99\n",
      "training Loss: 0.0107 Acc: 84.4979\n",
      "validation Loss: 0.0111 Acc: 84.1310\n",
      "Epoch 64/99\n",
      "training Loss: 0.0107 Acc: 84.4652\n",
      "validation Loss: 0.0110 Acc: 84.1667\n",
      "Epoch 65/99\n",
      "training Loss: 0.0107 Acc: 84.4831\n",
      "validation Loss: 0.0110 Acc: 84.2262\n",
      "Epoch 66/99\n",
      "training Loss: 0.0107 Acc: 84.3700\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 67/99\n",
      "training Loss: 0.0107 Acc: 84.4682\n",
      "validation Loss: 0.0110 Acc: 84.0714\n",
      "Epoch 68/99\n",
      "training Loss: 0.0107 Acc: 84.4325\n",
      "validation Loss: 0.0111 Acc: 84.1071\n",
      "Epoch 69/99\n",
      "training Loss: 0.0107 Acc: 84.5218\n",
      "validation Loss: 0.0110 Acc: 84.2381\n",
      "Epoch 70/99\n",
      "training Loss: 0.0107 Acc: 84.4533\n",
      "validation Loss: 0.0110 Acc: 84.1071\n",
      "Epoch 71/99\n",
      "training Loss: 0.0107 Acc: 84.4444\n",
      "validation Loss: 0.0110 Acc: 84.1905\n",
      "Epoch 72/99\n",
      "training Loss: 0.0106 Acc: 84.4593\n",
      "validation Loss: 0.0110 Acc: 84.1905\n",
      "Early stopped.\n",
      "Best val acc: 84.238095\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.6738\n",
      "validation Loss: 0.0122 Acc: 82.6071\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.9474\n",
      "validation Loss: 0.0119 Acc: 82.9881\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 83.1855\n",
      "validation Loss: 0.0117 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.2183\n",
      "validation Loss: 0.0116 Acc: 83.1190\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.3879\n",
      "validation Loss: 0.0116 Acc: 83.0595\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.3403\n",
      "validation Loss: 0.0115 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.4087\n",
      "validation Loss: 0.0115 Acc: 83.3810\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.3938\n",
      "validation Loss: 0.0115 Acc: 83.2857\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.4325\n",
      "validation Loss: 0.0114 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.6111\n",
      "validation Loss: 0.0114 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0113 Acc: 83.6111\n",
      "validation Loss: 0.0114 Acc: 83.4048\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.7688\n",
      "validation Loss: 0.0114 Acc: 83.5238\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.6885\n",
      "validation Loss: 0.0113 Acc: 83.4524\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.8343\n",
      "validation Loss: 0.0113 Acc: 83.5357\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.8164\n",
      "validation Loss: 0.0113 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.8492\n",
      "validation Loss: 0.0113 Acc: 83.5476\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.7956\n",
      "validation Loss: 0.0113 Acc: 83.5714\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.8968\n",
      "validation Loss: 0.0113 Acc: 83.5595\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.9117\n",
      "validation Loss: 0.0113 Acc: 83.5000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.8700\n",
      "validation Loss: 0.0113 Acc: 83.5357\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 83.9087\n",
      "validation Loss: 0.0113 Acc: 83.5119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0110 Acc: 83.9504\n",
      "validation Loss: 0.0113 Acc: 83.4286\n",
      "Epoch 22/99\n",
      "training Loss: 0.0110 Acc: 84.0396\n",
      "validation Loss: 0.0112 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0110 Acc: 83.9771\n",
      "validation Loss: 0.0112 Acc: 83.5000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 84.0188\n",
      "validation Loss: 0.0112 Acc: 83.5357\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 84.0069\n",
      "validation Loss: 0.0113 Acc: 83.5833\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 84.1170\n",
      "validation Loss: 0.0112 Acc: 83.6667\n",
      "Saving..\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 84.0426\n",
      "validation Loss: 0.0112 Acc: 83.5595\n",
      "Epoch 28/99\n",
      "training Loss: 0.0109 Acc: 84.1051\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0110 Acc: 84.0932\n",
      "validation Loss: 0.0112 Acc: 83.7262\n",
      "Epoch 30/99\n",
      "training Loss: 0.0109 Acc: 84.2182\n",
      "validation Loss: 0.0113 Acc: 83.2619\n",
      "Epoch 31/99\n",
      "training Loss: 0.0109 Acc: 84.0932\n",
      "validation Loss: 0.0112 Acc: 83.5238\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.1646\n",
      "validation Loss: 0.0113 Acc: 83.4524\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.1587\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 84.2599\n",
      "validation Loss: 0.0112 Acc: 83.4762\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.1914\n",
      "validation Loss: 0.0113 Acc: 83.7500\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.2986\n",
      "validation Loss: 0.0112 Acc: 83.3571\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.3462\n",
      "validation Loss: 0.0112 Acc: 83.5714\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.2896\n",
      "validation Loss: 0.0112 Acc: 83.5238\n",
      "Epoch 39/99\n",
      "training Loss: 0.0108 Acc: 84.2480\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Epoch 40/99\n",
      "training Loss: 0.0108 Acc: 84.3670\n",
      "validation Loss: 0.0112 Acc: 83.6190\n",
      "Epoch 41/99\n",
      "training Loss: 0.0108 Acc: 84.3849\n",
      "validation Loss: 0.0112 Acc: 83.5714\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.3700\n",
      "validation Loss: 0.0112 Acc: 83.4524\n",
      "Epoch 43/99\n",
      "training Loss: 0.0108 Acc: 84.2450\n",
      "validation Loss: 0.0112 Acc: 83.6786\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.3610\n",
      "validation Loss: 0.0112 Acc: 83.5714\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.3224\n",
      "validation Loss: 0.0112 Acc: 83.6548\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.3878\n",
      "validation Loss: 0.0112 Acc: 83.5714\n",
      "Epoch 47/99\n",
      "training Loss: 0.0108 Acc: 84.4116\n",
      "validation Loss: 0.0112 Acc: 83.6429\n",
      "Epoch 48/99\n",
      "training Loss: 0.0107 Acc: 84.5218\n",
      "validation Loss: 0.0112 Acc: 83.7500\n",
      "Early stopped.\n",
      "Best val acc: 83.750000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0134 Acc: 81.0666\n",
      "validation Loss: 0.0121 Acc: 82.4048\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.8463\n",
      "validation Loss: 0.0118 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 83.1349\n",
      "validation Loss: 0.0117 Acc: 82.9643\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.2599\n",
      "validation Loss: 0.0117 Acc: 83.0357\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.4206\n",
      "validation Loss: 0.0116 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.3522\n",
      "validation Loss: 0.0115 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.3641\n",
      "validation Loss: 0.0115 Acc: 83.2381\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0113 Acc: 83.5129\n",
      "validation Loss: 0.0115 Acc: 83.2738\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.4742\n",
      "validation Loss: 0.0115 Acc: 83.2500\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.6438\n",
      "validation Loss: 0.0114 Acc: 83.2738\n",
      "Epoch 10/99\n",
      "training Loss: 0.0112 Acc: 83.6290\n",
      "validation Loss: 0.0115 Acc: 83.4048\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.6647\n",
      "validation Loss: 0.0114 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.6528\n",
      "validation Loss: 0.0114 Acc: 83.4405\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.5665\n",
      "validation Loss: 0.0114 Acc: 83.4048\n",
      "Epoch 14/99\n",
      "training Loss: 0.0111 Acc: 83.7450\n",
      "validation Loss: 0.0114 Acc: 83.4762\n",
      "Epoch 15/99\n",
      "training Loss: 0.0111 Acc: 83.8283\n",
      "validation Loss: 0.0113 Acc: 83.4405\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.8938\n",
      "validation Loss: 0.0113 Acc: 83.4881\n",
      "Epoch 17/99\n",
      "training Loss: 0.0111 Acc: 83.8075\n",
      "validation Loss: 0.0113 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.8700\n",
      "validation Loss: 0.0114 Acc: 83.6071\n",
      "Epoch 19/99\n",
      "training Loss: 0.0110 Acc: 83.9146\n",
      "validation Loss: 0.0113 Acc: 83.4762\n",
      "Epoch 20/99\n",
      "training Loss: 0.0110 Acc: 83.9682\n",
      "validation Loss: 0.0113 Acc: 83.6429\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0110 Acc: 83.9414\n",
      "validation Loss: 0.0113 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0110 Acc: 83.8343\n",
      "validation Loss: 0.0113 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0110 Acc: 84.1587\n",
      "validation Loss: 0.0113 Acc: 83.6905\n",
      "Epoch 24/99\n",
      "training Loss: 0.0110 Acc: 84.0039\n",
      "validation Loss: 0.0113 Acc: 83.7500\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.9920\n",
      "validation Loss: 0.0113 Acc: 83.6905\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 84.0010\n",
      "validation Loss: 0.0113 Acc: 83.6190\n",
      "Epoch 27/99\n",
      "training Loss: 0.0109 Acc: 84.1438\n",
      "validation Loss: 0.0113 Acc: 83.4286\n",
      "Epoch 28/99\n",
      "training Loss: 0.0109 Acc: 84.1378\n",
      "validation Loss: 0.0113 Acc: 83.6667\n",
      "Epoch 29/99\n",
      "training Loss: 0.0109 Acc: 84.0664\n",
      "validation Loss: 0.0113 Acc: 83.6905\n",
      "Epoch 30/99\n",
      "training Loss: 0.0109 Acc: 84.1498\n",
      "validation Loss: 0.0113 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0109 Acc: 84.1974\n",
      "validation Loss: 0.0113 Acc: 83.7262\n",
      "Epoch 32/99\n",
      "training Loss: 0.0109 Acc: 84.1795\n",
      "validation Loss: 0.0113 Acc: 83.6310\n",
      "Epoch 33/99\n",
      "training Loss: 0.0109 Acc: 84.0962\n",
      "validation Loss: 0.0113 Acc: 83.6667\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 84.1259\n",
      "validation Loss: 0.0112 Acc: 83.6429\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.2063\n",
      "validation Loss: 0.0113 Acc: 83.6786\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.2569\n",
      "validation Loss: 0.0113 Acc: 83.5714\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.1855\n",
      "validation Loss: 0.0113 Acc: 83.8571\n",
      "Epoch 38/99\n",
      "training Loss: 0.0108 Acc: 84.2539\n",
      "validation Loss: 0.0113 Acc: 83.5595\n",
      "Epoch 39/99\n",
      "training Loss: 0.0108 Acc: 84.1855\n",
      "validation Loss: 0.0113 Acc: 83.8095\n",
      "Epoch 40/99\n",
      "training Loss: 0.0108 Acc: 84.1617\n",
      "validation Loss: 0.0113 Acc: 83.6190\n",
      "Epoch 41/99\n",
      "training Loss: 0.0108 Acc: 84.2212\n",
      "validation Loss: 0.0112 Acc: 83.8333\n",
      "Epoch 42/99\n",
      "training Loss: 0.0108 Acc: 84.3878\n",
      "validation Loss: 0.0113 Acc: 83.7857\n",
      "Epoch 43/99\n",
      "training Loss: 0.0107 Acc: 84.3105\n",
      "validation Loss: 0.0112 Acc: 83.8333\n",
      "Epoch 44/99\n",
      "training Loss: 0.0108 Acc: 84.2569\n",
      "validation Loss: 0.0113 Acc: 83.8333\n",
      "Epoch 45/99\n",
      "training Loss: 0.0108 Acc: 84.3164\n",
      "validation Loss: 0.0112 Acc: 83.7381\n",
      "Epoch 46/99\n",
      "training Loss: 0.0108 Acc: 84.2480\n",
      "validation Loss: 0.0113 Acc: 83.5714\n",
      "Epoch 47/99\n",
      "training Loss: 0.0108 Acc: 84.3938\n",
      "validation Loss: 0.0113 Acc: 83.7619\n",
      "Epoch 48/99\n",
      "training Loss: 0.0107 Acc: 84.5485\n",
      "validation Loss: 0.0113 Acc: 83.7738\n",
      "Epoch 49/99\n",
      "training Loss: 0.0107 Acc: 84.3045\n",
      "validation Loss: 0.0112 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0107 Acc: 84.3610\n",
      "validation Loss: 0.0113 Acc: 83.8452\n",
      "Epoch 51/99\n",
      "training Loss: 0.0107 Acc: 84.4622\n",
      "validation Loss: 0.0112 Acc: 83.7619\n",
      "Epoch 52/99\n",
      "training Loss: 0.0107 Acc: 84.3849\n",
      "validation Loss: 0.0112 Acc: 83.7857\n",
      "Epoch 53/99\n",
      "training Loss: 0.0107 Acc: 84.3730\n",
      "validation Loss: 0.0112 Acc: 83.8452\n",
      "Epoch 54/99\n",
      "training Loss: 0.0107 Acc: 84.4652\n",
      "validation Loss: 0.0112 Acc: 83.8333\n",
      "Epoch 55/99\n",
      "training Loss: 0.0107 Acc: 84.4325\n",
      "validation Loss: 0.0112 Acc: 83.8571\n",
      "Epoch 56/99\n",
      "training Loss: 0.0107 Acc: 84.4116\n",
      "validation Loss: 0.0112 Acc: 83.8690\n",
      "Epoch 57/99\n",
      "training Loss: 0.0107 Acc: 84.2866\n",
      "validation Loss: 0.0112 Acc: 83.7262\n",
      "Epoch 58/99\n",
      "training Loss: 0.0107 Acc: 84.4444\n",
      "validation Loss: 0.0112 Acc: 83.7857\n",
      "Epoch 59/99\n",
      "training Loss: 0.0107 Acc: 84.4801\n",
      "validation Loss: 0.0112 Acc: 83.8452\n",
      "Epoch 60/99\n",
      "training Loss: 0.0107 Acc: 84.4979\n",
      "validation Loss: 0.0113 Acc: 83.8095\n",
      "Epoch 61/99\n",
      "training Loss: 0.0107 Acc: 84.2986\n",
      "validation Loss: 0.0112 Acc: 83.8452\n",
      "Epoch 62/99\n",
      "training Loss: 0.0107 Acc: 84.4295\n",
      "validation Loss: 0.0112 Acc: 83.8333\n",
      "Epoch 63/99\n",
      "training Loss: 0.0107 Acc: 84.4087\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Epoch 64/99\n",
      "training Loss: 0.0107 Acc: 84.4057\n",
      "validation Loss: 0.0113 Acc: 83.8095\n",
      "Epoch 65/99\n",
      "training Loss: 0.0107 Acc: 84.4890\n",
      "validation Loss: 0.0112 Acc: 83.8214\n",
      "Epoch 66/99\n",
      "training Loss: 0.0107 Acc: 84.4771\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Epoch 67/99\n",
      "training Loss: 0.0107 Acc: 84.5604\n",
      "validation Loss: 0.0113 Acc: 83.7976\n",
      "Epoch 68/99\n",
      "training Loss: 0.0107 Acc: 84.3789\n",
      "validation Loss: 0.0112 Acc: 83.8095\n",
      "Epoch 69/99\n",
      "training Loss: 0.0107 Acc: 84.5545\n",
      "validation Loss: 0.0112 Acc: 83.7976\n",
      "Early stopped.\n",
      "Best val acc: 83.904762\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0135 Acc: 80.9208\n",
      "validation Loss: 0.0119 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0119 Acc: 82.8463\n",
      "validation Loss: 0.0116 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0117 Acc: 82.9713\n",
      "validation Loss: 0.0115 Acc: 83.3095\n",
      "Epoch 3/99\n",
      "training Loss: 0.0116 Acc: 83.1171\n",
      "validation Loss: 0.0114 Acc: 83.3452\n",
      "Epoch 4/99\n",
      "training Loss: 0.0115 Acc: 83.2867\n",
      "validation Loss: 0.0114 Acc: 83.4524\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0114 Acc: 83.2927\n",
      "validation Loss: 0.0113 Acc: 83.5357\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0114 Acc: 83.3968\n",
      "validation Loss: 0.0113 Acc: 83.4286\n",
      "Epoch 7/99\n",
      "training Loss: 0.0114 Acc: 83.4444\n",
      "validation Loss: 0.0113 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0113 Acc: 83.4296\n",
      "validation Loss: 0.0113 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0113 Acc: 83.5099\n",
      "validation Loss: 0.0113 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0113 Acc: 83.5040\n",
      "validation Loss: 0.0112 Acc: 83.8810\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0112 Acc: 83.5843\n",
      "validation Loss: 0.0112 Acc: 83.7262\n",
      "Epoch 12/99\n",
      "training Loss: 0.0112 Acc: 83.6528\n",
      "validation Loss: 0.0112 Acc: 83.7976\n",
      "Epoch 13/99\n",
      "training Loss: 0.0112 Acc: 83.7272\n",
      "validation Loss: 0.0112 Acc: 83.9286\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0112 Acc: 83.6825\n",
      "validation Loss: 0.0112 Acc: 83.5833\n",
      "Epoch 15/99\n",
      "training Loss: 0.0112 Acc: 83.6260\n",
      "validation Loss: 0.0111 Acc: 83.7619\n",
      "Epoch 16/99\n",
      "training Loss: 0.0111 Acc: 83.7629\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 17/99\n",
      "training Loss: 0.0112 Acc: 83.8462\n",
      "validation Loss: 0.0111 Acc: 83.7143\n",
      "Epoch 18/99\n",
      "training Loss: 0.0111 Acc: 83.6974\n",
      "validation Loss: 0.0111 Acc: 83.8333\n",
      "Epoch 19/99\n",
      "training Loss: 0.0111 Acc: 83.8789\n",
      "validation Loss: 0.0111 Acc: 83.8095\n",
      "Epoch 20/99\n",
      "training Loss: 0.0111 Acc: 83.8879\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 21/99\n",
      "training Loss: 0.0111 Acc: 83.8432\n",
      "validation Loss: 0.0111 Acc: 84.0476\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0111 Acc: 83.9533\n",
      "validation Loss: 0.0111 Acc: 84.0238\n",
      "Epoch 23/99\n",
      "training Loss: 0.0111 Acc: 83.8908\n",
      "validation Loss: 0.0111 Acc: 84.2143\n",
      "Saving..\n",
      "Epoch 24/99\n",
      "training Loss: 0.0111 Acc: 83.9176\n",
      "validation Loss: 0.0111 Acc: 83.9762\n",
      "Epoch 25/99\n",
      "training Loss: 0.0110 Acc: 83.9801\n",
      "validation Loss: 0.0111 Acc: 83.9286\n",
      "Epoch 26/99\n",
      "training Loss: 0.0110 Acc: 83.8522\n",
      "validation Loss: 0.0111 Acc: 84.0000\n",
      "Epoch 27/99\n",
      "training Loss: 0.0110 Acc: 83.9355\n",
      "validation Loss: 0.0110 Acc: 84.0595\n",
      "Epoch 28/99\n",
      "training Loss: 0.0110 Acc: 83.9533\n",
      "validation Loss: 0.0111 Acc: 84.0595\n",
      "Epoch 29/99\n",
      "training Loss: 0.0110 Acc: 83.9950\n",
      "validation Loss: 0.0110 Acc: 83.9286\n",
      "Epoch 30/99\n",
      "training Loss: 0.0110 Acc: 83.9593\n",
      "validation Loss: 0.0111 Acc: 83.9167\n",
      "Epoch 31/99\n",
      "training Loss: 0.0110 Acc: 84.0277\n",
      "validation Loss: 0.0111 Acc: 84.0000\n",
      "Epoch 32/99\n",
      "training Loss: 0.0110 Acc: 84.0456\n",
      "validation Loss: 0.0111 Acc: 83.9405\n",
      "Epoch 33/99\n",
      "training Loss: 0.0110 Acc: 84.1140\n",
      "validation Loss: 0.0110 Acc: 84.1786\n",
      "Epoch 34/99\n",
      "training Loss: 0.0109 Acc: 84.0515\n",
      "validation Loss: 0.0111 Acc: 84.0238\n",
      "Epoch 35/99\n",
      "training Loss: 0.0109 Acc: 84.1230\n",
      "validation Loss: 0.0110 Acc: 83.9762\n",
      "Epoch 36/99\n",
      "training Loss: 0.0109 Acc: 84.2242\n",
      "validation Loss: 0.0111 Acc: 84.0000\n",
      "Epoch 37/99\n",
      "training Loss: 0.0109 Acc: 84.1200\n",
      "validation Loss: 0.0111 Acc: 83.8571\n",
      "Epoch 38/99\n",
      "training Loss: 0.0109 Acc: 84.1468\n",
      "validation Loss: 0.0110 Acc: 84.1667\n",
      "Epoch 39/99\n",
      "training Loss: 0.0109 Acc: 84.0843\n",
      "validation Loss: 0.0110 Acc: 84.0833\n",
      "Epoch 40/99\n",
      "training Loss: 0.0109 Acc: 84.2896\n",
      "validation Loss: 0.0111 Acc: 84.2024\n",
      "Epoch 41/99\n",
      "training Loss: 0.0109 Acc: 84.1230\n",
      "validation Loss: 0.0110 Acc: 83.9762\n",
      "Epoch 42/99\n",
      "training Loss: 0.0109 Acc: 84.2093\n",
      "validation Loss: 0.0111 Acc: 84.0238\n",
      "Epoch 43/99\n",
      "training Loss: 0.0109 Acc: 84.2182\n",
      "validation Loss: 0.0110 Acc: 83.9643\n",
      "Early stopped.\n",
      "Best val acc: 84.214286\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9198337131\n",
      "New configuration: {'learning_rate': 0.001032203756189414, 'initial_nodes': 200, 'dropout': 0.1141905893098015, 'batch_size': 359, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0012 Acc: 81.1756\n",
      "validation Loss: 0.0010 Acc: 83.6706\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.2083\n",
      "validation Loss: 0.0010 Acc: 83.6944\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.4077\n",
      "validation Loss: 0.0010 Acc: 83.5872\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.3899\n",
      "validation Loss: 0.0010 Acc: 83.7658\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.5476\n",
      "validation Loss: 0.0010 Acc: 83.8848\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.6667\n",
      "validation Loss: 0.0010 Acc: 83.9324\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.6190\n",
      "validation Loss: 0.0010 Acc: 83.7658\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.8750\n",
      "validation Loss: 0.0010 Acc: 84.0752\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.8512\n",
      "validation Loss: 0.0010 Acc: 83.8015\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.8542\n",
      "validation Loss: 0.0010 Acc: 84.4204\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.9494\n",
      "validation Loss: 0.0010 Acc: 84.2418\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 84.0565\n",
      "validation Loss: 0.0010 Acc: 84.1109\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 84.1012\n",
      "validation Loss: 0.0010 Acc: 84.0157\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 84.0119\n",
      "validation Loss: 0.0010 Acc: 83.8253\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 83.9107\n",
      "validation Loss: 0.0010 Acc: 84.1585\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 84.2054\n",
      "validation Loss: 0.0010 Acc: 84.1109\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 84.1935\n",
      "validation Loss: 0.0010 Acc: 83.9919\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 84.1071\n",
      "validation Loss: 0.0010 Acc: 83.8253\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 84.2679\n",
      "validation Loss: 0.0010 Acc: 84.0395\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.4256\n",
      "validation Loss: 0.0010 Acc: 84.1823\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.3839\n",
      "validation Loss: 0.0010 Acc: 84.0633\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.3095\n",
      "validation Loss: 0.0010 Acc: 84.1228\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.4732\n",
      "validation Loss: 0.0010 Acc: 83.9681\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.4494\n",
      "validation Loss: 0.0010 Acc: 84.2537\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.4256\n",
      "validation Loss: 0.0010 Acc: 83.9919\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.5089\n",
      "validation Loss: 0.0010 Acc: 84.2657\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.5476\n",
      "validation Loss: 0.0010 Acc: 84.0514\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.6399\n",
      "validation Loss: 0.0010 Acc: 84.0871\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.6310\n",
      "validation Loss: 0.0010 Acc: 83.8610\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.6577\n",
      "validation Loss: 0.0010 Acc: 84.1347\n",
      "Early stopped.\n",
      "Best val acc: 84.420376\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0012 Acc: 80.9684\n",
      "validation Loss: 0.0011 Acc: 82.2381\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1677\n",
      "validation Loss: 0.0011 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.4563\n",
      "validation Loss: 0.0011 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.6825\n",
      "validation Loss: 0.0010 Acc: 83.2500\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.6944\n",
      "validation Loss: 0.0010 Acc: 83.0000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.9087\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.9236\n",
      "validation Loss: 0.0011 Acc: 82.5238\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.8998\n",
      "validation Loss: 0.0010 Acc: 83.2619\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 84.1081\n",
      "validation Loss: 0.0010 Acc: 83.2619\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 84.1111\n",
      "validation Loss: 0.0010 Acc: 83.3452\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 84.1021\n",
      "validation Loss: 0.0010 Acc: 83.3690\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 84.3015\n",
      "validation Loss: 0.0010 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 84.2837\n",
      "validation Loss: 0.0010 Acc: 83.2738\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 84.1378\n",
      "validation Loss: 0.0010 Acc: 83.4524\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 84.2212\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 84.4116\n",
      "validation Loss: 0.0010 Acc: 83.4286\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 84.3372\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 84.2956\n",
      "validation Loss: 0.0010 Acc: 83.3333\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 84.5545\n",
      "validation Loss: 0.0010 Acc: 83.4643\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.4801\n",
      "validation Loss: 0.0010 Acc: 83.3810\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.5366\n",
      "validation Loss: 0.0010 Acc: 83.2619\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.5009\n",
      "validation Loss: 0.0010 Acc: 83.4286\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.4593\n",
      "validation Loss: 0.0010 Acc: 83.2738\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.6765\n",
      "validation Loss: 0.0010 Acc: 83.3452\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.5009\n",
      "validation Loss: 0.0010 Acc: 83.1429\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.5991\n",
      "validation Loss: 0.0011 Acc: 82.9881\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.7301\n",
      "validation Loss: 0.0010 Acc: 83.4524\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.7896\n",
      "validation Loss: 0.0010 Acc: 83.6190\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.7717\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.8134\n",
      "validation Loss: 0.0010 Acc: 83.5000\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.8223\n",
      "validation Loss: 0.0010 Acc: 83.4167\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.8491\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Early stopped.\n",
      "Best val acc: 83.619048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0012 Acc: 81.2005\n",
      "validation Loss: 0.0011 Acc: 82.7976\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.2867\n",
      "validation Loss: 0.0011 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.4563\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.5873\n",
      "validation Loss: 0.0010 Acc: 83.3333\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.7480\n",
      "validation Loss: 0.0011 Acc: 83.2024\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.6647\n",
      "validation Loss: 0.0010 Acc: 83.4881\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.7599\n",
      "validation Loss: 0.0011 Acc: 83.2619\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.9444\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.9831\n",
      "validation Loss: 0.0010 Acc: 83.4405\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.8908\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 84.0843\n",
      "validation Loss: 0.0011 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 84.1289\n",
      "validation Loss: 0.0011 Acc: 82.8690\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 84.1230\n",
      "validation Loss: 0.0010 Acc: 83.4881\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 84.1259\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 84.3551\n",
      "validation Loss: 0.0010 Acc: 83.5476\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 84.4503\n",
      "validation Loss: 0.0010 Acc: 83.3571\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 84.5158\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 84.4414\n",
      "validation Loss: 0.0010 Acc: 83.7262\n",
      "Saving..\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 84.5218\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 84.5307\n",
      "validation Loss: 0.0010 Acc: 83.5238\n",
      "Epoch 20/99\n",
      "training Loss: 0.0009 Acc: 84.3908\n",
      "validation Loss: 0.0010 Acc: 83.3810\n",
      "Epoch 21/99\n",
      "training Loss: 0.0009 Acc: 84.5128\n",
      "validation Loss: 0.0010 Acc: 83.6905\n",
      "Epoch 22/99\n",
      "training Loss: 0.0009 Acc: 84.5962\n",
      "validation Loss: 0.0010 Acc: 83.6548\n",
      "Epoch 23/99\n",
      "training Loss: 0.0009 Acc: 84.6973\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 24/99\n",
      "training Loss: 0.0009 Acc: 84.6854\n",
      "validation Loss: 0.0010 Acc: 83.5357\n",
      "Epoch 25/99\n",
      "training Loss: 0.0009 Acc: 84.6497\n",
      "validation Loss: 0.0010 Acc: 83.4762\n",
      "Epoch 26/99\n",
      "training Loss: 0.0009 Acc: 84.7063\n",
      "validation Loss: 0.0010 Acc: 83.5000\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.7688\n",
      "validation Loss: 0.0010 Acc: 83.4762\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.7420\n",
      "validation Loss: 0.0010 Acc: 83.4405\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.7360\n",
      "validation Loss: 0.0010 Acc: 83.6667\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.7807\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.6348\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 84.7539\n",
      "validation Loss: 0.0010 Acc: 83.5000\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 84.8461\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 84.8521\n",
      "validation Loss: 0.0010 Acc: 83.7500\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 84.7747\n",
      "validation Loss: 0.0010 Acc: 83.3095\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 84.8461\n",
      "validation Loss: 0.0010 Acc: 83.5238\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 84.8640\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 84.8491\n",
      "validation Loss: 0.0010 Acc: 83.6667\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 84.8432\n",
      "validation Loss: 0.0010 Acc: 83.4286\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 84.8432\n",
      "validation Loss: 0.0010 Acc: 83.4167\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 84.9146\n",
      "validation Loss: 0.0010 Acc: 83.4524\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 84.8997\n",
      "validation Loss: 0.0010 Acc: 83.5000\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 84.9533\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Epoch 44/99\n",
      "training Loss: 0.0009 Acc: 84.9116\n",
      "validation Loss: 0.0010 Acc: 83.5357\n",
      "Epoch 45/99\n",
      "training Loss: 0.0009 Acc: 84.8997\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 46/99\n",
      "training Loss: 0.0009 Acc: 84.9265\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Epoch 47/99\n",
      "training Loss: 0.0009 Acc: 84.9116\n",
      "validation Loss: 0.0010 Acc: 83.6429\n",
      "Epoch 48/99\n",
      "training Loss: 0.0009 Acc: 84.9890\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 49/99\n",
      "training Loss: 0.0009 Acc: 84.8134\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 50/99\n",
      "training Loss: 0.0009 Acc: 84.9027\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 51/99\n",
      "training Loss: 0.0009 Acc: 84.9354\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 52/99\n",
      "training Loss: 0.0009 Acc: 84.9563\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 53/99\n",
      "training Loss: 0.0009 Acc: 84.9652\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Epoch 54/99\n",
      "training Loss: 0.0009 Acc: 84.9533\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Early stopped.\n",
      "Best val acc: 83.750000\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0012 Acc: 81.0547\n",
      "validation Loss: 0.0011 Acc: 83.0476\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.1349\n",
      "validation Loss: 0.0010 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.3879\n",
      "validation Loss: 0.0010 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.4593\n",
      "validation Loss: 0.0010 Acc: 83.4881\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.5129\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.6409\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.8313\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.8432\n",
      "validation Loss: 0.0010 Acc: 83.5357\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.8432\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.8849\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 83.8254\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 83.9861\n",
      "validation Loss: 0.0010 Acc: 83.6548\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 84.0129\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 84.1408\n",
      "validation Loss: 0.0010 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 84.1795\n",
      "validation Loss: 0.0010 Acc: 83.8810\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 84.2182\n",
      "validation Loss: 0.0010 Acc: 84.2381\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 84.2628\n",
      "validation Loss: 0.0010 Acc: 84.0833\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 84.1170\n",
      "validation Loss: 0.0010 Acc: 84.2024\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 84.2122\n",
      "validation Loss: 0.0010 Acc: 83.9286\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.2390\n",
      "validation Loss: 0.0010 Acc: 84.2024\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.3194\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.3343\n",
      "validation Loss: 0.0010 Acc: 84.0119\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.3640\n",
      "validation Loss: 0.0010 Acc: 84.0357\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.3313\n",
      "validation Loss: 0.0010 Acc: 84.1190\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.3878\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.5337\n",
      "validation Loss: 0.0010 Acc: 84.1310\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 84.4414\n",
      "validation Loss: 0.0010 Acc: 84.0476\n",
      "Epoch 27/99\n",
      "training Loss: 0.0010 Acc: 84.4563\n",
      "validation Loss: 0.0010 Acc: 84.3333\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.5843\n",
      "validation Loss: 0.0010 Acc: 84.1786\n",
      "Epoch 29/99\n",
      "training Loss: 0.0010 Acc: 84.5426\n",
      "validation Loss: 0.0010 Acc: 84.2619\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.4593\n",
      "validation Loss: 0.0010 Acc: 84.1071\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.5991\n",
      "validation Loss: 0.0010 Acc: 83.9762\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 84.6497\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 84.6587\n",
      "validation Loss: 0.0010 Acc: 84.1786\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 84.7003\n",
      "validation Loss: 0.0010 Acc: 83.9881\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 84.6646\n",
      "validation Loss: 0.0010 Acc: 84.3690\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 84.7152\n",
      "validation Loss: 0.0010 Acc: 84.1310\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 84.7777\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 84.7747\n",
      "validation Loss: 0.0010 Acc: 84.0238\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 84.7420\n",
      "validation Loss: 0.0010 Acc: 84.0595\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 84.8551\n",
      "validation Loss: 0.0010 Acc: 84.1190\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 84.8491\n",
      "validation Loss: 0.0010 Acc: 84.0357\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 84.7390\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 84.8134\n",
      "validation Loss: 0.0010 Acc: 83.9643\n",
      "Epoch 44/99\n",
      "training Loss: 0.0009 Acc: 84.7658\n",
      "validation Loss: 0.0010 Acc: 84.2262\n",
      "Epoch 45/99\n",
      "training Loss: 0.0009 Acc: 84.8491\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Epoch 46/99\n",
      "training Loss: 0.0009 Acc: 84.9592\n",
      "validation Loss: 0.0010 Acc: 84.1905\n",
      "Epoch 47/99\n",
      "training Loss: 0.0009 Acc: 84.7836\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0009 Acc: 84.9086\n",
      "validation Loss: 0.0010 Acc: 84.0595\n",
      "Epoch 49/99\n",
      "training Loss: 0.0009 Acc: 84.9086\n",
      "validation Loss: 0.0010 Acc: 84.2024\n",
      "Epoch 50/99\n",
      "training Loss: 0.0009 Acc: 84.9414\n",
      "validation Loss: 0.0010 Acc: 84.1786\n",
      "Epoch 51/99\n",
      "training Loss: 0.0009 Acc: 84.9563\n",
      "validation Loss: 0.0010 Acc: 84.2619\n",
      "Epoch 52/99\n",
      "training Loss: 0.0009 Acc: 84.9890\n",
      "validation Loss: 0.0010 Acc: 84.1310\n",
      "Epoch 53/99\n",
      "training Loss: 0.0009 Acc: 84.9711\n",
      "validation Loss: 0.0010 Acc: 84.2143\n",
      "Epoch 54/99\n",
      "training Loss: 0.0009 Acc: 84.8670\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Epoch 55/99\n",
      "training Loss: 0.0009 Acc: 84.8699\n",
      "validation Loss: 0.0010 Acc: 84.1667\n",
      "Early stopped.\n",
      "Best val acc: 84.369048\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0012 Acc: 80.5577\n",
      "validation Loss: 0.0011 Acc: 82.8571\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0010 Acc: 83.2361\n",
      "validation Loss: 0.0011 Acc: 83.0714\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0010 Acc: 83.4206\n",
      "validation Loss: 0.0011 Acc: 83.1667\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0010 Acc: 83.5397\n",
      "validation Loss: 0.0010 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0010 Acc: 83.7063\n",
      "validation Loss: 0.0010 Acc: 83.3452\n",
      "Epoch 5/99\n",
      "training Loss: 0.0010 Acc: 83.7510\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0010 Acc: 83.8938\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0010 Acc: 83.8551\n",
      "validation Loss: 0.0010 Acc: 83.5595\n",
      "Epoch 8/99\n",
      "training Loss: 0.0010 Acc: 83.8164\n",
      "validation Loss: 0.0010 Acc: 83.5714\n",
      "Epoch 9/99\n",
      "training Loss: 0.0010 Acc: 83.8522\n",
      "validation Loss: 0.0010 Acc: 83.3452\n",
      "Epoch 10/99\n",
      "training Loss: 0.0010 Acc: 84.0129\n",
      "validation Loss: 0.0010 Acc: 83.5476\n",
      "Epoch 11/99\n",
      "training Loss: 0.0010 Acc: 84.0099\n",
      "validation Loss: 0.0010 Acc: 83.4167\n",
      "Epoch 12/99\n",
      "training Loss: 0.0010 Acc: 84.1557\n",
      "validation Loss: 0.0010 Acc: 83.5238\n",
      "Epoch 13/99\n",
      "training Loss: 0.0010 Acc: 84.1140\n",
      "validation Loss: 0.0010 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0010 Acc: 84.0367\n",
      "validation Loss: 0.0010 Acc: 83.5833\n",
      "Epoch 15/99\n",
      "training Loss: 0.0010 Acc: 84.3134\n",
      "validation Loss: 0.0010 Acc: 83.6548\n",
      "Epoch 16/99\n",
      "training Loss: 0.0010 Acc: 84.4295\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0010 Acc: 84.4741\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Epoch 18/99\n",
      "training Loss: 0.0010 Acc: 84.4444\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0010 Acc: 84.4474\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 20/99\n",
      "training Loss: 0.0010 Acc: 84.5307\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 21/99\n",
      "training Loss: 0.0010 Acc: 84.5069\n",
      "validation Loss: 0.0010 Acc: 83.8452\n",
      "Epoch 22/99\n",
      "training Loss: 0.0010 Acc: 84.5099\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 23/99\n",
      "training Loss: 0.0010 Acc: 84.4831\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0010 Acc: 84.4414\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Epoch 25/99\n",
      "training Loss: 0.0010 Acc: 84.6319\n",
      "validation Loss: 0.0010 Acc: 83.5952\n",
      "Epoch 26/99\n",
      "training Loss: 0.0010 Acc: 84.5783\n",
      "validation Loss: 0.0010 Acc: 83.7262\n",
      "Epoch 27/99\n",
      "training Loss: 0.0009 Acc: 84.6706\n",
      "validation Loss: 0.0010 Acc: 83.7381\n",
      "Epoch 28/99\n",
      "training Loss: 0.0009 Acc: 84.6081\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 29/99\n",
      "training Loss: 0.0009 Acc: 84.7777\n",
      "validation Loss: 0.0010 Acc: 83.8214\n",
      "Epoch 30/99\n",
      "training Loss: 0.0009 Acc: 84.5753\n",
      "validation Loss: 0.0010 Acc: 83.9167\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0009 Acc: 84.7836\n",
      "validation Loss: 0.0010 Acc: 83.8810\n",
      "Epoch 32/99\n",
      "training Loss: 0.0009 Acc: 84.7598\n",
      "validation Loss: 0.0010 Acc: 83.6429\n",
      "Epoch 33/99\n",
      "training Loss: 0.0009 Acc: 84.8580\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 34/99\n",
      "training Loss: 0.0009 Acc: 84.8610\n",
      "validation Loss: 0.0010 Acc: 83.8929\n",
      "Epoch 35/99\n",
      "training Loss: 0.0009 Acc: 84.8967\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 36/99\n",
      "training Loss: 0.0009 Acc: 84.8134\n",
      "validation Loss: 0.0010 Acc: 83.9405\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0009 Acc: 84.8313\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 38/99\n",
      "training Loss: 0.0009 Acc: 84.8283\n",
      "validation Loss: 0.0010 Acc: 83.7500\n",
      "Epoch 39/99\n",
      "training Loss: 0.0009 Acc: 84.8313\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 40/99\n",
      "training Loss: 0.0009 Acc: 84.9443\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 41/99\n",
      "training Loss: 0.0009 Acc: 85.0277\n",
      "validation Loss: 0.0010 Acc: 83.6786\n",
      "Epoch 42/99\n",
      "training Loss: 0.0009 Acc: 84.9205\n",
      "validation Loss: 0.0010 Acc: 83.8333\n",
      "Epoch 43/99\n",
      "training Loss: 0.0009 Acc: 84.9622\n",
      "validation Loss: 0.0010 Acc: 83.6310\n",
      "Epoch 44/99\n",
      "training Loss: 0.0009 Acc: 84.9979\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Epoch 45/99\n",
      "training Loss: 0.0009 Acc: 85.0515\n",
      "validation Loss: 0.0010 Acc: 83.9048\n",
      "Epoch 46/99\n",
      "training Loss: 0.0009 Acc: 84.9235\n",
      "validation Loss: 0.0010 Acc: 83.7738\n",
      "Epoch 47/99\n",
      "training Loss: 0.0009 Acc: 84.9979\n",
      "validation Loss: 0.0010 Acc: 83.8571\n",
      "Epoch 48/99\n",
      "training Loss: 0.0009 Acc: 85.0128\n",
      "validation Loss: 0.0010 Acc: 83.7143\n",
      "Epoch 49/99\n",
      "training Loss: 0.0009 Acc: 84.9741\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 50/99\n",
      "training Loss: 0.0009 Acc: 85.1616\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Epoch 51/99\n",
      "training Loss: 0.0009 Acc: 84.9116\n",
      "validation Loss: 0.0010 Acc: 83.7143\n",
      "Epoch 52/99\n",
      "training Loss: 0.0009 Acc: 85.0604\n",
      "validation Loss: 0.0010 Acc: 83.7262\n",
      "Epoch 53/99\n",
      "training Loss: 0.0009 Acc: 85.1051\n",
      "validation Loss: 0.0010 Acc: 83.8095\n",
      "Epoch 54/99\n",
      "training Loss: 0.0009 Acc: 85.0098\n",
      "validation Loss: 0.0010 Acc: 83.7976\n",
      "Epoch 55/99\n",
      "training Loss: 0.0009 Acc: 84.9979\n",
      "validation Loss: 0.0010 Acc: 83.7143\n",
      "Epoch 56/99\n",
      "training Loss: 0.0009 Acc: 85.1289\n",
      "validation Loss: 0.0010 Acc: 83.7024\n",
      "Early stopped.\n",
      "Best val acc: 83.940476\n",
      "----------\n",
      "Average best_acc across k-fold: 84.0197895059\n",
      "New configuration: {'learning_rate': 0.0011150682649498811, 'initial_nodes': 200, 'dropout': 0.8609088486412559, 'batch_size': 512, 'max_depth': 1}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 77.9494\n",
      "validation Loss: 0.0008 Acc: 82.7660\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.4583\n",
      "validation Loss: 0.0007 Acc: 83.0040\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.7679\n",
      "validation Loss: 0.0007 Acc: 83.3492\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 82.8869\n",
      "validation Loss: 0.0007 Acc: 83.6587\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.2470\n",
      "validation Loss: 0.0007 Acc: 83.6468\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.2351\n",
      "validation Loss: 0.0007 Acc: 83.7420\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.2262\n",
      "validation Loss: 0.0007 Acc: 83.6706\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.1994\n",
      "validation Loss: 0.0007 Acc: 83.8015\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.3423\n",
      "validation Loss: 0.0007 Acc: 83.8134\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.1429\n",
      "validation Loss: 0.0007 Acc: 83.8253\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.0655\n",
      "validation Loss: 0.0007 Acc: 83.9086\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.3423\n",
      "validation Loss: 0.0007 Acc: 83.9086\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.3899\n",
      "validation Loss: 0.0007 Acc: 83.8491\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.3185\n",
      "validation Loss: 0.0007 Acc: 84.0990\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.3869\n",
      "validation Loss: 0.0007 Acc: 83.9086\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.4286\n",
      "validation Loss: 0.0007 Acc: 83.9919\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.2708\n",
      "validation Loss: 0.0007 Acc: 83.7539\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.4494\n",
      "validation Loss: 0.0007 Acc: 83.9324\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.4167\n",
      "validation Loss: 0.0007 Acc: 83.8372\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.3661\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.5268\n",
      "validation Loss: 0.0007 Acc: 83.8491\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.4881\n",
      "validation Loss: 0.0007 Acc: 83.8729\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.6190\n",
      "validation Loss: 0.0007 Acc: 83.8848\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.4821\n",
      "validation Loss: 0.0007 Acc: 83.8134\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.4077\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.5893\n",
      "validation Loss: 0.0007 Acc: 83.9443\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 83.6637\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 83.4554\n",
      "validation Loss: 0.0007 Acc: 83.8967\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 83.6042\n",
      "validation Loss: 0.0007 Acc: 83.8610\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 83.6905\n",
      "validation Loss: 0.0007 Acc: 84.0395\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 83.6488\n",
      "validation Loss: 0.0007 Acc: 84.1942\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 83.7292\n",
      "validation Loss: 0.0007 Acc: 84.0752\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 83.7173\n",
      "validation Loss: 0.0007 Acc: 83.9562\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 83.6310\n",
      "validation Loss: 0.0007 Acc: 83.9562\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 83.6696\n",
      "validation Loss: 0.0007 Acc: 83.9205\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 83.7530\n",
      "validation Loss: 0.0007 Acc: 83.9800\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 83.6310\n",
      "validation Loss: 0.0007 Acc: 83.9324\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 83.6815\n",
      "validation Loss: 0.0007 Acc: 83.8253\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 83.7083\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 83.8363\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 83.6964\n",
      "validation Loss: 0.0007 Acc: 84.0038\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 83.9256\n",
      "validation Loss: 0.0007 Acc: 83.9443\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 83.7381\n",
      "validation Loss: 0.0007 Acc: 83.9800\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 83.6935\n",
      "validation Loss: 0.0007 Acc: 84.0038\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 83.8185\n",
      "validation Loss: 0.0007 Acc: 84.0276\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 83.8899\n",
      "validation Loss: 0.0007 Acc: 83.9562\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 83.7351\n",
      "validation Loss: 0.0007 Acc: 83.9324\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 83.7232\n",
      "validation Loss: 0.0007 Acc: 83.9800\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 83.8274\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 83.7976\n",
      "validation Loss: 0.0007 Acc: 83.9681\n",
      "Epoch 50/99\n",
      "training Loss: 0.0007 Acc: 83.7173\n",
      "validation Loss: 0.0007 Acc: 84.0157\n",
      "Early stopped.\n",
      "Best val acc: 84.194239\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 78.5727\n",
      "validation Loss: 0.0008 Acc: 82.6667\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.4028\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.8076\n",
      "validation Loss: 0.0007 Acc: 83.3690\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 82.9088\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.0873\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 82.9951\n",
      "validation Loss: 0.0007 Acc: 83.4643\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.1855\n",
      "validation Loss: 0.0007 Acc: 83.6071\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.2004\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.2718\n",
      "validation Loss: 0.0007 Acc: 83.5595\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.2599\n",
      "validation Loss: 0.0007 Acc: 83.6548\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.2808\n",
      "validation Loss: 0.0007 Acc: 83.2500\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.3373\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.3046\n",
      "validation Loss: 0.0007 Acc: 83.4881\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.4385\n",
      "validation Loss: 0.0007 Acc: 83.6071\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.3403\n",
      "validation Loss: 0.0007 Acc: 83.5238\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.4147\n",
      "validation Loss: 0.0007 Acc: 83.5952\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.4028\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.5724\n",
      "validation Loss: 0.0007 Acc: 83.5357\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.6409\n",
      "validation Loss: 0.0007 Acc: 83.5952\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.7391\n",
      "validation Loss: 0.0007 Acc: 83.5595\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.6706\n",
      "validation Loss: 0.0007 Acc: 83.5952\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.7420\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.6766\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.6022\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.6914\n",
      "validation Loss: 0.0007 Acc: 83.7738\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.7182\n",
      "validation Loss: 0.0007 Acc: 83.6310\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 83.8402\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 83.7242\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 83.8492\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 83.8194\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 83.6944\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 83.9742\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 83.9027\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 83.8432\n",
      "validation Loss: 0.0007 Acc: 83.8095\n",
      "Saving..\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 83.7867\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 83.8730\n",
      "validation Loss: 0.0007 Acc: 83.7024\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 83.9027\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 83.8641\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 83.8343\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 83.9057\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 83.7867\n",
      "validation Loss: 0.0007 Acc: 83.7619\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 84.1051\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 83.9623\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 84.0069\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 83.8343\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 83.7926\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 83.8849\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 83.8016\n",
      "validation Loss: 0.0007 Acc: 83.7143\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 83.9474\n",
      "validation Loss: 0.0007 Acc: 83.7857\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 83.8462\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Epoch 50/99\n",
      "training Loss: 0.0007 Acc: 83.8789\n",
      "validation Loss: 0.0007 Acc: 83.7262\n",
      "Epoch 51/99\n",
      "training Loss: 0.0007 Acc: 83.9504\n",
      "validation Loss: 0.0007 Acc: 83.7619\n",
      "Epoch 52/99\n",
      "training Loss: 0.0007 Acc: 83.6468\n",
      "validation Loss: 0.0007 Acc: 83.7500\n",
      "Epoch 53/99\n",
      "training Loss: 0.0007 Acc: 83.7897\n",
      "validation Loss: 0.0007 Acc: 83.7381\n",
      "Early stopped.\n",
      "Best val acc: 83.809524\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 77.7930\n",
      "validation Loss: 0.0008 Acc: 82.3810\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.3433\n",
      "validation Loss: 0.0008 Acc: 82.9048\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.8760\n",
      "validation Loss: 0.0008 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 82.8671\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.1052\n",
      "validation Loss: 0.0007 Acc: 83.1071\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.2331\n",
      "validation Loss: 0.0007 Acc: 82.9762\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.1141\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.1736\n",
      "validation Loss: 0.0007 Acc: 83.0357\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.0367\n",
      "validation Loss: 0.0007 Acc: 83.1786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0008 Acc: 83.1796\n",
      "validation Loss: 0.0007 Acc: 83.3095\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.1528\n",
      "validation Loss: 0.0007 Acc: 83.1071\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.2956\n",
      "validation Loss: 0.0007 Acc: 83.2738\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.3641\n",
      "validation Loss: 0.0007 Acc: 83.2143\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.3105\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.5813\n",
      "validation Loss: 0.0007 Acc: 83.2857\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.5367\n",
      "validation Loss: 0.0007 Acc: 83.2262\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.2748\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.2569\n",
      "validation Loss: 0.0007 Acc: 83.2381\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.3581\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.5337\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.4801\n",
      "validation Loss: 0.0007 Acc: 83.2976\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.5992\n",
      "validation Loss: 0.0007 Acc: 83.3690\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.7004\n",
      "validation Loss: 0.0007 Acc: 83.2857\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.5397\n",
      "validation Loss: 0.0007 Acc: 83.2381\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.4772\n",
      "validation Loss: 0.0007 Acc: 83.3095\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.5754\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 83.6290\n",
      "validation Loss: 0.0007 Acc: 83.3452\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 83.7182\n",
      "validation Loss: 0.0007 Acc: 83.4762\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 83.5486\n",
      "validation Loss: 0.0007 Acc: 83.2500\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 83.8402\n",
      "validation Loss: 0.0007 Acc: 83.5119\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 83.6706\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 83.5992\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 83.7837\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 83.7569\n",
      "validation Loss: 0.0007 Acc: 83.2143\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 83.5992\n",
      "validation Loss: 0.0007 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 83.8224\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 83.7510\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 83.6528\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 83.8492\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 83.9146\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 83.7212\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 83.8730\n",
      "validation Loss: 0.0007 Acc: 83.3095\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 83.8105\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 83.8789\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 83.8105\n",
      "validation Loss: 0.0007 Acc: 83.4524\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 84.0158\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 83.8819\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 83.8492\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 83.8283\n",
      "validation Loss: 0.0007 Acc: 83.4524\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 83.8105\n",
      "validation Loss: 0.0007 Acc: 83.4405\n",
      "Epoch 50/99\n",
      "training Loss: 0.0007 Acc: 83.9801\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 51/99\n",
      "training Loss: 0.0007 Acc: 84.0813\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 52/99\n",
      "training Loss: 0.0007 Acc: 83.8700\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 53/99\n",
      "training Loss: 0.0007 Acc: 84.0634\n",
      "validation Loss: 0.0007 Acc: 83.3929\n",
      "Epoch 54/99\n",
      "training Loss: 0.0007 Acc: 83.9623\n",
      "validation Loss: 0.0007 Acc: 83.4405\n",
      "Early stopped.\n",
      "Best val acc: 83.583333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 78.4626\n",
      "validation Loss: 0.0008 Acc: 82.8095\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.3314\n",
      "validation Loss: 0.0008 Acc: 83.0833\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.5100\n",
      "validation Loss: 0.0008 Acc: 83.6190\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 82.7719\n",
      "validation Loss: 0.0007 Acc: 83.7738\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.0635\n",
      "validation Loss: 0.0007 Acc: 83.5357\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.1766\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.1766\n",
      "validation Loss: 0.0007 Acc: 84.0357\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0008 Acc: 83.2183\n",
      "validation Loss: 0.0007 Acc: 83.9524\n",
      "Epoch 8/99\n",
      "training Loss: 0.0008 Acc: 83.2331\n",
      "validation Loss: 0.0007 Acc: 83.7619\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.2569\n",
      "validation Loss: 0.0007 Acc: 83.8214\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.3165\n",
      "validation Loss: 0.0007 Acc: 83.8095\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.3492\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.3849\n",
      "validation Loss: 0.0007 Acc: 83.9286\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.3641\n",
      "validation Loss: 0.0007 Acc: 83.9286\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.2867\n",
      "validation Loss: 0.0007 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.2510\n",
      "validation Loss: 0.0007 Acc: 83.6667\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.5278\n",
      "validation Loss: 0.0007 Acc: 84.0714\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.4712\n",
      "validation Loss: 0.0007 Acc: 83.9167\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.3909\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.3700\n",
      "validation Loss: 0.0007 Acc: 83.8690\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.6170\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.4444\n",
      "validation Loss: 0.0007 Acc: 84.0119\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0007 Acc: 83.8929\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.4147\n",
      "validation Loss: 0.0007 Acc: 83.8929\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.5605\n",
      "validation Loss: 0.0007 Acc: 83.9286\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0007 Acc: 84.1071\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 83.7093\n",
      "validation Loss: 0.0007 Acc: 83.9286\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 83.5010\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 83.6051\n",
      "validation Loss: 0.0007 Acc: 83.8571\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 83.6141\n",
      "validation Loss: 0.0007 Acc: 84.1548\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 83.5605\n",
      "validation Loss: 0.0007 Acc: 83.9762\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 83.7182\n",
      "validation Loss: 0.0007 Acc: 84.0476\n",
      "Epoch 32/99\n",
      "training Loss: 0.0007 Acc: 83.7569\n",
      "validation Loss: 0.0007 Acc: 84.1429\n",
      "Epoch 33/99\n",
      "training Loss: 0.0007 Acc: 83.9325\n",
      "validation Loss: 0.0007 Acc: 83.8810\n",
      "Epoch 34/99\n",
      "training Loss: 0.0007 Acc: 83.5337\n",
      "validation Loss: 0.0007 Acc: 84.0714\n",
      "Epoch 35/99\n",
      "training Loss: 0.0007 Acc: 83.7778\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0007 Acc: 83.7897\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Epoch 37/99\n",
      "training Loss: 0.0007 Acc: 83.8432\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Epoch 38/99\n",
      "training Loss: 0.0007 Acc: 83.8670\n",
      "validation Loss: 0.0007 Acc: 84.1190\n",
      "Epoch 39/99\n",
      "training Loss: 0.0007 Acc: 83.9504\n",
      "validation Loss: 0.0007 Acc: 84.0357\n",
      "Epoch 40/99\n",
      "training Loss: 0.0007 Acc: 83.7093\n",
      "validation Loss: 0.0007 Acc: 84.2262\n",
      "Saving..\n",
      "Epoch 41/99\n",
      "training Loss: 0.0007 Acc: 83.8760\n",
      "validation Loss: 0.0007 Acc: 84.0000\n",
      "Epoch 42/99\n",
      "training Loss: 0.0007 Acc: 83.8879\n",
      "validation Loss: 0.0007 Acc: 84.0357\n",
      "Epoch 43/99\n",
      "training Loss: 0.0007 Acc: 83.8938\n",
      "validation Loss: 0.0007 Acc: 84.0833\n",
      "Epoch 44/99\n",
      "training Loss: 0.0007 Acc: 83.8373\n",
      "validation Loss: 0.0007 Acc: 84.1905\n",
      "Epoch 45/99\n",
      "training Loss: 0.0007 Acc: 83.8194\n",
      "validation Loss: 0.0007 Acc: 84.1667\n",
      "Epoch 46/99\n",
      "training Loss: 0.0007 Acc: 83.9176\n",
      "validation Loss: 0.0007 Acc: 84.0714\n",
      "Epoch 47/99\n",
      "training Loss: 0.0007 Acc: 84.0158\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 48/99\n",
      "training Loss: 0.0007 Acc: 83.9623\n",
      "validation Loss: 0.0007 Acc: 84.1190\n",
      "Epoch 49/99\n",
      "training Loss: 0.0007 Acc: 84.0129\n",
      "validation Loss: 0.0007 Acc: 84.2976\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0007 Acc: 83.8432\n",
      "validation Loss: 0.0007 Acc: 84.4524\n",
      "Saving..\n",
      "Epoch 51/99\n",
      "training Loss: 0.0007 Acc: 83.9593\n",
      "validation Loss: 0.0007 Acc: 84.1190\n",
      "Epoch 52/99\n",
      "training Loss: 0.0007 Acc: 83.8819\n",
      "validation Loss: 0.0007 Acc: 84.3095\n",
      "Epoch 53/99\n",
      "training Loss: 0.0007 Acc: 83.9801\n",
      "validation Loss: 0.0007 Acc: 84.1548\n",
      "Epoch 54/99\n",
      "training Loss: 0.0007 Acc: 84.0605\n",
      "validation Loss: 0.0007 Acc: 84.2500\n",
      "Epoch 55/99\n",
      "training Loss: 0.0007 Acc: 84.0307\n",
      "validation Loss: 0.0007 Acc: 84.3214\n",
      "Epoch 56/99\n",
      "training Loss: 0.0007 Acc: 84.0396\n",
      "validation Loss: 0.0007 Acc: 84.0952\n",
      "Epoch 57/99\n",
      "training Loss: 0.0007 Acc: 83.9087\n",
      "validation Loss: 0.0007 Acc: 84.1429\n",
      "Epoch 58/99\n",
      "training Loss: 0.0007 Acc: 83.9920\n",
      "validation Loss: 0.0007 Acc: 84.1548\n",
      "Epoch 59/99\n",
      "training Loss: 0.0007 Acc: 84.0664\n",
      "validation Loss: 0.0007 Acc: 84.1667\n",
      "Epoch 60/99\n",
      "training Loss: 0.0007 Acc: 84.1914\n",
      "validation Loss: 0.0007 Acc: 84.2262\n",
      "Epoch 61/99\n",
      "training Loss: 0.0007 Acc: 83.9920\n",
      "validation Loss: 0.0007 Acc: 84.1548\n",
      "Epoch 62/99\n",
      "training Loss: 0.0007 Acc: 83.9950\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 63/99\n",
      "training Loss: 0.0007 Acc: 83.8849\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 64/99\n",
      "training Loss: 0.0007 Acc: 83.9890\n",
      "validation Loss: 0.0007 Acc: 84.2143\n",
      "Epoch 65/99\n",
      "training Loss: 0.0007 Acc: 84.0188\n",
      "validation Loss: 0.0007 Acc: 84.2024\n",
      "Epoch 66/99\n",
      "training Loss: 0.0007 Acc: 83.9712\n",
      "validation Loss: 0.0007 Acc: 84.1667\n",
      "Epoch 67/99\n",
      "training Loss: 0.0007 Acc: 84.0873\n",
      "validation Loss: 0.0007 Acc: 84.1786\n",
      "Epoch 68/99\n",
      "training Loss: 0.0007 Acc: 84.0129\n",
      "validation Loss: 0.0007 Acc: 84.2500\n",
      "Epoch 69/99\n",
      "training Loss: 0.0007 Acc: 83.9771\n",
      "validation Loss: 0.0007 Acc: 84.2738\n",
      "Epoch 70/99\n",
      "training Loss: 0.0007 Acc: 84.0992\n",
      "validation Loss: 0.0007 Acc: 84.2143\n",
      "Early stopped.\n",
      "Best val acc: 84.452381\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0009 Acc: 77.8674\n",
      "validation Loss: 0.0008 Acc: 82.5238\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0008 Acc: 82.4445\n",
      "validation Loss: 0.0008 Acc: 83.1190\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0008 Acc: 82.6558\n",
      "validation Loss: 0.0008 Acc: 83.0357\n",
      "Epoch 3/99\n",
      "training Loss: 0.0008 Acc: 83.0010\n",
      "validation Loss: 0.0007 Acc: 83.0952\n",
      "Epoch 4/99\n",
      "training Loss: 0.0008 Acc: 83.1915\n",
      "validation Loss: 0.0007 Acc: 83.1905\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0008 Acc: 83.1320\n",
      "validation Loss: 0.0007 Acc: 83.1071\n",
      "Epoch 6/99\n",
      "training Loss: 0.0008 Acc: 83.1528\n",
      "validation Loss: 0.0007 Acc: 83.1429\n",
      "Epoch 7/99\n",
      "training Loss: 0.0007 Acc: 83.3879\n",
      "validation Loss: 0.0007 Acc: 83.1310\n",
      "Epoch 8/99\n",
      "training Loss: 0.0007 Acc: 83.3819\n",
      "validation Loss: 0.0007 Acc: 83.1786\n",
      "Epoch 9/99\n",
      "training Loss: 0.0007 Acc: 83.2897\n",
      "validation Loss: 0.0007 Acc: 83.2857\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0007 Acc: 83.4593\n",
      "validation Loss: 0.0007 Acc: 83.2024\n",
      "Epoch 11/99\n",
      "training Loss: 0.0007 Acc: 83.4355\n",
      "validation Loss: 0.0007 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0007 Acc: 83.4623\n",
      "validation Loss: 0.0007 Acc: 83.5000\n",
      "Epoch 13/99\n",
      "training Loss: 0.0007 Acc: 83.4474\n",
      "validation Loss: 0.0007 Acc: 83.5595\n",
      "Epoch 14/99\n",
      "training Loss: 0.0007 Acc: 83.5188\n",
      "validation Loss: 0.0007 Acc: 83.4286\n",
      "Epoch 15/99\n",
      "training Loss: 0.0007 Acc: 83.5040\n",
      "validation Loss: 0.0007 Acc: 83.0238\n",
      "Epoch 16/99\n",
      "training Loss: 0.0007 Acc: 83.4861\n",
      "validation Loss: 0.0007 Acc: 83.4643\n",
      "Epoch 17/99\n",
      "training Loss: 0.0007 Acc: 83.7807\n",
      "validation Loss: 0.0007 Acc: 83.3571\n",
      "Epoch 18/99\n",
      "training Loss: 0.0007 Acc: 83.6349\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 19/99\n",
      "training Loss: 0.0007 Acc: 83.6706\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 20/99\n",
      "training Loss: 0.0007 Acc: 83.6528\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Epoch 21/99\n",
      "training Loss: 0.0007 Acc: 83.6528\n",
      "validation Loss: 0.0007 Acc: 83.2857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0007 Acc: 83.7420\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 23/99\n",
      "training Loss: 0.0007 Acc: 83.6349\n",
      "validation Loss: 0.0007 Acc: 83.4524\n",
      "Epoch 24/99\n",
      "training Loss: 0.0007 Acc: 83.7182\n",
      "validation Loss: 0.0007 Acc: 83.5476\n",
      "Epoch 25/99\n",
      "training Loss: 0.0007 Acc: 83.6141\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 26/99\n",
      "training Loss: 0.0007 Acc: 83.7510\n",
      "validation Loss: 0.0007 Acc: 83.4167\n",
      "Epoch 27/99\n",
      "training Loss: 0.0007 Acc: 83.7004\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 28/99\n",
      "training Loss: 0.0007 Acc: 83.6676\n",
      "validation Loss: 0.0007 Acc: 83.3810\n",
      "Epoch 29/99\n",
      "training Loss: 0.0007 Acc: 83.7926\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Epoch 30/99\n",
      "training Loss: 0.0007 Acc: 83.7153\n",
      "validation Loss: 0.0007 Acc: 83.4048\n",
      "Epoch 31/99\n",
      "training Loss: 0.0007 Acc: 83.7420\n",
      "validation Loss: 0.0007 Acc: 83.3333\n",
      "Early stopped.\n",
      "Best val acc: 83.583333\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9245621791\n",
      "New configuration: {'learning_rate': 0.004307294075258005, 'initial_nodes': 547, 'dropout': 0.5375720342346063, 'batch_size': 437, 'max_depth': 4}\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 80.7470\n",
      "validation Loss: 0.0009 Acc: 83.2778\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 82.9970\n",
      "validation Loss: 0.0009 Acc: 83.2897\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.1786\n",
      "validation Loss: 0.0009 Acc: 83.0755\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.4137\n",
      "validation Loss: 0.0009 Acc: 83.3016\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.1964\n",
      "validation Loss: 0.0009 Acc: 83.3135\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.3780\n",
      "validation Loss: 0.0009 Acc: 83.3492\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.5327\n",
      "validation Loss: 0.0009 Acc: 83.3373\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.3869\n",
      "validation Loss: 0.0009 Acc: 83.3373\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.5714\n",
      "validation Loss: 0.0009 Acc: 83.3968\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.3185\n",
      "validation Loss: 0.0009 Acc: 83.4801\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.8006\n",
      "validation Loss: 0.0009 Acc: 83.1588\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.6667\n",
      "validation Loss: 0.0009 Acc: 82.8017\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.3988\n",
      "validation Loss: 0.0009 Acc: 83.3849\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.8185\n",
      "validation Loss: 0.0009 Acc: 83.5753\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.9435\n",
      "validation Loss: 0.0009 Acc: 83.5277\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 84.1935\n",
      "validation Loss: 0.0009 Acc: 83.3611\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 84.2202\n",
      "validation Loss: 0.0009 Acc: 83.5872\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 84.1161\n",
      "validation Loss: 0.0009 Acc: 83.2897\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 84.0268\n",
      "validation Loss: 0.0009 Acc: 83.5158\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 84.2381\n",
      "validation Loss: 0.0009 Acc: 83.4087\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 84.2024\n",
      "validation Loss: 0.0009 Acc: 83.4920\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.4851\n",
      "validation Loss: 0.0009 Acc: 83.5158\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.4851\n",
      "validation Loss: 0.0009 Acc: 83.5872\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.5774\n",
      "validation Loss: 0.0009 Acc: 83.5039\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.6101\n",
      "validation Loss: 0.0009 Acc: 83.4325\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.6101\n",
      "validation Loss: 0.0009 Acc: 83.3849\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.7292\n",
      "validation Loss: 0.0009 Acc: 83.3373\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.7143\n",
      "validation Loss: 0.0009 Acc: 83.6110\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.8125\n",
      "validation Loss: 0.0009 Acc: 83.7539\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.8542\n",
      "validation Loss: 0.0009 Acc: 83.5396\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 85.0506\n",
      "validation Loss: 0.0009 Acc: 83.5158\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.0536\n",
      "validation Loss: 0.0009 Acc: 83.5515\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.1280\n",
      "validation Loss: 0.0009 Acc: 83.5515\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.1369\n",
      "validation Loss: 0.0009 Acc: 83.5158\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.3155\n",
      "validation Loss: 0.0009 Acc: 83.5872\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.3006\n",
      "validation Loss: 0.0009 Acc: 83.4682\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 85.2619\n",
      "validation Loss: 0.0009 Acc: 83.5277\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 85.2946\n",
      "validation Loss: 0.0009 Acc: 83.4325\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 85.2798\n",
      "validation Loss: 0.0009 Acc: 83.4444\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 85.4048\n",
      "validation Loss: 0.0009 Acc: 83.4087\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 85.4018\n",
      "validation Loss: 0.0009 Acc: 83.5158\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 85.5833\n",
      "validation Loss: 0.0009 Acc: 83.4444\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 85.5119\n",
      "validation Loss: 0.0009 Acc: 83.4801\n",
      "Epoch 43/99\n",
      "training Loss: 0.0008 Acc: 85.5952\n",
      "validation Loss: 0.0009 Acc: 83.4920\n",
      "Epoch 44/99\n",
      "training Loss: 0.0008 Acc: 85.5417\n",
      "validation Loss: 0.0009 Acc: 83.4682\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 85.5565\n",
      "validation Loss: 0.0009 Acc: 83.3968\n",
      "Epoch 46/99\n",
      "training Loss: 0.0008 Acc: 85.6458\n",
      "validation Loss: 0.0009 Acc: 83.4206\n",
      "Epoch 47/99\n",
      "training Loss: 0.0008 Acc: 85.5298\n",
      "validation Loss: 0.0009 Acc: 83.5158\n",
      "Epoch 48/99\n",
      "training Loss: 0.0008 Acc: 85.5565\n",
      "validation Loss: 0.0009 Acc: 83.4444\n",
      "Early stopped.\n",
      "Best val acc: 83.753868\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 80.5518\n",
      "validation Loss: 0.0009 Acc: 83.3452\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 83.0278\n",
      "validation Loss: 0.0009 Acc: 83.0714\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.1081\n",
      "validation Loss: 0.0009 Acc: 83.7143\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.2808\n",
      "validation Loss: 0.0009 Acc: 82.8571\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.3671\n",
      "validation Loss: 0.0009 Acc: 83.6905\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.2361\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.2212\n",
      "validation Loss: 0.0009 Acc: 83.4048\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.3552\n",
      "validation Loss: 0.0009 Acc: 83.4881\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.3135\n",
      "validation Loss: 0.0009 Acc: 83.6667\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.8492\n",
      "validation Loss: 0.0008 Acc: 84.2143\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0008 Acc: 83.7182\n",
      "validation Loss: 0.0008 Acc: 83.8214\n",
      "Epoch 11/99\n",
      "training Loss: 0.0008 Acc: 83.9563\n",
      "validation Loss: 0.0008 Acc: 83.7381\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.8641\n",
      "validation Loss: 0.0009 Acc: 83.8690\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.8194\n",
      "validation Loss: 0.0008 Acc: 83.6071\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.8730\n",
      "validation Loss: 0.0008 Acc: 84.0476\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.8343\n",
      "validation Loss: 0.0009 Acc: 83.9048\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.8522\n",
      "validation Loss: 0.0009 Acc: 83.7381\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.9593\n",
      "validation Loss: 0.0009 Acc: 83.9762\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 83.9533\n",
      "validation Loss: 0.0008 Acc: 83.9524\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 84.1438\n",
      "validation Loss: 0.0009 Acc: 84.0238\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 83.8879\n",
      "validation Loss: 0.0009 Acc: 83.9643\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.1587\n",
      "validation Loss: 0.0008 Acc: 83.7381\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.1736\n",
      "validation Loss: 0.0008 Acc: 84.0119\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.4801\n",
      "validation Loss: 0.0008 Acc: 83.9762\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.6289\n",
      "validation Loss: 0.0008 Acc: 84.2024\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.4890\n",
      "validation Loss: 0.0008 Acc: 83.9524\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.6557\n",
      "validation Loss: 0.0009 Acc: 84.0119\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.7271\n",
      "validation Loss: 0.0008 Acc: 83.8333\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.7063\n",
      "validation Loss: 0.0008 Acc: 83.8571\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.9384\n",
      "validation Loss: 0.0008 Acc: 83.9643\n",
      "Early stopped.\n",
      "Best val acc: 84.214286\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 80.5577\n",
      "validation Loss: 0.0009 Acc: 82.8929\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 82.8820\n",
      "validation Loss: 0.0009 Acc: 83.5238\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.2331\n",
      "validation Loss: 0.0009 Acc: 83.8333\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.1260\n",
      "validation Loss: 0.0009 Acc: 83.6190\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.2599\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.4177\n",
      "validation Loss: 0.0009 Acc: 83.0833\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.3313\n",
      "validation Loss: 0.0009 Acc: 83.3214\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.3343\n",
      "validation Loss: 0.0009 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.5307\n",
      "validation Loss: 0.0009 Acc: 83.5238\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.4653\n",
      "validation Loss: 0.0009 Acc: 83.6786\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.4325\n",
      "validation Loss: 0.0009 Acc: 83.2976\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.6468\n",
      "validation Loss: 0.0009 Acc: 83.5119\n",
      "Epoch 12/99\n",
      "training Loss: 0.0008 Acc: 83.7599\n",
      "validation Loss: 0.0009 Acc: 83.5952\n",
      "Epoch 13/99\n",
      "training Loss: 0.0008 Acc: 83.8849\n",
      "validation Loss: 0.0009 Acc: 83.6548\n",
      "Epoch 14/99\n",
      "training Loss: 0.0008 Acc: 83.7897\n",
      "validation Loss: 0.0009 Acc: 83.7143\n",
      "Epoch 15/99\n",
      "training Loss: 0.0008 Acc: 83.9146\n",
      "validation Loss: 0.0009 Acc: 83.6667\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.9742\n",
      "validation Loss: 0.0008 Acc: 83.8571\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.8522\n",
      "validation Loss: 0.0009 Acc: 83.7976\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 83.9504\n",
      "validation Loss: 0.0009 Acc: 83.7500\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 83.9504\n",
      "validation Loss: 0.0009 Acc: 83.7738\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 84.0010\n",
      "validation Loss: 0.0009 Acc: 83.6310\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.3789\n",
      "validation Loss: 0.0009 Acc: 83.7857\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.3551\n",
      "validation Loss: 0.0009 Acc: 83.8571\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.4295\n",
      "validation Loss: 0.0008 Acc: 83.8333\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.5099\n",
      "validation Loss: 0.0008 Acc: 83.8095\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.6348\n",
      "validation Loss: 0.0009 Acc: 83.8333\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.4414\n",
      "validation Loss: 0.0009 Acc: 83.5714\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.6110\n",
      "validation Loss: 0.0009 Acc: 83.6190\n",
      "Early stopped.\n",
      "Best val acc: 83.892857\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 80.0339\n",
      "validation Loss: 0.0009 Acc: 83.0000\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 82.9445\n",
      "validation Loss: 0.0009 Acc: 82.8095\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 83.1141\n",
      "validation Loss: 0.0009 Acc: 82.7738\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.1558\n",
      "validation Loss: 0.0009 Acc: 83.2619\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.1915\n",
      "validation Loss: 0.0009 Acc: 82.9762\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.5843\n",
      "validation Loss: 0.0009 Acc: 82.9643\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.4593\n",
      "validation Loss: 0.0009 Acc: 83.3214\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.3909\n",
      "validation Loss: 0.0009 Acc: 83.3571\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.6051\n",
      "validation Loss: 0.0009 Acc: 83.3452\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.5546\n",
      "validation Loss: 0.0009 Acc: 83.3095\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.5337\n",
      "validation Loss: 0.0009 Acc: 83.3929\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.6200\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.5605\n",
      "validation Loss: 0.0009 Acc: 83.2619\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.8492\n",
      "validation Loss: 0.0009 Acc: 83.3333\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.8641\n",
      "validation Loss: 0.0009 Acc: 82.8095\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 83.7718\n",
      "validation Loss: 0.0009 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0008 Acc: 83.9117\n",
      "validation Loss: 0.0009 Acc: 83.3571\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.9206\n",
      "validation Loss: 0.0009 Acc: 83.5119\n",
      "Epoch 18/99\n",
      "training Loss: 0.0008 Acc: 84.1378\n",
      "validation Loss: 0.0009 Acc: 83.0000\n",
      "Epoch 19/99\n",
      "training Loss: 0.0008 Acc: 84.1170\n",
      "validation Loss: 0.0008 Acc: 83.3452\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 84.1795\n",
      "validation Loss: 0.0009 Acc: 83.1786\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 84.2361\n",
      "validation Loss: 0.0009 Acc: 83.3214\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 84.3224\n",
      "validation Loss: 0.0009 Acc: 83.2500\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 84.1914\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 84.3789\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 84.3700\n",
      "validation Loss: 0.0009 Acc: 83.1905\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 84.7063\n",
      "validation Loss: 0.0009 Acc: 83.4048\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 84.7360\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 84.7955\n",
      "validation Loss: 0.0009 Acc: 83.4286\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.8759\n",
      "validation Loss: 0.0009 Acc: 83.4405\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 84.8313\n",
      "validation Loss: 0.0009 Acc: 83.1905\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 85.0247\n",
      "validation Loss: 0.0009 Acc: 83.2738\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 85.0455\n",
      "validation Loss: 0.0009 Acc: 83.1429\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 85.1437\n",
      "validation Loss: 0.0009 Acc: 83.1667\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 85.1378\n",
      "validation Loss: 0.0009 Acc: 83.1190\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 85.2449\n",
      "validation Loss: 0.0009 Acc: 82.9881\n",
      "Early stopped.\n",
      "Best val acc: 83.583333\n",
      "----------\n",
      "Epoch 0/99\n",
      "training Loss: 0.0010 Acc: 80.4714\n",
      "validation Loss: 0.0009 Acc: 83.1310\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0009 Acc: 83.0546\n",
      "validation Loss: 0.0009 Acc: 83.4762\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0009 Acc: 82.9564\n",
      "validation Loss: 0.0009 Acc: 83.5833\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0009 Acc: 83.1468\n",
      "validation Loss: 0.0009 Acc: 83.4881\n",
      "Epoch 4/99\n",
      "training Loss: 0.0009 Acc: 83.2331\n",
      "validation Loss: 0.0009 Acc: 83.8929\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0009 Acc: 83.5546\n",
      "validation Loss: 0.0009 Acc: 83.6429\n",
      "Epoch 6/99\n",
      "training Loss: 0.0009 Acc: 83.1736\n",
      "validation Loss: 0.0009 Acc: 83.9048\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0009 Acc: 83.2242\n",
      "validation Loss: 0.0009 Acc: 83.7976\n",
      "Epoch 8/99\n",
      "training Loss: 0.0009 Acc: 83.2153\n",
      "validation Loss: 0.0009 Acc: 83.7738\n",
      "Epoch 9/99\n",
      "training Loss: 0.0009 Acc: 83.4236\n",
      "validation Loss: 0.0009 Acc: 83.7738\n",
      "Epoch 10/99\n",
      "training Loss: 0.0009 Acc: 83.3700\n",
      "validation Loss: 0.0009 Acc: 83.7500\n",
      "Epoch 11/99\n",
      "training Loss: 0.0009 Acc: 83.4385\n",
      "validation Loss: 0.0009 Acc: 83.6429\n",
      "Epoch 12/99\n",
      "training Loss: 0.0009 Acc: 83.4504\n",
      "validation Loss: 0.0009 Acc: 83.8095\n",
      "Epoch 13/99\n",
      "training Loss: 0.0009 Acc: 83.4087\n",
      "validation Loss: 0.0009 Acc: 84.0595\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0009 Acc: 83.6051\n",
      "validation Loss: 0.0009 Acc: 84.0119\n",
      "Epoch 15/99\n",
      "training Loss: 0.0009 Acc: 83.5307\n",
      "validation Loss: 0.0009 Acc: 83.8810\n",
      "Epoch 16/99\n",
      "training Loss: 0.0009 Acc: 83.4742\n",
      "validation Loss: 0.0009 Acc: 83.8095\n",
      "Epoch 17/99\n",
      "training Loss: 0.0008 Acc: 83.4891\n",
      "validation Loss: 0.0009 Acc: 83.7262\n",
      "Epoch 18/99\n",
      "training Loss: 0.0009 Acc: 83.6825\n",
      "validation Loss: 0.0009 Acc: 84.0833\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0009 Acc: 83.6587\n",
      "validation Loss: 0.0008 Acc: 83.9762\n",
      "Epoch 20/99\n",
      "training Loss: 0.0008 Acc: 83.7272\n",
      "validation Loss: 0.0009 Acc: 83.9167\n",
      "Epoch 21/99\n",
      "training Loss: 0.0008 Acc: 83.6468\n",
      "validation Loss: 0.0008 Acc: 84.0357\n",
      "Epoch 22/99\n",
      "training Loss: 0.0008 Acc: 83.5426\n",
      "validation Loss: 0.0008 Acc: 83.9881\n",
      "Epoch 23/99\n",
      "training Loss: 0.0008 Acc: 83.6349\n",
      "validation Loss: 0.0009 Acc: 83.9524\n",
      "Epoch 24/99\n",
      "training Loss: 0.0008 Acc: 83.8492\n",
      "validation Loss: 0.0009 Acc: 83.6310\n",
      "Epoch 25/99\n",
      "training Loss: 0.0008 Acc: 83.6944\n",
      "validation Loss: 0.0009 Acc: 83.5000\n",
      "Epoch 26/99\n",
      "training Loss: 0.0008 Acc: 83.6676\n",
      "validation Loss: 0.0009 Acc: 83.9762\n",
      "Epoch 27/99\n",
      "training Loss: 0.0008 Acc: 83.8254\n",
      "validation Loss: 0.0008 Acc: 84.0357\n",
      "Epoch 28/99\n",
      "training Loss: 0.0008 Acc: 83.9801\n",
      "validation Loss: 0.0009 Acc: 84.1905\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0008 Acc: 84.1170\n",
      "validation Loss: 0.0008 Acc: 83.9881\n",
      "Epoch 30/99\n",
      "training Loss: 0.0008 Acc: 84.3105\n",
      "validation Loss: 0.0009 Acc: 84.0476\n",
      "Epoch 31/99\n",
      "training Loss: 0.0008 Acc: 84.2628\n",
      "validation Loss: 0.0009 Acc: 84.1190\n",
      "Epoch 32/99\n",
      "training Loss: 0.0008 Acc: 84.2450\n",
      "validation Loss: 0.0009 Acc: 84.0000\n",
      "Epoch 33/99\n",
      "training Loss: 0.0008 Acc: 84.3581\n",
      "validation Loss: 0.0009 Acc: 84.0000\n",
      "Epoch 34/99\n",
      "training Loss: 0.0008 Acc: 84.4087\n",
      "validation Loss: 0.0009 Acc: 83.7024\n",
      "Epoch 35/99\n",
      "training Loss: 0.0008 Acc: 84.4384\n",
      "validation Loss: 0.0009 Acc: 84.1310\n",
      "Epoch 36/99\n",
      "training Loss: 0.0008 Acc: 84.5664\n",
      "validation Loss: 0.0009 Acc: 83.9762\n",
      "Epoch 37/99\n",
      "training Loss: 0.0008 Acc: 84.6973\n",
      "validation Loss: 0.0009 Acc: 83.8333\n",
      "Epoch 38/99\n",
      "training Loss: 0.0008 Acc: 84.9146\n",
      "validation Loss: 0.0009 Acc: 84.0000\n",
      "Epoch 39/99\n",
      "training Loss: 0.0008 Acc: 85.0009\n",
      "validation Loss: 0.0009 Acc: 83.9286\n",
      "Epoch 40/99\n",
      "training Loss: 0.0008 Acc: 84.9414\n",
      "validation Loss: 0.0009 Acc: 83.8095\n",
      "Epoch 41/99\n",
      "training Loss: 0.0008 Acc: 85.0664\n",
      "validation Loss: 0.0009 Acc: 84.1786\n",
      "Epoch 42/99\n",
      "training Loss: 0.0008 Acc: 85.1795\n",
      "validation Loss: 0.0009 Acc: 84.0119\n",
      "Epoch 43/99\n",
      "training Loss: 0.0008 Acc: 85.4681\n",
      "validation Loss: 0.0009 Acc: 84.0476\n",
      "Epoch 44/99\n",
      "training Loss: 0.0008 Acc: 85.4205\n",
      "validation Loss: 0.0009 Acc: 83.9881\n",
      "Epoch 45/99\n",
      "training Loss: 0.0008 Acc: 85.3253\n",
      "validation Loss: 0.0009 Acc: 84.1429\n",
      "Epoch 46/99\n",
      "training Loss: 0.0008 Acc: 85.3908\n",
      "validation Loss: 0.0009 Acc: 83.8810\n",
      "Epoch 47/99\n",
      "training Loss: 0.0008 Acc: 85.4473\n",
      "validation Loss: 0.0009 Acc: 83.9762\n",
      "Epoch 48/99\n",
      "training Loss: 0.0008 Acc: 85.7925\n",
      "validation Loss: 0.0009 Acc: 83.9048\n",
      "Early stopped.\n",
      "Best val acc: 84.190476\n",
      "----------\n",
      "Average best_acc across k-fold: 83.9269641015\n",
      "Best parameters: [3, 376, 0.0009993292097751978, 0.6279291595015061, 331]\n",
      "Save best configuration to BestConfig.json\n",
      "Epoch 0/99\n",
      "training Loss: 0.0014 Acc: 78.7649\n",
      "validation Loss: 0.0012 Acc: 83.3611\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0012 Acc: 82.7768\n",
      "validation Loss: 0.0011 Acc: 83.5039\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0012 Acc: 83.0119\n",
      "validation Loss: 0.0011 Acc: 83.7896\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0012 Acc: 83.0744\n",
      "validation Loss: 0.0011 Acc: 83.5039\n",
      "Epoch 4/99\n",
      "training Loss: 0.0012 Acc: 83.2500\n",
      "validation Loss: 0.0011 Acc: 83.3016\n",
      "Epoch 5/99\n",
      "training Loss: 0.0012 Acc: 83.5030\n",
      "validation Loss: 0.0011 Acc: 83.8967\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0012 Acc: 83.2738\n",
      "validation Loss: 0.0011 Acc: 83.8015\n",
      "Epoch 7/99\n",
      "training Loss: 0.0012 Acc: 83.4286\n",
      "validation Loss: 0.0011 Acc: 83.8253\n",
      "Epoch 8/99\n",
      "training Loss: 0.0011 Acc: 83.5030\n",
      "validation Loss: 0.0011 Acc: 83.9443\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0011 Acc: 83.4345\n",
      "validation Loss: 0.0011 Acc: 83.9205\n",
      "Epoch 10/99\n",
      "training Loss: 0.0011 Acc: 83.6161\n",
      "validation Loss: 0.0011 Acc: 84.1347\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0011 Acc: 83.4494\n",
      "validation Loss: 0.0011 Acc: 83.9562\n",
      "Epoch 12/99\n",
      "training Loss: 0.0011 Acc: 83.8482\n",
      "validation Loss: 0.0011 Acc: 83.8967\n",
      "Epoch 13/99\n",
      "training Loss: 0.0011 Acc: 83.8036\n",
      "validation Loss: 0.0011 Acc: 83.6587\n",
      "Epoch 14/99\n",
      "training Loss: 0.0011 Acc: 83.7976\n",
      "validation Loss: 0.0011 Acc: 83.6944\n",
      "Epoch 15/99\n",
      "training Loss: 0.0011 Acc: 83.7619\n",
      "validation Loss: 0.0011 Acc: 83.9205\n",
      "Epoch 16/99\n",
      "training Loss: 0.0011 Acc: 83.6220\n",
      "validation Loss: 0.0011 Acc: 83.9443\n",
      "Epoch 17/99\n",
      "training Loss: 0.0011 Acc: 83.7262\n",
      "validation Loss: 0.0011 Acc: 83.6944\n",
      "Epoch 18/99\n",
      "training Loss: 0.0011 Acc: 83.7946\n",
      "validation Loss: 0.0011 Acc: 83.7063\n",
      "Epoch 19/99\n",
      "training Loss: 0.0011 Acc: 83.8512\n",
      "validation Loss: 0.0011 Acc: 83.9443\n",
      "Epoch 20/99\n",
      "training Loss: 0.0011 Acc: 83.9643\n",
      "validation Loss: 0.0011 Acc: 83.6944\n",
      "Epoch 21/99\n",
      "training Loss: 0.0011 Acc: 83.9018\n",
      "validation Loss: 0.0011 Acc: 84.0157\n",
      "Epoch 22/99\n",
      "training Loss: 0.0011 Acc: 83.8780\n",
      "validation Loss: 0.0011 Acc: 83.7182\n",
      "Epoch 23/99\n",
      "training Loss: 0.0011 Acc: 83.8274\n",
      "validation Loss: 0.0011 Acc: 83.8253\n",
      "Epoch 24/99\n",
      "training Loss: 0.0011 Acc: 83.9881\n",
      "validation Loss: 0.0011 Acc: 84.0276\n",
      "Epoch 25/99\n",
      "training Loss: 0.0011 Acc: 83.9137\n",
      "validation Loss: 0.0011 Acc: 83.8015\n",
      "Epoch 26/99\n",
      "training Loss: 0.0011 Acc: 83.9286\n",
      "validation Loss: 0.0011 Acc: 83.8729\n",
      "Epoch 27/99\n",
      "training Loss: 0.0011 Acc: 83.9196\n",
      "validation Loss: 0.0011 Acc: 83.8372\n",
      "Epoch 28/99\n",
      "training Loss: 0.0011 Acc: 84.2500\n",
      "validation Loss: 0.0011 Acc: 83.9205\n",
      "Epoch 29/99\n",
      "training Loss: 0.0011 Acc: 84.0268\n",
      "validation Loss: 0.0011 Acc: 83.9800\n",
      "Epoch 30/99\n",
      "training Loss: 0.0011 Acc: 83.9554\n",
      "validation Loss: 0.0011 Acc: 83.7301\n",
      "Early stopped.\n",
      "Best val acc: 84.134730\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "space  = [Integer(1, 5, name='max_depth'),\n",
    "          Integer(200, 1000, name='initial_nodes'),\n",
    "          Real(10**-5, 10**0, \"log-uniform\", name='learning_rate'),\n",
    "          Real(0.01,0.9,name='dropout'),\n",
    "          Integer(32,512,name='batch_size')\n",
    "          ]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**X):\n",
    "    print(\"New configuration: {}\".format(X))\n",
    "    fom = []\n",
    "    for train_index, test_index in skf.split(data, label):\n",
    "        train_loader = DataLoader(EventHLF(data[train_index], label[train_index]), batch_size = X['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(EventHLF(data[test_index], label[test_index]), batch_size = X['batch_size'], shuffle=True)\n",
    "        data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "        model = SimpleDNN(X['max_depth'], X['initial_nodes'], X['dropout']).cuda()\n",
    "        optimizer = AMSGrad(model.parameters(), lr=X['learning_rate'])\n",
    "        criterion= nn.NLLLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=5)\n",
    "        best_acc = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "        fom.append(best_acc)\n",
    "    Y = np.mean(np.asarray(fom))\n",
    "    print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "    return -Y\n",
    "\n",
    "res_gp = gp_minimize(objective, space, n_calls=50, random_state=0)\n",
    "\n",
    "print(\"Best parameters: {}\".format(res_gp.x))\n",
    "best_max_depth = res_gp.x[0]\n",
    "best_initial_nodes = res_gp.x[1]\n",
    "best_learning_rate = res_gp.x[2]\n",
    "best_dropout = res_gp.x[3]\n",
    "best_batch_size = res_gp.x[4]\n",
    "\n",
    "bestconf = {\"max_depth\": best_max_depth,\n",
    "          \"initial_nodes\": best_initial_nodes,\n",
    "          \"learning_rate\": best_learning_rate,\n",
    "          \"dropout\": best_dropout,\n",
    "          \"batch_size\": best_batch_size}\n",
    "with open(config_file, 'w') as config:\n",
    "    json.dump(bestconf, config)\n",
    "    print(\"Save best configuration to {}\".format(config_file))\n",
    "\n",
    "train_index, test_index = skf.split(data, label).next()\n",
    "model = SimpleDNN(best_max_depth, best_initial_nodes, best_dropout).cuda()\n",
    "train_loader = DataLoader(EventHLF(data[train_index], label[train_index]), batch_size = best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(EventHLF(data[test_index], label[test_index]), batch_size = best_batch_size, shuffle=True)\n",
    "data_loader = {\"training\": train_loader, \"validation\": val_loader}\n",
    "optimizer = AMSGrad(model.parameters(), lr=best_learning_rate)\n",
    "criterion= nn.NLLLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=5)\n",
    "best_acc = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "\n",
    "torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best configuration from BestConfig.json\n",
      "Loaded best model parameter from SimpleDNN.torch\n"
     ]
    }
   ],
   "source": [
    "with open(config_file) as f:\n",
    "    best_conf = json.load(f)\n",
    "print(\"Loaded best configuration from {}\".format(config_file))\n",
    "model = SimpleDNN(best_conf[\"max_depth\"], best_conf[\"initial_nodes\"], best_conf[\"dropout\"]).cuda()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "print(\"Loaded best model parameter from {}\".format(model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "training Loss: 0.0099 Acc: 83.7444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:105: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation Loss: 0.0070 Acc: 88.6144\n",
      "Saving..\n",
      "Epoch 1/99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SimpleDNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0077 Acc: 88.8628\n",
      "validation Loss: 0.0061 Acc: 90.4371\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0074 Acc: 89.4164\n",
      "validation Loss: 0.0060 Acc: 90.5834\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0073 Acc: 89.5900\n",
      "validation Loss: 0.0059 Acc: 90.5747\n",
      "Epoch 4/99\n",
      "training Loss: 0.0072 Acc: 89.7367\n",
      "validation Loss: 0.0058 Acc: 90.6112\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0071 Acc: 89.8451\n",
      "validation Loss: 0.0060 Acc: 90.7697\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0070 Acc: 89.9182\n",
      "validation Loss: 0.0060 Acc: 90.3780\n",
      "Epoch 7/99\n",
      "training Loss: 0.0070 Acc: 90.0562\n",
      "validation Loss: 0.0058 Acc: 90.8863\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0069 Acc: 90.1485\n",
      "validation Loss: 0.0059 Acc: 90.4284\n",
      "Epoch 9/99\n",
      "training Loss: 0.0069 Acc: 90.2002\n",
      "validation Loss: 0.0059 Acc: 90.9472\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0068 Acc: 90.2586\n",
      "validation Loss: 0.0057 Acc: 90.8550\n",
      "Epoch 11/99\n",
      "training Loss: 0.0068 Acc: 90.2533\n",
      "validation Loss: 0.0059 Acc: 91.0186\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0068 Acc: 90.2703\n",
      "validation Loss: 0.0058 Acc: 90.6809\n",
      "Epoch 13/99\n",
      "training Loss: 0.0068 Acc: 90.3696\n",
      "validation Loss: 0.0055 Acc: 91.5809\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0068 Acc: 90.3073\n",
      "validation Loss: 0.0058 Acc: 91.2136\n",
      "Epoch 15/99\n",
      "training Loss: 0.0068 Acc: 90.4209\n",
      "validation Loss: 0.0062 Acc: 90.6321\n",
      "Epoch 16/99\n",
      "training Loss: 0.0067 Acc: 90.4801\n",
      "validation Loss: 0.0057 Acc: 90.4493\n",
      "Epoch 17/99\n",
      "training Loss: 0.0067 Acc: 90.3996\n",
      "validation Loss: 0.0055 Acc: 91.1805\n",
      "Epoch 18/99\n",
      "training Loss: 0.0067 Acc: 90.4588\n",
      "validation Loss: 0.0061 Acc: 90.8062\n",
      "Epoch 19/99\n",
      "training Loss: 0.0067 Acc: 90.5167\n",
      "validation Loss: 0.0056 Acc: 91.3720\n",
      "Epoch 20/99\n",
      "training Loss: 0.0062 Acc: 91.2500\n",
      "validation Loss: 0.0053 Acc: 91.9535\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0061 Acc: 91.3932\n",
      "validation Loss: 0.0053 Acc: 91.8107\n",
      "Epoch 22/99\n",
      "training Loss: 0.0061 Acc: 91.4428\n",
      "validation Loss: 0.0052 Acc: 91.9987\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0060 Acc: 91.4912\n",
      "validation Loss: 0.0053 Acc: 91.7916\n",
      "Epoch 24/99\n",
      "training Loss: 0.0060 Acc: 91.5216\n",
      "validation Loss: 0.0052 Acc: 92.0719\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0060 Acc: 91.5821\n",
      "validation Loss: 0.0052 Acc: 91.9761\n",
      "Epoch 26/99\n",
      "training Loss: 0.0060 Acc: 91.5913\n",
      "validation Loss: 0.0052 Acc: 92.0614\n",
      "Epoch 27/99\n",
      "training Loss: 0.0060 Acc: 91.5651\n",
      "validation Loss: 0.0052 Acc: 92.0005\n",
      "Epoch 28/99\n",
      "training Loss: 0.0060 Acc: 91.6283\n",
      "validation Loss: 0.0052 Acc: 92.0458\n",
      "Epoch 29/99\n",
      "training Loss: 0.0060 Acc: 91.6204\n",
      "validation Loss: 0.0052 Acc: 92.0666\n",
      "Epoch 30/99\n",
      "training Loss: 0.0060 Acc: 91.5678\n",
      "validation Loss: 0.0053 Acc: 92.1241\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0060 Acc: 91.6104\n",
      "validation Loss: 0.0053 Acc: 91.9657\n",
      "Epoch 32/99\n",
      "training Loss: 0.0060 Acc: 91.5878\n",
      "validation Loss: 0.0052 Acc: 92.1206\n",
      "Epoch 33/99\n",
      "training Loss: 0.0060 Acc: 91.5673\n",
      "validation Loss: 0.0053 Acc: 91.9117\n",
      "Epoch 34/99\n",
      "training Loss: 0.0057 Acc: 91.9786\n",
      "validation Loss: 0.0051 Acc: 92.3069\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0057 Acc: 92.0235\n",
      "validation Loss: 0.0051 Acc: 92.2651\n",
      "Epoch 36/99\n",
      "training Loss: 0.0057 Acc: 92.0935\n",
      "validation Loss: 0.0051 Acc: 92.3243\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0057 Acc: 92.0718\n",
      "validation Loss: 0.0051 Acc: 92.3504\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0056 Acc: 92.0387\n",
      "validation Loss: 0.0051 Acc: 92.4636\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0057 Acc: 92.0335\n",
      "validation Loss: 0.0051 Acc: 92.2547\n",
      "Epoch 40/99\n",
      "training Loss: 0.0056 Acc: 92.0752\n",
      "validation Loss: 0.0051 Acc: 92.2895\n",
      "Epoch 41/99\n",
      "training Loss: 0.0057 Acc: 92.0966\n",
      "validation Loss: 0.0052 Acc: 92.1607\n",
      "Epoch 42/99\n",
      "training Loss: 0.0056 Acc: 92.1044\n",
      "validation Loss: 0.0051 Acc: 92.4601\n",
      "Epoch 43/99\n",
      "training Loss: 0.0056 Acc: 92.1061\n",
      "validation Loss: 0.0051 Acc: 92.2512\n",
      "Epoch 44/99\n",
      "training Loss: 0.0056 Acc: 92.1027\n",
      "validation Loss: 0.0053 Acc: 92.1955\n",
      "Epoch 45/99\n",
      "training Loss: 0.0055 Acc: 92.2245\n",
      "validation Loss: 0.0051 Acc: 92.4375\n",
      "Epoch 46/99\n",
      "training Loss: 0.0055 Acc: 92.2376\n",
      "validation Loss: 0.0051 Acc: 92.4009\n",
      "Epoch 47/99\n",
      "training Loss: 0.0055 Acc: 92.3185\n",
      "validation Loss: 0.0051 Acc: 92.4618\n",
      "Epoch 48/99\n",
      "training Loss: 0.0055 Acc: 92.2937\n",
      "validation Loss: 0.0051 Acc: 92.4200\n",
      "Epoch 49/99\n",
      "training Loss: 0.0055 Acc: 92.3455\n",
      "validation Loss: 0.0052 Acc: 92.4914\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0055 Acc: 92.2702\n",
      "validation Loss: 0.0050 Acc: 92.4200\n",
      "Epoch 51/99\n",
      "training Loss: 0.0055 Acc: 92.2815\n",
      "validation Loss: 0.0051 Acc: 92.5280\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0055 Acc: 92.3399\n",
      "validation Loss: 0.0052 Acc: 92.3556\n",
      "Epoch 53/99\n",
      "training Loss: 0.0055 Acc: 92.3534\n",
      "validation Loss: 0.0051 Acc: 92.4218\n",
      "Epoch 54/99\n",
      "training Loss: 0.0055 Acc: 92.3438\n",
      "validation Loss: 0.0051 Acc: 92.3922\n",
      "Epoch 55/99\n",
      "training Loss: 0.0055 Acc: 92.3246\n",
      "validation Loss: 0.0052 Acc: 92.4566\n",
      "Epoch 56/99\n",
      "training Loss: 0.0055 Acc: 92.3177\n",
      "validation Loss: 0.0052 Acc: 92.5593\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0054 Acc: 92.4199\n",
      "validation Loss: 0.0051 Acc: 92.5297\n",
      "Epoch 58/99\n",
      "training Loss: 0.0054 Acc: 92.4121\n",
      "validation Loss: 0.0051 Acc: 92.5454\n",
      "Epoch 59/99\n",
      "training Loss: 0.0054 Acc: 92.4435\n",
      "validation Loss: 0.0051 Acc: 92.4862\n",
      "Epoch 60/99\n",
      "training Loss: 0.0054 Acc: 92.4478\n",
      "validation Loss: 0.0051 Acc: 92.4601\n",
      "Epoch 61/99\n",
      "training Loss: 0.0054 Acc: 92.4343\n",
      "validation Loss: 0.0052 Acc: 92.5384\n",
      "Epoch 62/99\n",
      "training Loss: 0.0054 Acc: 92.4104\n",
      "validation Loss: 0.0051 Acc: 92.4670\n",
      "Epoch 63/99\n",
      "training Loss: 0.0054 Acc: 92.4857\n",
      "validation Loss: 0.0051 Acc: 92.4827\n",
      "Epoch 64/99\n",
      "training Loss: 0.0054 Acc: 92.4717\n",
      "validation Loss: 0.0051 Acc: 92.5019\n",
      "Epoch 65/99\n",
      "training Loss: 0.0053 Acc: 92.4713\n",
      "validation Loss: 0.0052 Acc: 92.5019\n",
      "Epoch 66/99\n",
      "training Loss: 0.0054 Acc: 92.4691\n",
      "validation Loss: 0.0051 Acc: 92.4723\n",
      "Epoch 67/99\n",
      "training Loss: 0.0054 Acc: 92.5087\n",
      "validation Loss: 0.0052 Acc: 92.4827\n",
      "Epoch 68/99\n",
      "training Loss: 0.0054 Acc: 92.5035\n",
      "validation Loss: 0.0051 Acc: 92.4670\n",
      "Epoch 69/99\n",
      "training Loss: 0.0053 Acc: 92.5209\n",
      "validation Loss: 0.0051 Acc: 92.5053\n",
      "Epoch 70/99\n",
      "training Loss: 0.0053 Acc: 92.5157\n",
      "validation Loss: 0.0051 Acc: 92.5158\n",
      "Epoch 71/99\n",
      "training Loss: 0.0053 Acc: 92.4974\n",
      "validation Loss: 0.0051 Acc: 92.5645\n",
      "Saving..\n",
      "Epoch 72/99\n",
      "training Loss: 0.0053 Acc: 92.5344\n",
      "validation Loss: 0.0051 Acc: 92.5053\n",
      "Epoch 73/99\n",
      "training Loss: 0.0053 Acc: 92.5449\n",
      "validation Loss: 0.0051 Acc: 92.5350\n",
      "Epoch 74/99\n",
      "training Loss: 0.0053 Acc: 92.5514\n",
      "validation Loss: 0.0051 Acc: 92.5053\n",
      "Epoch 75/99\n",
      "training Loss: 0.0054 Acc: 92.5453\n",
      "validation Loss: 0.0051 Acc: 92.5158\n",
      "Epoch 76/99\n",
      "training Loss: 0.0053 Acc: 92.5740\n",
      "validation Loss: 0.0051 Acc: 92.5280\n",
      "Epoch 77/99\n",
      "training Loss: 0.0053 Acc: 92.5388\n",
      "validation Loss: 0.0051 Acc: 92.5158\n",
      "Epoch 78/99\n",
      "training Loss: 0.0053 Acc: 92.5544\n",
      "validation Loss: 0.0051 Acc: 92.5262\n",
      "Epoch 79/99\n",
      "training Loss: 0.0053 Acc: 92.5227\n",
      "validation Loss: 0.0051 Acc: 92.4984\n",
      "Epoch 80/99\n",
      "training Loss: 0.0053 Acc: 92.5248\n",
      "validation Loss: 0.0051 Acc: 92.5262\n",
      "Epoch 81/99\n",
      "training Loss: 0.0053 Acc: 92.5409\n",
      "validation Loss: 0.0051 Acc: 92.5332\n",
      "Epoch 82/99\n",
      "training Loss: 0.0053 Acc: 92.5849\n",
      "validation Loss: 0.0051 Acc: 92.5402\n",
      "Epoch 83/99\n",
      "training Loss: 0.0053 Acc: 92.5618\n",
      "validation Loss: 0.0051 Acc: 92.5228\n",
      "Epoch 84/99\n",
      "training Loss: 0.0053 Acc: 92.5583\n",
      "validation Loss: 0.0051 Acc: 92.5228\n",
      "Epoch 85/99\n",
      "training Loss: 0.0053 Acc: 92.5292\n",
      "validation Loss: 0.0051 Acc: 92.5332\n",
      "Epoch 86/99\n",
      "training Loss: 0.0053 Acc: 92.5357\n",
      "validation Loss: 0.0051 Acc: 92.5350\n",
      "Epoch 87/99\n",
      "training Loss: 0.0053 Acc: 92.4865\n",
      "validation Loss: 0.0051 Acc: 92.5367\n",
      "Epoch 88/99\n",
      "training Loss: 0.0053 Acc: 92.5779\n",
      "validation Loss: 0.0051 Acc: 92.5315\n",
      "Epoch 89/99\n",
      "training Loss: 0.0053 Acc: 92.5849\n",
      "validation Loss: 0.0051 Acc: 92.5384\n",
      "Epoch 90/99\n",
      "training Loss: 0.0053 Acc: 92.5318\n",
      "validation Loss: 0.0051 Acc: 92.5506\n",
      "Epoch 91/99\n",
      "training Loss: 0.0053 Acc: 92.5723\n",
      "validation Loss: 0.0051 Acc: 92.5524\n",
      "Early stopped.\n",
      "Best val acc: 92.564545\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "best_max_depth = 2\n",
    "best_initial_nodes = 128\n",
    "best_learning_rate = 0.0036864344871878527\n",
    "best_dropout = 0.6935861755978148\n",
    "best_batch_size = 40\n",
    "\n",
    "bestconf = {\"max_depth\": best_max_depth,\n",
    "          \"initial_nodes\": best_initial_nodes,\n",
    "          \"learning_rate\": best_learning_rate,\n",
    "          \"dropout\": best_dropout,\n",
    "          \"batch_size\": best_batch_size}\n",
    "#with open(config_file, 'w') as config:\n",
    "   # json.dump(bestconf, config)\n",
    "    #print(\"Save best configuration to {}\".format(config_file))\n",
    "\n",
    "train_index, test_index = next(skf.split(data, label))\n",
    "model = SimpleDNN(best_max_depth, best_initial_nodes, best_dropout).cuda()\n",
    "train_loader = DataLoader(EventHLF(data[train_index], label[train_index]), batch_size = best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(EventHLF(data[test_index], label[test_index]), batch_size = best_batch_size, shuffle=True)\n",
    "data_loader = {\"training\": train_loader, \"validation\": val_loader}\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "criterion= nn.NLLLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=5)\n",
    "best_acc = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC.\n",
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.2985  |       0.9602      |    0.1157 +/- 0.0017     |\n",
      "|   0.4618  |       0.9352      |    0.0776 +/- 0.0005     |\n",
      "|   0.6523  |       0.9002      |    0.0516 +/- 0.0015     |\n",
      "|   0.9287  |       0.7001      |    0.0124 +/- 0.0007     |\n",
      "|   0.9851  |       0.5001      |    0.0038 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 57440, index implies 143601.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-368ecd1c0b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# bkg_frame = pd.DataFrame.from_records(bkgnp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0msig_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NN_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mbkg_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NN_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbkg_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    312\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         raise ValueError(\n\u001b[0;32m--> 314\u001b[0;31m                             \u001b[0;34mf\"Length of passed values is {len(data)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m                             \u001b[0;34mf\"index implies {len(index)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                         )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of passed values is 57440, index implies 143601."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fprs = []\n",
    "base_tpr = np.linspace(0, 1, 5000)\n",
    "thresholds = []\n",
    "\n",
    "for train_index, test_index in skf.split(data, label):\n",
    "    x_val = data[test_index]\n",
    "    y_val = label[test_index]\n",
    "    train_weight, test_weight = weights[train_index], weights[test_index]\n",
    "    x_all_test = data_all[test_index]\n",
    "    model.eval()\n",
    "    pred = model(Variable(torch.from_numpy(x_val).float().cuda(), True)).cpu().data.numpy()\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_val, np.exp(pred)[:,1])\n",
    "    \n",
    "    fpr = np.interp(base_tpr, tpr, fpr)\n",
    "    threshold = np.interp(base_tpr, tpr, threshold)\n",
    "    fpr[0] = 0.0\n",
    "    fprs.append(fpr)\n",
    "    thresholds.append(threshold)\n",
    "    \n",
    "\n",
    "thresholds = np.array(thresholds)\n",
    "mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "fprs = np.array(fprs)\n",
    "mean_fprs = fprs.mean(axis=0)\n",
    "std_fprs = fprs.std(axis=0)\n",
    "fprs_right = np.minimum(mean_fprs + std_fprs, 1)\n",
    "fprs_left = np.maximum(mean_fprs - std_fprs,0)\n",
    "\n",
    "mean_area = auc(mean_fprs, base_tpr)\n",
    "\n",
    "\n",
    "with h5py.File(\"DNN_ROC.h5\",\"w\") as out:\n",
    "    out['FPR'] = mean_fprs\n",
    "    out['dFPR'] = std_fprs\n",
    "    out['TPR'] = base_tpr\n",
    "    out['Thresholds'] = mean_thresholds\n",
    "    print(\"Saved ROC.\")\n",
    "\n",
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(base_tpr>TPR_threshold)\n",
    "    NNtable.add_row([mean_thresholds[thres_idx], base_tpr[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs[thres_idx], std_fprs[thres_idx])])\n",
    "print(NNtable)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(mean_fprs, base_tpr,label=\"NN AUC = {}\".format(mean_area))\n",
    "plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(pred)[y_val==0,1], bins=60, label='ttH background',alpha=0.5, density =True)\n",
    "plt.hist(np.exp(pred)[y_val==1,1], bins=60, label='HH signal', alpha=0.5, density =True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "# sig_frame = pd.DataFrame.from_records(signp)\n",
    "# bkg_frame = pd.DataFrame.from_records(bkgnp)\n",
    "\n",
    "#sig_frame['NN_score'] = pd.Series(np.exp(pred[:len(signal),1]), index=sig_frame.index)\n",
    "#bkg_frame['NN_score'] = pd.Series(np.exp(pred[len(signal):,1]), index=bkg_frame.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale weights to match original\n",
    "sig_total_weight = np.sum(sig_frame_all.loc[:, b'genweight'].values)\n",
    "bgd_total_weight = np.sum(bkg_frame_all.loc[:, b'genweight'].values)\n",
    "\n",
    "sig_test_weights = test_weight[y_val == 1]\n",
    "bgd_test_weights = test_weight[y_val == 0]\n",
    "sig_test_total_weight = np.sum(sig_test_weights)\n",
    "bgd_test_total_weight = np.sum(bgd_test_weights)\n",
    "\n",
    "scale_sig = sig_total_weight / sig_test_total_weight\n",
    "scale_bgd = bgd_total_weight / bgd_test_total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyS0lEQVR4nO3dd5wV1f3/8ddnl7b0jvQOCiJtxYIKsRILqNgrlmBMbEk0ifmZxBJNjFETvxoVFQXs0RhRsWDBEqUsUpTee1lY+rKw5fP7Y2b1urnLLrB3Z/fu+/l43Ad3Zs7MfO7Zy3zunDlzxtwdERGRolKiDkBERComJQgREYlLCUJEROJSghARkbiUIEREJC4lCBERiUsJQiSGmc0xs8FlsJ3jzWxBzHR3M5tpZjvM7CYze8LMfn+w+5HSMbMWZvZZWP8PWuBZM9tiZlOjjq+iUoJIIma2M+ZVYGa7Y6aXxbzPN7OcmOnfheu3NLOnzGxtOH+pmT1nZofG7KOGmf3BzBaY2S4zW2Nm75rZqaWIr7GZvRGut8LMLolZNtjMVpfyc44wsy8OsI5qhAeI1eFnXG5mfy9c7u493X3SgWw7lrt/7u7dY2b9GvjE3eu5+yPu/lN3v+dg91OZmdlpMQftTDP71MyGlnLdSWZ27X7sbiSwCajv7r8CjgNOAdq4+4D9j75qUIJIIu5et/AFrATOipnXMWbZ58ANMcvuM7MmwJdAbeB4oB7QD/iU4D9SodeAYcAVQCOgI/AP4IxShPgYsBdoAVwKPG5mPcvgo++P24F0YADBZxwMfF0O+20PzCmH/VQKZnYe8C9gLNCG4DvxB+CsBO2yPTDXv78zuD2w3N13JWh/ycHd9UrCF7AcOLmYZZOAa4vM+xMwC0jZxzZPBnYT/Ora33jqECSHbjHzxgF/Cd8PBlbHLGsAPAOsA9aE8aUChwE5QD6wE9galj8dmAvsCMvfWkwcbwO3lKbegDRgDLAFmEdwFrC6SNlbgdnANuAVoFbRzwN8HMabE8bcDXgO+FPMtoYBM4HtwBJgSDj/qnDfO4ClwHUx6wwGVgO/AjaGdXVVzPI04EFgRRjfF0BauOxogh8EW8O/++Bi6uM3wGtF5v0DeCR8PyKMawewDLi0FN8FI/gBc9s+ytwJPB8z3QFwoBpwb5H6fDQscywwLfys04Bjw/nPAbkE37+dwHVFvkN3Rf3/taK+Ig9ArwT9Yfc/QUwG7ixhm38BJh1gPH2B7CLzbgXeCt9/d0ANp98AniRILM2BqYUHx/Cg9EWRba0Djg/fNwL6FRPHHeHB6WdAL8CKq7fw834abq8NQSIomiCmAq2AxgQH8p8W83l+UOfEJAiCs5ltBGdqKUBr4NBw2RlA5/CgOgjILvxs4T7ygLuB6gRJMhtoFC5/LNxva4LkeixQM5zeHJZPCfe7GWgWp77ah9usF06nhnV9dPi32Q50D5e1BHqW4rtwKMHBvuM+ytxJMQmimPpsTJDILydIIheH002K1ndx3yG9/velJiYp1BRYXzhhZkPNbGvYPvxBMWUah2W2mVlOCduvS3AwibWNoJnnB8ysBcHB6xZ33+XuG4GHgYv2sf1coIeZ1Xf3Le5eXLPRn4H7CZq4MoA1ZnZlMWUvAO4Lt7caeCROmUfcfa27ZwFvAX32EWNxrgFGu/tEdy9w9zXuPh/A3d9x9yUe+BT4gKAJsFAucLe757r7BIJfxN3NLAW4Grg53F6+u3/p7nuAy4AJ7j4h3N/EsC5OLxqYu68gaII7J5x1IkGinxxOFwCHm1mau69z99I0ozUJ/11X6hoq2RnAIncf5+557v4SMJ/ENVlVCUoQUmgzwS9AANx9vLs3BH4B1CimTFZYpj/BL9N92QnULzKvPkHTRFHtCX4RrwsT0FaCs4nm+9j+cIID3IrwYucx8QqFB8rH3H0g0JCguWK0mR0Wp3grYFXM9Ko4ZdbHvM8mSIT7qy1Bs9L/MLMfm9lkM8sK6+F0gkRdaLO758WJoSlQq5jttgfOL6zbcLvHEfO3LeJFgl/kAJeE03jQfn8h8FOCv9U7sR0a9mFz+G9x+zsQrQia0mKtIDhbkgOkBCGFPgLODn957qvMkWbW5gC2vxCoZmZdY+b1Jv6F21XAHqCpuzcMX/XdvfCC9v8MQezu09x9GEES+Q/wakkBuftud3+MoCmiR5wi6wialgq1LWmbB2gVQTPSD5hZTeB14G9AizAZTyBobirJJoJ29v/Zbri/cTF129Dd67j7X4rZ1r+AweHf/RzCBAHg7u+7+ykEB/v5wFOliG1BGMPwfZTZRdBhotAhRZYX/Q6sJUh8sdoRXI+SA6QEIYUeImhrH2dmncN+4vWIaTJx9w+AT4D/mNlRYZfR6gTt0fsU/tr8N3C3mdUxs4EEF2bHxSm7jqAp5UEzq29mKWFMg8IiG4A2ZlYDvuu6eqmZNXD3XIKmrIJ4cZjZLWGX2jQzqxY2L9UDZsQp/ipwu5k1MrPWwA0lfc4D9AxwlZmdFH7W1uEv8RoEZ2aZQJ6Z/RgosTsxgLsXAKOBh8yslZmlmtkxYdJ5Hjgr7Gaaama1wjqJm/jdPZOgzf9ZYJm7z4Pv7i0YZmZ1CBL6Toqp9yLbc+CXwO/N7KqYv/FxZjYqLDYTOMHM2plZA4LeZ7E2AJ1ipicA3czskvDveiFB0n+7xMqSYilBCADuvongQJ9D0NtlB8F/0nrA9TFFzyH4T/c8QQ+YZQTt+aeVYjc/I+hZsxF4Cbi+SJt17K/CKwgOkHMJfuG/xvdNEh8TnHmsN7NN4bzLgeVmtp2gyePSYmLIJujZs57gV/bPgeHuvjRO2bsJegktAz4MY9hTis+5X9x9KkFvpYcJrst8CrR39x3ATQSJagtB8874/dj0rcA3BD16sgiuvaS4+yqC5Pw7guSzCriNfR8PXiToxfZizLwUggP92nD7gwi/KxbcKLhzH5/5NYLmqavD9TcQ9FR7M1w+kaBX2GxgOv97oP8HcJ4FN7o94u6bgTMJenRtJuhxdmb4vZYDZEEyF4lWeIPU3e7eJ+pYimNm1wMXufugEguLJAGdQUjkzKwaQXt0RtSxxLLgzvKBYfNHd4Jfp29EHZdIeakWdQCSPMysHUGTUDw93H1lnHUaEDRxTCdoVqpIahD0nupI0Jz2MvDPKAMSKU9qYhIRkbjUxCQiInElTRNT06ZNvUOHDlGHISJSqUyfPn2TuzeLtyxpEkSHDh3IyKhQ1zhFRCo8Myt6B/p31MQkIiJxKUGIiEhcShAiIhKXEoSIiMSlBCEiInEpQYiISFxKECIiElfS3AchIrK/tmbv5YM5G9i2O5dWDdNo1bAWrRum0bRuTVJSSvNcpuSmBCEiVcr2nFwmztnA27PX8vmiTeQV/O94dNVTjUMa1KJVgzRaN0wLk8f3CaRVwzTq1Ez+w2fyf0IRqfKy9+bx4byNvD1rLZMWZrI3r4DWDdO45riOnNW7FW0b12bdtt2s3bqbNVtzWLt193evKcuyWL89h/wiiaRBWnVaNUyjdcNaHNu5KZcf057qqcnVaq8EISJJKSc3n0kLNvLWrHV8NH8DObkFtKhfk8uOas+ZvVvSt21DzL5vRmqQVp1DD6kfd1t5+QVs3LEnTCC7WRuTRJZvzubDeXN5cepK7hrak4FdmpbXR0w4JQgRSRp78vL5fOEm3p69lolzN7Brbz5N6tTg/P5tOfOIlhzZofEBXVuolpryXTNTepzlH83bwF1vzeXSp6dwRq+W/O6Mw2jdMO3gP1DElCBEpFLLyy/gyyWbeWvWWt6fs57tOXk0SKvOWb1bceYRrTi6U2OqJbjp56TDWjCwS1Oe+mwpj01azMfzN3LDiV249viO1KyWmtB9J1LSPDAoPT3dNZqrSNWSk5vPhaMmM2vVVurWrMapPVtw1hGtGNilKTWqRXM9YPWWbO59Zx7vfrueDk1q88ezevKjQ5tHEktpmNl0d493YqQzCBGpvO4cP4dZq7by53N7cU7f1tSqHv2v9TaNavP4Zf35bGEmd741h6uem8bJhzXnD2f2pF2T2lGHt1+S65K7iFQZ/8pYxcvTVvHzH3Xm4gHtKkRyiHVCt2a8d/MJ3P7jQ/lyyWZOfvhTHpq4kJzc/KhDKzUlCBGpdOat284d//mWYzo14Rcnd4s6nGLVqJbCdYM68/GvBjOk5yE88tEiTn7oU96fs57K0LyvBCEilcr2nFyuf346DdKq88jFfRN+AbosHNKgFo9c3JeXRx5NnRrVuG7cdK58dhpLM3dGHdo+VfyaFREJuTu//tdsVm3ZzWOX9qNZvZpRh7Rfju7UhHduOo4/ntWDGSu2cNrfP+P+9+aza09e1KHFpQQhIpXGM18s47056/ntkEM5skPjqMM5INVSU7hqYEc+vnUww/q05vFJSxj++JfsrIBJQglCRCqFacuz+PO78zmtZwuuPb5j1OEctGb1avK383szekQ6izbu5JaXZ1IQZ1yoKClBiEiFl7ljDz9/4WvaNkrjgfN7/2CIjMruxENb8PszDuPDeRv46/sLog7nB3QfhIhUaPkFzk0vzWDb7lyeu2oA9WtVjzqkMnflsR1YtHEnT3y6hK7N6zK8f5uoQwJ0BiEiFdxDExfw1dLN/Onsw+nRKv5gepWdmXHn0J4c27kJt//7G6avyIo6JEAJQkQqsI/mbeCxT5ZwYXpbzk9vG3U4CVU9NYV/XtqPVg1rMXLsdFZvyY46pMQmCDMbYmYLzGyxmf02zvIRZpZpZjPD17Xh/D5m9pWZzTGz2WZ2YSLjFJGKZ1VWNr94ZSY9WtbnrmE9ow6nXDSsXYOnrzySvfkFXDsmI/LurwlLEGaWCjwG/BjoAVxsZj3iFH3F3fuEr6fDednAFe7eExgC/N3MGiYqVhGpWHJy8/nZC1/jwOOX9atww2gkUpfmdXnskn4s3LCDW16JtmdTIs8gBgCL3X2pu+8FXgaGlWZFd1/o7ovC92uBjUCzhEUqIhXK3W/P5Zs123jw/N60b1In6nDK3QndmvH7M3swce4GHvggup5NiUwQrYFVMdOrw3lFDQ+bkV4zs/9pZDSzAUANYEmcZSPNLMPMMjIzM8sqbhGJ0L+/Xs2LU1Zy3aBOnNrzkKjDicyIYztw8YB2PD5pCf/+enUkMUR9kfotoIO7HwFMBMbELjSzlsA44Cp3Lyi6sruPcvd0d09v1kwnGCKV3fz12/ndG98woGNjbju1e9ThRMrMuHtYT47u1Jjfvh5Nz6ZEJog1QOwZQZtw3nfcfbO77wknnwb6Fy4zs/rAO8D/c/fJCYxTRCqAHTm5/Oz5r6lbszqPVpJB+BKtemoKj1/an5YNa3HduPLv2ZTIv8A0oKuZdTSzGsBFwPjYAuEZQqGhwLxwfg3gDWCsu7+WwBhFpAJwd37z+mxWZGXz6CV9aV6/VtQhVRiN6tTgmSvT2ZNb/j2bEpYg3D0PuAF4n+DA/6q7zzGzu81saFjsprAr6yzgJmBEOP8C4ARgREwX2D6JilVEovXsf5cz4Zv13HZad47u1CTqcCqcLs3r8X+X9GXhhh38ohx7NumZ1CISqekrsrjwyckM7t6cp67on1TjLJW10V8s4+635/KzwZ359ZBDy2Sbeia1iFRIq7Ky+enzX9OqYRoPXpBcg/AlwlUDO7Bo4w7+OWkJXZrX5dx+iR2zSVeBRCQSW7P3MuLZqeTk5vP0lek0SEu+QfjKmplx19DDOapjYc+mLQndnxKEiJS7nNx8fjI2g1VZu3nqinS6tagXdUiVRo1qKTxxWWHPpgzWbN2dsH0pQYhIucovcH7xykymLd/Cgxf01kXpA1BePZuUIESk3Lg797w9l3e/Xc8dZxzGWb1bRR1SpVXYs2nB+u0J69mki9QiUm6e+nwpz325nKsHduTa4ztFHU6lN7h7c+44owc5efkk4vq+EoSIlIvxs9Zy34T5nNGrJXeccVjU4SSNq49L3PO51cQkIgn31ZLN3PrqLAZ0bMyDF/QmJUXdWSsDJQgRSaj567czclwG7ZrU5qnL06vUsx0qOyUIEUmYddt2M2L0NNKqpzLm6gE0qK17HSoTXYMQkYTYnpPLiNHT2Lknj1evO4bWDdOiDkn2k84gRKTM7cnL57qx01mSuZMnLutPj1b1ow5JDoDOIESkTBUUOLf9azZfLd3Mwxf25riuTaMOSQ6QziBEpEzd/958xs9ay6+HdOecvokdTE4SSwlCRMrMc/9dxpOfLeXyo9tz/aDOUYcjB0kJQkTKxHvfruOut+dySo8W3Dm0p4buTgJKECJy0DKWZ3HzyzPp07Yhj1zUl1TdCJcUlCBE5KAs3riTa8Zk0KphGs9ceSRpNXQjXLJQghCRA7Y1ey9XPTeV6qnGmKsG0LhOjahDkjKkbq4ickDyC5wbX5rBhm17eOW6o2nXpHbUIUkZU4IQkQPytw8W8PmiTdw/vBd92zWKOhxJADUxich+e2f2Oh6ftIRLjmrHhUe2izocSRAlCBHZLwvW7+C212bRr11D/nhWj6jDkQRSghCRUtuWncvIcRnUqVmNxy/rT81q6rGUzJQgRKRU8gucm1+Zwdqtu3n80n60qF8r6pAkwZQgRKRU/v7hQiYtyOSPZ/UkvUPjqMORcqAEISIleu/b9fzfx4u5ML0tlx6li9JVhRKEiOzT4o07+NWrM+ndtiF3DdMYS1WJEoSIFGt7Ti4jx04nrUYqT1zWT8+TrmJ0o5yIxFVQ4PzylZmszMrmhWuPomUDPTK0qtEZhIjE9cjHi/hw3kZ+f2YPjurUJOpwJAJKECLyPybO3cDfP1zE8H5tuOKY9lGHIxFRghCRH1iSuZNfvjKTXq0bcO85h+uidBWmBCEi39mRk8vIsRlUr5bCE5f310XpKk4XqUUECC5K/+rVWSzfnM24awbQuqEuSld1CT2DMLMhZrbAzBab2W/jLB9hZplmNjN8XRuz7EozWxS+rkxknCIC/5y0mA/mbuB3px/GsZ2bRh2OVAAJO4Mws1TgMeAUYDUwzczGu/vcIkVfcfcbiqzbGPgjkA44MD1cd0ui4hWpyj6Zv5EHJy7k7D6tuHpgh6jDkQoikWcQA4DF7r7U3fcCLwPDSrnuacBEd88Kk8JEYEiC4hSp0pZv2sVNL8/gsEPq8+dzj9BFaflOIhNEa2BVzPTqcF5Rw81stpm9ZmZt93NdETkI23Ny+cnYDFJTjCcv709aDV2Ulu9F3YvpLaCDux9BcJYwZn9WNrORZpZhZhmZmZkJCVAkWeXmF/Cz579m2aZd/POSfrRtrGdKyw8lMkGsAdrGTLcJ533H3Te7+55w8mmgf2nXDdcf5e7p7p7erFmzMgtcJNm5O3e88S1fLN7Efef24tguuigt/yuRCWIa0NXMOppZDeAiYHxsATNrGTM5FJgXvn8fONXMGplZI+DUcJ6IlIEnPl3KKxmruOFHXbggvW3JK0iVlLBeTO6eZ2Y3EBzYU4HR7j7HzO4GMtx9PHCTmQ0F8oAsYES4bpaZ3UOQZADudvesRMUqUpW8M3sd9783n7N6t+KXp3SLOhypwMzdo46hTKSnp3tGRkbUYYhUaNNXbOHipybTq3UDXrj2KN0pLZjZdHdPj7cs6ovUIlJOVm7OZuTYDFo2qMUoDaMhpaAEIVIFbMvO5arnppJX4IwecSRN6taMOiSpBJQgRJLc3rwCfvr8dFZmZfPk5f3p3Kxu1CFJJaHB+kSSmLvzuze+4aulm3n4wt4crQf/yH7QGYRIEnvsk8W8Nn01t5zclXP6tok6HKlklCBEktSbM9fwtw8Wck7f1tx8Uteow5FKSAlCJAlNW57Fbf+azYCOjfnL8F4agE8OiBKESJJZtmkXI8dm0KZRGqMu70/NaurOKgdGCUIkiWzZtZernwsGIBg94kga1q4RcURSmakXk0iS2JOXz3XjprNmy25e/MlRdGhaJ+qQpJJTghBJAu7Ob16bzdTlWfzjoj6kd2gcdUiSBNTEJJIE/v7hIv4zcy23ntqNYX30bC0pG/t1BmFmdYAcd89PUDwish+ydu3ljRlr+MdHizivfxt+/qMuUYckSWSfCcLMUgie43ApcCSwB6hpZpuAd4An3X1xwqMUEdydlVnZTFu+hYzlWUxbnsWSzF0AHNu5Cfedo+6sUrZKOoP4BPgQuB341t0LAMysMfAj4H4ze8Pdn09smCJVT15+AXPXbSdj+RYyVmQxbfkWMncED2CsX6sa6R0aM7x/G9LbN6Zvu4ZUT1WLsZStkhLEye6eW3Rm+PCe14HXzax6QiITqWJ27cljxsqtTFueRcaKLGas3Er23qA1t02jNI7r0pT+7RtxZIfGdG1el5QUnS1IYu0zQRRNDmZWG+gBrHD3zHhlRKT0Fm3YwWtfr+bLxZuZu247+QVOisGhh9Tn/P5tSO/QmPQOjWjZIC3qUKUKKukaxFDgEYLHgd4BPAZsADqY2W/cfUziQxRJLttzcnl71jpezVjFzFVbqZZipHdoxM8Hdya9Q9BcVK+WTswleiU1Md0DnAo0ILgecYS7LzWz5sBHgBKESCm4O1OWZfHqtFVM+HYdObkFdG1elzvOOIyz+7amqR7gIxVQSQmiwN0XApjZMndfCuDuG80sL+HRiVRy67bt5vXpq/nX9NWs2JxNvZrVOLdfGy5Ib0vvNg3U60gqtJISRIqZNSK4oa4gfF/4jVaXCZE49uTl8+HcjbyasYrPFmXiDsd0asItJ3dlSM+WpNXQ4HlSOZSUIBoA0/k+KXwds8wTEpFIJTV37XZezVjFf2auYWt2Lq0a1OLGH3XhvP5tadekdtThiey3knoxdSinOEQqpey9efz76zW8PG0l367ZTo3UFE7t2YIL0tsysEtTUtUVVSqx/R1qo5G7b0lUMCKVxcbtOYz5ajkvTFnJ1uxcerSsz11DezKsTysNsS1JY39Hc/0I6JeIQEQqg/nrt/P058sYP3MtuQUFnNqjBT85vhP92zfSBWdJOvubIPQ/QKocd+ezRZt4+vOlfL5oE2nVU7l4QFuuGthRz1yQpFZigjCzKwrfAo1ipnH3sYkKTCRqe/LyeXPmWp75fBkLNuygeb2a3HZady49qp2akaRKKM0ZRMeY9zWBDgTJQr2YJClt2bWXF6asYMxXK8jcsYdDD6nH387vzVm9W+r5zlKllJgg3P2uwvdmNszd705sSCLRWLZpF898sZTXpq8mJ7eAQd2a8ZMLOjGwSxNdX5AqSdcgpMqbvXor//fxYj6ct4HqKSmc3bcV1x7fiW4t6kUdmkik9jdBXJ6QKEQismbrbi548ivSqqdyw4+6cPkx7Wler1bUYYlUCPubIBaYWV9gjbtvTERAIuXpr+/Nxx3euvE42jTS3c4isfY5npKZPWFmPcP3DYBZwFhghpldXA7xiSTMjJVbeHPmWq49vqOSg0gcJQ24d7y7zwnfXwUsdPdeQH/g1wmNTCSB3J173p5Ls3o1uX5wl6jDEamQSkoQe2PenwL8B8Dd1ycqIJHy8PbsdXy9ciu3ntqNujX3t6VVpGooKUFsNbMzw+sOA4H3AMysGqBnIEqllJObz1/enU+PlvU5r3/bqMMRqbBKShDXATcAzwK3xJw5nAS8U9LGzWyImS0ws8Vm9tt9lBtuZm5m6eF0dTMbY2bfmNk8M7u9dB9HpGTPfLGMNVt3c8eZh2m0VZF9KGm474XAkDjz3wfe39e6ZpZK8AzrU4DVwDQzG+/uc4uUqwfcDEyJmX0+UNPde5lZbWCumb3k7stL/kgixdu4I4d/frKYU3q04NjOTaMOR6RCK6kX0x3hU+SKW36imZ1ZzOIBwGJ3X+rue4GXgWFxyt0D3A/kxMxzoE5MU9ZeYPu+YhUpjYc+WMje/AJ+d/phUYciUuGVdHXuG+BtM8sheJpcJlAL6Ar0AT4E7itm3dbAqpjp1cBRsQXMrB/Q1t3fMbPbYha9RpBM1gG1gV+4e1bRHZjZSGAkQLt27Ur4KFLVzV27nVcyVnH1wI501CisIiUqqYnpTeBNM+tKcJG6JcEv+eeBke6++0B3bGYpwEPAiDiLBwD5QCugEfC5mX3o7kuLxDcKGAWQnp6uwQOlWO7On96ZS8O06tx0YteowxGpFPaZIMKLw++5+wxg0X5uew0Q20WkTTivUD3gcGBSOBDaIcB4MxsKXBLuNxfYaGb/BdKBHyQIkdL6cN5GvlyymbuG9qRB7epRhyNSKZTUi2kpcLOZzTCz58zswn1dkyhiGtDVzDqaWQ3gImB84UJ33+buTd29Q/js68nAUHfPAFYCJwKYWR3gaGD+fn0ykdDevALumzCPzs3qcMlRaooUKa2SmpheAV4BCO+FGAK8Hl48/pDgV/7UYtbNM7MbCHo7pQKj3X2Omd0NZLj7+HjrhR4DnjWzOQQjyD7r7rP387OJADBu8gqWbdrFsyOOpHpqSb+JRKRQSU1MRwKr3H29u88wsyOAnQQXq+cA1wJxEwSAu08AJhSZ94diyg6Oeb+ToKuryEHZsmsv//hwIcd3bcrg7s2iDkekUinp59SThMNtmNkJwJ+BMcBa4GJ3H5nY8EQOzj8+WsTOPXnccUYPPfRHZD+V1M01NaZ76YXAKHd/naCZaWZCIxM5SIs37mTc5BVcPKAd3Q/Rw39E9ldJZxCp4fUGCIbX+DhmmUY4kwrtvgnzqF09lV+e0i3qUEQqpZIO8i8Bn5rZJmA38DmAmXUBtiU4NpED9vmiTD6ev5Hbf3woTerWjDockUqppF5M95rZRwQ3yH3g7oU3o6UANyY6OJEDkZdfwJ/enke7xrUZMbBD1OGIVFolNhO5++Q48xYmJhyRg/dqxmoWbNjB45f2o2a11KjDEam01ClcksqOnFwemriAAR0bM+TwQ6IOR6RS04VmSSqPfbKETTv38uwIdWsVOVg6g5CksSorm9FfLOPcfq3p1aZB1OGIVHpKEJI0/vLufFJTjF+fdmjUoYgkBSUISQrTlmfxzjfruG5QJw5pUCvqcESSghKEVHoFBc49b8/lkPq1GHlCp6jDEUkaShBS6f1n5hpmr97Gr4d0p3YN9bsQKSv63ySV1p68fF6aspKHP1zEEW0acHaf1lGHJJJUlCCk0skvcN6cuYaHJi5k9ZbdHN2pMfed04uUFHVrFSlLShBSabg7H83byAPvL2DBhh0c3ro+953Ti+O7NtU9DyIJoAQhlcLUZVnc/958pq/YQsemdXj0kr6cfnhLnTWIJJAShFRoc9du54H35/PJgkxa1K/Jn8/txXn92+jRoSLlQAlCKqQVm3fx0MSFvDlzLQ3SqnP7jw/lymM7UKu6Bt8TKS9KEFKhbNyew/99vJiXpq6kWqrx8x91ZuQJnWmQVj3q0ESqHCUIqRC27c5l1GdLGP3FcnLzC7h4QDtuPLELzevrrmiRqChBSKRy8wt47r/LefSTxWzbncuwPq345SndaN+kTtShiVR5ShASmRkrt3D7v79h/vodDO7ejNtO607PVhqFVaSiUIKQcrdzTx5/e38BY75aTot6tRh1eX9O7amH+4hUNEoQUq4mzt3AH978lvXbc7ji6Pbcelp36tXSBWiRikgJQsrFhu053Dl+Du9+u57uLerx2KX96NeuUdRhicg+KEFIQhUUOC9OXcn9785nT34Bt53WnZEndNKNbiKVgBKEJMzCDTu4/d/fMH3FFo7t3IR7z+lFx6bqnSRSWShBSJnLyc3nn58s5vFPl1CnZjX+dn5vhvdrrQH1RCoZJQgpU5OXbuZ3//6GpZt2cU7f1txxxmE0qVsz6rBE5AAoQUiZ2Jq9lz9PmM8rGato2ziNsVcP4IRuzaIOS0QOghKEHBR3563Z67j7rTlsyc7lukGduOWkbqTV0KB6IpWdEoQcsAXrd/DH8d8yeWkWR7RpwJirB+hOaJEkogQh+23b7lz+/uFCxn61gro1q3HP2YdzyYB2pOrhPSJJRQlCSq2gwHlt+mruf28+Wdl7uXhAO249tTuN69SIOjQRSYCEJggzGwL8A0gFnnb3vxRTbjjwGnCku2eE844AngTqAwXhspxExivFm7VqK38YP4dZq7bSv30jxgwdwOGt1ZwkkswSliDMLBV4DDgFWA1MM7Px7j63SLl6wM3AlJh51YDngcvdfZaZNQFyExWrFG/zzj389b0FvDp9FU3q1OShC3pzTl/d0yBSFSTyDGIAsNjdlwKY2cvAMGBukXL3APcDt8XMOxWY7e6zANx9cwLjlDjy8gt4YcpKHvxgAdl787n2uI7cdFJXDawnUoUkMkG0BlbFTK8GjootYGb9gLbu/o6ZxSaIboCb2ftAM+Bld/9r0R2Y2UhgJEC7du3KOPyqa8rSzfxx/Bzmr9/BcV2acufQHnRpXi/qsESknEV2kdrMUoCHgBFxFlcDjgOOBLKBj8xsurt/FFvI3UcBowDS09M9oQFXAeu35XDfhHmMn7WW1g3TeOKyfpzW8xA1J4lUUYlMEGuAtjHTbcJ5heoBhwOTwgPQIcB4MxtKcLbxmbtvAjCzCUA/4AcJQsrGnrx8nvliGY9+vJi8Auemk7py/aDOutlNpIpLZIKYBnQ1s44EieEi4JLChe6+DWhaOG1mk4Bb3T3DzJYAvzaz2sBeYBDwcAJjrXKy9+YxeelmPl2QycS5G1i7LYdTe7Tg92f2oG3j2lGHJyIVQMIShLvnmdkNwPsE3VxHu/scM7sbyHD38ftYd4uZPUSQZByY4O7vJCrWqsDdWbRxJ58uyOTThZlMXZbF3vwC0qqncnSnxvx5+BEM0thJIhLD3JOj6T49Pd0zMjKiDqNC2Z6Ty5eLN/Hpwkw+XZDJ2m3BbSTdWtRlULdmDOrWnPQOjahVXU1JIlVVeH03Pd4y3UmdRAoKnLnrtn+XEKav3EJ+gVOvZjUGdmnKjSc1Y1C3ZrRqmBZ1qCJSCShBJIGJczfw7rfr+GzhJjbt3APA4a3r89NBnRjUrTl92zXUIz5FZL8pQVRiefkF3PP2XMZ8tYJGtatzQrfgDOH4rs1oVk8P6RGRg6MEUUnt3JPHjS9+zScLMvnJ8R357Y8P02iqIlKmlCAqobVbd3PNmAwWbtjBvecczqVHtY86JBFJQkoQlcw3q7dxzZhp7N6bz7MjjtRjPUUkYZQgKpEP5qzn5pdn0rhODcZdfxTdD9H4SCKSOEoQlYC788wXy7h3wjyOaNOQp69I10VoEUk4JYgKLi+/gD+On8MLU1Zyeq9DePD8PhojSUTKhRJEBbY9J5efv/A1ny/axPWDO3Pbqd1JUU8lESknShAV1Oot2Vz93DSWZu7i/uG9uPBIPe9CRMqXEkQFNHPVVq4dk8GevHzGXD2AgV2alrySiEgZU4KoYN79Zh23vDKT5vVr8vLIo/QkNxGJjBJEBeHuPPnZUv7y7nz6tWvIU1ek06SueiqJSHSUICqA3PwC7njjW17JWMVZvVvxwHlHaAhuEYmcEkSE3J3pK7bw1/cXMHVZFjee2IVfnNxNPZVEpEJQgohAbn4BE75Zx+gvljFr9TYapFXnwfN7M7x/m6hDExH5jhJEOdqWncuLU1cy9qvlrNuWQ6emdbjn7MMZ3q81tWvoTyEiFYuOSuVgaeZOnv3vcl6bvprdufkM7NKEe885nMHdmqs5SUQqLCWIBHF3vlqymWe+WMbHCzZSPSWFoX1acfXAjvRoVT/q8ERESqQEUcb25OUzfuZaRv93OfPWbadJnRrceGJXLju6Hc3r1Yo6PBGRUlOCKCObd+7hhSkrGfvVCjbt3EO3FnW5f3gvhvVprS6rIlIpKUEcpJzcfO59Zx6vZKxib14Bg7s345rjOnJcl6aY6fqCiFReShAHISc3n5+MzeDzRZu4eEBbrjmuo4bGEJGkoQRxgHbvzefasdP4cslm/jr8CC44sm3UIYmIlCkliAOQvTePq5+bxpRlWfztPN3gJiLJSQliP+3ak8dVz00jY3kWD1/Qh7P7to46JBGRhFCC2A879+QxYvRUZqzayj8u6stZvVtFHZKISMIoQZTS9pxcRoyeyuzV2/i/i/tyeq+WUYckIpJQShClsG13LleMnsqcNdt49JJ+DDn8kKhDEhFJOCWIEmzLzuXy0VOYt247j1/Wn1N6tIg6JBGRcqEEsQ9bdu3lsmemsGjDTp68vD8nHqrkICJVhxJEMbJ27eXSp6ewJHMno67oz+DuzaMOSUSkXClBxLFp5x4ue3oKyzbt4ukr0jmhW7OoQxIRKXdKEEVk7tjDJU9NZtWWbEaPOJKBXZpGHZKISCRSErlxMxtiZgvMbLGZ/XYf5YabmZtZepH57cxsp5ndmsg4C23cnsNFo75i9ZbdPDtigJKDiFRpCUsQZpYKPAb8GOgBXGxmPeKUqwfcDEyJs5mHgHcTFWOs9dtyuGjUZNZty2HM1QM4pnOT8titiEiFlcgziAHAYndf6u57gZeBYXHK3QPcD+TEzjSzs4FlwJwExgjAum27uWjUV2zcsYexVw9gQMfGid6liEiFl8gE0RpYFTO9Opz3HTPrB7R193eKzK8L/Aa4a187MLORZpZhZhmZmZkHFOS6bbu58MnJbN65l7HXDCC9g5KDiAgk+BrEvphZCkET0q/iLL4TeNjdd+5rG+4+yt3T3T29WbMD62lUv1Z1ujavy7hrj6Jfu0YHtA0RkWSUyF5Ma4DYhyS0CecVqgccDkwKn7x2CDDezIYCRwHnmdlfgYZAgZnluPujZR1knZrVeGbEkWW9WRGRSi+RCWIa0NXMOhIkhouASwoXuvs24LtuQmY2CbjV3TOA42Pm3wnsTERyEBGR4iWsicnd84AbgPeBecCr7j7HzO4OzxJERKQCM3ePOoYykZ6e7hkZGVGHISJSqZjZdHdPj7cssovUIiJSsSlBiIhIXEoQIiISlxKEiIjEpQQhIiJxJU0vJjPLBFYcxCaaApvKKJzKTnXxQ6qPH1J9fC8Z6qK9u8cdiiJpEsTBMrOM4rp6VTWqix9SffyQ6uN7yV4XamISEZG4lCBERCQuJYjvjYo6gApEdfFDqo8fUn18L6nrQtcgREQkLp1BiIhIXEoQIiISV5VKEGY2xMwWmNliM/ttnOUjzCzTzGaGr2ujiLO8lFQfYZkLzGyumc0xsxfLO8byVIrvx8Mx342FZrY1gjDLRSnqop2ZfWJmM8xstpmdHkWc5aUU9dHezD4K62KSmbWJIs4y5+5V4gWkAkuATkANYBbQo0iZEcCjUcdageqjKzADaBRON4867ijro0j5G4HRUccd4XdjFHB9+L4HsDzquCOuj38BV4bvTwTGRR13Wbyq0hnEAGCxuy91973Ay8CwiGOKUmnq4yfAY+6+BcDdN5ZzjOVpf78fFwMvlUtk5a80deFA/fB9A2BtOcZX3kpTHz2Aj8P3n8RZXilVpQTRGlgVM706nFfU8PA08TUzaxtnebIoTX10A7qZ2X/NbLKZDSm36Mpfab8fmFl7oCPfHxCSTWnq4k7gMjNbDUwgOKNKVqWpj1nAueH7c4B6ZtakHGJLqKqUIErjLaCDux8BTATGRBxP1KoRNDMNJvjF/JSZNYwyoAriIuA1d8+POpAIXQw85+5tgNOBcWZWlY8ntwKDzGwGMAhYA1T670dV+oOuAWLPCNqE877j7pvdfU84+TTQv5xii0KJ9UHwS2m8u+e6+zJgIUHCSEalqY9CF5G8zUtQurq4BngVwN2/AmoRDFyXjEpz7Fjr7ue6e1/g/4XztpZbhAlSlRLENKCrmXU0sxoE/8nHxxYws5Yxk0OBeeUYX3krsT6A/xCcPWBmTQmanJaWY4zlqTT1gZkdCjQCvirn+MpTaepiJXASgJkdRpAgMss1yvJTmmNH05gzqNuB0eUcY0JUmQTh7nnADcD7BAf+V919jpndbWZDw2I3hd05ZwE3EfRqSkqlrI/3gc1mNpfgwttt7r45mogTq5T1AcHB4WUPu6sko1LWxa+An4T/V14CRiRrnZSyPgYDC8xsIdACuDeSYMuYhtoQEZG4qswZhIiI7B8lCBERiUsJQkRE4lKCEBGRuJQgREQkLiUIqbTMLD8cWXWOmc0ys18V9kU3s8Fm5mZ2Vkz5t81scPh+kpllxCxLN7NJcfaRYmaPmNm3ZvaNmU0zs44J/3Df73+AmX0WjiQ6w8yeNrPa+yjf0Mx+VsptPxDW3QNm1szMpoT7OL7sPoFUZtWiDkDkIOx29z4AZtYceJFgALk/hstXE9zV+lYx6zc3sx+7+7v72MeFQCvgCHcvCIdx3nUwQZtZtbBvfUnlWhCMEnpReLcyZnYeUA/ILma1hsDPgH+WIpSRQGN3zzezi4Bv3D2ph7iX/aMzCEkK4UizI4EbzMzC2bOAbWZ2SjGrPUA4LMI+tATWuXtBuJ/VhaPbhs8I+Do8e/konNfYzP4TDvg42cyOCOffaWbjzOy/BOMWNTOz18MzkmlmNjDOvn8OjClMDuH+X3P3DeH2bi2cH57hdAD+AnQOz6wesMADMWdAF4blxwN1gelm9hvgr8CwcL20EupEqgidQUjScPelZpYKNI+ZfS9wD8Hgi0V9BZxjZj8CdhSz2VeBL8Jml4+A5919hpk1A54CTnD3ZWbWOCx/FzDD3c82sxOBsUCfcFkP4Dh3323Bw5cedvcvzKwdwV26hxXZ9+Hs/4CRvwUOjzmzGh7uvzfBWEnTzOwzdx9qZjtjym0A0t39hv3cnyQxnUFIUnP3zwDM7LhiivwJuGMf668GuhOMr1MAfGRmJwFHA5+Fgxji7lnhKscB48J5HwNNzKzwuQnj3X13+P5k4FEzm0kwrk99M6t7QB9y344DXnL3fHffAHwKHJmA/UgSUoKQpGFmnQiGWC76YKN7KSYJhAfxNIIDflzuvsfd33X324D7gLMPMMTYaxcpwNHu3id8tXb3nUXKz6H4EYXz+OH/31oHGJNIsZQgJCmETT5PEDwy9gcDjLn7BwQjsB5RzOp/An5dzHb7mVmr8H1KuI0VwGTghMIeTTFNTJ8Dl4bzBgOb3H17nE1/QMxDdsysT5wyjwJXmtlRMeXODS9eLwf6FcZI8AAjCJrK6sVs43PgQjNLDevoBGBqvM8qUpSuQUhllhY20VQn+EU9DniomLL3Am/GW+DuE8ysuKGqmxM8KKlmOD2VIAnlmNlI4N9h4tgInELwpLXRZjaboKfRlcVs9ybgsbBcNeAz4KdF4toQ9i76W9hLqyAs9x7wOnCFmc0BphA8qwN332zBEwC/Bd4lSHzHEFywd+DX7r6+mJhEfkCjuYqISFxqYhIRkbiUIEREJC4lCBERiUsJQkRE4lKCEBGRuJQgREQkLiUIERGJ6/8DbO1unouBe/EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def significance_file(k):\n",
    "    file.write(f'cutoff: {k}\\n')\n",
    "    bgd = np.exp(pred)[y_val == 0,1]\n",
    "    file.write(f'# background: {len(bgd)}\\n')\n",
    "    bgd_bin = []\n",
    "    for event in bgd:\n",
    "        if event >= k:\n",
    "            bgd_bin.append(1)\n",
    "        else:\n",
    "            bgd_bin.append(0)\n",
    "    file.write(f'# false signal: {sum(bgd_bin)}\\n')\n",
    "\n",
    "    sgl = np.exp(pred)[y_val == 1, 1]\n",
    "    file.write(f'# signal: {len(sgl)}\\n')\n",
    "    sgl_bin = []\n",
    "    for event in sgl:\n",
    "        if event >= k:\n",
    "            sgl_bin.append(1)\n",
    "        else:\n",
    "            sgl_bin.append(0)\n",
    "    file.write(f'# true signal: {sum(sgl_bin)}\\n')\n",
    "    bgd_bin_np = np.array(bgd_bin)\n",
    "    sgl_bin_np = np.array(sgl_bin)\n",
    "    sgl_weight = test_weight[y_val == 1].flatten()\n",
    "    bgd_weight = test_weight[y_val == 0].flatten()\n",
    "    file.write(f'original background: {np.sum(bkg_weight_np_sfl)}\\n')\n",
    "    file.write(f'original test background: {np.sum(bgd_weight)}\\n')\n",
    "    file.write(f'original signal: {np.sum(sig_weight_np_sfl)}\\n')\n",
    "    file.write(f'original test signal: {np.sum(sgl_weight)}\\n')\n",
    "\n",
    "    re_bkg = np.sum(bgd_bin_np * bgd_weight)\n",
    "    file.write(f'remaining background: {re_bkg}\\n')\n",
    "    re_sig = np.sum(sgl_bin_np * sgl_weight)\n",
    "    file.write(f'remaining signal: {re_sig}\\n')\n",
    "    file.write(f'original S/sqrt(S+B): {np.sum(sig_weight_np_sfl) / np.sqrt(np.sum(sig_weight_np_sfl) + np.sum(bkg_weight_np_sfl))}\\n')\n",
    "    file.write(f'original test S/sqrt(S+B): {np.sum(sgl_weight) / np.sqrt(np.sum(sgl_weight) + np.sum(bgd_weight))}\\n')\n",
    "    file.write(f'S/sqrt(S+B): {re_sig / np.sqrt(re_sig + re_bkg)}\\n')\n",
    "    file.write('\\n')\n",
    "    return re_sig / np.sqrt(re_sig + re_bkg)\n",
    "\n",
    "\n",
    "test_sc = np.linspace(0.5, 0.95, 20)\n",
    "sc_vals = []\n",
    "file = open('DNN_Score_Nums/ttH_score/' + names[i]+'_ttHscore_DNN_score.txt', \"w\")\n",
    "for value in test_sc:\n",
    "    sc_vals.append(significance_file(value))\n",
    "\n",
    "file.close()\n",
    "    \n",
    "plt.plot(test_sc, sc_vals, label = names[i])\n",
    "plt.xlabel('DNN Score Cutoff')\n",
    "plt.ylabel('S/(S+B)')\n",
    "plt.title(names[i] + ' Significance vs. Cutoff')\n",
    "plt.savefig(fname = f'DNN_Score_Plts/ttH_score/{names[i]}_ttHscore_significance', format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.ROOT.RDF.RResultPtr<ROOT::RDF::RInterface<ROOT::Detail::RDF::RLoopManager,void> > object at 0x955eb570>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgd_x_all = x_all_test[y_val == 0]\n",
    "bgd_y = np.exp(pred)[y_val == 0, 1]\n",
    "\n",
    "sig_x_all = x_all_test[y_val == 1]\n",
    "sig_y = np.exp(pred)[y_val == 1, 1]\n",
    "\n",
    "result_sig_df = pd.DataFrame(sig_x_all, columns = list(sig_df.columns))\n",
    "result_sig_df[b'genweight_scale'] = result_sig_df.loc[:, b'genweight'].values * scale_sig\n",
    "result_sig_df[b'DNN score'] = sig_y\n",
    "result_bgd_df = pd.DataFrame(bgd_x_all, columns = list(bkg_df.columns))\n",
    "result_bgd_df[b'genweight_scale'] = result_bgd_df.loc[:, b'genweight'].values * scale_bgd\n",
    "result_bgd_df[b'DNN score'] = bgd_y\n",
    "\n",
    "result_sig_df_alt = pd.DataFrame()\n",
    "result_bgd_df_alt = pd.DataFrame()\n",
    "for column in list(result_sig_df.columns):\n",
    "    result_sig_df_alt[column] = result_sig_df.loc[:, column].values\n",
    "    result_bgd_df_alt[column] = result_bgd_df.loc[:, column].values\n",
    "    \n",
    "from ROOT import RDF\n",
    "from ROOT import Internal\n",
    "\n",
    "result_sig_dict = {key.decode(\"utf-8\"): result_sig_df_alt[key].values for key in list(result_sig_df_alt.columns)}\n",
    "result_bgd_dict = {key.decode(\"utf-8\"): result_bgd_df_alt[key].values for key in list(result_bgd_df_alt.columns)}\n",
    "sig_rdf = rt.RDF.MakeNumpyDataFrame(result_sig_dict)\n",
    "bgd_rdf = rt.RDF.MakeNumpyDataFrame(result_bgd_dict)\n",
    "sig_rdf.Snapshot('tree', f'DNN_Trees/ttH_score/{names[i]}_ttHscore_sig_DNN_file.root')\n",
    "bgd_rdf.Snapshot('tree', f'DNN_Trees/ttH_score/{names[i]}_ttHscore_bgd_DNN_file.root')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28720,)\n",
      "(28720,)\n",
      "(5, 5000)\n",
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGECAYAAADdvJEhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABYfUlEQVR4nO3deXxU1f3/8dcnELawb2pAFhFjihRUqChfK1WjVPm5FBWXVhQq6hdDRWsL6hcopRVU3FEKgkRFkSoqRSgWBcGFClgUNFEhIpsF2dcAgfP7YyZhtiQzySQzmXk/H488Mvfec2c+N8OET84953zMOYeIiIhIokuJdQAiIiIiVUFJj4iIiCQFJT0iIiKSFJT0iIiISFJQ0iMiIiJJQUmPiIiIJIWasQ4g1po3b+7atWsX6zBEREQkSlasWLHNOdcicH/SJz3t2rVj+fLlsQ5DREREosTMvg+1X7e3REREJCko6REREZGkoKRHREREkkLSj+kRERHxdeTIETZu3EhBQUGsQ5Ey1KlTh9atW5OamhpWeyU9IiIiPjZu3EiDBg1o164dZhbrcKQEzjm2b9/Oxo0bad++fVjn6PaWiIiIj4KCApo1a6aEJ86ZGc2aNYuoR05Jj4iISAAlPNVDpO+Tkh4REZE4Y2bce++9xduPPvooo0aNqrTXe+utt/jqq68q7fnjhcb0iIiIlKLn2PfZtOtg1J6vVeO6fDTswlLb1K5dm1mzZjF8+HCaN28etdcuyVtvvUWfPn34yU9+UumvFUtKekREREqxaddB1o29PGrP127YO2W2qVmzJoMGDeLxxx/nL3/5i9+xdevWMWDAALZt20aLFi144YUXaNOmjV+bffv2kZ2dzfLlyzEzRo4cSd++falfvz779u0D4PXXX2fOnDkMGjSI2bNn88EHHzBmzBjeeOMNOnToELXrjSe6vSUiIhKHBg8ezPTp09m9e7ff/uzsbPr3788XX3zBTTfdxJAhQ4LO/fOf/0yjRo1YtWoVX3zxBRdeWHLP0nnnnccVV1zBI488wsqVKxM24QElPSIiInGpYcOG3HzzzTz11FN++z/55BNuvPFGAH7zm9/w4YcfBp27YMECBg8eXLzdpEmTyg22mlDSIyIiEqfuvvtupkyZwv79+6PyfL6znZJx8UUlPSKSmB7vDKMa+X893jnWUYlEpGnTplx33XVMmTKleN95553HjBkzAJg+fTrnn39+0HlZWVlMmDCheHvnzp0AnHDCCeTm5nLs2DHefPPN4uMNGjRg7969lXUZcUNJj4gkpt3rYdRu/6/d62MdlUjE7r33XrZt21a8/fTTT/PCCy/w05/+lJdeeoknn3wy6JwHH3yQnTt3csYZZ9ClSxcWLlwIwNixY+nTpw/nnXceJ510UnH766+/nkceeYQzzzyTtWvXVv5FxYg552IdQ0x169bNLV++PNZhiEi0jWrkSXR8Pd7ZP/Fp1AaGrqrauCTu5ebmkpmZWbwdiynrEr7A9wvAzFY457oFttWUdRFJHoEJzqhGsYlDqhUlKIlDt7dEREQkKSjpERERkaSgpEdERESSgsb0iEj1owHJIlIOSnpEpPopmo5eJMSA5Etbp7M5x39dnvS0dOZfM7+yoxOROKXbWyKSkDan1mRV/1V+X5v3b451WCJhefLJJznjjDPo1KkTTzzxhN+xp59+mtNPP51OnTrxhz/8AYCPPvqIn/70p3Tr1o1vv/0WgF27dnHJJZdw7NixsF934sSJvPjii1G7jiK9evUikuVh8vLy6Nq1a/G6QU899RSZmZncdNNNFYpDPT0iIiKlCbydWlFl3I5dvXo1kydP5tNPP6VWrVr07t2bPn36cOqpp7Jw4ULefvttPv/8c2rXrs3WrVsBGD9+PHPnzmXdunVMnDiR8ePHM2bMGO6//35SUsLv37jjjjsqfHnR8NZbb3HNNdfw4IMPAvDss8+yYMECWrduXaHnVdIjItVO4K2r9Nbp6KaVVJrA26kVVcb6ULm5uZxzzjnUq1cPgAsuuIBZs2bxhz/8geeee45hw4ZRu3ZtAFq2bAlAamoqBw4c4MCBA6SmprJ27Vo2bNhAr169SnydYcOGMXv2bGrWrMkll1zCo48+yqhRo6hfvz6///3vWbZsGQMHDiQlJYWsrCzmzZvH6tWrmTZtGrNnz+bAgQOsXbuWq6++mocffhiAO++8k2XLlnHw4EGuueYa/vSnP5V6rStWrOCee+5h3759NG/enGnTpvGf//yHJ554gho1avDee++RkZFBfn4+v/zlLxkwYABDhw4N9ycdREmPiFQ7RbeuinTOUU0tSRxnnHEGDzzwANu3b6du3brMnTuXbt08iwt/8803LFmyhAceeIA6derw6KOP0r17d4YPH87NN99M3bp1eemll/j973/PmDFjSnyN7du38+abb5KXl4eZsWvXrqA2t956K5MnT+bcc89l2LBhfsdWrlzJf/7zH2rXrk1GRgbZ2dmcfPLJ/OUvf6Fp06YcPXqUiy66iC+++IKf/vSnIWM4cuQI2dnZvP3227Ro0YLXXnuNBx54gKlTp3LHHXcUJ18A//znP1m4cCHNmzcv50/VQ0mPiIhIHMnMzOSPf/wjl1xyCWlpaXTt2pUaNWoAUFhYyI4dO1i6dCnLli3juuuuIz8/n65du7J06VIAFi9ezEknnYRzjn79+pGamsr48eM54YQTil+jUaNG1KlTh4EDB9KnTx/69OnjF8OuXbvYu3cv5557LgA33ngjc+bMKT5+0UUX0aiRp8fqJz/5Cd9//z0nn3wyM2fOZNKkSRQWFvLDDz/w1VdflZj0fP3116xevZqsrCwAjh496lcPrDJoILOIiEicGThwICtWrGDx4sU0adKE0047DYDWrVvzq1/9CjPjZz/7GSkpKX7FSJ1zjBkzhv/7v//jT3/6Ew8//DC33XYbTz31lN/z16xZk08//ZRrrrmGOXPm0Lt374jiK7q9BlCjRg0KCwv57rvvePTRR3nvvff44osvuPzyyykoKCjxOZxzdOrUiZUrV7Jy5UpWrVrFu+++G1EckVLSIyJJIz0tnc45nYu/Lm2dHuuQREIqGqC8fv16Zs2axY033gjAVVddVVwx/ZtvvuHw4cN+t3xefPFFLrvsMpo2bcqBAwdISUkhJSWFAwcO+D3/vn372L17N5dddhmPP/44n3/+ud/xxo0b06BBA/79738DMGPGjDJj3rNnD2lpaTRq1IgtW7Ywb968UttnZGTw448/8sknnwCe211ffvllma9TEbq9JSJJI3CNHo0FknjVt29ftm/fTmpqKhMmTKBx48YADBgwgAEDBnDGGWdQq1YtcnJyMDMADhw4wLRp04p7S+655x4uu+wyatWqxSuvvOL3/Hv37uXKK6+koKAA5xyPPfZYUAxTpkzhtttuIyUlhQsuuKD4dlZJunTpwplnnsnpp5/OySefTM+ePUttX6tWLV5//XWGDBnC7t27KSws5O6776ZTp07h/pgiZs65Snvy6qBbt24ukrUDRCT2Pjgnk5Y+k2m2NoIL/p3r16ZzTme/wc6hhNNGkk9ubi6ZmZnHd1TxlPV4sW/fPurXrw/A2LFj+eGHH3jyySdjHFWwoPcLMLMVzrlugW3V0yMi1U7L3ZCZ55PknJ5ZcmORiqoGCUpleOedd3jooYcoLCykbdu2TJs2LdYhVZiSHhFJDIFrn7RvE5s4RBJEv3796NevX6zDiColPSKSGAIWj3vmnExyH/LvAUpNT+fU99+ryqhEJI4o6RGRhBR0CwzI1W0wkaSmpEdEkkZqerpf4vNMI6B/7OIRkaqlpEdEEkLg9POZIdoE3dpSz49IUtHihCKSEFb1X+X3JVKdPfnkk5xxxhl06tSJJ554Iuj4+PHjMbPi1ZjfeOMNOnXqxPnnn8/27dsBWLt2bcQDkUeMGMGCBQsqHH+gdu3a+a0cXZYlS5bQqVMnunbtysGDB7nvvvvo1KkT9913X4XiUE+PiMS3kGukaCVlqTqXvn4pm/dvjtrzpaelBy2U6Wv16tVMnjyZTz/9lFq1atG7d2/69OnDqaeeCsCGDRt49913adPm+AzFp59+mmXLljFr1ixeeeUVsrOzefDBB0stOhrK6NGjy3dRUTZ9+nSGDx/Or3/9awAmTZrEjh07imuQlVdcJj1m1ht4EqgBPO+cGxtw/B7gt0Ah8CMwwDn3vffYUaDoz7z1zrkrqixwEYm+3euDZmYxQ7elpOps3r85qr2HZa0EnpubyznnnEO9evUAuOCCC5g1axZ/+MMfABg6dCgPP/wwV155ZfE5KSkpHDp0iAMHDpCamsqSJUs48cQT6dixY8jXOHr0KAMHDmT58uWYGQMGDGDo0KHccsst9OnTh2uuuYa5c+dyzz33kJaWRs+ePcnPz2fOnDmMGjWK9evXk5+fz/r167n77rsZMmQI4CmTsWHDBgoKCvjd737HoEGDSr3Wd999l5EjR3Lo0CE6dOjACy+8wIwZM5g5cybz589n3rx57N27l3379nH22WczfPjwCk2jj7ukx8xqABOALGAjsMzMZjvnvvJp9h+gm3PugJndCTwMFP0UDjrnulZlzCIiItFyxhln8MADD7B9+3bq1q3L3Llz6dbNs7jw22+/TatWrejSpYvfOcOHD+fiiy8mPT2dl19+mWuvvbbUelkrV65k06ZNrF69GvBUVfdVUFDA7bffzuLFi2nfvj033HCD3/G8vDwWLlzI3r17ycjI4M477yQ1NZWpU6fStGlTDh48SPfu3enbty/NmjULGcO2bdsYM2YMCxYsIC0tjXHjxvHYY48xYsQIPvzww+LkC6B+/fqsXLkykh9jSHGX9AA/A9Y45/IBzGwGcCVQnPQ45xb6tF8K/LpKIxQREakkmZmZ/PGPf+SSSy4hLS2Nrl27UqNGDQ4cOMBf//rXkJXIs7KyyMrKAo4XHf3mm2949NFHadKkCU8++WRxzxHAKaecQn5+PtnZ2Vx++eVccsklfs+Xl5fHKaecQvv27QG44YYbmDRpUvHxyy+/nNq1a1O7dm1atmzJli1baN26NU899RRvvvkm4LkN9+2335aY9CxdupSvvvqquEbX4cOHOffccyvwkytbPA5kbgVs8Nne6N1XkoGAbynXOma23MyWmtlVoU4ws0HeNst//PHHCgcsIiISTQMHDmTFihUsXryYJk2acNppp7F27Vq+++47unTpQrt27di4cSNnnXUW//3vf4vPKyo6OnjwYEaOHElOTg7/8z//w/Tp0/2ev0mTJnz++ef06tWLiRMn8tvf/jai+GrXrl38uEaNGhQWFrJo0SIWLFjAJ598wueff86ZZ55JQUFBic/hnCMrK4uVK1eycuVKvvrqK6ZMmRJRHJGKx56esJnZr4FuwAU+u9s65zaZ2SnA+2a2yjm31vc859wkYBJ4Co5WWcAiUikC198p2idSXW3dupWWLVuyfv16Zs2axdKlS2ncuDFbt24tbtOuXTuWL19O8+bNi/c98sgjDBkyhNTUVA4ePIiZkZKSwoEDB/yef9u2bdSqVYu+ffuSkZFRPGC4SEZGBvn5+axbt4527drx2muvlRnz7t27adKkCfXq1SMvL4+lS5eW2r5Hjx4MHjyYNWvWcOqpp7J//342bdrEaaedFs6PqFziMenZBJzss93au8+PmV0MPABc4Jw7VLTfObfJ+z3fzBYBZwJrA88Xkerh0tbpbC5jDR6VlpBE07dvX7Zv305qaioTJkygcePGZZ6zefNmPv30U0aOHAlAdnY23bt3p3Hjxrz11lt+bTdt2sStt97KsWPHAHjooYf8jtetW5dnn32W3r17k5aWRvfu3ct8/d69ezNx4kQyMzPJyMigR48epbZv0aIF06ZN44YbbuDQIc9/42PGjKnUpMeci6+ODjOrCXwDXIQn2VkG3Oic+9KnzZnA60Bv59y3PvubAAecc4fMrDnwCXBlwCBoP926dXPLly+vnIsRkQrrnNM5aOZM7umZQSUmyiNazyOJJTc3l8zM4z2HVT1lPV7s27eP+vXr45xj8ODBdOzYkaFDh8Y6rCCB7xeAma1wznULbBt3PT3OuUIzuwuYj2fK+lTn3JdmNhpY7pybDTwC1Af+bmZwfGp6JvA3MzuGZ7zS2NISHhGRoOrsjdrAUC1uKMdVhwSlMkyePJmcnBwOHz7MmWeeye233x7rkCos7pIeAOfcXGBuwL4RPo8vLuG8j4HSF0AQEfEVuAZQYBIkkqSGDh0alz07FRGXSY+IJLGAFZifmZdO7kMapCwiFaekR0TiS8AKzC1naNyNiESHkh4REV+N2vjf4tIYH5GEoaRHRMRXYIKjMT4iCSMeV2QWEakynXM6+31d+vqlsQ5JhCeffJIzzjiDTp068cQTTxTv37FjB1lZWXTs2JGsrCx27twJwBtvvEGnTp04//zz2b59OwBr166NuDjniBEjWLBgQdSuo0i7du3Ytm1b2O2XLFlCp06d6Nq1KwcPHuS+++6jU6dO3HfffRWKQz09IpLUAtcAKqsCtiSfNRdexJHN0VunJzU9vdQFNVevXs3kyZP59NNPqVWrFr1796ZPnz6ceuqpjB07losuuohhw4YxduxYxo4dy7hx43j66adZtmwZs2bN4pVXXiE7O5sHH3yQMWPGRBTb6NGjK3p5UTF9+nSGDx9evFL0pEmT2LFjBzVq1KjQ8yrpEZFq58X7P2bvjuM1fRo0rcPNfz0vhhFJIjuyeXNUB9MHlkwJOp6byznnnFNcIPSCCy5g1qxZ/OEPf+Dtt99m0aJFAPTv359evXoxbtw4UlJSOHToEAcOHCA1NZUlS5Zw4okn0rFjx5CvcfToUQYOHMjy5csxMwYMGMDQoUO55ZZbiqubz507l3vuuYe0tDR69uxJfn4+c+bMYdSoUaxfv578/HzWr1/P3XffzZAhQwC46qqr2LBhAwUFBfzud79j0KBBpV7ru+++y8iRIzl06BAdOnTghRdeYMaMGcycOZP58+czb9489u7dy759+zj77LMZPnx4xL1XvpT0iEi1s3dHAYMnXli8PeGO92MYjUh0nXHGGTzwwANs376dunXrMnfuXLp18ywuvGXLFk466SQATjzxRLZs2QLA8OHDufjii0lPT+fll1/m2muvZcaMGSW+xsqVK9m0aROrV68GYNeuXX7HCwoKuP3221m8eDHt27fnhhtu8Duel5fHwoUL2bt3LxkZGdx5552kpqYydepUmjZtysGDB+nevTt9+/Ytscr6tm3bGDNmDAsWLCAtLY1x48bx2GOPMWLECD788MPi5Augfv36rFy5MuKfZSCN6REREYkjmZmZ/PGPf+SSSy6hd+/edO3aNeRtHTPDW5WArKwsVqxYwT/+8Q/efvttLrvsMr755huuueYabrvttqCCo6eccgr5+flkZ2fzz3/+k4YNG/odz8vL45RTTqF9+/YAQUnP5ZdfTu3atWnevDktW7YsTr6eeuopunTpQo8ePdiwYQPffvstJVm6dClfffUVPXv2pGvXruTk5PD9999H/gOLgHp6RCSuBBYYDSwuWtnS09L9xvWkt04nOYsQSCwNHDiQgQMHAnD//ffTunVrAE444QR++OEHTjrpJH744Qdatmzpd96BAweYNm0a8+fPp0+fPsyaNYvXX3+d6dOnc9tttxW3a9KkCZ9//jnz589n4sSJzJw5k6lTp4YdX+3atYsf16hRg8LCQhYtWsSCBQv45JNPqFevHr169aKgoKDE53DOkZWVxauvvhr261aUkh4RiSubU2v6DS4OXI05XIHjfkKp02M0gc8eWGdJA5slFrZu3UrLli1Zv349s2bNYunSpQBcccUV5OTkMGzYMHJycrjyyiv9znvkkUcYMmQIqampHDx4EDMjJSUlqKdn27Zt1KpVi759+5KRkVE8YLhIRkYG+fn5rFu3jnbt2vHaa6+VGfPu3btp0qQJ9erVIy8vrzjmkvTo0YPBgwezZs0aTj31VPbv38+mTZsqtcq6kh4RSUiB435CmXDH+0GDSsuaWSNSFfr27cv27dtJTU1lwoQJNG7cGIBhw4Zx3XXXMWXKFNq2bcvMmcf7Qjdv3synn37KyJEjAcjOzqZ79+40btyYt956y+/5N23axK233sqxY8cAeOihh/yO161bl2effZbevXuTlpZG9+7dy4y5d+/eTJw4kczMTDIyMujRo0ep7Vu0aMG0adO44YYbOHToEABjxoxR0iMi4qtg9/OM7/fY8R0pDYHSE5ySBM7KKWtmjSSf1PT0qP67CKd23JIlS0Lub9asGe+9FzopT09P55133inevvbaa7n22mtDtu3SpQufffZZ0P5p06YVP/7FL35BXl4ezjkGDx5cPJh61KhRfucUDYYGmDdvXsjXW7duXcj9F154IcuWLSs1DoB9+/aFPD9SSnpEpPo5tod7X5tTvDm+X58YBiOJLll7/iZPnkxOTg6HDx/mzDPP5Pbbb491SBWmpEdEksbkuwaw58etx3dUoIdIJNENHTqUoUOHxjqMqFLSIyJJY8+PW9VDJJLElPSISEIKGvcDNGzRsoTWxwWO33imEfBdQNFRVV4XqZaU9IhIXHnm2UK/aerhDPoMKWDcT7iCxm+cngmjdvvvU+V1kWpJSY+IxJWWu4NnVImIRIPKUIiIiMSRr7/+mq5duxZ/NWzYkCeeeAKAHTt2kJWVRceOHcnKymLnzp0AvPHGG3Tq1Inzzz+f7du3A7B27dqIi3OOGDGCBQsWRPV6ANq1a8e2bdvCbr9kyRI6depE165dOXjwIPfddx+dOnXivvvuq1Ac6ukREREpRTire0eiQdM63PzX80o8npGRUVxc8+jRo7Rq1Yqrr74agLFjx3LRRRcxbNgwxo4dy9ixYxk3bhxPP/00y5YtY9asWbzyyitkZ2fz4IMPMmbMmIhiGz16dLmvK5qmT5/O8OHDi1eKnjRpEjt27AhZgywSSnpEJCGo0rpUlnBW945EJP9W33vvPTp06EDbtm0BePvtt1m0aBEA/fv3p1evXowbN46UlBQOHTrEgQMHSE1NZcmSJZx44ol07Ngx5PMePXqUgQMHsnz5csyMAQMGMHToUG655Zbi6uZz587lnnvuIS0tjZ49e5Kfn8+cOXMYNWoU69evJz8/n/Xr13P33XczZMgQAK666io2bNhAQUEBv/vd7xg0aFCp1/fuu+8ycuRIDh06RIcOHXjhhReYMWMGM2fOZP78+cybN4+9e/eyb98+zj77bIYPHx5x75UvJT0iEtfC/Ss78D+lwJlbItXRjBkz/Cqcb9myhZNOOgmAE088sbi6+fDhw7n44otJT0/n5Zdf5tprr2XGjBklPu/KlSvZtGlT8WrKu3bt8jteUFDA7bffzuLFi2nfvn1QlfW8vDwWLlzI3r17ycjI4M477yQ1NZWpU6fStGlTDh48SPfu3enbty/NmjULGcO2bdsYM2YMCxYsIC0tjXHjxvHYY48xYsQIPvzww+LkC6B+/frFvV8VoaRHROJaqL+yldBIMjh8+DCzZ88OqotVxMwwMwCysrLIysoC4MUXX+Syyy7jm2++4dFHH6VJkyY8+eST1KtXr/jcU045hfz8fLKzs7n88su55JJL/J47Ly+PU045hfbt2wNwww03MGnSpOLjl19+ObVr16Z27dq0bNmSLVu20Lp1a5566inefPNNADZs2MC3335bYtKzdOlSvvrqK3r27Fl8veeee255flRhU9IjIrHzeGfYvT5gZ/mmqLcb9o7fdnY5QxKJF/PmzeOss87ihBNOKN53wgkn8MMPP3DSSSfxww8/0LKl/9pTBw4cYNq0acyfP58+ffowa9YsXn/9daZPn85tt91W3K5JkyZ8/vnnzJ8/n4kTJzJz5kymTp0admy1a9cuflyjRg0KCwtZtGgRCxYs4JNPPqFevXr06tWLgoKSe2mdc2RlZfHqq6+G/boVpaRHRGJn9/rgNXBmlK+w47qxl/ttj+/3XFAi1KpxXT4aFvnYjM45nf2201unMz/yEEUi8uqrrwbdVrriiivIyclh2LBh5OTkcOWVV/odf+SRRxgyZAipqakcPHgQMyMlJYUDBw74tdu2bRu1atWib9++ZGRkFA8YLpKRkUF+fj7r1q2jXbt2vPbaa2XGu3v3bpo0aUK9evXIy8tj6dKlpbbv0aMHgwcPZs2aNZx66qns37+fTZs2qcq6iEh5BCZCgUlQuFb19199OTAJEom2/fv3869//Yu//e1vfvuHDRvGddddx5QpU2jbti0zZ84sPrZ582Y+/fRTRo4cCUB2djbdu3encePGvPXWW37Ps2nTJm699VaOHTsGEHQLrW7dujz77LP07t2btLQ0unfvXmbMvXv3ZuLEiWRmZpKRkUGPHj1Kbd+iRQumTZvGDTfcwKFDhwAYM2aMkh4RkdLsqdkgqI5WqJITrRrX9Ut8dAtMwtGgaZ2ozg5s0LROmW3S0tKK19vx1axZM957L3TV9/T0dN555/i/72uvvZZrr702ZNsuXbrw2WefBe2fNm1a8eNf/OIX5OXl4Zxj8ODBdOvWDYBRo0b5nVM0GBo8t+RCWbduXcj9F154IcuWLSs1DoB9+/aFPD9SSnpEJGYubZ3O5oBek5kltA3km7y06nxbWLetAtuM7/dcmK8myay0NXUS2eTJk8nJyeHw4cOceeaZ3H777bEOqcKU9IhIzGxOrRl068i37haELhy6p2aDoFtXIhJdQ4cOZejQobEOI6qU9IhIfAtROLTdsHf4U4zCEZHqK6Kkx8xqAScCh51z/w04Vh8YBWQBx4A5wF+dcwejE6qISNVLTU8n93T/3qdnGgH9YxOPiJRfpD09vwWeBnKAAQHH3gH+BzDv9k+B883sF845V6EoRURi5NT3QwwaPb180+pFJLYiTXou9X5/xXenmV0BnI+nh+cV4CBws3ffb4AXKxamiCSiZ54tDBrDk5oevDhhqPV2REQilRJh+6LfTisC9t8IOGCcc+43zrlBwN14en1urFCEIpKwWu6GzLxcv69QPSvrxl7u91WeBQZFqpN27drRuXNnunbtWjxVHGDHjh1kZWXRsWNHsrKy2LlzJwBvvPEGnTp14vzzzy+e6r527dqIi3OOGDGCBQsWRO9CvNq1a8e2bdvCbr9kyRI6depE165dOXjwIPfddx+dOnXivvvuq1Ackfb0tAAOOOd2Buz/hff78z77XgKeBbqUMzYREZGYm3zXAPb8uDVqz9ewRUtue6bskg8LFy6kefPmfvvGjh3LRRddxLBhwxg7dixjx45l3LhxPP300yxbtoxZs2bxyiuvkJ2dzYMPPsiYMWMiim306NERta8s06dPZ/jw4cUrRU+aNIkdO3ZQo0aNCj1vpElPGp5bV8XMrB2eZGi9c+67ov3Ouf1mtgtoWqEIRSSphFtVPVrKWnSuQdM6SbtOi3js+XFr0AzCighcSDMSb7/9NosWLQKgf//+9OrVi3HjxpGSksKhQ4c4cOAAqampLFmyhBNPPJGOHTuGfJ6jR48ycOBAli9fjpkxYMAAhg4dyi233FJc3Xzu3Lncc889pKWl0bNnT/Lz85kzZw6jRo1i/fr15Ofns379eu6++26GDBkCwFVXXcWGDRsoKCjgd7/7HYMGDSr1et59911GjhzJoUOH6NChAy+88AIzZsxg5syZzJ8/n3nz5rF371727dvH2WefzfDhwyPuvfIVadKzA2hhZo2dc7u8+4r6mT8u4fmjs4yiiCSFwKrqlV1RPbCCe6BorsQrEi4z45JLLsHMuP3224uThy1btnDSSScBcOKJJ7JlyxYAhg8fzsUXX0x6ejovv/wy1157LTNmzCjx+VeuXMmmTZuKV1PetWuX3/GCggJuv/12Fi9eTPv27YNqgOXl5bFw4UL27t1LRkYGd955J6mpqUydOpWmTZty8OBBunfvTt++fUussr5t2zbGjBnDggULSEtLY9y4cTz22GOMGDGCDz/8sDj5Aqhfvz4rV66M+OcYKNIxPUVrVg8EMLMU72MHLPRtaGYtgPqA39R2ERERKd2HH37IZ599xrx585gwYQKLFy8OamNmmHkmTGdlZbFixQr+8Y9/8Pbbb3PZZZfxzTffcM0113DbbbcFFRw95ZRTyM/PJzs7m3/+8580bNjQ73heXh6nnHIK7du3BwhKei6//HJq165N8+bNadmyZXHy9dRTT9GlSxd69OjBhg0b+Pbbb0u8xqVLl/LVV1/Rs2dPunbtSk5ODt9//33kP6wIRNrTkwP0Bsaa2cV4bmudBewF/h7Q9nzv99wKRSgiUomiVYldJJpatWoFQMuWLbn66qv59NNP+fnPf84JJ5zADz/8wEknncQPP/xAy5b+NeYOHDjAtGnTmD9/Pn369GHWrFm8/vrrTJ8+ndtuu624XZMmTfj888+ZP38+EydOZObMmUydWvY4oyK1a9cuflyjRg0KCwtZtGgRCxYs4JNPPqFevXr06tWLgoKSb1U758jKyuLVV18N+3UrKqKeHufca8A0oAae6etnAQXAHT63u4r0I0QPkIhIPAmcGbZpl9ZTldjav38/e/fuLX787rvvcsYZZwBwxRVXkJOTA0BOTg5XXnml37mPPPIIQ4YMITU1lYMHD2JmpKSkBPX0bNu2jWPHjtG3b1/GjBkTVHw0IyOD/Pz84kKhr732Wplx7969myZNmlCvXj3y8vJYunRpqe179OjBRx99xJo1a4qv9ZtvvinzdSoi4jIUzrkBZjYFOA/YBbznnMv3beNduXk3nvV55kYhThFJEoG1tvbUbBDDaESq3pYtW7j66qsBKCws5MYbb6R3794ADBs2jOuuu44pU6bQtm1bZs48XqJ38+bNfPrpp4wcORKA7OxsunfvTuPGjXnrrbf8XmPTpk3ceuutHDt2DICHHnrI73jdunV59tln6d27N2lpaXTv3r3MuHv37s3EiRPJzMwkIyODHj16lNq+RYsWTJs2jRtuuIFDhw4BMGbMGE477bQyX6u8LNkXS+7WrZtbvnx5rMMQSUq5p2eSmed/B3x8vz5+M2XaDXun0oqLhjUVOaUh977qtx5ryLglceTm5pKZeXzRzFhNWY+1ffv2Ub9+fZxzDB48mI4dO8ZlAdLA9wvAzFY457oFto209tbNwEHnXOD4nZLa/wqo75zTiswiEndC/ccTmGRVZHqxJIbqkKBUhsmTJ5OTk8Phw4c588wzuf3222MdUoVFentrGvADwYOWSzIeOBmVoRAREalWhg4dGpc9OxUR8ZgejhcUraz2IpLkfGdUqc6WiERLeZKeSDQEDlfya4hINfVxj9G8H2Lxv8oawyMSLudc8Ro4Er8iHZdcaUmPmZ0LNAHyy2ob4tzewJN4psY/75wbG3D8HuC3QCHwIzDAOfe991h/4EFv0zHOuZxyX4SIVKqCOs2CVkSu7BWYRcpSp04dtm/fTrNmzZT4xDHnHNu3b6dOnTphn1Nq0uNNIPoH7G5qZqWty25AY6ATnnV6gksml/6aNYAJQBawEVhmZrOdc1/5NPsP0M05d8DM7gQeBvqZWVNgJNDN+9orvOcGFkgVEREJqXXr1mzcuJEff/wx1qFIGerUqUPr1q3Dbl9WT087oFfAvloh9pXka2BU2NF4/AxYU7T2j5nNAK4EipMe55zvgodLgV97H18K/Ms5t8N77r/wrCBddcs9ikhyGNXo+ONGbWDoqtjFIlGVmppaXH5BEktZSc+igO2ReAqIji/lnGPAHmA1sMg5dzTCmFoBG3y2NwLnlNJ+IDCvlHNbRfj6IlJFAhcihGq0GOGo3T6PG5XcTkTiRqlJj3PuA+CDom0zGwnsc879qbIDC4eZ/RrPrawLIjxvEDAIoE2bNpUQmYiE5dgev4UIwTNzK5a/YFo1rus3eyw7hrGISHRFOpC5PRBpz02kNuFZ26dIa+8+P96Cpw8AFzjnDvmc2yvg3EWB5zrnJgGTwLMiczSCFpHEEFhsdHy/55gQMMOsTo/R+K//KiLVQURJT9EMqUq2DOhoZu3xJDHXAzf6NjCzM4G/Ab2dc75rg88H/mpmTbzblwDDKz9kEUlkgTPMApMgEakeKjRl3czq4JmWnlpaO+fc+nCf0zlXaGZ34UlgagBTnXNfmtloYLlzbjbwCFAf+Lt3OuF659wVzrkdZvZnPIkTwOiiQc0iItHUOadz8eP01unMj2EsIhKeiJMeM6sH/AG4ATg1jFNcpK/jnJtLQHV259wIn8cXl3LuVCA5C6WISJVZ1f/4bC3fBEhE4lekBUcbA4vxrMET7opNWtlJRKqtPTUbBBcdTWkIXBiyvYjEr0h7ev4POAM4AjwNvA1sxrMysohIwsk5+ddBZTFUeV2keoo06bkKz+2qu51zz0U/HBFJaI93ht2+Q/zOj1koFZV7+vH5W880InjtehGJO5EmPa3wLD74QiXEIiIJ7tIGhWxuenxtrFtyYxhMBWXm+QR/uiawi1QHkSY9O4A6zrmCyghGRBLb5tSafgOAx8/t47cQIHgWBxQRqQyRJj0fAn3NrJVzLmjBQBGRSAWOlxERqSwpEbYfh2fQ8v9VQiwiIiIilSbS9XNWmNktwFQzSwX+UlQNXUSkLM88W0juQz7jX7p0iF0wIpJ0Il2npyjBOQrcAtxiZjuAvaWc5pxz+s0mIrTc7T8AeG41mPodWIAUVIRUpLqKdExPuxD7mnm/SqKCniJSbQUWIAVPEVIRqX4iTXpurZQoRERERCpZpGN6ciorEBEREZHKFOnsLREREZFqSUmPiIiIJIVIx/QAYGatgXuAS4G2eFZprulzvAlwJ55BzI8451SQVET4uMdo3r/j/ViHISJJKuKkx8yygJlAQ8C8u/1maDnndprZVcDZwJfA7IqFKSKJoKBOMwZPPD4bany/x2IYjYgkm4hub5nZycDrQCPgH8A1wM4Smk/FkxRpjXkRERGJuUjH9NwLNABmOueucs7NAg6X0Ha+93v38gYnIiIiEi2RJj2X4rmVVWbtLefcd8AhoH054hIRERGJqkiTnjbAQefct2G23wekRfgaIiIiIlEX6UDmY0CNcBqaWU08g533RBqUiCSAxzvD7vUBO9+MSSgiIhB50vM9kGlmbZxzgb/NAv0cSAXC7RUSkQSy5qUCjhxI99tXcObzfjO29tRsUNVhRc0En6n3dXqMJrOUtiISHyJNehYAmcAdwP0lNTKzVOAveMb/zCt3dCJSbR05UJPrhvv/irll7h7ufW1O8Xa7Ye/wp6oOLEp8p95P0NpDItVCpEnP48DtwL1mttY5NyWwgZmd5W13Dp5bW89WOEoRqZZW9V/ltz1+bh/aDXuneLtV47pVHZKIJLFIC45+b2a/BXKASWb2Vzxr9mBmH+NZnflEPOvzFAI3O+e2RTdkEanO1o3V0l0iEhsRr8jsnJtuZluBCcCpPod6+DxeA9zhnFOfr4gknD01GzC+X5/jO1IaAheW2F5E4kO5am855/5lZhl4Biv3BNLxzOr6L/ARsNA5dzRqUYqIxJGck3/t12PllwCJSNwqV9ID4JxzwAfeLxEREZG4FmntrdaVFYiIiIhIZYq0p2edmS0CXgTecM7tj35IIpIIPu4xmvc1lVtE4kikSU8K8Avv17Nm9ibwEvAv7+0uEREACuo081vLBvBbmFBEpKpFWnvrIjy9PPuAesCNeBYf3GhmD5tZ5yjHJyIiIhIVka7TsxBYaGZ3AlcBNwNZwEnAvXgWLfwCzzo+rzrntkQ3XBGpLgp2Px/Us1Ody06ISPVX3inrBcAMYIaZtcTT4/Nr4CygCzAeeNjMFgA5zrnXohSviFQXx/xLTkD1Ljvhq1Xjun4rS2fHMBYRCV+5p6wXcc5tBZ4AnjCzTDy9PzcCJwO9gUsBJT0ikjA+GhY4Vum5GEUiIpGIdExPqZxzuc654cCvgOXRfG4RERGRiqhwT08RM2sF3AT8BviJz6Ej0XoNERERkfKqUNJjZmlAXzyJTi88PUfmPbwcz0yvVyvyGiIiIiLREHHSY2aGZ8bWzXhmcNXleKKzAXgZeNE593WUYhQRERGpsIiSHjN7FLgBOLFoF7AXeANPorMoqtGJiIiIREmkPT33eL8fBRbgWY35TefcwahGJSIJwXdaN3imeouIxEqkSc8XeBKd6c65/1ZCPCKSQNaNvTzWIYiIFIt0ReaulRSHiFRjl75+KZv3b/bbdwttYxRNfLh0amc21/Dfl34U5g9YFZuARCR6U9ZFJHnd/8h6Wu723ze3S2xiiReba8Cq/v4JTucclScUiaVyJT1mVg/4LZ7VltsCdZ1zHXyONwIuB5xzTlPWRRJcy92Qeb1/T8/c3A4ltBYRiY3yTFnvCrwNtOb4VHUX0GwP8CCQYWZbnHPvVyRIEakGRgV09fTrE5s44sQzzxaS+1Cm/75GQP/YxCMikU9Zbwa8g6eq+go8Cw+OAPxKJzvnnJlNAR4BrgCU9IhIYhvVyG+z5e50MvNy/duc7p8EiUjVirT21lA8Cc97wDnOuceAkqarF81VPTfSoMyst5l9bWZrzGxYiOM/N7PPzKzQzK4JOHbUzFZ6v2ZH+toiIuUyarf/l4jEnUhvb/0/PLey/uCcO1ZG26/x1N2K6Ma+mdUAJuBZ9XkjsMzMZjvnvvJpth64Bfh9iKc4qFlmIlLVAgcpz4xRHCJSskiTnlOAw8DKshp6b3HtARqV1TbAz4A1zrl8ADObAVwJFCc9zrl13mNlJV4iIlUicKZW4HgeEYm9SG9vpQCFzrnAgctBvDW66gP7I3yNVnhqeBXZ6N0XrjpmttzMlprZVRG+toiIiCSoSHt6NgEdzKylc25rGW27A7WB3DLaRVtb59wmMzsFeN/MVjnn1vo2MLNBwCCANm3aVHF4Ionn4x6jef+O5J6vkBswSDk1PT1GkYhISSJNehbhGaNzKzCujLYj8Yz/+VeEr7EJONlnu7V3X1icc5u83/PNbBFwJrA2oM0kYBJAt27dyuy1EpHSFdRpxuCJF/rtG9/vsRhFExu/vOpRv+1WjevyUYxiEZHQIk16ngQGAPeb2Qrn3ILABmZ2AvAY8EvgEJ5ByZFYBnQ0s/Z4kp3rgRvDOdHMmgAHnHOHzKw50BN4OMLXF5FyCCwumh2jOGIlsM7YX+9cwISA3q86PUajkT4isRNp7a0vzex+YCww38z+g3egspm9gmd15rOBVO8pv3POrY/wNQrN7C5gPlADmOp93dHAcufcbDPrDrwJNAH+n5n9yTnXCcgE/uYd4JwCjA2Y9SUilaBg9/Nk79zjt69hi5YxiiY+NHIpQb1fgUmQiFStiFdkds49bGbbgUeBs3wO9eP4Cs27gLudcy+WJyjn3FxgbsC+ET6Pl+G57RV43seAituIVLVje7j3tTmxjkJEpFTlqr3lnJtiZq8BffHcQkrH0yvzX+Aj4O/OOa3OJSJJq2D388HjmlIaAheGbC8ila/cVdadc/uAHO+XiEhSC7p1FaL3a3yS1yMTibVyJz0iInJcss9eE6kOIl2cUERERKRaUk+PiEgFNWzRMujW1Z6aDWIUjYiUREmPiEgF3fbM1KB97Ya9w59iEIuIlEy3t0RERCQpKOkRERGRpKCkR0RERJKCkh4RERFJCkp6REREJClo9paIRGzKwFkUpDaOdRhxrVXjuklfeV4k3pSY9JjZiJKORco5NzpazyUisVeQ2thvBWKtPhzso2HBNbbG93suBpGISJHSenpGAS5Kr6OkRySBBBbT3Fe3MIbRiIiEp7SkZzHRS3pEJJEc28O0y74v3kw/oqRHROJfiUmPc65XFcYhItXMqu/WFz/e6JrHMBIRkfBoILOIlM+o3cUP/2fYO6yLXSQiImHRlHURERFJCkp6REREJCmU+/aWmZ0H/A/QGkgDrISmzjk3sLyvIyLxyXcNmlaN68YwEhGR8ESc9JhZR+AV4KzAQwTP9irap6RHJMGsG3t5rEMQEYlIREmPmTUD3gdaAVuAD4DrgIPAG8CJwDlAA2Ab8E7oZxIRERGpWpGO6bkbT8Lzb6CDc+567/7dzrmbnXOXAOnAI0Bz4KBz7tZoBSsiIiJSXpHe3rocz+2q+51zB0I1cM7tB/5oZrWAIWa20Dn39wrGKSIiIlIhkfb0dMCT9CwJ2F8rRNux3u+DIg1KREREJNoiTXpSgZ3OOd815w/gGcPjxzm3BdgN/LT84YmIiIhER6S3tzYDJwXs2wK0M7NTnHP5RTvNLBVoCKgoj4iIV+7pmcWPdzSuQc+lq2MYjUhyibSn53ugjpm19tm3zPv91wFtb/E+/6byhSYikngy83KLv5ruOhrrcESSSqQ9PUuAXt6vl737XsIzbf1BMzsBWAl0AW7DM/7nrYqHKSKSIEY18tlIj1kYIsko0qTn70B/4CK8SY9z7h0zmwFcD9zh09aAXGB0FOIUEUkMPoVamZFZcjsRibqIkh7n3JdA+xCHbgIWAv2Ak/EMYP4nMN45tztEexGRpDThjveLH9fpMRqlPSJVp9y1t3w55xww2fslIgnkxfs/Zu+OgliHkTAGT7yw+LFvAiQilS8qSY+IJK69Owr8/qMGGN/vsRhFU73tqdmA8f36HN+R0pDOOb/za5Oels78a+ZXcWQiyUFJj4iUqmD380FJTt3DR2IUTfWWc/Kv/Qq1ju/Xh1Xfrfdr0znUAAIRiYpyJz1mdi6ehQeb4lm0sETOOQ1mFqmuju3hss/X+u3a2qiEthKx3Bn+M7ieaVTomS4iIlEXcdJjZhcDk4C2EZympEekGsvMy/Xbvi6nM6tiFEt11qpxXdoNe6d4O5vgny2na2izSGWJKOkxs58Bczhea+s7PKs0a9VlEZEyfDQscGzUczGKRCQ5RdrT8394Ep484DrnnNZPFxERkWoh0qTnXDyrLP9GCY9I8uic09lvO/2IOnejwaUEzOYCrOtPtXaPSCWJNOmpBxxwzq2ojGBEJD6t6h8wgmeURjJHwzNt/WdzAUFJkIhET6RJz/dAm8oIRETiWGCS00i/BkSk+ok06XkDGG5mP3fOLa6MgEQkDo1SNRkRqf5SImw/FsgHJphZs0qIR0RERKRSRNrTcxaeGVwTgC/NbBLwb2BvaSepV0hERERiLdKkZxGe2VtFHgjjHFeO1xERERGJqvIkI1bJ7UVERESiLqKkxzkX6RggERERkbig204iIjESWIsLPPW4RKRyxGXSY2a9gSeBGsDzzrmxAcd/DjyBp8r79c65132O9Qce9G6Occ7lVEnQIgnixfs/Zu+OgliHkRQCa3GB6nGJVKa4S3rMrAae2WFZwEZgmZnNds595dNsPXAL8PuAc5sCI4FueAZQr/Ceu7MqYhdJBHt3FDB44vH/jMf3eyyG0YiIRE+kVdZHRPj8BcAu4EtgmXPucBjn/AxY45zL977mDOBKoDjpcc6t8x47FnDupcC/nHM7vMf/BfQGXo0wbpGkVbD7eb9Ep+7hIzGMRkQkeiLt6RmF/5T1SOwws8eBsc65wGTFVytgg8/2RuCcMF8j1LmtAhuZ2SBgEECbNlpOX8TPsT1c9vna4s2tKrMlIgki0qRnMZ6kpwvQ2LtvA7DJ+7gVcLL38U5glbfd6UAz4M/ec/uVN+BocM5NAiYBdOvWrbxJnEjCyszLLX58XU5nVpXSVkSkuohoCrpzrhfwMZ5EJgc41TnX1jl3nverLdABeAFoAnzgnOuKJ+EZ5X2aa8zs6lJeZhPHEyeA1hxPqspSkXNFREQkgUWU9JhZX2AY8KRz7taicTe+nHPfOecG4pld9aCZXeGc2++cGw08hmexwv6lvMwyoKOZtTezWsD1wOwwQ5wPXGJmTcysCXCJd5+IiIgkuUgXG7wLz+2tP4fRdoz3+xCffU97v3cr6STnXKH3deYDucBM59yXZjbazK4AMLPuZrYRuBb4m5l96T13hze2Zd6v0UWDmkVERCS5RTqm56fArnASCefcDjPbBZzps2+9me0Fmpdx7lxgbsC+ET6Pl+G5dRXq3KnA1LLiE5GSdc7pXPw4/UhhDCMREYmeSJOe2kAtM6vnnDtQWkMzSwMaAodCHD4Y4euKSBVa9d364xuNNMNRRBJDpEnPt3h6e+4ExpfR9g48Kyp/W7TDzBoBDYA1Eb6uiFSlUbtjHYGISNRFOqbnJTwDkcea2XAzqxvYwMzqmtkwYCye8T8v+Rzu4f2+ujzBioiIiJRXpD09T+FZHfl8PAOV7zez5cBmPAlOOp5Byml4kqMl3nOK3OL9/q/yhywiksBSGjK+X5/izYYtWnLbMxqmKBINESU9zrlCM/slnqnnv8WT3FzA8VWazfv9GPA8cI93NlaR2/Dc9tpXkaBFJDpG/+ZG0g7v8d+Z0jA2wQgADepczZHazYq39/yo2mci0RJxwVHvAOY7zOwvQF/gLKCF9/CPwGfALOfc+hDnKtkRiSNph/dQp8k9fvvqFGyPUTQC8HSv0azqf3wNbBV8FYmecldZd85twLMAoYhUY74V1QFyT8/EswSWiEhiiXQgs4iIiEi1VO6eHhFJDJ6eneO2NoLMEtpK5Us/Uui3OOQttI1hNCKJpcSkx8yKVkDe5px7NmBfRLx1t0QkDvlWVAdVVY+1+Rs3+62TNH5un1Jai0gkSuvpGYVnVtbXwLMB+yKlpEdEJEzthr1T/Dg7hnGIJJrSkp7FeBKc9SH2iYhIJVk39vLix+P7PRfDSEQSS4lJj3OuVzj7REQkihq1gVGNfHacH7NQRBKNBjKLJDnfQbOgquoxNzRgRFU/jekRiRYlPSJJ5PGbbuJY4fFBsmZpfgvhAQG9DCIiiSMqSY+Z1QJ6AxnAIeAz59yH0XhuEYmeY4W7ufe1OcXbgdPVRUQSWalJj5k1AK72br7mnDsUok034A2gdcD+fwO/cs79N0qxioiIiJRbWT09FwHTgJXOuRcDD5pZS2Au0IzjxUaLnAPMBn5W8TBFpNIE3s5q1CY2cYiIVLKykp6iaQOvlHD8j0BzPNPYc4BJeCqo3wIMBc42s2ucc69XPFQRiQbfNWDmgd9CeCIiiayspOdneBKaf5Zw/Cbv8X8452712X+vmTUF+uOpxK6kRyRO+K4Bk/vW72MYiYhI1Sqr4OhJQCHwVeABM+sEtPRuPhXi3Ce9388sd3QiIiIiUVJW0nMCsMc5dyzEsaKxOoeBUDO1VuPpBUovf3giIiIi0VFW0lMDaFjCsbO933Odc4cDDzrnCoGdQN3yhyciIiISHWWN6dkKnGxmHZxzawOOnYunJ2dZKefXB/ZXID4RiTLftXm2NgKt1CMiyaKspOcz4GRgEJ6ZWgCYWUegq3fzg1AnmllboBawpsJRikjUZOblFj++Lqczq0ppK/HBd8YdQKvGdflo2IUxikak+ior6XkVuAoYambb8Ky70xoYj2ddnn3AP0o49+fe76srHqaISPLynXEHwUmQiISn1KTHOfd3MxuMJ4EZ6/0qPgw85pzbW8Lp/bxtVI5CRKS8UhoyPqDoaP+aDYDLQ7cXkRKFU3vrSuAlwPdT54DngdGhTvDe/urt3ZxbkQBFRJJZ49pXU1Cnmf/OnY/FJhiRaq7MpMc5txu4wsxO5fg4nmXOue9LOe0InmTpiHMuv8JRikjEeo59n027Dvrty45RLFJ+p+aOoGXAotlzu3SITTAi1VzYVdadc2sIc1Cyc24dsK58IYlINGzadTBoLMj4fs/ROadz8Xb6kcKqDksidNf/1mRVf//h5nMDbneJSHjCTnpEJDH4/QcaWGxU4k76UfwSVYBbaBujaESqNyU9IsnGN9FRRfW4N39A8KIC4+eqp0ekPJT0iCSou75/mfH9nvPbZ5amquoikrSU9IgkKDu2l3tfm+O3z3c1ZqneJtzxfvHjBk3rcPNfz4thNCLVg5IeEZFqaPDE4ysy+yZAIlIyJT0iItVM3cNH/BcsTGkIqCyFSFmU9IgkiMB1ebQmT+L6Re56fnnVo8Xb2d89V0prESmipEckQVy8ajINC32qwqQ0DBrDo6rqicN3DabAAesiEpqSHpEE0bBwL3Wa3FO8XadgO5l5r/i1UVV1EUlmSnpEEojv4Nbc0zPpnONfHk8rMItIMlPSI5LAAssXaAVmEUlmKbEOQERERKQqKOkRERGRpKDbWyKJLPB2lmptJYStjQDfmXldOsQsFpHqREmPSDU0+a4B7Plxq//OlIbBDVVnKyHd9b81/cZrze2nAqQi4VDSI1IN7flxa1BdrQl3vO+3Lo/W5Elc6Uehc07n4u1baBvDaESqDyU9IgkkMy+3+LHW5Elc8wf4v7Pj56qnRyQccZn0mFlv4EmgBvC8c25swPHawIvA2cB2oJ9zbp2ZtQNyga+9TZc65+6ossBFqlBgkck6Bdv9/vrXmjzJpd2wd/y2WzWuy0fDVI9LxFfcJT1mVgOYAGQBG4FlZjbbOfeVT7OBwE7n3Klmdj0wDujnPbbWOde1KmMWiYVHGh/025731ggGTjve06M1eZJISsOg+lt7ajZARUhF/MVd0gP8DFjjnMsHMLMZwJWAb9JzJTDK+/h14Bkzs6oMUiTWfGsvAeS+9Xv/REcztZJGnUa/9VuNG/Cvwi4iQHwmPa2ADT7bG4FzSmrjnCs0s91AM++x9mb2H2AP8KBzbkklxysSPzRbKynVKdgedLtTRILFY9JTET8AbZxz283sbOAtM+vknNvj28jMBgGDANq00V/DIlK9nbd0hN8gdoDx/R6LUTQi8Ssek55NwMk+2629+0K12WhmNYFGwHbnnAMOATjnVpjZWuA0YLnvyc65ScAkgG7durnKuAiRaOk59n027fIfv5Mdo1hERKqzeEx6lgEdzaw9nuTmeuDGgDazgf7AJ8A1wPvOOWdmLYAdzrmjZnYK0BHIr7rQRaLv4lWTaVi4139nqIUIRXylNPQb19OwRUtue2ZqDAMSib24S3q8Y3TuAubjmbI+1Tn3pZmNBpY752YDU4CXzGwNsANPYgTwc2C0mR0BjgF3OOd2VP1ViERPw8K91Glyj9++Okd2+S1ECFqMUPwdajyARu54ecU9P+p2l0jcJT0Azrm5wNyAfSN8HhcA14Y47w3gjUoPUKSKBc7MyT09M2gMhxYjFF/vtE3xuy2avTOGwYjEibhMekSkbL4LEYIWIxR/gQsTju/3XAktRZKHkh6Rasq34CSgxQhFRMqgpEekugpMcrQYYdLa2ggIGOOVmp7Oqe+/F5uAROKUkh6R6koLEYrXXf9bM6jnL3Cgu4hAStlNRERERKo/9fSIxJnAxQizCf6rXdPTxVf6kcKgge0zYxSLSDxT0iMSZzbtOuhXTHR8v+e4brj/RzX9SCEXVHVgErfmb9wcdLvzg2cz/cf5dOlQxVGJxB8lPSLVgGZqSakatQn6N3HX/7bx+3czV1XXRZT0iMSb/hte9l9TRSUnpCxDQyxLGXC7K7AsBag0hSQfJT0iMRSymGjhXu59bU7x9oQ73q/qsCQBBI7zubv2CArqNPNro9IUkmyU9IjEUOD4HfCM4fFNdOoUbNeaPBKxwHE+uQ8Fly4Z309JjyQXJT0iVSiwZ2fApunB5QFSGvrV2so9PROmaU0eEZGKUtIjUoVCzcwKrKDeoGmdqg5LElHQ4Ob0mIUiEi+U9IjEWGAFdZGoCBzcPEMrO4ko6RGpQpqZJSISO0p6RKpQwzBmZq258CKObN5cvK3Vl0VEokNJj0gV8010Qo3fObJ5s98sm+tyOhNiFRaRqGg37B2/7VaN6/LRMN1ylcSkpEekkoRcgweN4ZHY2NoI/7IUAF06BC2ZEJgEiSQSJT0ilaSkNXjC4buoXPqRwqjGJcnprv+tGVTORKUpJNko6RGpJEGDliHsgct+/zmpzpaISFQo6RGJksDbWdmFe7UGj8SNwLIUALekdA6qx9W/ZgPAv4dSJFEo6RGJkotXTaZh4d7jOwJWVo6Ib++OSk5IFASWpQCYsvDvQfW42KnSFJK4lPSIREk409HDNkplJ6Tynbd0RMh6XL6DmTWbSxKJkh6RKCprOrpIzASVpYCSSlP4DsDXbC5JJEp6RMqhvNPRAxceDEWLEUqlCCxLAaFLU6Q09BvnozE+kkiU9IiUQ3mnowcuPBiKFiOUWGrR/i727ig4vkNjfCSBKOkRqWJ+a/AchfkDlOJI/Lj5r+f5bQeO8QGN85HqS0mPSBkm3zWAPT9u9duXTYienXKswXPp85lB04i1GKHEG63aLIlCSY9IGfb8uJWn29/pty/UX7rlma0VahqxFiOUqhKqNEVqejqnvv/e8R0BY3wABtRqSLthx7fV8yPVhZIekTAE/qX74v0fByU55ZqtFWpGjdblkSry19tgc6r/fwMzH/IfaB80xgdI2TuF7O+O93TuqdkAUNIj8U9Jj0iA0b+5kbTDe4q399cKvm21d0dB0EytNRdeRO7px//DCPqLOZRQM2pEqkionsbch/x7fgLH+AC8eH8dDXaWaklJjyS1wAQHgFoN/RYZDLdXJ3BmVm5gRWuReBPB2j2+Qg12FqkOlPRIUks7vMcvwQklVK9OSXwHJc+sUGQiVSDctXvCoBleUh0o6ZGkV9YA5EjG6qz6bn3x4w8apQcNEtXCgxLvAgc3h3ObNqVmI78xPgAuReN8JP4o6ZGkElQJnbJXUY6Iz/iIu9p39pueDlp4UOLfXf9b0+/fbTi3aYdOnx60L3DGl0g8UNIjSSVwJeVwVlEur/QjhVqDR5KaCpdKvFHSI1JJtAaPVEeByfrExjWCenvCmpmICpdK/FHSIwkjVBHQQK0a143Ka4UqHBo0Xkdr8Eg1FJisd6Yzq/r714sLa2aiCpdKHFLSI3EvVDITqqs8VBHQyhKqcGjQeB2twSPVUUCynt6mTdBt2sDen1A9P6EKl2qGl8Sakh6Je6GSmZ5j3w/5C7Qsj990E8cKj/8Vm1Kz/LebNF5HElJAsj5/VKOg27SXpl3K5v3HezoDV3GG0Gv5hPs5ViIklUVJj8RUOLekBmyaHjTg+JYWLbntmal++ybfNaDsRdJS/Bce9KyiXPp4hY96nEHTXUf92uxo6D89HdCtK0ka86+Z77cduIpzKCk1GwXN6LqmZqOgmV8a+yOVSUmPxFQ4t6TG93uOOk3u8du3b+eUoF+gDVu0LHOhwcA1eY5s3sx1w/0/BhOf28IRn0SosBFBt7II8devSEIqaWyaT49QOIVLw53W3qpxXc36kkqjpEdiqv+Gl4N6cRqG6MUJXEsnqPYPcLiwfAsNBq6lEzhwU2vrSFILNTbt8c5+idBd/9sm6HNU3jIsgQmOen4kmpT0SKUJ59ZVduHeoN4Zz20qn78AU4ILfoYqglhugX/Ftg9xm0qzsESOC0iE0p/PDBrjFlYZloAZXqEMqNWQdsNKfxr1Bkm4lPRIkMBkJdQvlFAJTf8NL9OwcG/x9qW1GjLipVdKfa1QiwPWbvhb6hQe78WJpAxEuQTcpkqf2tnvF3j60eA2InJcONXaQwma4RXKzuAB0IE0IFrCZc65WMcQU926dXPLly+PdRgxEyp5GbBpul/l8f21GjK11U1+bUL9Qhnfr49fr83kuwaw58etpb5+Ss1G1Gow0G9fg6Z1otuT4yNwfZ2tjeCCf+eWcoaIlOnxzrDbf2D/B/PSaVnG3wrhLHIY+HslXOH88SaJy8xWOOe6Be1X0pM4SU95emiilbxAcAITKnl58f6P/f6yCyfBCTl7qnENei5dXep5oRYQ3NG4BnfcacXb6UcKmf9bJT0i0Xbp85lsTi39ZsLMhwqDJwkECFxmIlyBYwPDud0OSo4SRbVKesysN/AkUAN43jk3NuB4beBF4GxgO9DPObfOe2w4MBA4CgxxzvnPrQxQmUlPuCsEh3PrKByBzzX6Nzf69diAp9fG95ZTqIQm1EDicEy4432/AceBCQ6Urxcn9/RMMq/3T14+mptO0z0lnOC1oyHcMdj/l276kUJPV3yRgFkoIhIlgb0/IT5ruadnlpn0hBLqd0uggp2PqYcoiZWU9MTdmB4zqwFMALKAjcAyM5vtnPvKp9lAYKdz7lQzux4YB/Qzs58A1wOdgHRggZmd5pzz7yaoIuFMxy7pXnQ0VhZOO7wn6EMfOGhwz49bg6aDhzMLKpTAsTcd3h8UsocGjvfQhN2LEzBeoGej4O70zu39Z5B0zgmuci4iVSTwj4mAGV8e6UGnheqhDXReOLfFbni+XJXeA9cAC/U7OpASo+oj7pIe4GfAGudcPoCZzQCuBHyTniuBUd7HrwPPmJl5989wzh0CvjOzNd7n+6SKYvcTznTsWza+HLKnBfw/QOHeYgp+nuB9gTOjAqeDX/q6/2qr6WnpQYuRhfLRzzLJneX7YgT10OTO8P8l13TX0aC/9D7qcYbfdNcdwZO3QvbOBM4g0QrJInEkxGd2x9zMoKntWxvBXcPLui3m/3slVKLUuOdfKEhtHHGYe370X+D0mlVTgm6vpQQsqhhOYlRe5R2mIKHF3e0tM7sG6O2c+613+zfAOc65u3zarPa22ejdXgucgycRWuqce9m7fwowzzn3ekmvV5m3t8b36xPUi1Kw+3k45nNfJqUhdRr91q/N4b3BHzKz+tRuPCjqMaYe2s75n4yIynOFMyj4o59l+t2W2tEQel5W+l91Yd+CCqM7XUTiSIgB0OF8bj84J9NvkHSoW9nhjBcKlSzN69oF5/YVb6ekpDE0459+bcZ/fbn/7/FyCmcowaM33IgFvJZLacjvXz0+TKG8QyICBc7ABdhTswE5J/864ueKdSJWbcb0VEXSY2aDgKIMIgP4upIupzmwrZKeO17oGhNDMlwjJMd16hoTg66xYto651oE7ozH21ubgJN9tlt794Vqs9HMagKN8AxoDudcnHOTgElRjDkkM1seKtNMJLrGxJAM1wjJcZ26xsSga6wcKVX5YmFaBnQ0s/ZmVgvPwOTZAW1mA/29j68B3neeLqvZwPVmVtvM2gMdgU+rKG4RERGJY3HX0+OcKzSzu4D5eKasT3XOfWlmo4HlzrnZwBTgJe9A5R14EiO87WbiGfRcCAyO1cwtERERiS9xl/QAOOfmAnMD9o3weVwAXFvCuX8B/lKpAYav0m+hxQFdY2JIhmuE5LhOXWNi0DVWgrgbyCwiIiJSGeJxTI+IiIhI1CnpCZOZTTWzrd7p8kX7rjWzL83smJl189nfzswOmtlK79fEEp6zqZn9y8y+9X5vUhXXUpIIr/Emn+tb6T3eNcRzjjKzTT7tLquiywmphGt8xMzyzOwLM3vTzBr7HBtuZmvM7Gszu7SE52xvZv/2tnvNOwA/ZiK5RjPLMrMVZrbK+z3kwhrV+X1MsM9jSddYLT+P3phCXeefvde40szeNbN0734zs6e8n7UvzOysEp7zbO+/6TXe9haqXVWJ8Bpv8u5fZWYfm1mXEp5zmpl95/Nedq2iywkpwmvsZWa7fWIPuVhcpfxudc7pK4wv4OfAWcBqn32ZeNb5WQR089nfzrddKc/5MDDM+3gYMK66XGPAeZ2BtSUcGwX8PtbvXxnXeAlQ0/t4XNH7APwE+ByoDbQH1gI1QjznTOB67+OJwJ3V6BrPBNK9j88ANiXg+5hIn8eQ1xhwXrX5PJZynQ19Hg8BJnofXwbMAwzoAfy7hOf81HvcvO1/WY2u8TygiffxL0u5xmnANbF+/8p5jb2AOWE8Z9R/t6qnJ0zOucV4Zor57st1zlVkYcMrgRzv4xzgqgo8V4VV4BpvAGZUWmBRVMI1vuucK6pZsRTP+k7gU9bEOfcdUFTWpJj3L8gL8ZRDgfh9H0Neo3PuP865oiVpvwTqmqegb1yL8H0MV3X4PIZzjdXm8wglXqfvEsRpQNHg0yuBF53HUqCxmZ3ke653u6Fzbqnz/G/5IvH5Xoa8Rufcx865nd795fl3HBMRvo9lqqzfrUp6Kk97M/uPmX1gZueX0OYE59wP3sf/BU6ootiirR/wainH7/J2cU6N9S2DMAzA85chQCtgg8+xjd59vpoBu3z+IwrVJt74XqOvvsBnzlO7LpTq+j5CYn4eS3ofE+LzaGZ/MbMNwE1A0e2PcD6Trbz7S2sTF0q4Rl8DCf0eF/mL9718PF7/WCnlGs81s8/NbJ6ZdQpxaqX8blXSUzl+ANo4584E7gFeMbNQZTOLef8iqXZT6czsHOCAc251CU2eAzoAXfH8XMZXUWgRM7MH8KzvNL2sttVVSdfo/aUzDri9hFOr8/uYcJ/HUt7HhPk8OucecM6djOca7yqrfXVU2jWa2S/wJD1/LOH04cDpQHegaSntYqqEa/wMT5mILsDTwFtVFY+SnkrgvR2y3ft4BZ6xIKeFaLqlqGvW+z2yMurx4XpK+avSObfFOXfUOXcMmEzA7aF4YWa3AH2Am7z/4UF4ZU224+lir1lKm7hQwjViZq2BN4GbnXNrQ51bnd/HRPs8lvQ+eiXE5zHAdDy9kBB+maLWZbSJN77XiJn9FHgeuLLo324g59wP3tt8h4AXiP/3svganXN7nLeqq/Osy5dqZs0D2lfK71YlPZXAzFqYWQ3v41PwlMPID9HUt5xGf+DtqokwOswsBbiOUsYPBNxvvxoo6S/QmDGz3sAfgCuccwd8DpVZ1sT7n85CPOVQIE7fx5Ku0Tyzf97BM4D3o1LOr7bvYyJ9Hkv5t5own0cAM+vos3klkOd9PBu42Tx6ALt9bkkCnmQA2GNmPbzjQm4mPt/LkNdoZm2AWcBvnHPflHJ+UYJueMa6xN17Wco1nuiNGzP7GZ5cxC+5q7TfrRUdCZ0sX3j+evoBOILn3uJAPL80NgKHgC3AfG/bvngGha7E0433/3ye53m8s6Dw3LN8D/gWWAA0rS7X6G3fC09V+8Dn8b3Gl4BVwBd4fmGdFIfXuAbPOIGV3q+JPu0fwNMz8DU+M0DwrBheNOvpFDzJ0Brg70Dt6nKNwIPAfp/9K4GWifQ+JtjnsbR/q9Xu81jKdb6B5z/xL4B/AK28bQ2Y4P1MrsJ/1uxKn8fdvOevBZ7BuxBvNbnG54GdPu/xcp/n8f298773Z7AaeBmoX42u8S7vZ/JzPIO1zyvhGqP+u1UrMouIiEhS0O0tERERSQpKekRERCQpKOkRERGRpKCkR0RERJKCkh4RERFJCkp6REREJCko6REREZGkoKRHROKWmT1qZs7M3vFu32hm75nZdjPbY2aLvTWKitqnmtntZvaxme0ys31mNt+7rL+IJDktTigiccvM3gMuBJ7EUyizD3AYz6qvad5mh4GeeCqjz8JTgPEgntV763jbbAU6Ouf2VFnwIhJ31NMjIvGsq/d7f+CneMqi1HfO1Qd6AwVALTwVp//pffwLPAlRGnCr9/yWHK+rJSJJSkmPiMQlM2sLNPVuHgbOdc695Zw7AuCcm4+n5hDAr/D07PR0zi1yHsecc9OAxd42p1dd9CISj5T0iEi8OtPn8W3Ouc0h2mz0fndAP+fc/hBtNnm/p0YzOBGpfpT0iEi8Kkp6NgNzSmjTzvv9Y+fc6hLatPd+Xx+luESkmlLSIyLxqijpecc5d6yENl29398OddDMUoDO3s0voheaiFRHSnpEJF519X5fFuqgmdUCOnk3l5fwHB05Psvrs6hFJiLVkpIeEYk7ZtYMONm7WVKy0hnPOB1XSpui3qKtzrmi8T+Y2Tgz22tm/xfitVPM7HMzW21mj4TZrka41yYisaOkR0TiUVGycgRYVUKbs7zf1zrndpfxPP8J2D8X+BIYYWYNA44NxDM9/h48Y4nKbOecO1rShYhI/FDSIyLxqChZ+dI5d7iMNqXdtuoaqo1z7gPgIaAmkFm038waAH8G5jrn3g23XTgXJCKxp6RHROJROAlNUU/PinI+T573e4bPvuFAM+D35WgnInFOSY+IxKNSkx7vGJqielohkx4zawW0KOV58oFCvMmMdzHEocDfnHO55WgnInFOSY+IxBUzqwec5t0sqafndKBuGW2KEqddzrn8wIPelZ3XcbwHZxyeshajytNOROJfzVgHICLiyzl3ACh1NpRz7ks8ZSdKazOnrDbAN0CGmZ0L9APudc5tq0A7EYlj6ukRkWT2LZ61fB4H1gDPVLCdiMQx9fSISDL7BqgNnAP8qpSZYuG2E5E4pp4eEUlm33i/f+CcezMK7UQkjinpEZFkVtRj83SU2olIHFPSIyLJrKv3++dRaicicUxJj4gksy7APmBtlNqJSBxT0iMiyawLsNo556LUTkTimOkzLCIiIslAPT0iIiKSFJT0iIiISFJQ0iMiIiJJQUmPiIiIJAUlPSIiIpIUlPSIiIhIUlDSIyIiIklBSY+IiIgkBSU9IiIikhT+P/VprNnZnvKfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bkg_test_mjj = bgd_x_all[:, 19]\n",
    "bkg_test_mgg = bgd_x_all[:, 20]\n",
    "sig_test_mjj = sig_x_all[:, 19]\n",
    "sig_test_mgg = sig_x_all[:, 20]\n",
    "print(bkg_test_mjj.shape)\n",
    "print(bgd_y.shape)\n",
    "print(thresholds.shape)\n",
    "print(len(TPR_thresholds))\n",
    "\n",
    "fontsize=25\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(bkg_aux_frame[b'dibjet_mass'], bins=100, histtype='step', label='No cut', density=True, range=(20, 250))\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(TPR_threshold)\n",
    "    #print(thres_idx)\n",
    "    #thres = thresholds[thres_idx]\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(bkg_test_mjj[bgd_y > TPR_threshold], \n",
    "             bins=100, histtype='step', \n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True, range=(20, 250))\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Background events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{bb}$',fontsize=fontsize)\n",
    "plt.savefig(fname = f'Mass_Sculpt_Plts/ttH_score/{names[i]}_bgd_mjj_mass_sculpt')\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(sig_aux_frame[b'dibjet_mass'], bins=100, histtype='step', label='No cut', density=True, range=(20, 250))\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(sig_test_mjj[sig_y > TPR_threshold], \n",
    "             bins=100, histtype='step', \n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True, range = (20, 250))\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Signal events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{bb}$',fontsize=fontsize)\n",
    "plt.savefig(fname = f'Mass_Sculpt_Plts/ttH_score/{names[i]}_sig_mjj_mass_sculpt')\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(bkg_aux_frame[b'diphoton_mass'], bins=100, range=(115,135), histtype='step', label='No cut', density=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(bkg_test_mgg[bgd_y > TPR_threshold], \n",
    "             bins=100, histtype='step', \n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True, range = (115, 135))\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Background events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{\\gamma \\gamma}$',fontsize=fontsize)\n",
    "plt.savefig(fname = f'Mass_Sculpt_Plts/ttH_score/{names[i]}_bgd_mgg_mass_sculpt')\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(sig_aux_frame[b'diphoton_mass'], bins=100, range=(115,135), histtype='step', label='No cut', density=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(sig_test_mgg[sig_y > TPR_threshold], \n",
    "             bins=100, histtype='step', \n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True, range=(115, 135))\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Signal events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{\\gamma \\gamma}$',fontsize=fontsize)\n",
    "plt.savefig(fname = f'Mass_Sculpt_Plts/ttH_score/{names[i]}_sig_mgg_mass_sculpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDT benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReferenceError",
     "evalue": "attempt to access a null-pointer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReferenceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-5716bc0839b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbdtfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TMVA.root\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbdttree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdtfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TestTree\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m bdtscore = tree2array(bdttree,\n\u001b[1;32m      4\u001b[0m                         branches = ['classID','BDT'])\n\u001b[1;32m      5\u001b[0m \u001b[0mbdtframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdtscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReferenceError\u001b[0m: attempt to access a null-pointer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in <TFile::TFile>: file TMVA.root does not exist\n"
     ]
    }
   ],
   "source": [
    "bdtfile = rt.TFile.Open(\"TMVA.root\")\n",
    "bdttree = bdtfile.Get(\"TestTree\")\n",
    "bdtscore = tree2array(bdttree,\n",
    "                        branches = ['classID','BDT'])\n",
    "bdtframe = pd.DataFrame.from_records(bdtscore)\n",
    "bdtarray = bdtframe.values\n",
    "\n",
    "bdt_pred = bdtarray[:,1]\n",
    "bdt_target = np.zeros(shape=len(bdt_pred))\n",
    "bdt_target[bdtarray[:,0]==0] = 1 # Signal\n",
    "bdt_target[bdtarray[:,0]==1] = -1 # Background\n",
    "\n",
    "fpr_bdt, tpr_bdt, thresholds_bdt = roc_curve(bdt_target, bdt_pred)\n",
    "area_bdt = auc(fpr_bdt, tpr_bdt)\n",
    "\n",
    "with h5py.File(\"BDT_ROC.h5\",\"w\") as out:\n",
    "    out['FPR'] = fpr_bdt\n",
    "    out['TPR'] = tpr_bdt\n",
    "    out['Thresholds'] = thresholds_bdt\n",
    "    print(\"Saved ROC.\")\n",
    "\n",
    "TPR_thresholds = [0.96, 0.94, 0.935, 0.9, 0.7, 0.5, 0.3]\n",
    "print(\"BDT performance\")\n",
    "print(\"Threshold \\tSignal Efficiency \\tBackground contamination\")\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_bdt_idx = np.argmax(tpr_bdt>TPR_threshold)\n",
    "    print(\"{0:.4}  \\t   {1:.4}   \\t\\t  {2:.4}\".format(thresholds_bdt[thres_bdt_idx], tpr_bdt[thres_bdt_idx],  fpr_bdt[thres_bdt_idx]))\n",
    "\n",
    "\n",
    "# TPR_threshold = 0.96\n",
    "# thres_bdt_idx = np.argmax(tpr_bdt>TPR_threshold)\n",
    "# print(\"Signal efficiency = {} @ {} ttH background contamination\".format(tpr_bdt[thres_bdt_idx], fpr_bdt[thres_bdt_idx]))\n",
    "# print(\"NN score threshold = {}\".format(thresholds_bdt[thres_bdt_idx]))\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(fpr_bdt,tpr_bdt,label=\"AUC = {}\".format(area_bdt))\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "plt.axhline(tpr_bdt[thres_bdt_idx],ls='--',color='tab:gray')\n",
    "plt.axvline(fpr_bdt[thres_bdt_idx],ls='--',color='tab:gray')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(bdt_pred[bdt_target==-1], bins=60, label='ttH background',alpha=0.5, normed=True)\n",
    "plt.hist(bdt_pred[bdt_target==1], bins=60, label='HH signal', alpha=0.5, normed=True)\n",
    "#plt.axvline(thresholds_bdt[thres_bdt_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f34ec877210>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34ec837290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Compare\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(fpr,tpr,label=\"NN AUC = {}\".format(area))\n",
    "plt.plot(fpr_bdt,tpr_bdt,label=\"BDT AUC = {}\".format(area_bdt))\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "thres_idx = np.argmax(tpr>0.96)\n",
    "# plt.xlim(0.01,0.6)\n",
    "# plt.ylim(0.2,1)\n",
    "plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.legend(loc='best',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show variables used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d2208fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34ec766790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36319c99d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d21e6f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d1f57e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d197b650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3502187b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d1bfea50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEKCAYAAAD0Luk/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt4VOW5/vHvYzifEdCNBBusQA0EggRQkNp6oCIIVOPPYK1EQSvKtuKvaNhSRey2YNnVUqCKihsP1Kotiornw25B5CgKEdlGoBJFBZSTHIPP/mNWpsMQyEAmycC6P9eVi7Xe9a41zwyTubNO75i7IyIi4XRcdRcgIiLVRyEgIhJiCgERkRBTCIiIhJhCQEQkxBQCIiIhphAQEQkxhYCISIgpBEREQqxGdRcQr3nz5p6RkVHdZYiIHFWWLFmy0d1bHO56KRcCGRkZLF68uLrLEBE5qpjZP49kPR0OEhEJsYRCwMwuMLNVZlZkZgVlLP+hmS01sxIzy41pzzaz+WZWaGYfmNllySxeREQqptwQMLM0YArQF8gEBptZZly3T4F8YGZc+w7gSnfvAFwA3GdmTSpatIiIJEci5wS6A0XuvhrAzJ4EBgIflnZw97XBsu9iV3T3/42Z/tzMvgJaAJsrXLlIJdi7dy/FxcXs2rWruksRKVOdOnVIT0+nZs2aSdleIiHQClgXM18M9DjcBzKz7kAt4JPDXVekqhQXF9OwYUMyMjIws+ouR2Q/7s6mTZsoLi6mTZs2SdlmlZwYNrOWwGPAVe7+XRnLrzWzxWa2eMOGDVVRkkiZdu3aRbNmzRQAkpLMjGbNmiV1TzWREPgMaB0znx60JcTMGgEvAre5+7tl9XH3ae6e4+45LVoc9mWuIkmlAJBUluz3ZyIhsAhoa2ZtzKwWkAfMTmTjQf9ZwKPu/syRlykiIpWh3HMC7l5iZiOAV4A0YLq7F5rZOGCxu882s25EPuybAheZ2Z3BFUH/D/gh0MzM8oNN5rv7ssp4MiLJ1mv8m3y2eWfStteqSV3mFZxzyD5ffvklI0eO5N1336Vp06bUqlWLW265hZ/+9Ke8/fbbTJw4kRdeeKHStrFp0ybOPfdcAL744gvS0tIo3UNfuHAhtWrVOmCdkpISmjdvzubN/7rm46GHHmLFihXcd999jBkzhubNm3PTTTdFl6enp7N06VL69OlT5mMtWbKEtLS0Ax5rzZo15Ofns2HDBsyMESNGMHz4cADy8vJYuHAhjRo1YteuXQwZMoTRo0cDkJuby/Lly6lZsya9evViypQp1KgR+Qh8/vnnufPOO/n222+pXbs2mZmZTJw4kZNOOmm/xy4sLGT48OFs3bqV3bt3c+655zJ58mTeeecdZs2axe9+97tD/r8croKCAtLT0xkxYkRStxsroTuG3X0OMCeu7faY6UVEDhPFr/c48HgFazws8b+0ifzSiRzMZ5t3snZ8v6RtL6PgxUMud3cGDRrEkCFDmDkzcsX1P//5T2bPTmjnOynbaNasGcuWRf5OGzt2LA0aNOBXv/pVwo9/OGrVqhV9rLKCoiw1a9Zk0qRJdO7cmc2bN9OlSxf69OnD97//fQAmTZpE//792bFjB+3atSM/P5+WLVty9dVX07dvX9yd3NxcHnvsMa666iqWLl3KqFGjeP7552nbti3uzqxZs/j0008PCIHrr7+e2267jZ/85Ce4OytWrACgZ8+e9OzZsxJeocp3zN0xXPpLW/qTzL/iRCrbm2++Sa1atbjuuuuibd/73vf493//9wP6jh07lokTJ0bnO3bsyNq1a5OyjUO555576NixIx07duSPf/zj4Ty9w/bb3/42+lhTp04FInsQnTt3BqBJkya0a9eOzz///IB1d+7cSVpaGnXr1gXgwgsvxMw47rjj6NatG8XFxdHHuOOOO2jbti0QOeZ+8cUXc8YZZxywzfXr15Oenh7tl5WVBcDLL79Mbm7kPtkvvviCH//4x3Ts2JHrr7+ef/u3f2P79u189NFHdO7cmauuuorMzEz69evHnj17AJgyZQrdunWjU6dOXHbZZVV6ifIxFwIiR7PCwkJOP/30at/GwSxYsIAnnniCRYsWMX/+fKZOncry5csB2LZtG9nZ2dGfcePG7bfu7373u/2Wf/XVV4d8rHnz5vH000+zaNEi3nnnHf7whz/w4Ycf7tfn448/5qOPPqJr167RthtvvJHs7GxOPvlkhg4dSpMm+9+funv3bmbOnMkFF1wAHN7rdfPNN9OzZ0/69evHpEmT2Lp16wF9xowZw4ABA1ixYgV9+vThyy+/jC5buXIlo0aN4sMPPyQtLS26d5aXl8eiRYv44IMPaN26NY899lhC9SSDQkAkhd1www107tyZbt26Ves2Ss2dO5dLLrmEunXr0rBhQwYNGsQ//vEPABo2bMiyZcuiP7fffvt+644aNWq/5SeccEK5j5Wbm0vdunVp1KgRAwYMYO7cudHlW7ZsITc3l6lTp1KvXr1o+6RJk1i2bBnr16/n2WefZcmSJftt95prrqFfv35lvh6ff/452dnZtG3blsmTJx+w/LrrrqOwsJCLL76YV155hV69elFSUnJA3Xl5eQAMGjRov9rat29PZmZkwIWuXbtG97qWLVvGWWedRVZWFk8//TSFhYWHfG2SSSEgkkI6dOjA0qVLo/NTpkzhjTfeoKz7Z2rUqMF33/3rtpvSQwjJ2Eaq2717N4MGDeLaa6+lX7+yz9k0atSI3r17M2/evGjb6NGj2b17N7/97W+jbbGv10knncSyZcsYMmQI27dvL3O76enpDB06lBdffJGdO3eyatWqhOuuXbt2dDotLS0aIFdeeSUPPvggy5cvZ/To0TocJBJW55xzDrt27eJPf/pTtG3Hjh1l9s3IyIh+eC1dupQ1a9YkbRsH07t3b2bNmsXOnTvZvn07zz33HL179078CR6G3r1789e//pVdu3axbds2nn/+ec466yzcnZ///Of06NGDG2644aDr7927l4ULF0ZPGE+ePJn58+fz6KOP7net/a233srYsWP5+OOPo20He71eeuml6Ad3cXExW7dupWXLlvv16dWrF0899RQAs2fPPui2Srk7O3bs4MQTT2TPnj3Rk/lVJeW+T0AklbRqUrfcK3oOd3uHYmY8++yzjBw5knvuuYcWLVpQv359JkyYcEDfSy65hEcffZQOHTrQo0cP2rVrl7RtHEz37t0ZPHhw9FDK8OHDycrKOuCQSDL07NmT3NxccnJygMix/szMTF5//XWefvppOnXqxMsvvwzAxIkTOe+886L9xowZw+7du7nwwgujJ2B/+ctf0qZNG3r0iIx6k5eXR0FBATk5OUyYMIHLLruMHTt20KxZM9q0aXPAOQ2AF198kZtuuok6depgZvzxj3/k+OOP36/PXXfdxeWXX86DDz7I2WefTYsWLaInp8tiZowdO5auXbtywgkn0LVr1/32ziqbuXuVPVgicnJyvCJfKpNR8OJ+l/TFz4scysqVKznttNOquww5iu3atYuaNWuSlpbG22+/za233sqCBQuS+hhlvU/NbIm75xzutrQnICKSRJ988glXXHEF+/bto06dOjzwwAPVXdIhKQRERJKoQ4cOvPfee9VdRsJ0YlhEJMQUAiIiIaYQEBEJMYWAiEiI6cSwyKHcmwVbPk3e9hqfDCOXH3Tx5s2bmTlzJtdffz0Aa9eu5Z133uHyyy8HKHMY6Pz8fPr3788TTzzBmjVr2L59Oxs2bIh+/eDUqVMPOsLl3r17+fWvf80zzzxDgwYNMDMuu+wyCgoKgMiAaSNHjmThwoU0bdqU2rVrU1BQwIABA/bbTlFREVlZWbRv3x53p0GDBvz3f/93dFC2w3HWWWcxefJksrOzD3vdylRUVERubm501NNjhUJA5FC2fApjtyRve2MbH3Lx5s2bmTp16n4hMHPmzGgIHMqsWbOAsoPiYEaPHs0333xDYWEhtWvXZtu2bfz+978HIneyDhw4kGuuuYYnn3wSiIzlP2fOnDK31b59++gH5JQpUxg/fjwPP/xwuTUkU0lJSfQ7AiQxOhwkkkIKCgr45JNPyM7OZtSoURQUFPCPf/yD7Oxs7r333iPe7quvvkp2djZZWVlcc8017Nmzh23btjFjxgwmTZoUHdOmYcOG3HHHHdF1GjZsyDXXXBPdTps2bQ45VEOprVu30rRpUyBy3Xzv3r3p0qULXbt23e/GqbvvvpusrCw6d+7Mbbfdtt829u3bxxVXXMHYsWMBeOCBB2jXrh09evRg2LBh0e8duOKKKxg+fDjdu3fnP/7jP9i4cSMDBgygU6dO9OzZMzrm/5gxY7jvvvui2//BD35AcXExRUVFdOzYkaFDh9KhQwf69u0bHbtn0aJFdOrUiezsbO6///7Des2PFopMkRQyfvx4VqxYEf2LOv6v+rfffjsaCqU+/fRT+vfvf9Bt7tixg6uvvpr/+Z//4fvf/z4/+9nPmDZtGj179iQjI4P69euXud7hDkm9atUqsrOzo9+6Vfph37JlS1577TXq1KnDRx99xJAhQ1iwYAHPP/88L730EgsXLqRu3bp8/fXX0W3t3buXwYMH07VrV2699VbWrVvH+PHjWbp0KfXr1+dHP/oR3bt3j/Zfv3497777LscddxzDhw+nR48ezJ49m1dffZX8/HzKG4Vg1apV/PnPfyYrK4uLL76YZ599lry8PPLz85k2bRq9evVi5MiRCb8WRxPtCYgcZXr37r3fkMzxx+fjrVy5knbt2kUHUrvyyiv5+9//fkC/hx56iOzsbNLT01m/fv0By6+77jo6derEmWeeWebjlB4OWr16Nffcc0/0S212797N0KFD6dixI3l5edHvBHj99de5+uqro+PqxI7BM2zYsGgAQOR7DM4555zoV2WWfoFLqUsvvZTjjot8nM2dO5ef//znAPTp04fPP/+cb7/99pCv0amnnhr9gpjSIZ43btzIzp076dWrF0B0m8cahYBISLVt25Y1a9ZEPyCHDRvGsmXLaNCgAfv27TtgSOr777+fV199tcwhqeMNGDAgGjT/9V//RevWrVm+fDkLFy5k9+7d5a7fs2dP3njjjYT6Agfdm4l1qGGzDzbEcxgoBERSSMOGDdm2bdtB54/Eaaedxscff8zq1asBePzxxzn77LNp2LAhV155JTfeeGP0w7akpIS9e/cCkb+it2zZwoMPPhjdVnnDIpeaO3dudM9jy5YttGzZEjNjxowZlA5aef755zN9+nR27ox8BWzs4aBf/OIXnHfeeeTl5VFSUkL37t1566232Lx5M3v37uVvf/vbQR+7d+/ePPHEE0Bkb6NVq1bUr1+fjIyM6BfMLFy4kHXr1h3yOTRv3py6desyf/58gOg2jzU6JyByKI1PLveKnsPe3iE0a9aMXr160bFjR/r27cvdd99NWloanTt3Jj8/ny5duhz2Q9arV4+HH36Yiy++mH379tGjR4/oyd7x48czZswYMjMzadSoEfXq1WPYsGGceOKJmBnPPfccI0eO5O677+aEE06gXr16jB8/vszHKT0n4O7Url2badOmATBixAhyc3OZPn06/fr1i/7V3b9/f95//31ycnKoWbMmF110EXfddVd0e7fccgu33XYb+fn5PProo4waNYpu3bpx/PHH0759exo3Lvv/Zdy4cVx99dV06tSJBg0a8MgjjwCRQ0aPP/44HTt25IwzzuCUU04p97V75JFHGDZsGMcddxznn39+4i/6UURDSYvE0FDSqWv79u00aNCAvXv3MnDgQIYPH85FF11U3WVVi2QOJa3DQSJyVPj1r39Nly5d6NSpE+3btz/kFVGSOB0OEpGjQkXuk5CD056ASJxUO0QqEivZ70+FgEiMOnXqsGnTJgWBpCR3Z9OmTdSpUydp29ThIJEY6enpFBcXJ3QtvEh1qFOnDunp6UnbXkIhYGYXAH8A0oCH3H183PIfAvcBnYA8d38mZtkQYEww+xt3n5GMwkUqQ82aNaOjb4qEQbmHg8wsDZgC9AUygcFmlhnX7VMgH5gZt+7xwB1AD6A7cIeZNa142SIikgyJnBPoDhS5+2p33wM8CQyM7eDua939A+C7uHV/Arzm7l+7+zfAa8AFSahbRESSIJEQaAXE3l9dHLQlIqF1zexaM1tsZot1LFZEpOqkxNVB7j7N3XPcPadFixbVXY6ISGgkEgKfAa1j5tODtkRUZF0REalkiYTAIqCtmbUxs1pAHjA7we2/AvQxs6bBCeE+QZuIiKSAckPA3UuAEUQ+vFcCT7l7oZmNM7MBAGbWzcyKgUuBB8ysMFj3a+AuIkGyCBgXtImISApI6D4Bd58DzIlruz1mehGRQz1lrTsdmF6BGkVEpJKkxIlhERGpHgoBEZEQUwiIiISYQkBEJMQUAiIiIaYQEBEJMYWAiEiIKQREREJMISAiEmIKARGREFMIiIiEmEJARCTEFAIiIiGmEBARCTGFgIhIiCkERERCTCEgIhJiCgERkRBTCIiIhJhCQEQkxBQCIiIhphAQEQkxhYCISIgpBEREQkwhICISYgoBEZEQSygEzOwCM1tlZkVmVlDG8tpm9pdg+QIzywjaa5rZDDNbbmYrzWx0cssXEZGKKDcEzCwNmAL0BTKBwWaWGddtKPCNu58K3AtMCNovBWq7exbQFfhFaUCIiEj1S2RPoDtQ5O6r3X0P8CQwMK7PQGBGMP0McK6ZGeBAfTOrAdQF9gBbk1K5iIhUWCIh0ApYFzNfHLSV2cfdS4AtQDMigfAtsB74FJjo7l/HP4CZXWtmi81s8YYNGw77SYiIyJGp7BPD3YF9wElAG+D/m9kp8Z3cfZq757h7TosWLSq5JBERKZVICHwGtI6ZTw/ayuwTHPppDGwCLgdedve97v4VMA/IqWjRIiKSHImEwCKgrZm1MbNaQB4wO67PbGBIMJ0LvOnuTuQQ0DkAZlYfOAP4KBmFi4hIxZUbAsEx/hHAK8BK4Cl3LzSzcWY2IOj2MNDMzIqAm4HSy0inAA3MrJBImDzi7h8k+0mIiMiRqZFIJ3efA8yJa7s9ZnoXkctB49fbXla7iIikBt0xLCISYgoBEZEQUwiIiISYQkBEJMQUAiIiIaYQEBEJMYWAiEiIKQREREJMISAiEmIKARGREFMIiIiEmEJARCTEFAIiIiGmEBARCTGFgIhIiCkERERCTCEgIhJiCgERkRBTCIiIhJhCQEQkxBQCIiIhphAQEQkxhYCISIgpBEREQkwhICISYgoBEZEQSygEzOwCM1tlZkVmVlDG8tpm9pdg+QIzy4hZ1snM5ptZoZktN7M6yStfREQqotwQMLM0YArQF8gEBptZZly3ocA37n4qcC8wIVi3BvA4cJ27dwB+BOxNWvUiIlIhiewJdAeK3H21u+8BngQGxvUZCMwIpp8BzjUzA/oAH7j7+wDuvsnd9yWndBERqahEQqAVsC5mvjhoK7OPu5cAW4BmQDvAzewVM1tqZreU9QBmdq2ZLTazxRs2bDjc5yAiIkeosk8M1wDOAn4W/PtTMzs3vpO7T3P3HHfPadGiRSWXJCIipRIJgc+A1jHz6UFbmX2C8wCNgU1E9hr+7u4b3X0HMAc4vaJFi4hIciQSAouAtmbWxsxqAXnA7Lg+s4EhwXQu8Ka7O/AKkGVm9YJwOBv4MDmli4hIRdUor4O7l5jZCCIf6GnAdHcvNLNxwGJ3nw08DDxmZkXA10SCAnf/xsx+TyRIHJjj7i9W0nMREZHDVG4IALj7HCKHcmLbbo+Z3gVcepB1HydymaiIiKQY3TEsIhJiCgERkRBTCIiIhJhCQEQkxBQCIiIhphAQEQkxhYCISIgpBEREQiyhm8WOJnNr3whjL4/Or60DjA1mGp8MI5dXR1kiIinpmAuBdNsIY7dE5zMKXmTt+H6RmbGNq6kqEZHUpMNBIiIhphAQEQkxhYCISIgpBEREQkwhICISYgoBEZEQUwiIiISYQkBEJMQUAiIiIaYQEBEJMYWAiEiIKQREREJMISAiEmIKARGREFMIiIiEmEJARCTEEgoBM7vAzFaZWZGZFZSxvLaZ/SVYvsDMMuKWn2xm283sV8kpW0REkqHcEDCzNGAK0BfIBAabWWZct6HAN+5+KnAvMCFu+e+BlyperoiIJFMiewLdgSJ3X+3ue4AngYFxfQYCM4LpZ4BzzcwAzGwQsAYoTE7JIiKSLImEQCtgXcx8cdBWZh93LwG2AM3MrAFwK3BnxUsVEZFkq+wTw2OBe919+6E6mdm1ZrbYzBZv2LChkksSEZFSNRLo8xnQOmY+PWgrq0+xmdUAGgObgB5ArpndAzQBvjOzXe4+OXZld58GTAPIycnxI3kiIiJy+BIJgUVAWzNrQ+TDPg+4PK7PbGAIMB/IBd50dwd6l3Yws7HA9vgAEBGR6lNuCLh7iZmNAF4B0oDp7l5oZuOAxe4+G3gYeMzMioCviQSFiIikuET2BHD3OcCcuLbbY6Z3AZeWs42xR1CfiIhUIt0xLCISYgoBEZEQUwiIiISYQkBEJMQUAiIiIaYQEBEJMYWAiEiIKQREREJMISAiEmIKARGREFMIiIiEmEJARCTEFAIiIiGmEBARCTGFgIhIiCkERERCTCEgIhJiCgERkRBTCIiIhJhCQEQkxBQCIiIhphAQEQkxhYCISIgpBEREQkwhICISYgoBEZEQSygEzOwCM1tlZkVmVlDG8tpm9pdg+QIzywjazzezJWa2PPj3nOSWLyIiFVFuCJhZGjAF6AtkAoPNLDOu21DgG3c/FbgXmBC0bwQucvcsYAjwWLIKFxGRiktkT6A7UOTuq919D/AkMDCuz0BgRjD9DHCumZm7v+funwfthUBdM6udjMJFRKTiEgmBVsC6mPnioK3MPu5eAmwBmsX1uQRY6u67j6xUERFJthpV8SBm1oHIIaI+B1l+LXAtwMknn1wVJYmICIntCXwGtI6ZTw/ayuxjZjWAxsCmYD4dmAVc6e6flPUA7j7N3XPcPadFixaH9wxEROSIJRICi4C2ZtbGzGoBecDsuD6ziZz4BcgF3nR3N7MmwItAgbvPS1bRIiKSHOWGQHCMfwTwCrASeMrdC81snJkNCLo9DDQzsyLgZqD0MtIRwKnA7Wa2LPg5IenPQkREjkhC5wTcfQ4wJ67t9pjpXcClZaz3G+A3FaxRREQqie4YFhEJMYWAiEiIKQREREJMISAiEmIKARGREFMIiIiEmEJARCTEFAIiIiGmEBARCTGFgIhIiCkERERCTCEgIhJiCgERkRBTCIiIhJhCQEQkxBQCIiIhphAQEQkxhYCISIgpBEREQkwhICISYgoBEZEQUwiIiISYQkBEJMQUAiIiIaYQEBEJMYWAiEiIJRQCZnaBma0ysyIzKyhjeW0z+0uwfIGZZcQsGx20rzKznySvdBERqaga5XUwszRgCnA+UAwsMrPZ7v5hTLehwDfufqqZ5QETgMvMLBPIAzoAJwGvm1k7d9+X7CeSkMYnw9jGR77uyOXJrUdEpJqVGwJAd6DI3VcDmNmTwEAgNgQGAmOD6WeAyWZmQfuT7r4bWGNmRcH25ien/MNUkQ/xe7OOPEBAISIiKSmREGgFrIuZLwZ6HKyPu5eY2RagWdD+bty6rY642upU0Q/wiobIkVL4iMghJBIClc7MrgWuDWa3m9mqCmyuOXfaxv22P6ECW6sazYGN5fY6IivgZquMDVdizZXqaKxbNVedo7Hu0pq/dyQrJxICnwGtY+bTg7ay+hSbWQ2gMbApwXVx92nAtMTLPjgzW+zuOcnYVlVRzVXnaKxbNVedo7HuitacyNVBi4C2ZtbGzGoROdE7O67PbGBIMJ0LvOnuHrTnBVcPtQHaAguPtFgREUmucvcEgmP8I4BXgDRgursXmtk4YLG7zwYeBh4LTvx+TSQoCPo9ReQkcglwQ7VdGSQiIgdI6JyAu88B5sS13R4zvQu49CDr/ifwnxWo8XAl5bBSFVPNVedorFs1V52jse4K1WyRozYiIhJGGjZCRCTEjpkQKG9oi+pkZtPN7CszWxHTdryZvWZmHwf/Ng3azcwmBc/jAzM7vZpqbm1mb5nZh2ZWaGa/TPW6zayOmS00s/eDmu8M2tsEw5kUBcOb1AraDzrcSTXUnmZm75nZC0dRzWvNbLmZLTOzxUFbyr4/gjqamNkzZvaRma00szNTuWYzax+8vqU/W83spqTW7O5H/Q+RE9afAKcAtYD3gczqriumvh8CpwMrYtruAQqC6QJgQjB9IfASYMAZwIJqqrklcHow3RD4XyAzlesOHrtBMF0TWBDU8hSQF7TfDwwPpq8H7g+m84C/VON75GZgJvBCMH801LwWaB7XlrLvj6COGcCwYLoW0CTVa46pPQ34gsj9AEmrudqeUJJfnDOBV2LmRwOjq7uuuBoz4kJgFdAymG4JrAqmHwAGl9Wvmut/jsj4UUdF3UA9YCmRu9s3AjXi3ytErng7M5iuEfSzaqg1HXgDOAd4IfgFTumag8cvKwRS9v1B5P6lNfGvVyrXHFdnH2Besms+Vg4HlTW0RaoPT3Giu68Ppr8ATgymU+65BIccuhD5yzql6w4OqywDvgJeI7KHuNndS8qoa7/hToDS4U6q2n3ALcB3wXwzUr9mAAdeNbMlFrnrH1L7/dEG2AA8Ehx6e8jM6pPaNcfKA/4cTCet5mMlBI5qHonslLxMy8waAH8FbnL3rbHLUrFud9/n7tlE/rruDvygmks6JDPrD3zl7kuqu5YjcJa7nw70BW4wsx/GLkzB90cNIodl/+TuXYBviRxKiUrBmgEIzgkNAJ6OX1bRmo+VEEhoeIoU86WZtQQI/v0qaE+Z52JmNYkEwBPu/regOeXrBnD3zcBbRA6lNLHIcCbxdUVrtv2HO6lKvYABZrYWeJLIIaE/kNo1A+DunwX/fgXMIhK6qfz+KAaK3X1BMP8MkVBI5ZpL9QWWuvuXwXzSaj5WQiCRoS1STexQG0OIHHMvbb8yOMt/BrAlZrevypiZEbkTfKW7/z5mUcrWbWYtzKxJMF2XyDmMlUTCIPcgNZc13EmVcffR7p7u7hlE3rdvuvtWfDFwAAACOUlEQVTPSOGaAcysvpk1LJ0mcrx6BSn8/nD3L4B1ZtY+aDqXyGgGKVtzjMH861AQJLPm6jrJUQknTS4kcgXLJ8Bt1V1PXG1/BtYDe4n8NTKUyHHcN4CPgdeB44O+RuRLfD4BlgM51VTzWUR2MT8AlgU/F6Zy3UAn4L2g5hXA7UH7KUTGrCoisjtdO2ivE8wXBctPqeb3yY/419VBKV1zUN/7wU9h6e9cKr8/gjqygcXBe+RZoOlRUHN9Int7jWPaklaz7hgWEQmxY+VwkIiIHAGFgIhIiCkERERCTCEgIhJiCgERkRBTCIgcRDBaY70E+o0IRm10M2teFbWJJItCQOTgbiIyEF155gHnAf+s3HJEki+hr5cUOZYFA+S9DCwhMoxAIfB34CTgLTPb6O4/NrM+wJ1AbSI341zl7tvd/b1gO9VQvUjFaE9AJKI9MNXdTwO2Ehlr/nPgx0EANAfGAOd5ZNC0xUS+A0DkqKY9AZGIde4+L5h+HLgxbvkZRL5UZ17wF38tYH7VlSdSORQCIhHx46fEzxvwmrsPrqJ6RKqEDgeJRJxsZmcG05cDc4FtRL5aE+BdoJeZnQrRUTTbVX2ZIsmlEBCJWEXki1FWEhlZ8k/ANOBlM3vL3TcA+cCfzewDIoeCfgBgZjeaWTGRsds/MLOHquMJiBwJjSIqoRdcHfSCu3es5lJEqpz2BEREQkx7AiIiIaY9ARGREFMIiIiEmEJARCTEFAIiIiGmEBARCTGFgIhIiP0fkvb2gtnC9bUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d1d152d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d253cc90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3501d49d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d1d458d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d159e2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d1fc8290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HLFs = ['sumEt','MET','dPhi1','dPhi2','PhoJetMinDr','njets','Xtt0',\n",
    "        'Xtt1','pte1','pte2','ptmu1','ptmu2','fabs_CosThetaStar_CS','fabs_CosTheta_bb']\n",
    "\n",
    "for hlf in training_vars:\n",
    "    plt.figure()\n",
    "    plt.hist(sig_frame[hlf], bins=40, normed=True, histtype='step', label='GluGluToHHTo2B2G Signal')\n",
    "    plt.hist(bkg_frame[hlf], bins=40, normed=True, histtype='step', label='ttHToGG Background')\n",
    "    plt.xlabel(hlf)\n",
    "    plt.legend(loc='best')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34d1d45110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_corr(df,size=10):\n",
    "    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns);\n",
    "\n",
    "plot_corr(bkg_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
